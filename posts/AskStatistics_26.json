[
    {
        "title": "Where do I go to find a statistical consultant?",
        "author": "BlargAttack",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98bz9n/where_do_i_go_to_find_a_statistical_consultant/",
        "text": "I’m looking to replicate a result from a published paper and am having a bear of a time doing it. I think it’s time to hire a statistical programmer, preferably one who understands econometrics. How do I find one?",
        "created_utc": 1534601797,
        "upvote_ratio": ""
    },
    {
        "title": "Figuring out odds the suit a card in a deck would be",
        "author": "slippinJimmy93",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9897gc/figuring_out_odds_the_suit_a_card_in_a_deck_would/",
        "text": "So, in an average deck of 52 cards there are 13 of each suit, so the odds of picking a correct suit is 1/4=25%.\n\nWhat I want to know is, what would be the odds of picking the suit of a random card if, after the random card had been chosen, I was exposed to 8 other cards (at random) that have not been chosen. \n\nThe hard part here is that it depends on what those 8 cards are to understand the odds. If there are two of each suit, there will still be an equal amount (11) of each suit and it would still be 25%. However, if say you see 2 diamonds 3 clubs 3 hearts then you know that 13/44 (29.5%) cards left are spades and that would be your chance at selecting the right option.\n\nHow would I go about aggregating all of these possibilities to understand my odds? And how would I go if I saw 10, 20, etc cards instead of 8.\n\nThanks in advance!",
        "created_utc": 1534568451,
        "upvote_ratio": ""
    },
    {
        "title": "Probably a really easy question, but here goes",
        "author": "fellowhumandude2021",
        "url": "https://www.reddit.com/r/AskStatistics/comments/988zud/probably_a_really_easy_question_but_here_goes/",
        "text": "Say I could potentially get cookies from three different people. \n\nThere is 13/29 chance Person 1 will give me one cookie.\nThere is a 14/26 chance Person 2 will give me one cookie.\nThere is a 4/11 chance Person 3 will give me one cookie.\n\nWhat are the odds that I'll get one cookie from anyone? In other words, how do you \"combine,\" for lack of a better word, the odds that any one of the three people will give me a cookie into one probability of getting a cookie? \n\nHopefully that makes sense. It was a little hard to phrase.",
        "created_utc": 1534566168,
        "upvote_ratio": ""
    },
    {
        "title": "What are the odds of someone being famous in the United States?",
        "author": "xSylk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/987cko/what_are_the_odds_of_someone_being_famous_in_the/",
        "text": "Based off the 325.7mil people that live in the United States what are the odds of you being famous? That also begs the question of what is considered famous in the United States? Let's say .10% of america, that's 325,700 people. How would one gather the statistics and different variables that would contribute to this answer(Followers/views on YouTube, Twitter, TV, movies, etc.)? I don't know, so I'm asking statistics.",
        "created_utc": 1534550802,
        "upvote_ratio": ""
    },
    {
        "title": "Epidemiology texts and advice for going from statistics -&gt; public health/epidemiology",
        "author": "lolpdb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9856l2/epidemiology_texts_and_advice_for_going_from/",
        "text": "Hey /r/askstatistics,\n\nI just finished a Bayesian-driven MS.  I'll be diving into a public health/epidemiology-driven statistics role soon.  \n\nWhat texts do you recommend for learning epidemiological modeling?  If that's too broad, I'm looking to gain expertise in survival analysis.\n\nAlso, I'm curious if anyone has made this sort of transition, from a pure statistics degree to a very much applied discipline.  Any advice?\n\nThanks!\n\n",
        "created_utc": 1534533825,
        "upvote_ratio": ""
    },
    {
        "title": "Recode Different Variable in SPSS, how to decide range?",
        "author": "notevelvet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/984crq/recode_different_variable_in_spss_how_to_decide/",
        "text": "I need to recode ”the total number of drinks consumed in the last week”. There are 966 responses but I'm unsure of how to breakdown the ranges. I was thinking of dividing the range into 7 (for the days of the week) which would be 138/range. But that doesn't seem to be correct. Should I be looking at the max number consumed and the least consumed to find the range? \n\nThanks in advance for any advice",
        "created_utc": 1534527881,
        "upvote_ratio": ""
    },
    {
        "title": "Standard Error",
        "author": "hadanangel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/983lww/standard_error/",
        "text": "I need to calculate standard error for a study which has 2*2 within design. I have group means and standard deviations. My advisor said that you should first calculate the pooled SD and then calculate SE. I search little bit but I am really confused .  What should I do? ",
        "created_utc": 1534522615,
        "upvote_ratio": ""
    },
    {
        "title": "Meditation Analysis Help",
        "author": "Huck_Fer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/983hy0/meditation_analysis_help/",
        "text": "I am pretty new to statistics, but I was wondering what is the best way to do mediation?\n\n I have heard SEM, bootstrapping, PROCESS, or Sobel's test are all viable options, but what is the easiest/most effective? \n\nBased on the steps outlined by Baron and Kenny (1986), I tried to just manually enter the variables as bivariate regressions and a multiple regression and examined whether the presence of the mediator reduced the statistical significance of the relationship between the IV and DV, but I don't think this is advisable? \n\nAlso, when looking mediation, is it possible to have more than one IV in the model, or would I have to do separate analyses?\n\nThanks!",
        "created_utc": 1534521828,
        "upvote_ratio": ""
    },
    {
        "title": "What topics in calculus should I review before starting a Master's program in Applied Statistics?",
        "author": "mrbabynugget",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9831nt/what_topics_in_calculus_should_i_review_before/",
        "text": "I'm not sure if this is the right subreddit, but can someone let me know which topics in calculus I'll be using most in statistics, and point me in the direction of some good practice problems? Thank you!",
        "created_utc": 1534518597,
        "upvote_ratio": ""
    },
    {
        "title": "Everyone is different?",
        "author": "Je_pense_je_suis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/981au9/everyone_is_different/",
        "text": "What is there logically fallacious about countering every statistical claim with the statement \"everyone is different, so your claim is not true. You can't generalize\"? It is definitely wrong to try to dismiss every statistic by saying \"you can't generalize\". I would like to see how it can be logically disproved.\n\nAn example of a fallacious statement:\n\n\\- All women have a primal instinct that makes them attracted to strong males. Counterargument: \"Not every woman is the same, so you can't generalize\"\n\nI haven't paid much attention in my Statistics 101 classes for my degree (even though I got it), but I remember something alone the lines of the following being said:\n\n\\- There's a 1 in 3 chance that you will die if you get that operation. 2 already died from it, so that means you're going to die from it. This is an incorrect estimate.\n\n\\- Statistics can generalize a sample, but they can't predict something about one particular individual.\n\n\\- Janet argues that an ex-criminal shouldn't be kept in jail just because the statistics prove that a person of that category is likely to commit a crime within the year. Person A isn't person B. -&gt; Janet doesn't understand statistics\n\nThese statements are rather confusing to me and I'm having trouble finding the truth and which statements are applicable where.\n\nWhen can you generalize something? When can't you?",
        "created_utc": 1534503455,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for an intermediate book on hypothesis testing.",
        "author": "CuckedByJaredFogle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97zo8z/looking_for_an_intermediate_book_on_hypothesis/",
        "text": "I just completed my first college level stats class a few months ago and fell in love with statistics.  Our book was called *Elementary Statistics* by Farber and Larson and I have completed nearly problem in the book.  I can find some good sources for furthering my learning in probabilities and Bayesian stuff, but I am having trouble finding good material on hypothesis testing, everything I have seen so far is either a repeat of what I have just learned or *waaaay* to difficult.  \n\n At the current moment, I have basic knowledge of one way and two way ANOVA, Chi-square tests, some of the nonparametric tests like the sign test as well as the easy stuff like one and two sample Z/T tests (the kinds of things people usually learn in their first stats class).  I also have decent programming skills.\n\nMy ultimate goal is to have a better understanding of the things I read in psychology and genetics papers.  I am not crazy about understanding proofs or about memorizing long formulas (although I will put the work in if need be) rather, I am more interested in understanding the subtle differences between different tests for significance, tests for normality, and ways of dealing with things like heteroskedasticity and skewness. \n\nI hope that wasn't too vague and thanks in advance for the suggestions for  reading material.",
        "created_utc": 1534483953,
        "upvote_ratio": ""
    },
    {
        "title": "Normalising normalised data?",
        "author": "GinDingle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97yg7i/normalising_normalised_data/",
        "text": "Hi there,\n\nI'm doing a gene expression analysis and I'm trying to figure out what to do with my data set. In summary, I measured a relative \"quantity\" of a gene of interest against a standard curve, then normalised that to the \"quantity\" of a housekeeper gene (so GOI/HK).\n\nThe result of this normalisation is that the final number is either 0-1 or 1-infinity depending on whether the GOI or the HK was higher. I met previously with a statistician who recommended I log-transform this data before doing my final analysis to account for this. I understand why this would be the case if I was measuring a fold-change difference, but is this still relevant to my data, given that I quantified expression against a standard curve?\n\nIs log-transforming still a reasonable option for this type of data? Transformation does reduce the number of outliers in my data, but also adversely affects some of my results. Not to mention that essentially no-one using this technique log-trasnforms their data before analysis, although I wouldn't necessarily trust a bunch of biologists on this.\n\nDoes anyone have any advice regarding the best way to approach my data in this case?",
        "created_utc": 1534472025,
        "upvote_ratio": ""
    },
    {
        "title": "Help finding a best-fit geomapping software",
        "author": "zesty1989",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97xbt9/help_finding_a_bestfit_geomapping_software/",
        "text": "I'm looking for a geomapping software that will allow me to plug in about 18 months worth of data to view trends from month to month and show changing trends in a geographic area. I will input a series of customer's zip codes and the months in which those customers came to visit us. I want to be able to see changing trends for the past 18 months in some sort of animation.  \n\n\nI know that Batchgeo will allow me to plug in the locations, but I'm not certain I will be able to view the changing trends in an animation. Any tips?",
        "created_utc": 1534462454,
        "upvote_ratio": ""
    },
    {
        "title": "Curious about step in Bias-variance decomposition of MSE",
        "author": "ztnq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97wsd7/curious_about_step_in_biasvariance_decomposition/",
        "text": "https://wikimedia.org/api/rest_v1/media/math/render/svg/d9d5b73ef1ffab0e22636fdce3dfb69da36cef35\nI'm curious about the claim that E(theta_hat)-theta =const.\nWouldn't it be more accurate to say they are deterministic? Or are deterministic variables just called constants when\ntalking about random variables.\nThanks",
        "created_utc": 1534458156,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to determine whether one-way ANOVA or Krusal Wallis is the more appropriate test for my data",
        "author": "PlatinumKn1ght",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97wqef/trying_to_determine_whether_oneway_anova_or/",
        "text": "I read through [the link](https://stats.idre.ucla.edu/other/mult-pkg/whatstat/) on the sidebar and did some investigating on my data and the type of scales my data represents.  In my data set, I have my independent variable called ASA status (this is a categorical variable with 2+ groups or levels), and my dependent variable which is the number of events, or count, based on a patient's ASA status.  What I would like to do is to use a statistical test to determine whether an increase in the patient's ASA status and the number of events that patient has during their procedure are statistically significant.  So basically I want to prove that a higher ASA status would result in a higher count of events for each patient.\n\nMy data looks something like this (pairing each patient's ASA Status with the # of events they had per case) with over 100 samples:\n\nASA Status | # of Events\n\n      2               1\n      2               3\n      3e             1\n      3               2\n      5               1\n      4e             2\n\n...etc (just an FYI the 'e' is not a typo next to the numbers; those denote a specific ASA Status, which is why I said my IV are categorical).\n\nWhat I noticed for the one-way ANOVA and the Kruskal Wallis tests is that both tests allow for interval-type dependent variables.  I imagine that this has to do with parametric versus non-parametric assumptions, correct?  The problem I'm having is determining whether they fit one or the other, so here is the structure of my data in a nutshell:\n\n* The data appears to be normal and were pulled with a specific case type in mind, but were randomly selected.\n* There are a total of 6 levels for this study.\n* # of events is strictly a whole number (cannot have fractions of an event)\n* Some of the ASA Status levels have very few cases (one of them has only 1 case)\n\nSo I'm pretty lost at this point in determining whether I should use one-way ANOVA or Kruskal Wallis.  Or is there another test that I did not consider that I should use?\n\nAny assistance with my question would be a great help.  Thank you.",
        "created_utc": 1534457739,
        "upvote_ratio": ""
    },
    {
        "title": "(sanity check) If your priors are uniform and you spin a slot machine 6 times, the probability of winning 3 times is the same as the probability of winning 0 times!?",
        "author": "Fatlark",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97wgm8/sanity_check_if_your_priors_are_uniform_and_you/",
        "text": "Am building something, and I am calculating the likelihood of winning a slot machine X number of times given a uniform prior. eg. I am calculating the likelihood of winning 0 times out of 6, 1 time out of 6, etc.\n\nI am finding that the probability of winning X times is uniform (!?). \n\nIs this correct? It seems surprising because there are six times as many ways to win 1 time out of 6 than there are to win 0 times out of 6 (n!/(k!(n-k)!) and whatnot). This makes me think that winning 3 times would have the highest likelihood, but that is not the result I'm getting.\n\nDo I need to double check my code or is this correct? \n\nThanks",
        "created_utc": 1534455680,
        "upvote_ratio": ""
    },
    {
        "title": "How would you represent these odds mathematically?",
        "author": "redasda",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97wg3z/how_would_you_represent_these_odds_mathematically/",
        "text": "If I have 1/3 chance of being right on a question with 3 answers , and eliminate one of the choices immediately... my odds improve to 1/2... and if I am 1/2 sure on the chance of one of the remaining 2 answers being right... my odds of getting the answer right are 75%? ",
        "created_utc": 1534455575,
        "upvote_ratio": ""
    },
    {
        "title": "Are there more dead bodies at sea or under ground?",
        "author": "PIGEXPERT",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97wddd/are_there_more_dead_bodies_at_sea_or_under_ground/",
        "text": "Got into an argument and I'm a facts guy, so even though logic dictates there are more bodies in the ground I want proof.\n\nIf anyone got something like that for me that would be amazing",
        "created_utc": 1534455003,
        "upvote_ratio": ""
    },
    {
        "title": "Weighing Data based on Recency",
        "author": "smallchimp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97w4og/weighing_data_based_on_recency/",
        "text": "Looking to make projections for NFL fantasy football and one thing I want to incorporate is something where data that is more recent is worth more than data that is more distant, i.e. if something happened last week, the value would be weighed more heavily than something that happened 2 weeks ago. So if a player earned 20 fantasy points in the past two weeks but their average over 10 weeks was 12, the weighted average would be closer to 20 than 12 because the mean has been shifting upwards given whatever thing is happening in the actual game.\n\nWhat property would this be, and how could I represent it in Google Sheets? \n\nEDIT: I know how to weigh numbers that are assigned with weights, like 20 items at $1 and 40 items at $0.75 (20x1),(40x.75), but I don't know how to weight things on a scale that isn't provided\n\nEDIT 2: If it were Week 10, Week 9's value would count more than Week 8's value, which would count more than Week 7's value and so on. There'd be a non-weighted average for the 9 previous weeks, but the weighted average I'm looking to find would be either greater or lesser depending on what the recent trends are",
        "created_utc": 1534453241,
        "upvote_ratio": ""
    },
    {
        "title": "Basic Level Statistics Understanding with Lottery slots",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97vd0u/basic_level_statistics_understanding_with_lottery/",
        "text": "[deleted]",
        "created_utc": 1534447758,
        "upvote_ratio": ""
    },
    {
        "title": "Is Wilcoxon Rank Sum Test right here?",
        "author": "deepcdaniell",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97u0jb/is_wilcoxon_rank_sum_test_right_here/",
        "text": "Hey folks!\n\nI'm comparing two datasets on the populations of 7 species, divided into 3 zones, at 2 points in time (2009 vs 2017) looking for population increases. The species present looks like this:\n\nZone 1\n\nSpecies B\n\nZone 2\n\nSpecies B,C,D,E\n\nZone 3\n\nSpecies A,B,C,F\n\nSo far, I've done a bunch of Wilcoxon Rank Sums comparing Species B in Zone 1, Species B in Zone 2, Species A in Zone 3, etc at each point in time. If I wanted to compare all species across all zones between the two years, would a Wilcoxon Rank Sum work? Should I put it in like this:\n\n2009 Zone1(B) + Zone2(B+C+D+E) + Zone3(A+B+C+F) vs 2017 \n\nor \n\nZone1(A+B+C+D+E+F) + Zone2(A+B+C+D+E+F) + Zone3(A+B+C+D+E+F)\n\nOtherwise, what test can compare 2 irregularly distributed datasets that are in turn subdivided?\n\nI considered a Non-parametric one-way ANOVA, but to my understanding that wouldn't work here.\n\nMany thanks! ",
        "created_utc": 1534438534,
        "upvote_ratio": ""
    },
    {
        "title": "Probability of finding someone I know on a dating app",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97sjdb/probability_of_finding_someone_i_know_on_a_dating/",
        "text": "[deleted]",
        "created_utc": 1534428185,
        "upvote_ratio": ""
    },
    {
        "title": "How to determine variability",
        "author": "phantom0511",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97shbd/how_to_determine_variability/",
        "text": "I have a device that has 5 channels for measuring eg. refractive index. Each measurement will give 5 outputs from 5 samples at a time. Let's say I measure 50 samples, ie. each channel measures 10 samples. I would like to determine the channel to channel variability. Could l calculate the coefficient variation for each measurement and average the 10 coefficient variations from the 10 measurements to represent the channel to channel variability?",
        "created_utc": 1534427750,
        "upvote_ratio": ""
    },
    {
        "title": "Justification using stats",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97r3y9/justification_using_stats/",
        "text": "[deleted]",
        "created_utc": 1534415114,
        "upvote_ratio": ""
    },
    {
        "title": "How to structure data with multiple visits in a hospital setting?",
        "author": "G3N3IO",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97mkkh/how_to_structure_data_with_multiple_visits_in_a/",
        "text": "I have raw hospital infection data and was wondering how to structure it. It currently looks like this:\n\n    ID    Admit            Discharge          Infection\n    1     08/10/2018      08/12/2018             Yes\n    1     08/14/2018                             Yes \n    2     08/02/2018      08/08/2018             No\n\nThe data includes patient ID, admit &amp; discharge date, and whether or not they had an infection. If there was a blank in the discharge column, that means the patient hasn't been discharged yet. Patient ID #1 has multiple repeats and I was wondering how I should shape this data.\n\nMy question: What do I do about patients that visit more than once? Do I treat them as independent observations? The thing is if the patient got an infection, both rows would get \"Yes\" for infection, since they are the same person, and in turn biases the data.\n\nThanks!",
        "created_utc": 1534370995,
        "upvote_ratio": ""
    },
    {
        "title": "Is it possible to do a statistical test for this?",
        "author": "aspevack",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97kz0l/is_it_possible_to_do_a_statistical_test_for_this/",
        "text": "I'm being asked to do some sort of statistical verification on a project for work, and I don't even know if it's possible for this particular situation:\n\nI have a complete list of addresses of participants in a particular statewide program, and no other information about them. We are making some inferences of the demographic makeup of the program participants by linking the addresses to Census block groups. For example, participants tend to live in higher income block groups, and I've quantified this by doing some weighted averages. \n\nThe project manager has asked for some quantification of how likely it is that the demographics of the participants in the program match the demographics of the block groups where they are most concentrated. Put another way, I’m being asked to test the likelihood that the correlation between program participant and high income areas is not random chance. \n\nSo...is there some way of measuring the likelihood that, for example, the 45% of census block group X who are participants in the program have an average income lower (or higher) than the average for the whole block group? Framed like that, it sounds more like a complex probability problem than a statistical test. Framed another way, we know the demographic characteristics by census block group, so how confidently can we predict the demographic characteristics of a subset of people based on their block group? \n\nAny advice?",
        "created_utc": 1534359342,
        "upvote_ratio": ""
    },
    {
        "title": "Dichotomous predictor has very uneven group sizes - should it be omitted from multiple regression analysis?",
        "author": "BorisMalden",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97kp08/dichotomous_predictor_has_very_uneven_group_sizes/",
        "text": "Let's say I want to include a dichotomous variable (e.g. gender) as a predictor in a multiple regression analysis, but by some unforeseen circumstance during sampling the two groups ended up being very uneven (e.g. out of a sample of 500 participants, 475 were male and only 25 were female). \n\nIn such a circumstance, should you omit gender (or whatever your dichotomous predictor is) from the analysis, or can it still be used? Are there any rules of thumb for when the uneven group sizes becomes a problem for multiple regressions?",
        "created_utc": 1534357320,
        "upvote_ratio": ""
    },
    {
        "title": "Expected value and implications on consecutive overlapping bets",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97im29/expected_value_and_implications_on_consecutive/",
        "text": "[deleted]",
        "created_utc": 1534342773,
        "upvote_ratio": ""
    },
    {
        "title": "What's the name of this algorithm for selecting between models of different polynomial orders?",
        "author": "Celebrate710b",
        "url": "https://i.imgur.com/MMfrles_d.jpg?maxwidth=640&amp;shape=thumb&amp;fidelity=medium",
        "text": "",
        "created_utc": 1534342402,
        "upvote_ratio": ""
    },
    {
        "title": "Which coefficient matches which scale of measure?",
        "author": "Arjab",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97hz1e/which_coefficient_matches_which_scale_of_measure/",
        "text": "I got this training question where I have to fit a coefficient and sclaes of measure to different variables. Can anybody check, if I'm correct and if possible tell me in case why not. I would be super thankful!\nThe variables are set and I had to fill in the coefficient and scale of measure:  \n  \n* X= Sex; Y= prefered political party; X= *nominal*; Y= *ordinal*; coefficients= *Chi-squared*, *Cramers V*  \n  \n* X= age in years; Y= IQ; X= *ratio*; Y= *ratio*; coefficients= *F-test*, *T-test*, *covariance*, *correlation*  \n  \n* X= social stratum; Y= income as income group; X= *ordinal*; Y= *interval*; coefficients= *Gamma*, *Kendalls Tau*  \n  \n* X= education of father; Y= education of child; X= *ordinal*; Y= *ordinal*; coefficients= *Gamma*, *Kendalls Tau*  ",
        "created_utc": 1534337556,
        "upvote_ratio": ""
    },
    {
        "title": "Which type of ANOVA analysis is best for this set of data?",
        "author": "tealpeak",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97h419/which_type_of_anova_analysis_is_best_for_this_set/",
        "text": "Good morning, \n\nI have the data as following: \nTotal Polyphenol Content was measured in triplicate. \nDifferent ABV were used  - 4%, 6%, 8%, in both a beer matrix (just fermented beer) and an ethanol matrix (alcohol and water mix with an adjusted pH). \nTwo hop concentrations were used 5 and 10 g/L across both matrices and all ethanol concentrations. \n\n\t            1E\t  2E\t          3E\n5g/l\t          66.42\t77.08\t92.66\n\t         68.88\t74.62\t91.02\n\t         72.98\t83.64\t86.1\n10g/L\t130.38\t143.5\t191.29\n\t        143.5\t141.04\t198.44\n\t        120.54\t146.78\t189.49\n\n\t            1B\t2B\t          3B\n5g/l\t         243.54\t398.52\t552.68\n\t         225.50\t431.32\t512.50\n\t        193.52\t418.20\t587.94\n10g/L\t319.80\t536.28\t836.40\n\t        291.92\t492.82\t609.26\n\t         337.02\t510.04\t700.00\n\nI am struggling to understand the optimal statistical method to interpret the data. \nEthanol concentration is the key variable of the study. I have ran 2-way ANOVAs with ethanol against the matrices - doing so twice with the different hop concentrations (5 &amp; 10). And ethanol against hop concentrations - again twice with the different beer and ethanol matrices. \n\nI am now thinking this data may suit a factorial ANCOVA? \n\nMy fundamental understanding of statistical analysis is shallow to say the least. If anybody could weigh in with their opinions it would be more than appreciated. \n\n",
        "created_utc": 1534328369,
        "upvote_ratio": ""
    },
    {
        "title": "Data organization in a 3rd world country",
        "author": "ThatShitTay",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97gomc/data_organization_in_a_3rd_world_country/",
        "text": "Hello! I'm a Peace Corps volunteer charged with helping my host organization organize 40 years of survey data. Right now each year's results are stored in a separate database. For simplicity's sake, let's assume that the survey questions are exactly the same year over year. The variations are in the count of individuals surveyed and how many people completed the survey each year. I feel like the logical thing to do is combine all of the data in one table and add a field for \\[year\\]. I'm getting some push back (maybe a bit of language barrier involved), but I'm curious if--from a statistical point of view--there are issues with combining the data in this way? ",
        "created_utc": 1534322849,
        "upvote_ratio": ""
    },
    {
        "title": "Did I do the math right?",
        "author": "YUH8_SpartaN",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97gmmz/did_i_do_the_math_right/",
        "text": "Did I do the math right on the [chances of getting a foil](https://yuh8spartan.blogspot.com/2018/08/What-are-the-odds-of-getting-legendary-foil.html) in my post? I know that it relies on many assumptions, that I've hopefully made clear are just assumptions. I'm not great at math and I don't want to spread misinformation by screwing up the math. Thanks in advance.",
        "created_utc": 1534322084,
        "upvote_ratio": ""
    },
    {
        "title": "Question about the central limit theorem",
        "author": "InfiniteV",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97gbgs/question_about_the_central_limit_theorem/",
        "text": "So my understanding of the central limit is basically this:\n\nTake a sample and average the values out. Repeat this process from the same population sufficient times and you end up with a normal distribution of the means around the true mean. So if you have 2 samples you'll have a graph with 2 plotted values.\n\n\n Problem I'm having is that I've seen examples where there's a single observation but the given plot is not a uniform distribution. What am I looking at? An example would be this example from Wikipedia https://en.m.wikipedia.org/wiki/Central_limit_theorem#/media/File%3ADice_sum_central_limit_theorem.svg where the plot of n=2 definitely has more than 2 columns. What's wrong with my understanding here?",
        "created_utc": 1534317897,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate this probability?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97fa85/how_to_calculate_this_probability/",
        "text": "[deleted]",
        "created_utc": 1534306290,
        "upvote_ratio": ""
    },
    {
        "title": "Easiest way to calculate likelihood of a series occurring in die results",
        "author": "falcon4287",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97es7i/easiest_way_to_calculate_likelihood_of_a_series/",
        "text": "I enjoy designing mechanics for board games and RPGs, but I'm unfortunately not as strong in statistics as I should be, and sometimes a more complex mechanic creates trouble for me.  \n\nIn this case, I want to create a game where the goal is to roll a pool of 6-sided dice and have a result that is a sequence of numbers, such as 2, 3, 4. The number of dice in the sequence is multiplied by the top number of the sequence, giving you the end result.\n\nWhat I don't quite get is how to figure probability on that. I've tried to use AnyDice, but I can't quite figure out how to calculate how likely a certain number of dice rolled are to pass a target number. Or the likelihood of getting each combination.   \n\nAny help or references would be appreciated. Thanks in advance.",
        "created_utc": 1534301676,
        "upvote_ratio": ""
    },
    {
        "title": "Help identifying this function/method",
        "author": "[deleted]",
        "url": "https://i.imgur.com/MMfrles_d.jpg?maxwidth=640&amp;shape=thumb&amp;fidelity=medium",
        "text": "[deleted]",
        "created_utc": 1534291419,
        "upvote_ratio": ""
    },
    {
        "title": "Adding draw prediction to an Elo rating via logistic regression?",
        "author": "Butterbrezel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97cprx/adding_draw_prediction_to_an_elo_rating_via/",
        "text": "Hi, \n\nI recently built an Elo model to predict results of football competitions. While the traditional Elo model has a way to handle draws in the calculation of scores it does not have a way to predict the probability of a draw occuring.\n\nI already built a database based with match results as well as the expected winning probability (P) based on the teams Elo rating.\n\nThe simplest way would be to just look at the likelihood of a draw occuring in general. But if a strong team with a high rating plays a weak team, the likelihood of a draw is much less than in a competition between two teams with similiar ratings. \nMy goal would be to built a function F(P) that can give me an estimate of the likelihood of a draw occuring based on the winning percentage of a team.\n\nI thought about using P as my sole independent variable and use game drawn as a binary variable.\nIs a logistic regression with one independent variable (in my case P) the right way to approach the problem? Or are there other ways to obtain such a function?\n\nMany thanks in advance.",
        "created_utc": 1534284664,
        "upvote_ratio": ""
    },
    {
        "title": "Any issues with using relative importance/shapley regression values as weights?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97b2uy/any_issues_with_using_relative_importanceshapley/",
        "text": "[deleted]",
        "created_utc": 1534272941,
        "upvote_ratio": ""
    },
    {
        "title": "What is the correct multiple regression method to answer my hypotheses?",
        "author": "BorisMalden",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97a8yo/what_is_the_correct_multiple_regression_method_to/",
        "text": "I have a dataset in which I'm looking to explore the relationships between nine input variables (X1 - X9) and five outputs (Y1 - Y5). For each output, I have a hypothesis based on the previous literature relating to some combination of the inputs, or none of the inputs at all, e.g.\n\nH1: X1-X6 will predict Y1 H2: X5, X6 and X7 will predict Y2 H3: X2, X4, X8 and X9 will predict Y3 H4: None of the inputs will predict Y4 H5: None of the inputs will predict Y5\n\nI'm currently considering three regression procedures I could use to explore these hypotheses:\n\n  *  **Forced entry**. Construct a model for each output including all potential predictors. From the coefficients table, report the predictors which are significant at the P&lt;0.05 level. Maybe re-run the regression, dropping all the non-significant predictors, and then report the adjusted r-squared value.\n\n  *  **Stepwise**. Construct a model for each output including all potential predictors, but specify the stepwise procedure. Report all of the (significant) predictors retained in the final model, along with the adjusted r-squared value.\n\n  *  **All-possible-subsets**. Use the all-possible-subsets approach within SPSS to identify the model with the lowest AIC. Report all of the predictors in the identified model OR report only the predictors which are significant at the P&lt;0.05 level, along with the adjusted r-squared value.\n\nCan anybody help me to work out which of these (if any) is the most appropriate approach? Many thanks in advance\n",
        "created_utc": 1534267108,
        "upvote_ratio": ""
    },
    {
        "title": "How to run A/B test to measure incremental and continuous conversion impact?",
        "author": "HelloWorld1111111",
        "url": "https://www.reddit.com/r/AskStatistics/comments/97a69h/how_to_run_ab_test_to_measure_incremental_and/",
        "text": "Hi,\n\nI need advice on how to correctly run A/B testing on website.\n\nMy company own retail website. And let’s say we want to test how one feature (video about the product) contributes to overall sales. We already have this videos, and we run A/B testing \\[ Control: no video, Treatment: video\\] to see if there is difference in sales when video is presented vs not presented. The difference was positively significant, so we decided to launch the product video project and place these videos next to the product description.\n\nNow, we want to reevaluate our videos, because we still not quite sure, how much $$ videos brings to the company. Not just rerun same A/B test, but create methodology that will incrementally AND continuously measure sales($$) impact of the videos.\n\n**One of possible solution is implementing continuous A/B testing by running SAME A/B experiment for 1 week every 2 months, where:**  \n Control - hide videos widget - 3% of users  \n Treatment - show video widget (Current State) - 97% of users\n\nThe point is to **run similar experience continuously, every months and only for small amount of customers** (do not suppress widget for 50%, only for 3% of users).\n\nAs a result, using the output from these experiments we wanted to create graph, that shows the difference in average spending per session in Control(no videos) and Treatment(has videos) over the last year.\n\nSo we will be able to measure incremental and continuous impact of videos on purchases on our website.\n\n?Could you advice us if there are any other risks of getting incorrect result with such way of attributed sales measurements?\n\nWhat if after a few iterations of such experiment, our org decides to launch another experiment to test some new feature for our widget? For example, make size of the widget bigger. How this would affect significance this another experiment (hide/show widget).",
        "created_utc": 1534266590,
        "upvote_ratio": ""
    },
    {
        "title": "Main effects plot and count data",
        "author": "cmcrae1019",
        "url": "https://www.reddit.com/r/statistics/comments/979dcl/main_effects_plot_and_count_data/",
        "text": "",
        "created_utc": 1534264257,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating chi-square values",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/979exs/calculating_chisquare_values/",
        "text": "[deleted]",
        "created_utc": 1534261239,
        "upvote_ratio": ""
    },
    {
        "title": "Ecological network analysis - nestedness and null models",
        "author": "xaratco",
        "url": "https://www.reddit.com/r/AskStatistics/comments/978yfp/ecological_network_analysis_nestedness_and_null/",
        "text": "Hi!\n\n\nI hope I am posting this in the correct place, but am in need of help with ecological network analyses. \n\n\nI am creating five weighted bipartite networks of plant-pollinator mutualisms and would like to conduct analyses to determine the robustness of the networks, then compare the values between networks. I have looked into calculating the nestedness of each network, but am having issues choosing the correct metric (WNODF etc.). I am also having issues with choosing the correct null model to test it against.\n\n\nMy adjacency matrices have differences in the number of samples per site, so I will need to account for this. As I focused specifically on bumblebees, the network sizes are relatively small, ranging from five pollinators and four plants in the smallest network, to 13 pollinators and 22 plants in the largest network.\n\nI would really appreciate any tips for choosing the correct metric and null model for my nestedness analysis! I will need to conduct the analyses using bipartite in R, but I'm not all too great at coding, so would rather decide the method of analysis before getting stuck in.\n\nThank you!",
        "created_utc": 1534257948,
        "upvote_ratio": ""
    },
    {
        "title": "Definition of random effects",
        "author": "nadezdhaw27",
        "url": "https://www.reddit.com/r/AskStatistics/comments/977m03/definition_of_random_effects/",
        "text": "Hello there! I'm currently working on a linear mixed effect model and I just realized maybe my random effects structure doesn't make much sense so I'd rather see what you think!\nThe model is analysing how much the manner of delivery of an utterance (fluent vs disfluent) influences where participants looked at. Therefore, for items and participants I added a random slope for manner of delivery. However, I realized that this means that each item was delivered in different ways, and that the disfluency may vary across items. Yet, because of the experimental design, fluent and disfluent utterances were the same (i.e. The same was the same, I just cross-spliced a disfluency and put it at the end of the fluent version to create the disfluent one and then added recording of each item). Therefore, disfluency does not vary because it is the same. \nMy educated guess is that the random slope per manner of delivery should be dropped by-items (but not by-subjects because each participant may react in a different fashion). What do you think?\nCheers! ",
        "created_utc": 1534246791,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a summary metric that takes in compositional data and a graph structure as input?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/976295/is_there_a_summary_metric_that_takes_in/",
        "text": "[deleted]",
        "created_utc": 1534228284,
        "upvote_ratio": ""
    },
    {
        "title": "Could someone explain how a control variate reduces variance?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/975c7j/could_someone_explain_how_a_control_variate/",
        "text": "[deleted]",
        "created_utc": 1534220547,
        "upvote_ratio": ""
    },
    {
        "title": "distribution of tests between events with some likelihood of happening (it's for rewriting a twitter bot)",
        "author": "jhizzle4rizzle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/974w8l/distribution_of_tests_between_events_with_some/",
        "text": "So I currently have a twitter bot with logic that looks something like this:\n\n    while True:\n        time.sleep(random.uniform(0, 12 * HOURS))\n        tweet_something_funny()\n\nThis, by my understanding, tweets something on average every 6 hours, so about 4 times a day, but sporadically, sometimes in quick succession and sometimes only much later. Works for me! I'm not a statistician, but on the face that seems like a sane non-spammy frequency.\n\nProblem though: It's potentially cheaper for me to do this, because of on-demand compute:\n\n    # Ran by cron every SOME_INTERVAL minutes\n    if random.random() &gt; SOME_THRESHOLD:\n        tweet_something_funny()\n\nI want to know how to say the same kinds of things about my bot for this algorithm, but I don't even know what a reasonable threshold would be! Is there a way I can tune this to have similar behavior to the existing algorithm? Or, perhaps, do better?\n\nTIA!\n",
        "created_utc": 1534216370,
        "upvote_ratio": ""
    },
    {
        "title": "Pearson correlation in a dataset-- can someone provide insight into how ratios change linearity?",
        "author": "knotswag",
        "url": "https://www.reddit.com/r/AskStatistics/comments/972pfe/pearson_correlation_in_a_dataset_can_someone/",
        "text": "This is a question related to a dataset I'm analyzing and has been posed on /r/bioinformatics, [where there's been debate](https://www.reddit.com/r/bioinformatics/comments/95sis3/moronic_question_about_pearson_correlation/) but not a clear answer for my own moronic brain to comprehend.\n\nIn short:\nI'm comparing whether changes in time between datasets are consistent compared to a reference. Testing linearity between each independent replicate to each other, without a reference, shows strong linearity via Pearson. However when I do the ratios to a reference this linearity no longer holds up. Intuitively I would have expected independent linear datasets to maintain their linearity even after doing a ratio. I wrote it out in the discussion of the thread via this sample data of what I'm trying to do:\n\n\"Reference point 1: &lt;1, 5, 10&gt;\n\nTime point, replicate 1: &lt;10, 50, 100&gt;\n\nTime point, replicate 2: &lt;100, 500, 1000&gt;\n\n\n\nRatio vector 1: &lt;100/10, 50/5, 10/1&gt;= &lt;10,5,10&gt;\n\nRatio vector 2: &lt;1000/10, 500/5, 100/1&gt;=  &lt;100, 50, 100&gt;\n\nRunning Pearson, linearity should not be unchanged.\"\n\nBut this hasn't been the case in my own dataset. Are there incorrect assumptions I'm making here? Are there other statistical tests you might suggest? Thank you for any help.",
        "created_utc": 1534198220,
        "upvote_ratio": ""
    },
    {
        "title": "How to create interaction variable between 1-7 value and dummy variable (0-1)?",
        "author": "R4ikuma",
        "url": "https://www.reddit.com/r/AskStatistics/comments/971y78/how_to_create_interaction_variable_between_17/",
        "text": "I am trying to use [this Excel tool (2_way unstandardized.xls)](http://www.jeremydawson.co.uk/slopes.htm) to create a plot. It asks for unstandardized regression coefficients and means. \n\nThe issue is that my independent variable is a dummy (?) variable, with a value of either 1 or 0, and the Excel tool asks for an Interaction. Does it mean an interaction variable? How can I calculate it since I use 0 or 1 as an independent variable value? If I multiply the independent variable with the moderating variable obviously it would either result in a 0 or the same value. \n\nThank you in advance!   ",
        "created_utc": 1534192739,
        "upvote_ratio": ""
    },
    {
        "title": "Could somebody give me their thoughts on this method I am using to calculate beliefs about the true probability of a binomial distribution when priors are changing",
        "author": "Fatlark",
        "url": "https://www.reddit.com/r/AskStatistics/comments/971nar/could_somebody_give_me_their_thoughts_on_this/",
        "text": "Sorry for this long (bad?) title, I will try to best elaborate here:\n\nI am trying to calculate an agent's beliefs about the true probability of success from two slot machines. The agent samples the slot machines and forms a probability density distribution related to each one. (ie after experiencing 1 success and 1 failure, the agent will believe the true probability being 50% is twice as likely as the true probability being 20%...)\n\nHowever, the agent's priors are warped because the agent knows that p1 &gt; p2 (the probability of success in machine one is greater than the probability of success in machine two). So if the agent believes p2 = 50% then the probability of p1 = 50% is zero (this example is hugely simplified).\n\nTo calculate this in a computationally manageable way, I want plan to first calculate p1 and p2 distributions assuming uniform priors. Then I will recalculate p1 and p2 using priors dependent on the probability distributions from the first calculations. I then plan to repeat this process a few times using the (i-1) distributions to create priors for the (i) distributions.\n\nEXAMPLE OF MY MATH: Suppose for p2 you have 5 success and 5 failures, and suppose you know p1 = 60%. Before I control for p1, I can calculate that p2 has a 0.25 likelihood of being 40%, a 0.5 likelihood of being 50% and a 0.25 likelihood of being 60% (suppose this is the entire distribution). I then change the likelihood of being 60% from being 0.25 to being 0 because if p2 = 60% then p1 is not greater than p2. After changing p2 values &gt;=60% to zero, I then re-normalize so everything adds up to 100%: 0.2 (for 40%) becomes 0.33 and 0.5 (for 50%) becomes 0.66.\n\nDoes this sound reasonable? I imagine this converges to the true answer.\n\nIs there a name for this process?\n\nNote: I imagined a way to calculate this by creating a single likelihood distribution for all combinations of p1 and p2 (ie the likelihood that p1=20% and p2=10% is 0.1, the likelihood that p1=20% and p2=30% is 0.0, etc.), but this is far too computationally intense to be feasible (in my actual problems I have multiple probabilities so this space would be massive).",
        "created_utc": 1534190632,
        "upvote_ratio": ""
    },
    {
        "title": "Gage R&amp;R calculations",
        "author": "Mathsy_maths",
        "url": "https://www.reddit.com/r/AskStatistics/comments/971km0/gage_rr_calculations/",
        "text": "I need to know how to calculate Gage R&amp;R by hand. I know there are easier methods and software to calculate but I cannot use them. \n\nWould anyone be able to explain the calculations please?\n\nThanks in advance! \n",
        "created_utc": 1534190090,
        "upvote_ratio": ""
    },
    {
        "title": "Choosing a study design without a control group (repeated measures vs interrupted time-series vs pre-test/post-test)",
        "author": "Bokonomy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/971k2o/choosing_a_study_design_without_a_control_group/",
        "text": "Hello! I am supposed to come up with a realistic proposal to complete over an 8-month period. I will probably be completing the intervention (a psychoeducation program) over the course of 8 weeks. I'm worried I won't have enough participants for a control group for significance, so I thought just having the experimental group would be better. Which of these methods do you think would be realistic, and the best of those? Thanks. ",
        "created_utc": 1534189985,
        "upvote_ratio": ""
    },
    {
        "title": "Follow up for Introduction to Probability by Blitzstein?",
        "author": "photographic_amnesia",
        "url": "https://www.reddit.com/r/AskStatistics/comments/971dyn/follow_up_for_introduction_to_probability_by/",
        "text": "I've recently finished working through [Introduction to Probability by Blitzstein](https://www.goodreads.com/book/show/21558327-introduction-to-probability), the text based on Harvard's STAT 110. I really enjoyed it and I'm interested in the follow-up class: STAT 111. Unfortunately, there doesn't seem to be an equivalent textbook for 111.\n\nWhat would be a good follow up? I found this [quora post](https://www.quora.com/What-are-the-top-10-big-ideas-in-Statistics-111-Introduction-to-Theoretical-Statistics-at-Harvard) that lists the topics covered in 111, but I've had trouble finding a suitable textbook that both covers that material and is still at the undergraduate level.\n\nSo far, I've considered the following texts:\n\n- [Probability and Statistics by Morris H. DeGroot,  Mark J. Schervish](https://www.goodreads.com/book/show/979760.Probability_and_Statistics)\n- [Introduction To Probability And Mathematical Statistics\nby Lee J. Bain,  Max Engelhardt](https://www.goodreads.com/book/show/4022552-introduction-to-probability-and-mathematical-statistics)\n- [Mathematical Statistics with Applications by Dennis D. Wackerly,  William Mendenhall, Richard L. Scheaffer](https://www.goodreads.com/book/show/115105.Mathematical_Statistics_with_Applications_Mathematical_Statistics)\n- [Introduction to Mathematical Statistics\nby Robert V. Hogg, Joseph W. McKean, Allen T. Craig](https://www.goodreads.com/book/show/756267.Introduction_to_Mathematical_Statistics)\n- [Statistical Inference by George Casella,  Roger L. Berger](https://www.goodreads.com/book/show/383472.Statistical_Inference)\n\nI'm including the Casella text because, while aimed at the first-year graduate student, it definitely covers all the material from the quora post and it's one I'd eventually like to work through. I'm just worried it's too much of a complexity jump from Blitzstein.\n\nLast, I'm sure I'd be ok with any of these texts. But given that I devoted almost three months to Blitzstein, I'd prefer a recommendation here as I'll probably have to invest even more time in whichever one I pick.",
        "created_utc": 1534188841,
        "upvote_ratio": ""
    },
    {
        "title": "What is the correct way to convert to numeric data?",
        "author": "ASPNetthrow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/970ul1/what_is_the_correct_way_to_convert_to_numeric_data/",
        "text": "In a statistical analysis, I'd like to include a user's choice of four different scholarly journals. Let's call them Journal A, Journal B, Journal C, Journal D. What is the correct way to \"encode\" their journal choice so that it can be included with otherwise numeric data?",
        "created_utc": 1534185131,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate height for biserial correlation?",
        "author": "Squareheaded",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96zn0o/how_to_calculate_height_for_biserial_correlation/",
        "text": "Hi all,\n\nMay I know how do I manually calculate the height for a biserial correlation? I've seen an earlier [thread](https://www.reddit.com/r/AskStatistics/comments/57h6w1/calculating_biserial_correlation/) on this issue but I still do not understand how I can calculate the Z-score for p1. Also, why must I calculate the Z-score for p1 only and not p2? I've created some hypothetical data for anyone who can help to enlighten me with. I would really appreciate it if someone could show me the step by step workings as I feel that it will help me better grasp this concept.\n\nThank you very much.\n\n|Test 1|Test 2|Test 1|Test 2|\n|:-|:-|:-|:-|\n|0|85|1|125|\n|0|32|1|130|\n|0|122|1|122|\n|0|89|1|118|\n|0|92|1|104|\n|0|58|1|111|\n|0|99|1|94|\n|0|101|1|98|\n|0|77|1|116|\n\n\\*Note: 0=\"fail\" and 1=\"pass\"",
        "created_utc": 1534176861,
        "upvote_ratio": ""
    },
    {
        "title": "I don't get dis-/con-cordant pairs in a crosstable!",
        "author": "Arjab",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96y6mx/i_dont_get_disconcordant_pairs_in_a_crosstable/",
        "text": "Why is the rule of thumb that you build concordant pairs from the top left to the bottom right and the other way around with discordant pairs. Shouldn't that be relative to the sorting of the x- and y-variables of the crosstable? I hope you get me, hard to ask about something you don't get in a language other than your mother language.",
        "created_utc": 1534165965,
        "upvote_ratio": ""
    },
    {
        "title": "Any downsides in giving a participant A and B then compare the results?",
        "author": "ohai123456789",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96v78t/any_downsides_in_giving_a_participant_a_and_b/",
        "text": "Almost all the experimental design studies that I came across randomly separates participants into bucket A or B. Then they study the effects to identify a causal relationship.\n\nAre there any downsides to assigning all participants both A and B 100 times (50 times each randomly)? The number of times is arbitrary..it can be 1000x (500:500) but you get the point.\n\nIsn't that a better way to identify whether there is a casual relationship? Let's assume that the participants don't realized that there are 2 variations.\n\nAlso what kind of test is that called? Is there a textbook name for that?",
        "created_utc": 1534132969,
        "upvote_ratio": ""
    },
    {
        "title": "Biostats/Statistics PhD/Masters Chances?",
        "author": "maryjotw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96uy7s/biostatsstatistics_phdmasters_chances/",
        "text": "Hello. I am thinking about applying to PhD/Masters schools in Stats fields. My undergrad GPA is \\~2.7 and I am a Biology major with an emphasis on Cell and Molecular biology. I do research at UCSD on my own project, however I attend SDSU. My GRE scores are Verbal: 150 Quantitative Reasoning: 168 and Writing: 4.\n\nI have taken Stats twice: I took intro to stats and stats with a computer science emphasis. I failed both classes, and am planning to retake in the future. I have taken all the calculus series up to linear algebra and differentials with B's and C's. I am still very interested in becoming What are my chances? What are my chances of getting into a top 20 school? How about any stats schools in general?\n\nThank you.",
        "created_utc": 1534130636,
        "upvote_ratio": ""
    },
    {
        "title": "Need help for research project!!",
        "author": "nuromania",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96r6ux/need_help_for_research_project/",
        "text": "Hi,\n\nI need to perform analysis with using SPSS. I am using Early Trauma Inventory Scale to see if diagnosis type of people with eating disorders differ in trauma response. So my IV is type of diagnosis with 4 levels: (coded as) 1=Individuals with anorexia nervosa binge-purge type, 2= individuals with restrictive subtype, 3=individuals with recovered anorexia and 4=healthy controls. The type of diagnosis is categorical variable. My DV is trauma scale which have several domains such as general trauma, emotional trauma, physical trauma and sexual trauma. Participants gave yes/no answers to questions. (0=no, 1=yes). As directed, I simply sum up the scores for each domain (for ex total score for emotional trauma) and then, calculate the sum total by sum up each domain together. My question is, how should I code my domain variables and sum total in the spss? For example, is the emotional trauma, sexual trauma etc CATEGORICAL VARIABLE and sum total is CONTINUOUS/INTERVAL VARIABLE? Or shall I code all of them as categorical variables?\n\nHelp please! Thank you!!",
        "created_utc": 1534098823,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a way to aggregate odds ratios together into a summary statistic?",
        "author": "jamhsijedi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96qvig/is_there_a_way_to_aggregate_odds_ratios_together/",
        "text": "Hey everyone. I'm currently trying to perform a systematic review and meta-analysis, but of the papers (roughly 50), they only provide the odds ratios, and not raw data (for producing our own risk/odds ratios). Thus, is there a way for me to aggregate these odds ratios into a forest plot or something like that?\n\nI am familiar with R and the metafor package, but I don't know if its possible with these.\n\nThank you for your time and help!",
        "created_utc": 1534096309,
        "upvote_ratio": ""
    },
    {
        "title": "Help identifying this formula",
        "author": "Celebrate710b",
        "url": "https://i.imgur.com/zjmGNET_d.jpg?maxwidth=640&amp;shape=thumb&amp;fidelity=medium",
        "text": "",
        "created_utc": 1534057862,
        "upvote_ratio": ""
    },
    {
        "title": "Multivariate Prediction Model Help",
        "author": "wowoweewoo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96n4x1/multivariate_prediction_model_help/",
        "text": "Looking for some guidance on how to go about developing a model for this: \n\nI have a categorical outcome (O0 or O1) that I'm trying to predict based on multiple continuous (D1,D2) and categorical data (D3, D4). \n\nMy end goal is to develop a decision tree such that entering the values for D1-4 will give me the predicted outcome with various levels of accuracy (95, 99, etc). \n\nCould someone please point me in the right direction for this project? ",
        "created_utc": 1534053042,
        "upvote_ratio": ""
    },
    {
        "title": "What test should I use for comparing distributions over time?",
        "author": "Stallob",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96l23c/what_test_should_i_use_for_comparing/",
        "text": "Hey guys, I have an experiment looking at age of animals fed different diets. The 'age' is depicted as different categories - young, medium age, old - and the diet is also variable, with a control diet and multiple other diets. I sampled animals from a population for each diet every day to get a distribution of their ages over time on the different. Essentially my data looks something like this:  [https://i.imgur.com/2EkkNWG.png](https://i.imgur.com/2EkkNWG.png)\n\nMy hypothesis is that the age distribution of the animals is altered on the high and low fat diets compared to the control diet. My question is, which statistical test do I need to use to determine whether the distributions on the other diets are significantly different?\n\nThanks",
        "created_utc": 1534031914,
        "upvote_ratio": ""
    },
    {
        "title": "is this a correct stratified randomization ?",
        "author": "Asmaa_Saeed",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96jkfu/is_this_a_correct_stratified_randomization/",
        "text": "I'm going to be dealing with a certain disease  and there are scoring systems for measuring that disease activity. So based on the score, the patients can be classified into strata into mild , moderate or severe .\n\nThe total patients (52 patients) are going to be distributed on 2 groups ( intervention and control) and both disease activity itself and quality of life are going to be measured at baseline and at the  end as an outcome.\n\nMy question is : Can I categorize the patients into strata at baseline based on the disease activity and then perform block randomization for each strata onto the 2 groups ( the intervention and control) to obtain a homogeneous baseline distribution of disease activity in the 2 groups ....is this a correct application of stratified randomization ?\n\nor in other words can the stratification factor be the same as the outcome I'm going to be measuring ?\n\nthank you ",
        "created_utc": 1534019030,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a uniform method for analysing ecological presence/absence data?",
        "author": "SquiffyRae",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96hcj1/is_there_a_uniform_method_for_analysing/",
        "text": "Because every paper I've read so far seems to suggest a different method. From what I've read the two best options I can come up with are logistic regression modelling and analysis or Pearson's correlation test.\n\nI'm doing an experiment looking into the composition of invertebrate microhabitats both at the base of trees and in the tree canopy sampling from two different tree species at three sites in the forest. In short, looking for differences in community composition between habitat location (base/canopy), tree species and each of the three sites.\n\nAs I said, from my reading so far I'm leaning towards logistic regression modelling. Am I on the right track or have I just not seen the wood for the trees while reading and missed a really obvious method I haven't thought of?",
        "created_utc": 1534001345,
        "upvote_ratio": ""
    },
    {
        "title": "Could somebody tell me if the model described here is reasonable and or give suggestions for improvement? Thanks. :)",
        "author": "danielw29",
        "url": "http://discourse.mc-stan.org/t/regression-discontinuity-with-binomial-logit-model/5137",
        "text": "",
        "created_utc": 1533987932,
        "upvote_ratio": ""
    },
    {
        "title": "Back transforming variables on a logged scale in LMM output",
        "author": "insufferablemoron",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96ftl5/back_transforming_variables_on_a_logged_scale_in/",
        "text": "Hi guys,\n\nI’ve read a fair bit about this online but my model is not providing the values I was expecting so I just wanted a bit of reaffirmation that I am using the correct formula.\n\nI have log transformed my response variable so that it has a normal distribution and now I want to back transform the intercept and coefficients to the original data scale. Can somebody please confirm if this formula is right? Or correct me please.\n\nY=exp(Bo+ 1/2var) \n\nApologies for the weird formula I’m on my phone and can’t get the correct symbols \n\nThanks in advance ",
        "created_utc": 1533985968,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing two groups within two groups of people ?",
        "author": "LostMeatball",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96dbej/comparing_two_groups_within_two_groups_of_people/",
        "text": "So I had two independent groups, Group A and Group B, perform several tasks and I measured their mean heart rate during each task (not quite but similar). About 40% of Group A and 60% of Group B missed 1 or 2  out of 7 tasks. \n\nI want to compare the means of the people who missed tasks in Group A and B against those that completed all tasks in Group A and Group B. So I would have two new Groups to compare- A/B people who missed tasks vs A/B who did not miss tasks. I would also like to look withing Group A and compare those who did miss tasks against those that did not miss tasks in Group A, and do the same within Group B.\n\nAny idea on how to do this? The data is not normally distributed, and I've used independent samples t-tests but I am not sure if that is valid in this case. as for the missing values, I selected case-by-case analysis over listwise. \n",
        "created_utc": 1533955852,
        "upvote_ratio": ""
    },
    {
        "title": "Stats test counting number of data points more strongly than the values of data points?",
        "author": "gravitydefyingturtle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96c7e6/stats_test_counting_number_of_data_points_more/",
        "text": "So I need to find a specific kind of test, as the usual linear models and non-parametric systems aren't working for what I want to do. What I need is a test where the number of data points in a set are more important that the values of those data points (but where the values are still considered).  These data points are essentially 'hits' within a pre-selected range, which is why the number of them is more important than the actual values. \n\nI've made some dummy data to highlight what I mean. Let's say I have 4 treatments, which have the following values:\n\nA - 17, 9\n\nB - 10, 7, 14, 19, 20, 2, 6, 19, 4, 8\n\nC - 15, 8, 18, 15, 5, 1, 15, 9, 4, 3, 11, 11, 2, 5, 8, 10, 13, 15, 17, 14, 20\n\nD - 4, 19, 8, 14, 6, 8\n\nTreatment A has 2 data points, B has 10, C has 22, and D has 6. As I said, the data values do have importance, just not as much as the number of points.\n\nAny suggestions on what kinds of tests I might use on this? I thought ranked non-parametric tests would work, but they seem to only be calculating based on data values. Any help would be greatly appreciated!",
        "created_utc": 1533945738,
        "upvote_ratio": ""
    },
    {
        "title": "Question regarding statistical analysis of a repeated measures design",
        "author": "raencoat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96ai9t/question_regarding_statistical_analysis_of_a/",
        "text": "I'm developing a research proposal where I analyze the effects of an intervention on Social Connectedness, assessed using a 6-point Likert scale. Since the dependent variable is ordinal, it's my understanding that I shouldn't use a repeated measures ANOVA. Does anyone have guidance regarding the best statistical approach for this? \n\nMore context if needed: I originally developed a 2x2 factorial design: (1) Social Connectedness (Low / High) using a median split, and (2) Intervention (Y / N). I considered analyzing the social connectedness data as interval. If this is a possible approach, I can change the design if needed.",
        "created_utc": 1533932663,
        "upvote_ratio": ""
    },
    {
        "title": "Assign index to categorical data for Pearson Correlation Coefficient.",
        "author": "herbyhancock2323",
        "url": "https://www.reddit.com/r/AskStatistics/comments/96ae0a/assign_index_to_categorical_data_for_pearson/",
        "text": "Hello, I’m using Power BI and one of the plug in graphs automatically uses Pearson. Can I assign each category a unique identifier and get accurate results? I would typically use Spearman for this but the graph doesn’t let me. ",
        "created_utc": 1533931806,
        "upvote_ratio": ""
    },
    {
        "title": "Basic Statistics Question",
        "author": "Misanthropic_Cynic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/969u0x/basic_statistics_question/",
        "text": "I was never really good at statistics in my college class, so hoping someone can help me out with a problem at work\n\nThe Background:\n\nA test was performed to measure something from a few samples (less than 10). We didn't like the results, so we changed those samples in some way. The same test was performed again with the same number of samples from the newly-upgraded ones.\n\nThe Problem: \n\nThe mean of the 2nd set of samples is very similar to the mean of the 1st set of samples, and well within the standard deviation of the 1st set. Basically, it looks like nothing changed. BUT, management wants me to report some kind of improvement so that we don't have to keep changing the part and re-doing these tests and close the file on this already.\n\nThe Solution:\n\nOne thing I have going for me is that the standard deviation of the 2nd set of samples is lower than the 1st set (about a 20% decrease). So I am wondering if there is a way to statistically demonstrate that the decrease in standard deviation means something like a 'more consistently made part' or something like that. \n\nThanks",
        "created_utc": 1533927895,
        "upvote_ratio": ""
    },
    {
        "title": "Back transforming rescaled variables",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/968doy/back_transforming_rescaled_variables/",
        "text": "[deleted]",
        "created_utc": 1533918039,
        "upvote_ratio": ""
    },
    {
        "title": "I am doing a retrospective stat analysis of patients who have diabetes, and comparing their heart rates and blood pressures against those without diabetes.",
        "author": "st3v0943",
        "url": "https://www.reddit.com/r/AskStatistics/comments/967i6g/i_am_doing_a_retrospective_stat_analysis_of/",
        "text": "I have a diabetic sample size of n = 70. My control will be those without diabetes. What test/control split would be most effective? What control size do I need?",
        "created_utc": 1533912059,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical difference between two values that aren't themselves statistically different from zero?",
        "author": "monakajgod",
        "url": "https://www.reddit.com/r/AskStatistics/comments/964zlq/statistical_difference_between_two_values_that/",
        "text": "I have this hypothesis:\n\nx-y=z=0\na-b=c=0\n\nz and c weren't exactly 0, so I tested if that was significant using two z-tests which showed that x wasn't different form y, and a wasn't different from b which lead to the conclusion that z and c weren't really statistically different from 0.\n\nHowever, I went to check if z and c were different from each other and they were! How does that make sense? ",
        "created_utc": 1533887609,
        "upvote_ratio": ""
    },
    {
        "title": "How can I create a prediction curve for percentages in a regression analysis",
        "author": "bakja",
        "url": "https://www.reddit.com/r/AskStatistics/comments/961ft5/how_can_i_create_a_prediction_curve_for/",
        "text": "I am working with a data-set with four independent variables that are a percent of time spent on one of four topics. The four variables add up to 100%. The dependent variable is customer satisfaction. I would expect that if a variable is not addressed by the salesperson, that would decrease overall satisfaction, but if it were the only topic, i.e. 100% of the time, that would also decrease satisfaction. I am looking for a method that would pinpoint the ideal percentage for each factor. ",
        "created_utc": 1533855762,
        "upvote_ratio": ""
    },
    {
        "title": "Need help figuring out a formula for this problem",
        "author": "Geryth04",
        "url": "https://www.reddit.com/r/AskStatistics/comments/960myc/need_help_figuring_out_a_formula_for_this_problem/",
        "text": "So I'm playing a mobile game that has a system setup of fusing experience containers into units to give the unit experience points. Each \"fuse\" has a 5% chance of \"great success\" which adds 1.5x the experience of the container and a 1% chance of \"amazing success\" which adds 2x the experience of the container. The catch here is that we can now fuse experience containers into other experience containers. What I'm trying to find out is how much extra experience we can get by fusing our containers first one at a time.\n\nSo let's say I have 100 separate experience containers and each one is worth 100,000 experience. With regular fusing the 5% chance of 1.5x and 1% chance of 2x should equate to a simple 3.5% extra experience. I don't know the statistics formula for this, but I calculated that out this way:\n\nSay I fused 100 containers and I hit the expected 5% and 1% chances. That would look something like this:\n\n* 100 \\* 100,000 = 10,000,000 experience base experience from the containers.\n* 5 \\* 50,000 = 250,000 extra experience that we gained because 5 of that 100 gave 150,000 experience instead of 100,000.\n* 1\\* 100,000 = 100,000 extra experience that we gained because 1 of that 100 gave 200,000 experience instead of 100,000\n* 10,000,000 + 250,000 + 100,000 = 10,350,000 which is our total experience gain.\n* 10,350,000 (total) / 10,000,000 (base) = 1.035, so we gained 3.5% extra experience from our containers because of the great and amazing successes.\n\nI think that's accurate, and wasn't too hard. Where I get stuck is where we can add extra great and amazing chances by fusing containers into each other one at a time. It would looks something like this:\n\n* Fuse 1: Modifier 1 (94% chance of 1x, 5% chance of 1.5x, and 1% chance of 2x) \\* 100,000 + 100,000 = 200,000 (for example's sake assume a 1x modifier)\n* Fuse 2: Modifier 1 \\* 200,000 + 100,000 = 300,000 (again assume modifier was 1)\n* Fuse 3: Modifier 1.5 \\* 300,000 + 100,000 = 550,000 (we got lucky and hit a 5% chance of \"great success\" for 1.5 modifier)\n\nSo as you can see, we greatly increase our experience because every single container from the previous fuse gets a chance to get 1.5x or 2x modifiers (excluding the target container which always adds a flat 100,000). What I'm trying to figure out is a mathematical formula for this because I want to know how much extra experience we are getting for this kind of fusing. Without it we get 3.5%, with it we get how much extra % do we get?\n\nA wrench in this formula is the level cap. A container can not hold more than 4,500,000 experience. Part of the problem is knowing when to stop and start on a new container because we're at risk of a great or amazing success hitting our level cap, in the interest of maximizing experience gain from smart fusing.\n\nWhile I am not a mathematics major I am a computer programmer, and I tried to get these numbers by hitting them with millions of simulated fuses and getting averages. The trouble is - so did other programmers! And our results vary by a lot. But if we had the mathematical formula behind what we're doing we wouldn't need to worry about simulation programs that probably have bugs in them and could get an exact amount.\n\nI'm deeply appreciative of any help. Thanks!",
        "created_utc": 1533849991,
        "upvote_ratio": ""
    },
    {
        "title": "If I spin a slot machine 10 times and I win 1 time, the probability of winning is 16.7%!? (Assuming a uniform distribution of priors)",
        "author": "Fatlark",
        "url": "https://www.reddit.com/r/AskStatistics/comments/960g8j/if_i_spin_a_slot_machine_10_times_and_i_win_1/",
        "text": "Could anybody clarify if my thinking is moving along on the correct path. I was thinking about the following situation: if I spin a slot machine a couple times, and I never win, that doesn't mean that the probability of winning is 0%. \n\nReturning to the 1 in 10 example: I am calculating the probability of winning by calculating the probability that a given \"true probability\" (TP) would result in winning 1 in 10 times. I am taking the integral of p(TP) \\* TP for all TP in the range of 0 to 100%. Doing this gives me the result 16.7%. \n\nI feel like I am assuming a uniform distribution of priors, but I cannot say for certain that this is correct. \n\nThanks",
        "created_utc": 1533848693,
        "upvote_ratio": ""
    },
    {
        "title": "Survey about familiarity and attitudes towards statistical programs",
        "author": "Lady__Otter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95z6gw/survey_about_familiarity_and_attitudes_towards/",
        "text": "Hello everyone!\n\nAs part of a UX research course I am taking, I have to develop a series of small studies concerning a topic of my choice. Being a nerd, I choose statistics.\n\nIn this case, I designed a little survey to better understand peoples' familiarity and attitudes towards different statistical programs and tools.\n\nIt is a short survey and should only take a couple of minutes to complete. \n\nI would be very happy if you choose to participate!\n\n[Click here to participate in the survey!](https://sjsu.qualtrics.com/jfe/form/SV_37YIXWTsJCPrrmt)\n\n\nThank you!",
        "created_utc": 1533840246,
        "upvote_ratio": ""
    },
    {
        "title": "Which model to use to analyse pollution temporal patterns?",
        "author": "tony4ward",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95xc16/which_model_to_use_to_analyse_pollution_temporal/",
        "text": " Hello everyone,\n\nI am a senior student in college and for a research project I would like to analyse temporal patterns of different pollutants in Rotterdam. I have obtained hourly data of 5 pollutants for 3 years, distributed in 8-10 stations across the city. In addition, I have obtained hourly weather measurements such as wind speed, temperature, humidity...\n\nI would like to ask you if you could advice me which model would be the best fit for the analysis of temporal patterns, such as day-night changes and seasonality.\n\nThank you in advance for the help :)",
        "created_utc": 1533827940,
        "upvote_ratio": ""
    },
    {
        "title": "SPSS Recoding variables",
        "author": "Statisticshelp7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95x9if/spss_recoding_variables/",
        "text": "I've currently got a multiple - choice question from a questionnaire which is split into individual variables for answers. These split variables are binary in answer format (e.g. What car do you drive? Q1\\_1 Honda (1=Honda), Q1\\_2 Nissan (1= Nissan), Q1\\_3 (1=Hyundai) etc...  \n\n\nHow do I group this into one variable so it answers 1=Honda, 2= Nissan, 3= Hyundai etc.  \n\n\nThanks in advance ",
        "created_utc": 1533827467,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for a metric to help determine which data to fit against",
        "author": "edwwsw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95wta4/looking_for_a_metric_to_help_determine_which_data/",
        "text": "This may seem backwards at first glance, but I’m looking for the data (dependent variables) that best explains my model (independent variables) in a multi variable system. \n&amp;nbsp; \n&amp;nbsp;\n\nSpecifically I have spectrum data with absorption features in it. Typically when fitting this data, a subset of the spectrum is used to avoid areas of high interference.  These interferences are areas of the spectrum that are significantly affected by unknowns (variables not in the model), signal starvation, modeling imprecision, etc.\n&amp;nbsp; \n&amp;nbsp;\n\nI’m looking to create a training algorithm that automatically determines the “best” areas to match against.  This problem becomes one choosing the data that best fits the model.  \n&amp;nbsp; \n&amp;nbsp;\n\nIn model selection, you can use adjusted R^2, AIC, BIC to compare the fit of one model vs another.  Metrics like AIC and BIC require that the dependent variables are unchanging.   So I can’t use these metrics to compare say spectrum vs spectrum minus 1 data point.   I suspected R^2 has the same issue.  So what I’m looking for is a metric that allows me to compare fits between subsets of the data.    ",
        "created_utc": 1533824280,
        "upvote_ratio": ""
    },
    {
        "title": "What test of significance should I use?",
        "author": "Renyudaishu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95sw8u/what_test_of_significance_should_i_use/",
        "text": "For a project that I am working on, I am trying to see if the difference in the amount of housework done by men and women is statistically significant. \n\nThe variable for housework is a categorical variable.\n\n  \nThis is what I have for women:\n\nFreq: Housework |      Freq.     Percent        Cum.\n\n\\----------------------+-----------------------------------\n\nNot applicable |         18       10.17       10.17\n\nAlmost every day |     149       84.18       94.35\n\n Several times a week |       5        2.82       97.18\n\nSeveral times a month |     3        1.69       98.87\n\n once every few month |    2        1.13      100.00\n\n\\----------------------+-----------------------------------\n\nTotal |                177      100.00\n\nThis is what I have for men:\n\nFreq: Housework |      Freq.     Percent        Cum.\n\n\\----------------------+-----------------------------------\n\nNot applicable |           68       32.38       32.38\n\nAlmost every day |        117       55.71       88.10\n\n Several times a week |       16        7.62       95.71\n\nSeveral times a month |       8        3.81       99.52\n\nOnce a month |            1        0.48      100.00\n\n\\----------------------+-----------------------------------\n\nTotal |                  210      100.00\n\nSuggestions on how to go about this would be extremely appreciated!",
        "created_utc": 1533784879,
        "upvote_ratio": ""
    },
    {
        "title": "Final Homework Help (R-studio and T-test",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95sjs2/final_homework_help_rstudio_and_ttest/",
        "text": "[deleted]",
        "created_utc": 1533781800,
        "upvote_ratio": ""
    },
    {
        "title": "So any other options apart from spss?",
        "author": "kindessissupreme",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95rxgt/so_any_other_options_apart_from_spss/",
        "text": "I'm trying to do my own market research, but I really can't afford spss and have to go to my uni to use it, is there any other option? As I'd like to use it personally, also, where's the best place to actually learn to use spss or any other data entry software like spss? ",
        "created_utc": 1533776527,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a statistical test for improvement in accuracy in parameter estimation?",
        "author": "mehtaters",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95r3tr/is_there_a_statistical_test_for_improvement_in/",
        "text": "If I'm analyzing the results of different methods used to estimate parameters, for example an EM algorithm vs a Bayesian algorithm is there a statistical test to indicate which one performed better? I was thinking about using one way anova but that would only tell me if the means of the parameter estimations are the same or not. Is the variance the best indicator of accuracy in this case?\n\n*I'm working with simulated data so I know the parameter values I'm looking for. I don't know if that changes anything.",
        "created_utc": 1533769987,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a better option than Chi-Square for several binomial dependent variables?",
        "author": "jasinjicama",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95q8nl/is_there_a_better_option_than_chisquare_for/",
        "text": "Survey respondents have been grouped into 3 cohorts by age (child, adolescent, adult).  I am analyzing data from a multiple response (check all symptoms that apply) question.  I want to know for which symptoms there are statistically significant differences in frequencies between cohorts.  My understanding is that using a Chi-Square test for each of the 20 symptoms is problematic because it significantly increases risk of Type 1 error. Is there a better way to do this analysis?  \n\nMy goal is to evaluate symptomatology differences between cohorts.",
        "created_utc": 1533763570,
        "upvote_ratio": ""
    },
    {
        "title": "How do you find the upper and lower bounds of a 95% confidence interval using only sample size and a sample proportion?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95q5zy/how_do_you_find_the_upper_and_lower_bounds_of_a/",
        "text": "",
        "created_utc": 1533763055,
        "upvote_ratio": ""
    },
    {
        "title": "What is there that variance can do that the standard deviation can't?",
        "author": "PauperPasser",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95pxbq/what_is_there_that_variance_can_do_that_the/",
        "text": "In everything I have read about these two (admittedly that may just be too little) it seems that variance is just mentioned but has no use other than being a measure of spread, but, since the standard deviation does the same thing and in the same units as the data set, it leaves me wondering what use there actually is for variance.  ",
        "created_utc": 1533761425,
        "upvote_ratio": ""
    },
    {
        "title": "Advice for explaining to a database person why we need our data in one big table?",
        "author": "swill128",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95oubk/advice_for_explaining_to_a_database_person_why_we/",
        "text": "We've been asking a database programmer to change the data export for our website for longitudinal data collection from a series of Excel sheets with each subject visit as its own row to a single sheet with each subject as a row and the visits as columns. This is the standard format for analyzing data but they just dont get it and won't change. So far their best excuse is that it could lead to empty columns as some subjects won't have all of the following up data. We have told them we know this and expect it and that it really isn't anything we can't handle from a statistics perspective.\n\nThe data we have been getting is unusable in this format and unnecessarily cumbersome to manipulate it in SPSS or R which are our only choices for software.",
        "created_utc": 1533754214,
        "upvote_ratio": ""
    },
    {
        "title": "How can I convert mean of logarithm of number of people/income per capita/land area to absolute value?",
        "author": "UKBua",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95ogea/how_can_i_convert_mean_of_logarithm_of_number_of/",
        "text": "I conduct descriptive statistics on my independent variables and some variables I applied in my Thesis analysis was converted to \"normal\" distribution, however, when I would like to interpret my result, I am not sure how to convert those mean value of the logarithms of income per capita for example to absolute number? I would like to ask if it is sensible to just provide \"absolute number\" although in my analysis I converted them into \"logarithms form,\" does anybody familiar with how to convert? I'd appreciate any kindness and generosity in advance. Thank you very much for your reading!",
        "created_utc": 1533751676,
        "upvote_ratio": ""
    },
    {
        "title": "Experiment sizing A/B test for increased adoption, rather than increased CTR?",
        "author": "abstract17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95nri4/experiment_sizing_ab_test_for_increased_adoption/",
        "text": "Most experiment sizing tools for A/B testing seem to focus on increased CTR for changing a button color, for example, like this tool: [link](https://www.optimizely.com/sample-size-calculator/)\n\nI'm asking how to size an experiment that might add a new exposure point for a feature, rather than change the CTR of an existing exposure point.   \n\nFor example, we let you access a certain page through an additional point in the UI in the treatment, and we want to measure incremental actions on that page due to the extra point. How is that sized? Any way to use an existing tool like this?\n\n\nThanks!",
        "created_utc": 1533747197,
        "upvote_ratio": ""
    },
    {
        "title": "Modeling question - Preference change based on learning",
        "author": "savedross",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95n7vw/modeling_question_preference_change_based_on/",
        "text": "I have some data from a survey where responds were asked to make a binary choice, then provided additional information, then asked if they'd like to change their choice. I'm not sure how to model this data. I am  familiar with probit and logit models, but don't know how to incorporate the preference shift. Any advice or direction is much appreciated.",
        "created_utc": 1533743535,
        "upvote_ratio": ""
    },
    {
        "title": "Probablity question - percentages",
        "author": "FthkuMan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95mp1c/probablity_question_percentages/",
        "text": "Hi all,\n\nSo this is kind of a silly question, I don't know if it's really a statistics question or a semantics one, but nonetheless; I've got an argument with someone with regards to the terminology of probability percentages: he claims you can say something \"has gone up 1000%\", but I think he's just confusing general use of percentages. Now he made me doubt myself though, is it technically possible to say that a probability has gone up by a 1000%, relative to what it was? (obviously without going over 100%)  \nSay, 0.005% goes up 1000%, and now it's 0.05%. To me it seems wrong, since probability is already in percentage units, but I guess I don't want to claim something without being sure.  \n\n\nSorry if this isn't the right place or a serious enough question for the sub!",
        "created_utc": 1533740075,
        "upvote_ratio": ""
    },
    {
        "title": "What do artificial neural networks typically do with missing data?",
        "author": "dijete_u_vremenu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95ko80/what_do_artificial_neural_networks_typically_do/",
        "text": "I plan to use ANN for my masters. In my opinion it is not used as much as it could be used in my field (and it would be good because we work with 'soft' data) so I wanted to start. I learned some basics on theory of ANN and how to do it in SPSS21 but I need help with question in title.\n\nThank you!\n\nEDIT: Also I got to a bit of contradiction here. First when I searched research on ANN and comparison with other more traditional statistical methods in older studies they said that it is good for incomplete data (which makes sense for me, if I understood it correctly). When I searched for question in tittle I found this article: Dealing with Missing Values in Neural Network-Based Diagnostic Systems that says \"Backpropagation neural networks have been applied to prediction and classification problems in many real world situations. However, a drawback of this type of neural network is that it requires a full set of input data, and real world data is seldom complete\" and also some that said that missing data is ok if it is unsystematic. So what is the deal with this? Pls help!",
        "created_utc": 1533722299,
        "upvote_ratio": ""
    },
    {
        "title": "Finding function(s) from datasets",
        "author": "Bromy2004",
        "url": "https://www.reddit.com/r/HomeworkHelp/comments/95ipez/finding_functions_from_datasets/",
        "text": "",
        "created_utc": 1533702217,
        "upvote_ratio": ""
    },
    {
        "title": "Standard Error of Cohen's d?",
        "author": "willbell",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95im0w/standard_error_of_cohens_d/",
        "text": "I have bootstrapped standard errors for my original point estimates of the statistics I'm interested in.  I want to graph the Cohen's d of those statistics with a confidence interval for their standard error.  Is it reasonable for me to divide the standard errors of the raw statistics by the pooled standard deviations to get the standard errors of Cohen's d?  Or should I go back and bootstrap my Cohen's d as well?  Is the former 'quick and dirty' but probably correct?",
        "created_utc": 1533699833,
        "upvote_ratio": ""
    },
    {
        "title": "Proving sample of (0,1) variable is representative of population",
        "author": "JDGinDC",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95h59k/proving_sample_of_01_variable_is_representative/",
        "text": "With my limited knowledge of statistics, I'm struggling to determine how to approach a problem and could really use your help. I've changed the context a bit to make it less confusing. Thank you in advance!\n\nLet's say there is a total population of 1M frogs. I have access to a random sample of 50,000 frogs at a nearby zoo. I'm trying to determine what % of all frogs have a wart on their butt.\n\nI can go to the zoo and one-by-one pull out a frog, look at its butt, and then give it a 1 if has a wart, or a 0 if not. Let's say I do this for 50 frogs and get that 60% of frogs have a butt wart. What statistical test(s) can I run to demonstrate that I manually sampled enough frogs to show that 60% is accurate with some level of confidence? Or do I need to test 500 frogs to prove it? How would I determine the necessary 'n' to have a statistically significant value?",
        "created_utc": 1533687390,
        "upvote_ratio": ""
    },
    {
        "title": "Meta Analysis (means, standard deviation), how to conduct? (excel)",
        "author": "lillychrissy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95f3rw/meta_analysis_means_standard_deviation_how_to/",
        "text": "hello! for my bachelor's thesis I was told to conduct a meta-analysis. after googling and researching I feel even more confused than before. I have the population size, means, and standard deviations of 14 studies in excel and I have no clue what to do with that. Since R is for free I would prefer to do analyses in either excel or R (not SPSS which costs $$)\n\nHelp would be very much appreciated :) Thanks in advance!\n\nupdate: he said we should change it to a correlation analysis :) so no meta-analysis needed anymore! Thanks for all your advices",
        "created_utc": 1533672449,
        "upvote_ratio": ""
    }
]