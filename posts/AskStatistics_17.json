[
    {
        "title": "Help sampling method",
        "author": "FormalPapaya7955",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zbe1pq/help_sampling_method/",
        "text": "Hi I just need some help over my quantitative research. I cannot find a data of the overall population for my study, so the title was changed to selected social workers but I do know how many social workers per offices that I targeted are. Panelists want to change the sampling method I did to stratified or clustered sampling which are both probability sampling, but I think quota sampling applies to the study more since its non probability. Am I wrong?",
        "created_utc": 1670066212,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are some good books on probability?",
        "author": "Amurie_Reddit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zbbmpf/what_are_some_good_books_on_probability/",
        "text": "Hello\n\nI'm looking for an introductory probability book for self-studying that is rigorous, formal and clear... I know calc 1 and 2...\n\nmost people seem to recommend Bertsekas' book or Ross' book\n\nI personally kind of liked Berstekas' book more ... as it seems more rigorous and formal than Ross' book? correct me if I'm wrog.\n\n\\-Thanks",
        "created_utc": 1670056394,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone dumb down the concept of regression analysis for a medical student?",
        "author": "Sufficient-Peach6365",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zba28d/can_someone_dumb_down_the_concept_of_regression/",
        "text": "The title",
        "created_utc": 1670050289,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need Help in R Hypothesis Questions",
        "author": "Stats-R-Python-Coder",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zb9pl6/need_help_in_r_hypothesis_questions/",
        "text": "[removed]",
        "created_utc": 1670049002,
        "upvote_ratio": 1.0
    },
    {
        "title": "Raw graphs",
        "author": "ReadyPlayerOne27",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zb9ee4/raw_graphs/",
        "text": "s there an app/website that allows me to combine graphs from Raw graphs together/edit. I know the most simple fix would just be illustrator but I just don't have access to it. \n\nThanks",
        "created_utc": 1670047886,
        "upvote_ratio": 1.0
    },
    {
        "title": "Do I need to reduce sample size of some categories for logistic model stability?",
        "author": "Naj_md",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zb8zcp/do_i_need_to_reduce_sample_size_of_some/",
        "text": "I want to compare non-normal weight with normal weight to explore interactions of each weight categories with different variables in a logistic model \n\nthe majority of my sample is normal weight patients:\n\n    Normal Weight     Obesity I    Obesity II   Obesity III    Overweight   Underweight \n            65746          3448          3614          3554          1352          1044  \n\nWill modeling with this huge discrepancy affect my results? \n\nIf I use propensity scores to match normal weight to non-normal weight (based on comorbidities), will that be helpful to produce better correlation with the outcomes?\n\nThanks",
        "created_utc": 1670046462,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question: Statistical test for ranked list",
        "author": "Colddustmass",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zb4wzb/question_statistical_test_for_ranked_list/",
        "text": "I have data on a series of medical diagnosis that happened pre-covid, during covid, and \"post\" covid. I would like to compare the frequency of these diagnoses through time and select those that are statistically significantly different. The data for example: \n\nHeart attacks:\n\nPeriod 1 = 100, Period 2 = 120, Period 3= 97. \n\nWith this dataset there are 100s of different diagnoses and I can normalize the particular diagnosis in each period against the total number of diagnoses made in that same interval. (Heart attack, period 1 = 5.2% for example... etc. ) \n\nThe patients are from the same sample population  (given US city), and the time windows are all the same length. This data set is large, with many individual diagnoses, and I would like to select the ones that differ significantly across those time windows. I was thinking T test at first but since there is no mean / SD that does not make sense. I could subdivide each period into week or month long blocks, then find the mean number of heart attack diagnoses and SD within each month of that period then do a T test on that data, but I would prefer to avoid slicing the data in such a manner just to perform a t test. Instead I'm wondering if there is a better test for something like this that I am not aware of.",
        "created_utc": 1670033447,
        "upvote_ratio": 1.0
    },
    {
        "title": "How would I go on about analyzing the correlation between likert scale data and data like age, occupation, level of education etc?",
        "author": "barrencold",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zb3los/how_would_i_go_on_about_analyzing_the_correlation/",
        "text": "Hello everyone! I would really appreciate your help.\n\nSo, I'm doing my bachelor thesis on financial behavior and one of my research questions is to analyze whether the financial behavior differs among gender, age, occupation and level of education. For gender, I assume (from what I read) I can use t-tests, but what about the other data? \n\nI use JASP because it's free. I mention this because every time I try to use t - test or ANOVA for the other data, JASP shows me \"Number of factor levels is ≠ 2 in How old are you?\"\n\nThis is the questionnaire (it's not finished yet, so please don't judge lol). Maybe seeing how the questions are asked will help you answer my question. [https://docs.google.com/forms/d/e/1FAIpQLSffXyrQWXkCbFc6sgq6y1MJWO47JvSrVeqwW\\_IB-AKHnVwc\\_g/viewform](https://docs.google.com/forms/d/e/1FAIpQLSffXyrQWXkCbFc6sgq6y1MJWO47JvSrVeqwW_IB-AKHnVwc_g/viewform)",
        "created_utc": 1670029588,
        "upvote_ratio": 1.0
    },
    {
        "title": "I am disagreeing with my probability teacher about the correction to his exam, his answer to my email was unhelpful, please help me.",
        "author": "DoctorFuu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zb3109/i_am_disagreeing_with_my_probability_teacher/",
        "text": "Hi,\nFor context, I had already followed (quite successfully) probability and statistics courses prior to this one. This course is a \"catch up\" course in the master's degree I enrolled in in September, and I had already seen most of the material. I still did all the exercises diligently to try and find all misconceptions I may have to correct them, and didn't find any during the whole duration of the course, except apparently for one of the questions of the final exam, here it is (translated):\n\n&gt; In order to evaluate the cost (C) of something, a company asked three consultant firms for help. Each of these firms proposed an unbiased estimator for the cost, C1, c2 and C3, with variances respectively s^2, 2*s^2 and 3*s^2.  \n&gt; Three synthetic estimators are constructed from these:  \n&gt; T1 = (C1 + C2 + C3)/3  \n&gt; T2 = C1  \n&gt; T3 = 0.6*C1 + 0.2*C2 + 0.2*C3\n\n&gt; Show that T1 T2 and T3 are unbiased. Is there one estimator better than the others? If yes which one? And why?  \n\nI'll go through my answer for the second part, as showing that they are unbiased is trivial. I'll add that in this course, the only criteria introduced to compare estimators was the MSE (mean squared error), so this is the tool that he expected us to use.\n\n&gt; Since all estimators are unbiased, we only need to compare their variances in order to find which estimator is the best estimator according to the mean squared error.  \n&gt; var(T1) = var((C1 + C2 + C3)/3) = 1/9 * var(C1 + C2 + C3)  \n&gt; var(T2) = var(C1) = s^2\n&gt; var(T3) = var(0.6*C1 + 0.2*C2 + 0.2*C3) = 1/25 * var(3*C1 + C2 + C3)  \n\n&gt; Since we don't have any information about the independence of these estimators, in particular we have no information about their covariance and we can't compute their variances in order to compare them. C1, C2 and C3 are all estimators for the same parameter, and if they are based even partially on the same data then they are not independent, and therefore it would be problematic to assume independence in this setting. It is therefore impossible to conclude about which of these estimators is the best according to the mean squared error.  \n\n\nNow here is the correction given by the teacher:  \n\n&gt; var(T1) = 1/9 * (var(C1) + var(C2) + var(C3)) = (s^2 + 2*s^2 + 3*s^2)/9 = 2/3 * s^2  \n&gt; var(T2) = s^2  \n&gt; var(T3) = ... = 14/25 * s^2\n&gt; var(T3) &lt; var(T1) &lt; var(T2), therefore T3 should be the preferred estimator. We are allowed to use var(C1 + C2) = var(C1) + var(C2) since we can expect the consulting firms to be independent.  \n\nI was shocked by his correction. I sent an e-mail detailing my point of view and asking for clarifications about where my reasoning could be flawed (I even included proofs that independence of firms cannot imply independence of their estimators, asked for extra material about the theory behind combining estimators...etc...). His answer was very short:  \n\n&gt; In order to reduce the variance this technique is often used. This exercise resembles closely X and Y that we did in the course. I understand your point but if we don't assume independence we can't answer the question.  \n\nAnd that's it. I went back to check, X and Y were only exercises where we compared two estimators via the MSE, so basically just \"compute expectation, compute bias, computer expectation + bias^2\".  \n\nPlease explain to me where my misconception lies. (don't be afraid to link me to more material if needed, I'm eager to learn). I am both angry about him being wrong and pushing for it with something that looks a lot like **\"Shut up, it's magic\"**, and worried about having completely misunderstood the concept and importance of independence. I hope that I'm wrong, as we'll have other courses with him in the future, and this time about topics I have not studied yet, so I don't want to follow a course from a teacher I'm not trusting.  \n\nSorry for the long post, thanks in advance.\n\nPS: The assignment was not written in english, but I tried to translate it as well as possible. I don't think I mistranslated anything but feel free to ask for clarifications if needed. I'll just add that there was no mention of independence, not any hint about independence. Not even about the firms.",
        "created_utc": 1670027954,
        "upvote_ratio": 1.0
    },
    {
        "title": "Modeling considerations in block designs?",
        "author": "lilseabreeze",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zb1bcq/modeling_considerations_in_block_designs/",
        "text": "\nI was recently reading about how blocking is used as a technique to deal with nuisance factors in an experiment. Once this is done, I see most articles run an ANOVA model to determine if the mean effect of whatever factor they are interested in is significant. I’ve seen some compare the sum of squared residuals to the model without blocking to show how the blocking variable captures some of the variance in the model to give an improvement in fit. However, I have not seen them check linearity assumptions using plots of the residuals, or use a model selection technique to determine if the blocking variables should stay in the model. Does this concern only extend to regression settings? I would think that one should first identify the best fitting parsimonious model, then move on to make inferences about the effect of the predictors in the model.",
        "created_utc": 1670023254,
        "upvote_ratio": 1.0
    },
    {
        "title": "Rudimentary statistics question -- need to know which formula(s) to use?",
        "author": "SingleTMat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zawpx6/rudimentary_statistics_question_need_to_know/",
        "text": "I'm playing a game and am trying to calculate the odds of a certain item dropping.\n\nI am trying to figure out the correct formula to use to calculate the inclusive or odds of either of two independent events occurring.\n\nExample:\n\nEvent A (boss1 dropping the item) is 1:745  \nEvent B (boss2 dropping the item) is 1:702  \nBoth events occurring at the same time is 1:522,990 (right?)\n\nIn one iteration, how do I calculate the \"inclusive or\" odds of either event happening?\n\nAssuming multiple iterations (example, 500 iterations) -- how would I calculate this?\n\nPlease correct me if I am using any incorrect terms. Thank you!",
        "created_utc": 1670013030,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best ways to compare time series?",
        "author": "estanislaojoiko",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zawg43/best_ways_to_compare_time_series/",
        "text": "I want to compare time series (~800 points each) from two groups (14 subjects each).\nWhat is the best way to compare these data? (Correlation or other)",
        "created_utc": 1670012413,
        "upvote_ratio": 1.0
    },
    {
        "title": "[University Psychology Statistics: True Scores and Cut-Offs] How to get true score and its confidence?",
        "author": "AnotherStarLight",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zauwfl/university_psychology_statistics_true_scores_and/",
        "text": " \"A  College has a policy that in order for an individual to be allowed into  a special program for gifted students, the individual needs to be above  average on scholastic aptitude, measured by the Scholastic Aptitude  Tests (SAT). The SAT has a mean of 32, a standard deviation of 4.55, and  a reliability of .95. How confident would you be that a student who  scored 35 has a true score that is above average?\"\n\nAll  I understand is that the student who scored 35 is 3 standard deviations  away from the mean (top .3%), then i Convert that raw score to z scores  using z = (x – mean)/sd so Z = (35-32)/4.55 = 0.65 and then get that  Z=0.66 is the 75th percentile.\n\nAfter that, I dont know where to go.",
        "created_utc": 1670008871,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confusion around Margin of Error formulas",
        "author": "Crowford99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zaufd6/confusion_around_margin_of_error_formulas/",
        "text": " \n\nI am a little confused about the margin of error formulas.\n\n* The first formula contains the Z-score, standard deviation, and sample size.\n\nIt can be used in this cases such as this:\n\n900 students were surveyed and had an average GPA of 2.7 with a standard deviation of 0.4. Calculate the margin of error for a 90% confidence level.\n\nBut in the example where is the population size?\n\n&amp;#x200B;\n\n* The other formula can be used in this example: 1000 people were surveyed and 380 thought that climate change was not caused by human pollution. Find the MoE for a 90% confidence interval.\n\nAgain, how is it related to the population size?\n\nOnline calculators, like surveymonkey's ask for the population size, the sample size, and the confidence level. According to the site, it uses the first formula. But from this information how SD is calculated?\n\nHow to calculate the Margin of error in the below examples?\n\nPopulation size = 3000, sample size = 550 and 80% answered yes. Cl=95%\n\nPopulation size = 1000, sample size = 550 and 80% answered yes. Cl=95%\n\nAccording to the mentioned calculator, the MoE is different in these cases, but after a certain boundary, increasing the population will not increase the MoE.\n\nIn this case, how can we calculate the standard deviation?\n\nThanks for the help!",
        "created_utc": 1670007814,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does it make sense to calculate percentiles of percentages?",
        "author": "pglfreire",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zarb27/does_it_make_sense_to_calculate_percentiles_of/",
        "text": " This may sound like a rudimentary, dumb question, but I'm trying to understand if it makes sense to calculate percentiles of percentages.\n\nFor example, let's assume I have the attendance of students per class. Each class has a different number of students enrolled, and I can get the attendance percentage for each class. Doing so, I would end up with an array of percentages like \\[50%, 50%, 60%, 80%, 95%\\]. The P50 of that array would be 60%, so I would be able to say that half of the classes had an attendance of up to 60%.\n\nThis sounds OK, but I don't know, there is something about calculating percentiles on top of another aggregate measure that bothers me, though I cannot articulate why, exactly.\n\nAny pointers? Thanks!",
        "created_utc": 1670000468,
        "upvote_ratio": 1.0
    },
    {
        "title": "Random and fixed effects questions",
        "author": "luchins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zaqf3f/random_and_fixed_effects_questions/",
        "text": "I have some questions about hierarchical models\n\n1) As seen that random mixed effects models try to gauge if there is some difference between groups why cant this be done with an ANCOVA? Whats the problem in doing that with ANCOVA?\n\n2) When we have the  mixedmodel how can we understand/estimate the effect size? Example: Based on my model in school A kids learn faster because they are smarter and have better teacher than school B ...how can I Know that  they learn faster because of these 2 variables (smarter and better school teacher) since the final model accounts for **both** fixed and random effects at the same time? Basically where should I look to get that it is the smartness and having better teacher to determine the better results of kids in school A compared to school B kids?",
        "created_utc": 1669998290,
        "upvote_ratio": 1.0
    },
    {
        "title": "Significant interaction, but post-hoc analysis reveals no significant contrast",
        "author": "overlysaccharine",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zaorky/significant_interaction_but_posthoc_analysis/",
        "text": "Hello!\n\nI was wondering about a situation I observed a bunch of times while analysing two experimental datasets (behavioral data). For some lmer (lme4 package) analyses I observed a significant interaction between the independent variables. I explored it visually and some differences appeared to be there - this plot was based on the linear mixed effects model estimates.\n\nHowever, upon further inspection of the analysis using emmeans, no  e contrast appeared - that is there seemed to be no clear source of the previously observed interaction. I was wondering the following thing: is it because the effect wasn't there in the first place, or because there is too much penalization brought by each comparison (Note that I used 6 Bonferroni-corrected comparisons).\n\nLooking forward to hearing your thoughts or about your experience with this!",
        "created_utc": 1669994107,
        "upvote_ratio": 1.0
    },
    {
        "title": "Biostatistician salary",
        "author": "Overall_Percentage29",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zaoiwi/biostatistician_salary/",
        "text": "How much could I realistically expect to earn as a biostatistician in the UK? (Bachelors in Biology and considering a Masters in Biostatistics next year).",
        "created_utc": 1669993492,
        "upvote_ratio": 1.0
    },
    {
        "title": "Model convergence issues",
        "author": "frauensauna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zao1hi/model_convergence_issues/",
        "text": "Hi. I have not yet run the linear mixed-effects model, but I have many binary predictors (dummy coded  categorical variables) and some of them are of relatively low frequency. Can I expect convergence issues based on the large number of variables (about 20) and/or the low frequency of some of them? \n\nI am preregistering the study so that is why I cannot run the model yet, but I do have to have a plan, as well as possible solutions in case of convergence issues. Possible solutions could be dropping some of the very low frequency variables and/or grouping them (although I'd lose information, but there would be a theoretical logical way of grouping them).\n\nWhat do you think? Or are convergence issues not usually caused by low frequency variables?",
        "created_utc": 1669992269,
        "upvote_ratio": 1.0
    },
    {
        "title": "Single Factor ANOVA Test; P-value greater than Alpha, F value greater than F critical",
        "author": "jklyons13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zanlee/single_factor_anova_test_pvalue_greater_than/",
        "text": "Is it possible to have a P-value higher than the alpha (retain null hypothesis) but have an F value higher than the F critical value (significant test).\n\nThis is what my calculations are saying but it seems counterintuitive to retain the null hypothesis when the test is significant",
        "created_utc": 1669991092,
        "upvote_ratio": 1.0
    },
    {
        "title": "Got at this at an interview: relationship between three columns?",
        "author": "unnamed_scholar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zan9h6/got_at_this_at_an_interview_relationship_between/",
        "text": "So we have 3 Excel columns. The first is a list with random numbers\n\nthe second is the first \\* 0.75\n\nthe third is the square root of the first\n\n&amp;#x200B;\n\nwhat's the very first thing that comes to mind when you look at these data? (relation between column 1 and 2, and relation between C1 and C3)\n\n&amp;#x200B;\n\nI answered that both are dependent on C1 data.\n\n&amp;#x200B;\n\nI don't think the interviewer got the answer he's looking for. He said something about correlation but moved on to another question.\n\n&amp;#x200B;\n\nI'm quite curious to know, what did I miss?",
        "created_utc": 1669990195,
        "upvote_ratio": 1.0
    },
    {
        "title": "Good statistics aggregator?",
        "author": "saurgalen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zalvqx/good_statistics_aggregator/",
        "text": "I know Statista.com\n\nBut maybe someone knows a better aggregator that would not cost me $5000?",
        "created_utc": 1669986761,
        "upvote_ratio": 1.0
    },
    {
        "title": "What type of test should I do?",
        "author": "Glads0001",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zakavo/what_type_of_test_should_i_do/",
        "text": "I’m finding out how temperature affects the ripening rate of a banana and was wondering what correlation test is best for this experiment.",
        "created_utc": 1669982691,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to reduce the data’s coefficient of variation?",
        "author": "Jensfw1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zahs1p/how_to_reduce_the_datas_coefficient_of_variation/",
        "text": "I’ve got data of an income range.\nTook the midpoint of the data.\nVariation is 70%\nHow do I reduce it\n(Tampering w the data)\n150 entries",
        "created_utc": 1669975635,
        "upvote_ratio": 1.0
    },
    {
        "title": "Central limit theorem",
        "author": "AccurateCantaloupe35",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zagfch/central_limit_theorem/",
        "text": "Could someone explain the steps:\n\nIf 35 percent of the population of a large community is in favor of a legislation, using the Central Limit theorem approximate The probability that in a random sample of 100, exactly 35 support the legislation.",
        "created_utc": 1669971068,
        "upvote_ratio": 1.0
    },
    {
        "title": "Birthday Odds",
        "author": "Thealcoholsalesrep",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zaeopc/birthday_odds/",
        "text": "What are the odds that myself, my mother, my father, my husband, and his father, all share the same day of the month as a birthday?",
        "created_utc": 1669965137,
        "upvote_ratio": 1.0
    },
    {
        "title": "What really is support of a random variable?",
        "author": "RecentUnicorn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/za9dwi/what_really_is_support_of_a_random_variable/",
        "text": "We do not really know the true distribution of random variable. We only know the samples from true distribution and hence know the sample distribution.\n\nIn that case, what is the support of random variable? If it is all possible values that a random variable can take, how will we know that if we only know sample distribution and not true distribution (sample distribution only has some samples, not all possible values of a random variable)?",
        "created_utc": 1669950300,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trying to create hypotheses",
        "author": "Boosaknudel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/za92x4/trying_to_create_hypotheses/",
        "text": "Hi, so i was given a question, its estimated that men were more likely to die from covid than women. \n\nLet Ppop = the population proportion of men who passed away from covid\n\nUsing a 1 sided sign test to test the hypothesis, i need to create a null and alternative hypothesis. So far i got this:\n\nH0 : Ppop &gt; 1\nH1: Ppop &lt; 1\n\nim not really sure which number i should be using given the situation.",
        "created_utc": 1669949579,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hello, I need some help determining what type of test or analysis I should do to determine something related to fantasy football.",
        "author": "dshmitty",
        "url": "https://www.reddit.com/r/AskStatistics/comments/za6z48/hello_i_need_some_help_determining_what_type_of/",
        "text": "Hello, I have a project I am doing for Statistics class, but I am apparently really stupid and I have basically learned nothing throughout this semester; It has all been online, the textbook is terrible and doesn't help at all (i read every chapter), and my professor is no help whatsoever. It really sucks, because I hate not understanding stuff, and it's something I've never had a problem with in the past. I tried to use the link on the sidebar with the chart that shows which tests to use for which situations, but again, I'm dumb af and I have no idea what I should be looking at/for. \n\nAnyways, my project is extremely broad. I have to decide something to test/analyze and basically use any 2-3 topics/ideas/tests to do it. I would like to see if there is a statistically significant difference in fantasy production at home vs. on the road, and if so how big is it, is there differences between positions, etc.\n\nWould I start by writing hypothesis, and would it be that the alternative outcome is there is a significant difference, and null outcome is there is no difference?\n\nAnd, what type of test would be best to determine this? You could even just give me a brief idea and I can try to figure it out from that chart, and then do more research specifically on that topic/test to actually learn and understand it. I just really need to at least know how to start. I am weeks behind at this point and I am so frustrated it's ruining my days. \n\nThank you for reading and your consideration!\nDshmitty",
        "created_utc": 1669944617,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question on formula for standardization of a normally distributed variable",
        "author": "fuufufufuf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/za0njm/question_on_formula_for_standardization_of_a/",
        "text": "Why is sometimes the formula for standardization (X − µ)/σ and sometimes (X − µ)/(σ/√n)? When do I have to use each one?",
        "created_utc": 1669930623,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I do a one sample t-test in this case?",
        "author": "jpsoccer171717",
        "url": "https://www.reddit.com/r/AskStatistics/comments/za0c3t/can_i_do_a_one_sample_ttest_in_this_case/",
        "text": "Hello, I’m a complete stats newbie when it comes to stats. Say I have a small group of people (&lt;15) that I am comparing to a single person. Can I run a one sample t-test where I compare say the weight of the group to the weight of single person? I’m leaning towards no, but am having a hard time rationalizing why.",
        "created_utc": 1669929958,
        "upvote_ratio": 1.0
    },
    {
        "title": "Pharmacy tries Statistics",
        "author": "Mysterious-Ad-22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9za5l/pharmacy_tries_statistics/",
        "text": "Hello Everyone!\n\nI am a Pharmacy Student with absolutely zero statistics background working on a research project looking at covid vaccination rates and characteristics of the counties to include things such as ethnicity, income, how they voted in the last election, and other social disparities. I was hoping to get some opinions on if we are on the right track with our statistical analysis.\n\nCurrently we are a running an ordinal logistic fit model using the JMP Pro 16.2 software, we have divided each data point into 5 categories evenly split which brings each category to roughly 670 counties. Is this enough categories to give accurate data on each category? I ask this because with some factors such as access to a vehicle the majority of people do so some of the categories are split 100-99, 99-98, 97-90, 90-50, less than 50. Should we focus on keeping the amount of counties in each category similar or should we focus on creating normal distributions such as 100-80, 79-60, 59-40, 39-20, less that 20.\n\nTLDR: Should we add more categories (like 10 instead of 5)? How should we divide the categories (evenly or nominal)?",
        "created_utc": 1669927712,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to tell if two groups of frequencies are significantly different",
        "author": "gaymer8138",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9yfla/how_to_tell_if_two_groups_of_frequencies_are/",
        "text": "Hello! I’m a biochemist and I work with gene editing. I have data from an experiment where I look at the rate of modification before and after gene editing.\n\nEach group (pre and post editing) have been tested three times to asses the frequency (%) that the gene has been modified. In total I have three modification rates pre editing and three modification rates post editing.\n\nIs there a way I can compare these groups of rates to asses if they are statistically different from each other?",
        "created_utc": 1669925824,
        "upvote_ratio": 1.0
    },
    {
        "title": "Knowing combined effect of predictor variables on dependent variable in a standard multiple regression",
        "author": "francescar182",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9x47u/knowing_combined_effect_of_predictor_variables_on/",
        "text": "On SPSS I ran a multiple regression with self_blame &amp; circumstance_blame as predictor variables, and distress as the DV. In the coefficients table in the output I can see how much self_blame &amp; circumstance_blame individually predict distress. However, how can I know the degree to which self_blame &amp; circumstance_blame combined predict distress? Where would this data be reported in the output?",
        "created_utc": 1669922937,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate critical values",
        "author": "Hailtotherainbow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9wqnq/how_to_calculate_critical_values/",
        "text": "How to calculate critical values??\n\nProblem: Medical Research Laboratories claims that at least 4 out of 10 patients showed improvement from a new allergy treatment. Find the critical value(s) for a hypot test at the 0.01 level of significance.\n\nThe way we were taught to find critical probability was just a simple formula of 1 - (Alpha / 2). The answer is -2.575. I know this because we did this example in class, and the teacher told us to use this formula for the homework.\n\nI’m so lost and confused, every time I do it I get a completely different answer that isn’t even on the multiple choice options. If anyone could help it’d be great",
        "created_utc": 1669922092,
        "upvote_ratio": 1.0
    },
    {
        "title": "Resources and textbooks on how to do appropriate statistical test for different clinical trial designs",
        "author": "yanjama",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9vue7/resources_and_textbooks_on_how_to_do_appropriate/",
        "text": "Hi everyone,\n\nI'm a first year biostats ms student, and would love some help finding resources or textbooks where I can read more about how to do the right statistical analysis for different clinical trial designs. I know there's an abundant resources online, but a bit overwhelmed and would like guidance on where to start. Would also be interested in practicing identifying the right stat test with different clinical trial designs, if those exists in problem sets.\n\nThank you so much!",
        "created_utc": 1669920198,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need a refesher on a probability scenario",
        "author": "Griswold27",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9vafq/need_a_refesher_on_a_probability_scenario/",
        "text": "I was just wondering how to add probabilities properly.   if an event has 1/10 chances of occuring and you roll 10 times it comes to a probability of 1.  But of course that is an average over time.  How would you calculate and describe the probability of the event occurring  at least once in the first 10 rolls?",
        "created_utc": 1669919017,
        "upvote_ratio": 1.0
    },
    {
        "title": "I want to determine which among the groups I'm comparing is the best but I seem to have conflicting results (Kruskal-Wallis and Post Hoc test).",
        "author": "Fit_Distribution3192",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9stir/i_want_to_determine_which_among_the_groups_im/",
        "text": "I have non parametric data from four groups which will be referred to as group A, B, C, and D.   \nI want to find out which among the group performs the best so I first used Kruskal-Wallis to find out if there are any significant difference between them. Given the results suggest that at least one of the group is significantly different than the rest, I proceeded with Post Hoc testing (Dunn's test) . The results seems to be conflicting and I am unable to determine which among them is the best which is the answer I am looking for.  \n\n\nA is not significantly different with D  \nD is significantly different with B and C  \nIt makes sense so far but  \nA is not significantly different with B and C  \n\n\nDoes it make sense that A and D are not significantly different with each other but one differs significantly with B and C while the other doesn't? I am stumped by this and I don't know what sort of conclusion I can make which satisfies the question \"Which among them is the best?\"",
        "created_utc": 1669913597,
        "upvote_ratio": 1.0
    },
    {
        "title": "Working up to Estimation statistics as someone who is completely maths illiterate: Where should I start?",
        "author": "Organic-Morning9304",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9sh6d/working_up_to_estimation_statistics_as_someone/",
        "text": "*(CONTEXT: I am completely maths illiterate, I did not even take Algebra and Trig in Highschool. So when I say I am starting from scratch I REALLY mean it. However, many of my relatives are fantastic at maths so if I decide to learn I will have plenty of assistance, so I do not need to necessarily do everything on my own.)*\n\n So I have been learning to code recently, and have actually enjoyed myself and have made good progress. However, I recently started a project that requires working knowledge of Estimation statistics (a complex American election simulator that does a governor and senate election for each state. **This will involve a lot of estimation statistics because the aim of this project is to simulate polling data. So I can have a script write me up 5000 'responses' to a mock poll, from which I can construct my variables. Essentially I am trying to understand estimation statistics from a practical standpoin**t) and oh boy was I filtered hard. Its not that I cannot figure out how to program the maths. The problem is that I do not know how to come up with the maths in the first place. For example I want to have multiple changing variables that infleunce the outcome of the race, such as the approval rating of the incumbant and the president, multiple political issues such as the economy and education etc. But I really cannot put all the pieces together. I have always wanted to learn about Probability and statistics.",
        "created_utc": 1669912758,
        "upvote_ratio": 1.0
    },
    {
        "title": "Partial least squares model selection",
        "author": "Low_Teach7736",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9rqf7/partial_least_squares_model_selection/",
        "text": "I am trying to determine which PLS model is better to use. I have two models that I am using a k folds cross validation to get RMSE and R2 values: \n\n1.  RMSE (0.9) and R2 (0.81), but I use 8 components\n2. I remove two of the least important variables (as determined by a variable importance plot) my RMSE (1.3) and R2 (0.41) decrease but I only use 2 components\n\nI have read that the more components you use in a PLS model, the more similar it is to a multiple linear regression model. My variables are highly correlated and would be very overfit. So would the better model be #2 with lower RMSE and R2, but less chance of being overfit?",
        "created_utc": 1669910951,
        "upvote_ratio": 1.0
    },
    {
        "title": "Creating a Standard deviation variable with exclusion of the case",
        "author": "ObscureIs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9pldc/creating_a_standard_deviation_variable_with/",
        "text": " Greetings,\n\nPsychology student here. I could use some help because I struggle to find it online.\n\nI  have some data on students from various courses and need to calculate  the Standard deviation of certain variables for each students' course  but excluding the student. For example, let's I know the weight of each  student in each course.\n\nSo let's say I have 3 variables for 500 students from 50 courses:\n\n* StudentID,\n* student course\n* student weight.\n\nI need to create a new fourth variable for each student which represents the SD of the weight in the course the student is in, **but which does not include the student.**  So basically the SD of all students' coursemates excluding oneself. And  that done for all the participants. I can use SPSS or Excel.\n\nAggregating  data in SPSS includes all the students and excel functions I know can  only exclude a case if it is equal to something, but the weight can be  the same for 2 students, then it would exclude them both and I would  only need the one.\n\nIt would be  best if it was automatic because there's a too much data (cases and  variables) to do by hand. So if any ideas and help comes to mind how to  do that automatically using spss or excel, I would greatly greatly  appreciate it!\n\nThank you!",
        "created_utc": 1669905781,
        "upvote_ratio": 1.0
    },
    {
        "title": "What’s the difference between conditional density estimation and functional quantile regression?",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1669905159,
        "upvote_ratio": 1.0
    },
    {
        "title": "What type of categorical data is this please?",
        "author": "SlytherinSimsBunny",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9o1us/what_type_of_categorical_data_is_this_please/",
        "text": "Please can anybody tell me what type of data this is, I am not sure what type of categorical data it is as I need to perform the correct statistical test:\n\nBirth weight : low (&lt;2500g) or high (&gt;2500g)\n\nAs there are only two categories I am not sure what type it is (ordinal, nominal or binary). Thank you!\n\n\\#askstatistics",
        "created_utc": 1669901735,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can't keep up with stat class",
        "author": "beeeeeaaaammmmm111",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9li6a/cant_keep_up_with_stat_class/",
        "text": "Can't keep up with statistics class\n\nHello\n\nI just started my economics degree and  at first everything seemed pretty okay, but now my statistics class consists of me looking at a bunch of numbers and letters and not knowing whats going on or what the topic im supposed to study is even about\n\nI started my degree a little late and didnt do any math for a while so I have to work on that, but nevertheless I am really struggling with understanding and memorising how to calculate stuff \n\nPerhaps anyone has some advice for me ?\n\nDid you use any online sources when learning ? \nI don't really have anyone to ask about things I struggle with so im really lost in coursework thats keep stacking up..:(",
        "created_utc": 1669894174,
        "upvote_ratio": 1.0
    },
    {
        "title": "Check if reduction in number of students is significant",
        "author": "Euphoric-Ruin-6450",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9k1zo/check_if_reduction_in_number_of_students_is/",
        "text": "I want to check if the difference in student drop out of my course this year compared to last year is significant. I have data on how many students we had at the start of the semester, throughout (how many handed in assignments) and how many took the final exam.  \n\n\n&amp;#x200B;\n\n|2021|2022|\n|:-|:-|\n|325|284|\n|311|265|\n|256|227|\n|226|206|\n|207|192|\n\n&amp;#x200B;\n\nWhat statistical test can I utilize here? Thanks :)",
        "created_utc": 1669888916,
        "upvote_ratio": 1.0
    },
    {
        "title": "Given P(A), P(B|A), and P(C|B), how do you calculate P(B|C)?",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1669884344,
        "upvote_ratio": 1.0
    },
    {
        "title": "What statistical test for observational study?",
        "author": "Longjumping-Hawk-561",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9hhij/what_statistical_test_for_observational_study/",
        "text": "To keep it short: \nPre made observation grid was made stating different politicians participating in the debate we then observed. Next to names two columns with 2 different debate techniques - during observation we placed a tick each time a member used X/Y technique - we now want to see if there is a difference between political blocks (right/left - but we have data for all politicians individually) in their use of these two debate techniques. We did an independent t-test - is this the right method to use?",
        "created_utc": 1669879057,
        "upvote_ratio": 1.0
    },
    {
        "title": "MANOVA for Outcomes as Ratios?",
        "author": "squags",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9gaw1/manova_for_outcomes_as_ratios/",
        "text": "I have some data where the outcome variable is the 'percentage of a whole' that 3 categories take up. I.e. there are three regions, which vary in the proportion of the total whole they take up (area), but which always add to 1 in terms of the total area (so they are not independent?).\n\nThere are 2 factors which I believe influence these proportions (one treatment condition, and one to categorise the two groups receiving treatment). So the structure looks something like:\n\nGroup 1: Treatment, No Treatment\nGroup 2: Treatment, No Treatment\n\nDV: 3 regions, each a different proportion of the whole.\n\nI want to compare how:\n\n- the mean proportions of each region change with treatment (within groups, but only comparing differences between equivalent regions)\n- whether there are group differences in these regions between the groups overall (particularly the non-treatment groups)\n- whether the effect of treatment is different between the two groups (interaction)\n\nMy smooth brain tells me that a MANOVA might be the way to go as far as hypothesis testing, but having little experience with MANOVA, I wasn't sure this was appropriate for a DV that is a proportion like this?\n\nIn terms of the contrasts, how does MANOVA perform these? For example, I don't want the contrast between Region A and Region B between treatment conditions.\n\nI am running this in R, so open to alternatives, or for methods of modifying the contrasts performed etc. if suitable.\n\nThanks!",
        "created_utc": 1669874916,
        "upvote_ratio": 1.0
    },
    {
        "title": "Regarding Corr(X,Y) computation",
        "author": "Mappy39",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9f4xa/regarding_corrxy_computation/",
        "text": "Suppose there are 2 variables, X and Y, and Y = b0+2b1X, where b0 and b1 are constants.\n\nCalculate Corr(X,Y) [Express your answer in terms of bs]\n\nWhat I did was as follows:\n\nCov(X,Y)= E(XY) - E(X)E(Y) = 2b1std^2\n\nVar(X) = std^2\nVar(Y) = 4b1^2 std^2\n\nApplying the Corr formula [Corr(X,Y) = Cov(X,Y) / (Var(X)^0.5 × Var(Y)^0.5 )], (2b1std^2 /(std^2)^0.5 x (4b1^2(std^2))^0.5 = (2b1std^2)/(2b1std^2) = 1 [Answer]\n\nDoes this make sense?",
        "created_utc": 1669871331,
        "upvote_ratio": 0.33
    },
    {
        "title": "Tools and Advice to Learn Stats",
        "author": "jastorga99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9dex8/tools_and_advice_to_learn_stats/",
        "text": "Hello all,\n\nI am familiar with the very basica of stats and can kinda read a bell chart and somewhat understand it. However, I want to improve my knowledge. Is there any\n advice on where I could start? Sorry for the vagueness but I can't really explain what I need to learn since I'm not sure if what there is to learn besides...stats!",
        "created_utc": 1669866667,
        "upvote_ratio": 0.5
    },
    {
        "title": "What should I do?",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1669865270,
        "upvote_ratio": 0.5
    },
    {
        "title": "What type of hypothesis or statistical test should I use ?",
        "author": "avaiiya",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9c4hw/what_type_of_hypothesis_or_statistical_test/",
        "text": "I’m doing a research study for my intro to psych class. My hypothesis is taking xyz courses increases self esteem in college students. \n\nFor my survey, I used the Rosenberg self-esteem scale and the likert-scale. The respondents each got a score between 10-50, and the scale was ranked 1 to 5.\n\nI want to compare the self-esteem scores of people who took xyz courses to the scores of people who did not take xyz courses\nand determine if there is a difference. What type of hypothesis test or analysis should I use? I’m really new to this.",
        "created_utc": 1669863408,
        "upvote_ratio": 0.67
    },
    {
        "title": "when would ANOVA, T-test and linear regression not yield equivalent results",
        "author": "Aggravating_Hope2390",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9azk2/when_would_anova_ttest_and_linear_regression_not/",
        "text": "not really sure about this? Any thoughts.\n\n&amp;#x200B;\n\nThanks in advance :))",
        "created_utc": 1669860550,
        "upvote_ratio": 0.5
    },
    {
        "title": "when would ANOVA, T-test and linear regression would not yield equivalent results",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1669860510,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does multicollinearity matter for interaction terms?",
        "author": "GhostGlacier",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9a4nm/does_multicollinearity_matter_for_interaction/",
        "text": "I have a MLR model w/ 2 predictors: 1 quantitatitve variable and 1 indicator variable w/ 2 levels (0 &amp; 1).\n\nMy orignial model did not have an interaction term, but there appeared good reason to add one, and when I did my R2 adj increased and my regression standard error decreased.\n\nThe VIF for my interaction and one of the other terms increased significantly, to around 200.\n\nMy question is - does this VIF matter for interaction terms? It should be expected right? Or do I need to try &amp; reduce it, sort of along the lines of centering a predictor when you add a quadratic term, or some other method, or does it not matter?",
        "created_utc": 1669858381,
        "upvote_ratio": 0.6
    },
    {
        "title": "Help with statistical analysis for final year psychology project please",
        "author": "ttaf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z99vb0/help_with_statistical_analysis_for_final_year/",
        "text": "Hi, I was wondering could anyone please advise me as to which statistical method to use. This may appear quite elementary but I just cannot seem to get my head around statistical analysis. \n\nI have three IVS - Mental Toughness, athletic identity, and social support\n\nAnd I have two dvs, rehab beliefs, and coping with pain capabilities. \nIt appears that a multi variate regression may be most appropriate but I am not certain. Any help at all would be greatly appreciated. Thank you",
        "created_utc": 1669857734,
        "upvote_ratio": 0.67
    },
    {
        "title": "Why is the complement version of Naive Bayes' \"particularly suited for imbalanced data\"?",
        "author": "dcfan105",
        "url": "/r/datascience/comments/z99234/why_is_the_complement_version_of_naive_bayes/",
        "text": "",
        "created_utc": 1669855716,
        "upvote_ratio": 0.67
    },
    {
        "title": "sample size and p-value Relationship",
        "author": "Educational-House382",
        "url": "https://i.redd.it/85bf01gt563a1.png",
        "text": "",
        "created_utc": 1669849403,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Parametric survival model likelihood with time-varying covariates",
        "author": "sonicking12",
        "url": "/r/statistics/comments/z909zn/q_parametric_survival_model_likelihood_with/",
        "text": "",
        "created_utc": 1669835726,
        "upvote_ratio": 1.0
    },
    {
        "title": "Assumptions for a linear mixed effects model with a random intercept",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z9097e/assumptions_for_a_linear_mixed_effects_model_with/",
        "text": "Hi all,\n\n&amp;#x200B;\n\nI have a model that looks like this:\n\n    lme([skewed ordinal variable with 4 levels] ~ [binary variable ] * [time (2 timepoints)], random = ~ 1 | subject_ID, data = data)\n\nMy question is whether any assumptions are broken given that the outcome variable is a skewed ordinal variable? Or is there a better model that I can use?",
        "created_utc": 1669835657,
        "upvote_ratio": 1.0
    },
    {
        "title": "PRODUCT INNOVATION AND ENTERPRISE AI IN FINANCE",
        "author": "TemporaryPrize198",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z901kg/product_innovation_and_enterprise_ai_in_finance/",
        "text": "[removed]",
        "created_utc": 1669835193,
        "upvote_ratio": 1.0
    },
    {
        "title": "What method to use when comparing two groups, categorical ordinal variable",
        "author": "Ill_Pineapple3927",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8zs5y/what_method_to_use_when_comparing_two_groups/",
        "text": "I have collected data on students, they got a questionnaire where they rank order a statement from 1-9. They got randomly assigned to group A or B. Group A were told a fellow student made the statement. Group B were told a professor made the statement. 1 is not believable and 9 is very believable.\n\nNow my data is not normalised etc. which is needed for T test.\n\nI landed on using a ordinal regression with 1-9 as dependent variable and a dummy variable as dependent variable. 1 for group A, 0 for group B.\n\nIs this OK, anyone have any suggestions/ can confirm.\n\nAlso, no one selected 1 so my regression thinks it’s 8 categories. Is this a problem?",
        "created_utc": 1669834653,
        "upvote_ratio": 1.0
    },
    {
        "title": "Weighted Average with Multiple Weights",
        "author": "oxgtu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8y4ud/weighted_average_with_multiple_weights/",
        "text": "Sorry if this has been asked before but I didn’t find an answer and I might be over thinking this. \n\nI have a set of data that is used to calculate a weighted average with the weight being a dollar amount assigned to each data point. \n\nI would like to include a time based weight so that older data points don’t have as much influence on the final weighted average. \n\nIs it acceptable to make the new weight_i = $_i * (1/(today’s date - date of x_i)) ? And then divide by the sum of weight_i. \n\nThanks in advanced.",
        "created_utc": 1669831065,
        "upvote_ratio": 1.0
    },
    {
        "title": "Fear of brain cancer from CT scans",
        "author": "Nice-Brief-2761",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8vnyr/fear_of_brain_cancer_from_ct_scans/",
        "text": "Male - 27\n\nIve had a couple CT scans over the past 2 year and am now worried about cancer from these scans . Im not looking for medical or doctor advice , just wondering if anyone has the same worry or similar number of scans \n\nCT Chest Abdomen &amp; Pelvis Dec 2020\nCT Head Dec 2020\nCT Neck Soft Tissue May 2021\nCT Head Dec 2021\nCT Head+CTA Head/Neck Sep 2022",
        "created_utc": 1669825253,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with an article",
        "author": "Psyman8888",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8vngk/help_with_an_article/",
        "text": "Dear Redditors,\n\nI'm in the last year of my economics degree and i'm getting introduced to econometrics.\n\nI have to do an essay about a published article and provide points of improvement.\n\nSadly, I've chosen an article that uses panel data and I'm not very comfortable with these types of models. It will be a great opportunity to learn tho.\n\nI would be very grateful if you could shine some light on the path, or preferably tell me some points of improvement you felt the article falls short.\n\nThe article is:\n\n[https://www.ecb.europa.eu/pub/pdf/scpwps/ecb.wp2741\\~90de4c7390.en.pdf](https://www.ecb.europa.eu/pub/pdf/scpwps/ecb.wp2741~90de4c7390.en.pdf)",
        "created_utc": 1669825219,
        "upvote_ratio": 1.0
    },
    {
        "title": "Low power when modeling stimuli as random factors",
        "author": "NarcissusLovesEcho",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8uvsd/low_power_when_modeling_stimuli_as_random_factors/",
        "text": "I have a set of stimuli that vary in terms of a quality, let’s call this quality variable X. Participants rate each stimulus in terms of a second variable, let’s call variable Y. In a previous study, X was shown to positively correlate with Y. A third variable (another quality of the stimuli), let’s call variable M, was also shown to statistically mediate the X-Y association (i.e., the indirect effect of X on Y through M was significant). \n\nSo, I want to do a second study that manipulates M in such a way that holds it constant (methodologically rather than statistically) across all the stimuli. If M is truly a mediator of the X-Y association, then holding it constant should eliminate this association, which would provide much stronger evidence of mediation than my earlier statistical mediation. \n\nI figured out how to methodologically hold M constant across all the stimuli. One problem is that it takes quite a bit of work to do this manipulation on each stimulus. I have nearly 200 stimuli in my set, and it doesn’t look feasible to manipulate all of them. So, I thought what I might do is sort the stimuli by X and select the top and bottom 20 stimuli (40 stimuli total) and do the manipulation on them. Then, I would have 80 total stimuli (40 stimuli that vary in terms of X and M, and 40 stimuli that vary in terms of X but are held constant in terms of M). My hypothesis is that I would replicate the original effect in the first condition (where X and M both vary) and not observe it in the second condition (when X varies but M is held constant). In other words, I expect to observe an X by M interaction such that X and Y are only related when M is allowed to vary.\n\nThe problem I’m running into is statistical power. It seems like the best way to analyze these data is to model the stimuli (and participants) as random factors. In the “old days” I would have just run a 2 (high vs low X) by 2 (varying vs constant M) ANOVA, which models the stimuli as fixed factors. When stimuli are modeled as random factors, power appears to be largely determined by the number of stimuli rather than the number of participants. Unfortunately for me, participants are easy to get; it’s the stimuli that are difficult. Even with 40 stimuli, my power estimates are kind of dreadful. \n\nHas anyone else run into this problem, and if so, are there any options that don’t involve greatly increasing the number of stimuli? Are there good papers that discuss different options, or just the issue in general? Most of my understanding of this comes from either (a) discussions with cognitive psychologists who often run into this issue, especially in psycholinguistic research, and (b) readings, such as the ones below:\n\nJudd, C. M., Westfall, J., &amp; Kenny, D. A. (2012). Treating stimuli as a random factor in social psychology: A new and comprehensive solution to a pervasive but largely ignored problem. Journal of Personality and Social Psychology, 103(1), 54–69. [https://doi.org/10.1037/a0028347](https://doi.org/10.1037/a0028347)\n\nWestfall, J., Kenny, D. A., &amp; Judd, C. M. (2014). Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli. Journal of Experimental Psychology: General, 143(5), 2020–2045. [https://doi.org/10.1037/xge0000014](https://doi.org/10.1037/xge0000014)",
        "created_utc": 1669823397,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do hierarchical and Bayesian hierarchical models work?",
        "author": "luchins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8uk57/how_do_hierarchical_and_bayesian_hierarchical/",
        "text": "A commonly used example, that I find sufficient to understand the  scenario is that of the performance of students in a school. In a school  with students in multiple classes, the academic performance of a  student is influenced by their individual capabilities (called fixed  effects) and the class they’re a part of (called random effects). Maybe  the teacher assigned to a particular class teaches better than the  others, or a higher proportion of intelligent students in a class  creates a competitive environment for the students to perform better.\n\nOne approach to handle the case is to build multiple models for each class, referred to as pooling.  But such an approach might not always produce reliable results. For  example, the model corresponding to a class with very few students will  be very misleading. A single unpooled  model might not be able to fit sufficiently on the data. We want to find  a middle ground that finds a compromise between these extremes —  partial pooling. This brings us to Bayesian hierarchical modeling**,** also known as multilevel modeling.\n\nIn this method, parameters are nested within one another at different  levels of groups. Roughly, it gives us the weighted average of the  unpooled and pooled model estimates\n\nPlotting the math scores of the students against homework along with the **unpooled** OLS regression fit gives us this:\n\n[https://miro.medium.com/max/640/1\\*gVjMdU4z5p8J9usBflRBoQ.webp](https://miro.medium.com/max/640/1*gVjMdU4z5p8J9usBflRBoQ.webp)\n\nVisualizing the data at the school level reveals some interesting patterns. We also plot the **pooled** regression lines fit on each school and the unpooled regression fit for reference. For simplicity, we use OLS regression.\n\n[https://miro.medium.com/max/1400/1\\*kJxp4vuJBK0t-HBTqegHDw.webp](https://miro.medium.com/max/1400/1*kJxp4vuJBK0t-HBTqegHDw.webp)\n\nMy question: Werent unpooled models meant to be models fit to classroom with less students, thus producing misleading results? Why does he have one pooled OSL fit and an unpooled one for each class now? The website here: [https://towardsdatascience.com/introduction-to-hierarchical-modeling-a5c7b2ebb1ca](https://towardsdatascience.com/introduction-to-hierarchical-modeling-a5c7b2ebb1ca)",
        "created_utc": 1669822607,
        "upvote_ratio": 1.0
    },
    {
        "title": "How Many Types Of Statistics",
        "author": "aesh12100",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8uf6y/how_many_types_of_statistics/",
        "text": "This articles \" how many types of statistics[How many types of statistics](https://www.statistics.today/2022/08/how-many-types-of-statistics-2-types-of.html) \" explains the 2 types of statistics, Types of statistics with examples, types of statistical data, Types of statistics descriptive and inferential. \nFor more info click here[here](https://www.statistics.today/2022/08/how-many-types-of-statistics-2-types-of.html)",
        "created_utc": 1669822272,
        "upvote_ratio": 1.0
    },
    {
        "title": "Follow-up on chi-square",
        "author": "frauensauna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8uf3p/followup_on_chisquare/",
        "text": "Hi. I am testing whether a categorical variable is associated with a bunch of other categorical variables using a chi-square test. If it turns out significant, I want to examine more closely which of the ten categorical variables are responsible for the result. Performing ten separate post-hoc chi-square tests would require a conservative Bonferroni correction. Would there be another way of doing this? I could logically group them in two variables as well, but then I'd lose some of the details. Maybe there are other ways, such as calculating log odds (not entirely sure how... any advice is welcome). Thanks!",
        "created_utc": 1669822266,
        "upvote_ratio": 1.0
    },
    {
        "title": "Evaluating Parametric Models",
        "author": "ReadEditName",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8ucgu/evaluating_parametric_models/",
        "text": "I am years away from school and I am very rusty on parametric modeling.  I have a few questions:\n\n* How do I evaluate how well a parametric model, models a dataset\n* How do I compare different parametric models to each other\n* Is there a package in R or a function in Excel that compares different models for me\n\nMy goal is to look at this historic data and estimate how likely I am to see results in certain ranges in the future (my diagram below has the 3 areas of interest).  I have:\n\n* Fit the data to a normal distribution  with the intention of saying: \"if we performed this experiment many number of times, we would expect to see X% of the results to be in area 1, area 2, area 3\".\n* I modeled the results as a Bernoulli distribution (estimated p for each area and if we do this trial 10 times we would expect this percent of occurences in those areas)\n*  I then tried to use a Chi\\^2 test test if the distibution was normal by evaluating the normal expected values vs the observed values for each of the bins.  My result was a very very high Chi\\^2 value to reject that this was a normal distibution.  Upon further research I think this was incorrect for two reasons: Chi\\^2 is used for categorical variables and some of my bins had fewer than 5 observations. \n\nAdditional information about the data:\n\n* The x-axis bins are feet (how much something moved over a set amount of time)\n* I care most about the area estimates being \"accurate\", less concern about the other data areas\n\nBecause the normal distibution doesnt model Area 2 well or fit the data well in general, i want to use a different distibution that better models the data.  Any feedback or questions are greatly appreciated.  Thanks!\n\n[Frequency Diagram with normal distribution overlay](https://imgur.com/a/3cnwBQr)",
        "created_utc": 1669822088,
        "upvote_ratio": 1.0
    },
    {
        "title": "Main 8 Descriptive Statistics",
        "author": "aesh12100",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8uavi/main_8_descriptive_statistics/",
        "text": "This article \"Main 8 Descriptive Statistics [Main 8 descriptive statistics](https://www.statistics.today/2022/10/main-8-descriptive-statistics-main.html)will hep you to understand main descriptive statistics and what are the four descriptive statistics.\nhttps://www.statistics.today",
        "created_utc": 1669821980,
        "upvote_ratio": 1.0
    },
    {
        "title": "Method of transformation",
        "author": "lightsnooze",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8pvy5/method_of_transformation/",
        "text": " Hi all,\n\nI'm studying the Method of transformations to obtain the density function of a function of random variables. I'm having some trouble moving onto the bivariate case.\n\nI get the univariate case where we can get f\\_U(u) by differentiating F\\_U(u) and applying the chain rule. But I'm having trouble seeing how this translates to the bivariate case. Why are we allowed to obtain g(y\\_1, u) by multiplying f(y\\_1, h-1(u)) with the derivative of h-1 wrt u?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/s6rwf9ehx23a1.png?width=1325&amp;format=png&amp;auto=webp&amp;s=2f2852a2121abe7aeb706f1d17c225b9d2fbc50b",
        "created_utc": 1669810193,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a textbook that meticulously catalogues all currently known statistical tools along with their descriptions?",
        "author": "Shining_Silver_Star",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8phen/is_there_a_textbook_that_meticulously_catalogues/",
        "text": "Please forgive my ignorance. Such a work would be useful, I wager.",
        "created_utc": 1669808913,
        "upvote_ratio": 1.0
    },
    {
        "title": "Choosing seasonal order in SARIMAX for Quarterly Macro Data",
        "author": "AnalysisPerspective",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8oycq/choosing_seasonal_order_in_sarimax_for_quarterly/",
        "text": "Which seasonal order in SARMIAX is (more) correct to choose?  \n\n\nThe goal is to predict future level of dependent variable given macro scenarios. There is a seasonal component in the dependent variable. When I run the autocorrelation/partial autocorrelation plots there is a significant lag at t-5, t-10 etc. However, as I am working with quarterly data I would (naturally) assume that the seasonal order input for the SARMIAX should be 4, not 5. But since the 5th lag is significant, this suggest having seasonal order of 5. This means that predictions would be period with s=5 and thus roll across future quarterly predictions I assume, but like will give better predictive power? Both s = 4 and s = 5 with an AR(1) term gives significant results. Explainability is very important in this case, however, if prediction is off, that is also bad. Any thoughts of which is more correct to choose?\n\nThus the question is in technical terms:  \nmodel=sm.tsa.arima.ARIMA(dependent\\_variable,order=(0,1,0),seasonal\\_order (1,1,0,5),exog=exogenous\\_variables)\n\nmodel=sm.tsa.arima.ARIMA(dependent\\_variable,order=(0,1,0),seasonal\\_order (1,1,0,4),exog=exogenous\\_variables)\n\nhere is the seasonal component from running the model with the change in seasonal order  \ncoef        std err          z          P&gt;|z|      \\[0.025      0.975\\] \n\n ar.S.L5                                       -0.5577      0.141     -3.950      0.000      -0.834      -0.281   \n ar.S.L4                                       -0.4117      0.186     -2.209      0.027      -0.777      -0.046",
        "created_utc": 1669807144,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which test to use",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1669805983,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dummy coding or frequencies",
        "author": "frauensauna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8o1ep/dummy_coding_or_frequencies/",
        "text": "Hi all. I want to examine frequency effects (count data) on a continuous outcome variable. My plan was to run a linear regression model. The question is which Xn relate to higher Y scores. The data frame is currently in long format with many categorical variables. I was thinking of converting them to dummy coding which would create a huge data frame. Something like this:\n\nhttps://preview.redd.it/gnfnhbr3e23a1.png?width=936&amp;format=png&amp;auto=webp&amp;s=8cbc370c5e7ca14167a9c1b4dd6b2e946e9c3b80\n\nAnd model y \\~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8\n\nNow, I was wondering whether it would make more sense to convert them all to continuous variables, like:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/q04klttie23a1.png?width=936&amp;format=png&amp;auto=webp&amp;s=ea88b1015da32fb2c30a0529ab0061e2e793fb53\n\nAnd run the same model (y \\~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8). What is the difference? Which would be more appropriate?",
        "created_utc": 1669803917,
        "upvote_ratio": 1.0
    },
    {
        "title": "Justify statistical power and R^2",
        "author": "Intelligent_Put_412",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8nx5a/justify_statistical_power_and_r2/",
        "text": "Hello, I am a year two student who is studying a basic statistic course for psychology.\n\nI cam across with the two ideas \"stat power\" and \"R\\^2\" and I am confused with how they are justified. For stat-power, as the population distribution often remains unknown, how do we conclude a beta value out of this situation?\n\nFor R\\^2, how come a square of correlation strength would give an explanation of variability? I came across with this video but there are too many terms I haven't learnt yet... thank you redditors!",
        "created_utc": 1669803513,
        "upvote_ratio": 1.0
    },
    {
        "title": "a question about independent variables",
        "author": "sanadbenali222",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8nqov/a_question_about_independent_variables/",
        "text": "\"Two events are independent, statistically independent, or stochastically independent[1] if, informally speaking, the occurrence of one does not affect the probability of occurrence of the other or, equivalently, does not affect the odds. Similarly, two random variables are independent if the realization of one does not affect the probability distribution of the other.\"\n\n\" In medical research, independence is generally defined in a statistical sense: a variable is called an independent risk factor if it has a significant contribution to an outcome in a statistical model that includes established risk factors.\"\n\n\nThe second definition seems to completely contradict the first one \n\nI have heard this principle a few times in medical research so it's not just this source its widespread \n\nCould someone explain how does\" medical research\"  define and calculate independent risk factors vs how regular statistician would?",
        "created_utc": 1669802925,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why is Naive Bayes' giving me results biased towards the category with fewer values?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8gjl0/why_is_naive_bayes_giving_me_results_biased/",
        "text": "For context, this is the data set: [https://www.kaggle.com/datasets/jonathanbesomi/superheroes-nlp-dataset](https://www.kaggle.com/datasets/jonathanbesomi/superheroes-nlp-dataset) \n\nI cleaned it up by using sklearn's impution functions to handle all the missing data, as well as fixing the formatting of the height and weight variables, etc.  \n\nThis is how the data is distributed between amongst the different creators:\n\nhttps://preview.redd.it/ctjrbk26803a1.png?width=1652&amp;format=png&amp;auto=webp&amp;s=ecda83bd88355b5b0b1cb83488e80a8a9d757fc0\n\nCurrently, I'm looking only at the portion of the data from the top two creators, DC and Marvel.  As you can see from the bar  graph, there are significantly more characters in the Marvel category than in DC.  \n\nAnd yet, when I applied the Naive Bayes' algorithm, the results were HEAVILY skewed towards DC.  Specifically, I applied 400-fold cross validation and chose the model with the lowest proportion of misclassifications.  Here are histograms of the computed probabilities for each category for the model with the smallest error:\n\nhttps://preview.redd.it/daj65bc5903a1.png?width=1657&amp;format=png&amp;auto=webp&amp;s=69a618f264489c704310a22e4a319192f1465bb4\n\nhttps://preview.redd.it/mvb47by7903a1.png?width=1635&amp;format=png&amp;auto=webp&amp;s=1ea661e5769e16dad391c93787f80c721a4f9c2e\n\nAs you can see, for most of the data, it computed a high probability of DC and a low probability of Marvel, which just doesn't make any sense when the data has nearly twice as many Marvel characters as DC characters.\n\nTo be clear, I know I ought to try to balance out this imbalanced this imbalanced data -- the error rate is currently about 66%, which is ridiculous -- I just wanted to see the results on the original data first, than try balancing the data and see how the results differed.  But what I don't get right now is why there's a bias to the LOWER FREQUENCY category.  I expected the bias would be towards the higher frequency category, and, when I applied KNN to the same data, that is what happened.  So why is the opposite happening with Naive Bayes'?  \n\nIf it helps, here's my Python code:\n\n    import numpy as np\n    import pandas as pd\n    \n    from dfply import *\n    \n    from numpy.random import MT19937\n    from numpy.random import RandomState, SeedSequence\n    \n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.metrics import accuracy_score\n    \n    import plotly.express as px\n    \n    @pipe\n    def Rename(df, names):\n        return df.rename(columns = names)\n    \n    \n    def NaiveBayesClassification(df):\n    \n      RandomState(MT19937(SeedSequence(123))) #equivalent to R's set.seed(123)\n    \n      #select function and pipe operator are from dfply library\n      x = df &gt;&gt; select(range(10,67)) #numeric independent variables\n      y = df &gt;&gt; select(X.creator)\n    \n      model = GaussianNB()\n    \n      train_set_predictions = []\n      test_set_predictions = []\n      in_sample_error = []\n      out_of_sample_error = []\n      in_sample_probs = []\n      out_of_sample_probs = []\n    \n      #split data, using kfold cross validation\n      data_split = StratifiedKFold(n_splits = 400, shuffle = True)\n      index = 0\n        \n      with warnings.catch_warnings(): #ignore needless dataconversionwarning\n          warnings.simplefilter('ignore')\n    \n          #fit model to each training set and then get and store predictions, errors, \n      #and probabilities\n          for train_index, test_index in data_split.split(x, y):\n            x_train, x_test =  x.iloc[train_index], x.iloc[test_index] #train-test split\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n            model.fit(x_train, y_train)\n    \n            train_set_predictions.append(model.predict(x_train))\n            test_set_predictions.append(model.predict(x_test))\n    \n            in_sample_error.append(1 - accuracy_score(y_train.to_numpy(),     \n                                                train_set_predictions[index]\n                                                     ) \n                                   )\n            out_of_sample_error.append(1 - accuracy_score(y_test.to_numpy(), \n                                                       test_set_predictions[index]\n                                                          ) \n                                       )\n            \n            index += 1\n    \n            in_sample_probs.append(model.predict_proba(x_train))\n            out_of_sample_probs.append(model.predict_proba(x_test))\n    \n          #get probabilities for best model\n          best_model = np.argmin(out_of_sample_error)\n    \n          creator_names = dict(enumerate(model.classes_.flatten(), start = 0))\n    \n          out_of_sample_probs_best_model = (pd.DataFrame(in_sample_probs[best_model]) &gt;&gt;         \n                                            Rename(names = creator_names))\n    \n          print(\"Average in-sample error:\")\n          print(np.mean(in_sample_error))\n          print(\"\\nAverage out of sample error:\")\n          print(np.mean(out_of_sample_error))\n          print(\"\\nDescriptive statistics for out-of-sample predicted \\\n                  probabilities by best model:\")\n          print(out_of_sample_probs_best_model.describe())\n    \n          print(\"\\nHistograms of Probabilities for Top Two Creators:\")\n    \n          fig1 = px.histogram(out_of_sample_probs_best_model, x = \"DC Comics\")\n          fig1.show()\n          fig2 = px.histogram(out_of_sample_probs_best_model, x = \"Marvel Comics\")\n          fig2.show()\n    \n    NaiveBayesClassification(OnlyTop2Creators)",
        "created_utc": 1669780726,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need help in reading and concluding about an Xbar control chart. Is it out of control? What do experienced people infer only by looking at it?",
        "author": "Specialist-Leg-3613",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8etkn/i_need_help_in_reading_and_concluding_about_an/",
        "text": "&amp;#x200B;\n\n[The spike on 3\\/4 seems weird to me and 13-20 seems a large quantity of occurences above the mean, but I don't know exactly what this means as I'm really new to this](https://preview.redd.it/5og5047d403a1.png?width=1586&amp;format=png&amp;auto=webp&amp;s=60f80c8ce6b4f3bd93b82c90a9067dac5b471079)",
        "created_utc": 1669776287,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does correlation apply only to linear models? Or can we say that two variables \"correlate\" in a non-linear model?",
        "author": "Friendly-Hooman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8emhl/does_correlation_apply_only_to_linear_models_or/",
        "text": "",
        "created_utc": 1669775808,
        "upvote_ratio": 1.0
    },
    {
        "title": "Writing a conclusion about an Xbar and R chart",
        "author": "hrmantovani",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z8e99u/writing_a_conclusion_about_an_xbar_and_r_chart/",
        "text": "Hey there Reddit!\n\nI'm currently on the last semester of a small Uni in Brazil. We have to choose an elective discipline in the last semester in order to graduate, but there were only two choices and I went with the Six Sigma one. The problem is that due to work I haven't been able to attend many classes and I've got a final \"project\" to do at home to replace my grade. This is LITERALLY the last discipline I have and I'm really concerned about losing an entire semester in 2023.\n\nThe entire thing is pretty basic as they basically want you to graduate and leave. They send you 30 sets of 5 numbers each and ask you to:\n\n* Build a table indicating Xbar, Xbar bar, R, Rbar, LCLx, UCLx and UCLr\n* Build two graphs (I don't know the exact translation of each one but I assume, according to the English Wikipedia, that they are called **Process variation chart** and **Process mean chart**)\n* Write a conclusion about the graphs\n\nThat's it. There's no background to the numbers, they represent a random process somewhere. I'm pretty sure I nailed the table and the graphs because you just need to follow formulas, but I need the last question. I would really appreciate if someone could help me and [I'm leaving an Imgur link](https://imgur.com/gallery/ac0Rpzr) with the pics. Please help a student in need lol",
        "created_utc": 1669774929,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for Resources",
        "author": "Designer-Buy9564",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z893ak/looking_for_resources/",
        "text": "Just learning how to use excel for stats and looking for resources to refer to in order to gain a better understanding of the perks of using excel\nThanks",
        "created_utc": 1669762686,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help me understand why my P value is an error when generating Kaplan Meier curves",
        "author": "3234234234234",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z88q42/help_me_understand_why_my_p_value_is_an_error/",
        "text": "I paid for stats software to help with a project. I'm generating Kaplan Meier curves to differentiate survival in 2 groups. When I run the software with my data, it gives me the word summary:\n\n&gt;\\&gt; \"A log-rank test was calculated to see if there was a difference between groups X and Y in terms of the distribution of time to event occurrence. For the present data, the log-rank test showed that there is difference between the groups in terms of the distribution of time until the event occurs, p=aN. The null hypothesis is thus rejected.\"\n\nIt gives me the means and medians and yep pretty big difference between them, it would appear to be statistically significant. But then p=aN and the Chi Square for the log rank test =NaN as well. I understand this is because it's trying to divide by zero somewhere but at a loss as to why/what is trying to divide by zero. How can I reformat things to get a P value?\n\nAppreciate any help, I'm supposed to be presenting this data in 3 days!",
        "created_utc": 1669761862,
        "upvote_ratio": 1.0
    },
    {
        "title": "The margin of error in the confidence interval formula",
        "author": "ikashifkhan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z879lc/the_margin_of_error_in_the_confidence_interval/",
        "text": " Somebody, please explain to me this **confidence interval formula** especially the **margin of error**.\n\nWhy do we divide the population standard deviation by the root of the sample population?\n\n# CI = X  ±  Zs√n",
        "created_utc": 1669758467,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help on statistical testing on categorical data",
        "author": "Mystique22910",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z86uik/help_on_statistical_testing_on_categorical_data/",
        "text": "Let's say I am trying to understand the distribution of users who favourited a song\n\nThe dataset could look something like this\n\nage | total\\_favourited\\_song\n\n0-16| 2\n\n17-34 | 300\n\n35-50 | 200\n\n50+|700\n\nHow do I know if this is statistically significant? For example : how do I know that there are quit a bit of users in 50+ range that favourited the song did not happen by chance.\n\nIs chi-square test the best way to do this?",
        "created_utc": 1669757487,
        "upvote_ratio": 1.0
    },
    {
        "title": "Test for statistically significant difference in survival curves between two groups",
        "author": "Dr_Devilish",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z85pjm/test_for_statistically_significant_difference_in/",
        "text": "Hi there,\n\nI have data on the survival probability (%) of cells over time (s) from two different trains (\"WT\" and \"rad16\"). I want to compare the two to see if there is a statistically significant difference in the survival probability over time; a survival plot of the data suggest that cells from the rad16 strain die earlier. I am using R for my analysis.\n\nI understand a Cox Proportional-Hazards Model or log rank test would be appropriate, however the data that I have been given are not in a format which is typical for this sort of analysis. If I am not mistaken, this sort of analysis normally works with status (censored = 1, dead = 2) as a binary response variable and time as a continuous predictor variable. If I had data like this then I could just use the coxph() or survdiff() functions in the survival package, as seen here: [https://www.sthda.com/english/wiki/cox-proportional-hazards-model](https://www.sthda.com/english/wiki/cox-proportional-hazards-model) and [https://www.statology.org/log-rank-test-in-r/](https://www.statology.org/log-rank-test-in-r/), respectively. \n\nHowever, the data that I have are just the percentage survival at each time point. I don't have the number of censored and dead cells, just the final percentage.\n\nIs there a way that I could use these data in a Cox Proportional-Hazards Model or log rank test to see if there is a significant difference in survival between strains?\n\nExample of the data and a survival curve of the data below.\n\nThanks in advance for your support.\n\n[Example of the data](https://preview.redd.it/x9347bnhay2a1.png?width=208&amp;format=png&amp;auto=webp&amp;s=9cfb9acce83cf81037ee569946ac405764d5f67e)\n\n&amp;#x200B;\n\n[Example plot of survival curves](https://preview.redd.it/91vaa1suay2a1.png?width=771&amp;format=png&amp;auto=webp&amp;s=b40e194db5dd1537a471576e6d7d46b549450ee3)",
        "created_utc": 1669754968,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dichotomous Variables",
        "author": "Euphoric-Look3542",
        "url": "https://i.redd.it/lp1qb1l4mz2a1.png",
        "text": "I am given a table containing dichotomous variables, but I don't understand why the categories \"Have/Yes\" and \"None/No\" next to each variable together do not add up to 100%? Can someone help me understand this?\n\nAppendix/Annex table 1:\nThe proportion of the Roma women giving birth under the age of eighteen in the different categories of the dichotome explaining variables\n\n/ Explanatory variables / No / Yes / :\nIs there any railway? No/None: 34% Yes/Have: 26%\nIs there an economic centre? N: 29% Y: 28%\nIs there a primary school? N: 31% Y: 28%\nAre only gypsies living in the neighbourhood? N: 27% Y: 32% \nWhether the dwelling is outside the municipality N:27% Y:42%\nIs the dwelling of Oláhgypsy origin?  N: 28% Y: 28%\nIs it Beás origin? No: 30% Yes: 14%\nDid he/she continue their studies? No: 34% Yes: 4%",
        "created_utc": 1669752052,
        "upvote_ratio": 1.0
    },
    {
        "title": "Understanding the value of AIC and cross-validation",
        "author": "TK-710",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z82o5l/understanding_the_value_of_aic_and_crossvalidation/",
        "text": "Hello,\n\nI'm often in a situation where I need to identify relevant predictors of some outcome in some exploratory research project. The standard in this field is to use some combination of p values and confidence intervals to do so. \n\nI certainly understand the problems related to tossing a bunch of predictors in a regression and seeing which come out as significant. The alternatives to doing this that I see most commonly are to look at the AIC of the resulting model and/or to do some cross-validation. My confusion comes from the fact that AIC seems to identify irrelevant variables as relevant about 16% of the time and that cross-validation seems to be equivalent to AIC, at least under some circumstances. \n\nSo, my question is: why are these methods preferable to using some significance test in this context?\n\nThanks!",
        "created_utc": 1669748329,
        "upvote_ratio": 1.0
    },
    {
        "title": "Right A/B testing calcs for order size / lift?",
        "author": "petecious",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z82fwi/right_ab_testing_calcs_for_order_size_lift/",
        "text": "Hi!\n\nI'm trying to test if an introduction of a UX element in our app would have such effect on the Experiment population that they spend more in their online orders with us. So this is not a classic case of just measuring number of \"conversions\". Everybody in both Control and Experiment are customers that placed an order with us. We hope to measure the difference &amp; lift in $ spent per order. \n\nIs there a well-known equation I could use? Lets say I have:\n\n**Control group:**\n\n* Order count = CO\n* Average order spend amount = CA\n* Variance on the order spend amount = CV\n\n.. and the same for the **Exposed / Experiment** group (EO, EA, EV).\n\nWould there be an easy way to calculate that the average observed order spend amount lift, **L**, is within its true value with **95%** confidence level?\n\nNot sure what terms to even google up to come up with the equation. Everything I have googled so far leads me to Facebook's \"lift tests\" HOWTOs and not the actual math.\n\nThanks!",
        "created_utc": 1669747805,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bonferroni correction",
        "author": "cchhiicchhaarriittoo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z80xvv/bonferroni_correction/",
        "text": "Hey all,\nLets say I am comparing smokers vs non smokers. I want to see how smokers differ from non smokers on multiple factors, (bmi, sleep, blood sugar) … etc\n\nI will compute multiple independent sample t test for this, but is a bonferroni correction needed as these are two groups?\n\nCheers",
        "created_utc": 1669744489,
        "upvote_ratio": 1.0
    },
    {
        "title": "R coding is needed",
        "author": "Far_Most_5965",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z80cuu/r_coding_is_needed/",
        "text": "[removed]",
        "created_utc": 1669743179,
        "upvote_ratio": 1.0
    },
    {
        "title": "R code help is needed",
        "author": "Far_Most_5965",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z80c8c/r_code_help_is_needed/",
        "text": "[removed]",
        "created_utc": 1669743138,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to do my powercalculations in biomarker discovery?",
        "author": "justitia_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7wu1d/how_to_do_my_powercalculations_in_biomarker/",
        "text": "I will be starting with diagnosed patients group and they either develop chemoresistance or chemosensitivity. I want to find biomarkers identifying them (for before exposure). So they will be tested before treatment and after they either develop one of them. I kept looking at many clinical studies but I fail to find the one that suits my study design.\n\nI wanted to try this website ([https://riskcalc.org/samplesize/](https://riskcalc.org/samplesize/)) but I am really unsure if my study design fits to any of them as mine is observational but everyone is exposed to the drug they just give different responses. Please help!",
        "created_utc": 1669735057,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question on which statistical analysis to use.",
        "author": "UnusualEffort",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7wg1h/question_on_which_statistical_analysis_to_use/",
        "text": "I have data showing yearly drug-related deaths for about a two-decade period. This data is divided into all Persons, Male and then Female. It is also divided into if the drug of choice is found in the death certificate at all, with alcohol, without alcohol and without other drugs.\n\n[Picture to help me describe the data I have.](https://preview.redd.it/q2l3ij65nw2a1.png?width=2111&amp;format=png&amp;auto=webp&amp;s=683407842c7f7cfd61ccf44dc81aca686389f7d6)\n\nI would like to know if there are any useful statistical approaches to see if males or females are more likely to die via drug overdose. Further analysis is to see if either males or females are more likely to have deaths with alcohol or other drugs in the death certificate than the other sex.",
        "created_utc": 1669734141,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting the intercept of a multiple regression",
        "author": "westfloor22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7vxam/interpreting_the_intercept_of_a_multiple/",
        "text": "I started doing a regression analysis of how long a person watches a video according to their profession and I got this equation:\n\n    Duration = 0.105 + 0.032(job2) + 0.019(job3)\n\nI have interpreted the intercept as being the mean duration of subjects with job1. The other coefficients are the difference between the intercept and the mean duration for each respective job.\n\nI then added a binary variable for gender to the analysis and got this:\n\n    Duration = 0.096 + 0.033(job2) + 0.02(job3) + 0.025(male)\n\nI thought that the intercept would be the mean duration when the subject is female and has job1, but when I actually calculate the mean duration for this subgroup in the data, the value is not 0.096. It is 0.101.\n\nWhat am I doing wrong when interpreting the intercept? I have very little experience in statistics.",
        "created_utc": 1669732936,
        "upvote_ratio": 1.0
    },
    {
        "title": "Computational complexity of Metropolis-Hastings and potential speed-up?",
        "author": "tanishqkumar07",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7lf8s/computational_complexity_of_metropolishastings/",
        "text": "The MH algorithm essentially involves generating a sample destination state from a proposal distribution, computing the acceptance probability as a function of that sample, and checking whether a uniform random variable is above the acceptance probability to determine whether to move to the new state or not.\n\n* What is the time complexity of each step? Is it the case that each step in this algorithm is really fast, and so MH is time-bottlenecked by the mixing time it takes for the distribution to converge?\n* How does the time complexity depend on the dimension of the data in the underlying joint distribution being approximated in the first place (recall MH seeks to provide samples drawn from a PDF approximating some target distribution/PDF)?\n* For the exponential family, the acceptance probability involves a min(1, exp\\_1Q\\_2/exp\\_2Q\\_2) where both exponentials contain a dot product involving the sufficient statistic. Since approximate isometries are guaranteed to exist for data sets (by JL Lemma), why can't we always project data down into a much lower dimension such that all data points have the same pairwise dot products, then compute the acceptance probability/run M-H with the lower dimensional data points (much faster, since in lower dimension), if the only thing we use about the data points are their dot products with other data points?",
        "created_utc": 1669702393,
        "upvote_ratio": 1.0
    },
    {
        "title": "If I need to apply some method to balance out imbalanced data (e.g. under/oversampling, up/downweighting) prior to using a classification algorithms, should I do it before or after doing the train/test split?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7kp4t/if_i_need_to_apply_some_method_to_balance_out/",
        "text": "",
        "created_utc": 1669700146,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone help me analyze this (short) research study?",
        "author": "Academic-Anybody63",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7hzq2/can_someone_help_me_analyze_this_short_research/",
        "text": "The conclusion states they \"found no benefit when pantoprazole is added to enteral nutrition..\" but the p-value=0.99, meaning there is no statistically significant information that was found, which is not the same as \"no benefit\"...right? Thanks!\n\nDOI: 10.1016/j.jcrc.2017.08.036",
        "created_utc": 1669692347,
        "upvote_ratio": 1.0
    },
    {
        "title": "Significant Pearson Correlation but Insignificant in ANCOVA?",
        "author": "igloogaboo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7gdn4/significant_pearson_correlation_but_insignificant/",
        "text": "Hi there! I am somewhat of a statistics newbie so please forgive me if this answer is obvious. I was wondering if it was possible that ANOVA test is not significant when the Pearson correlation is significant.\n\nFor context, I have 3 variables; one continuous dependent variable, one continuous independent variable, and one categorical independent variable (confounding variable). When I run two-way ANOVA, I used both independent variables as \"fixed factors\". I didn't use ANCOVA because the independent confounding variable is categorical and has 6 levels (and I heard your ANCOVA covariate cannot be a categorical variable with more than 2 levels, so I am using two-way ANOVA). The independent variable is significant with Pearson correlation, yet is not significant when run in this two-way ANOVA. Does that make sense?",
        "created_utc": 1669688064,
        "upvote_ratio": 1.0
    }
]