[
    {
        "title": "Help with Running a Conditional Logit Regression",
        "author": "ay1mao",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119kcyj/help_with_running_a_conditional_logit_regression/",
        "text": "I'm attempting to run a conditional logit to predict the winning swimmers from 30 collegiate swim races of the same stroke and distance.  All swimmers are of one sex.  There are 8 swimmers per race (n = 240).  My \"y\" variable is \"win?\" (1 for yes).  My x variables are:\n\nx1: time in most recent race\n\nx2: average time in last 4 races (standardized)\n\nx3: dummy variable-- \"freshman?\" (1 for yes)\n\nI use GRETL and have never run a conditional logit before.  I have attached a screenshot of the prompt that comes up when I try to run the regression.  Would somebody please tell me what data to put where?  \n\n\nPS: I took a few stats and probability classes in undergrad and undergrad, but I never got as far as logit, let alone conditional logit.",
        "created_utc": 1677115476,
        "upvote_ratio": 1.0
    },
    {
        "title": "question about three-level categorical dependent variables",
        "author": "rurunner7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119jvre/question_about_threelevel_categorical_dependent/",
        "text": "Hi everyone!\n\nI am trying to build a model for a study that has a three-level categorical dependent variable (choice between A, B, C)  and an independent variable with 5-levels (group). I am trying to measure if choice is significantly influenced by group. Normally, I am working with binary models, where logistic mixed effects models are my go-to.  \n\nI have read that multinominal logistic regression is good for dependent variables with more than 2 levels, but it is not clear to be that this is the best option here. Does anyone have suggestions?",
        "created_utc": 1677114251,
        "upvote_ratio": 1.0
    },
    {
        "title": "Including an interaction term in a regression model versus fitting separate models",
        "author": "-curious-cheese-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1198w6m/including_an_interaction_term_in_a_regression/",
        "text": "This might be a dumb question and I might just be confusing myself, but I am wondering what is the difference between fitting a regression model with interaction versus fitting separate regression model at each level of the grouping variable and comparing the results? My data is all categorical predictors and quantitative responses with a grouping factor. The categorical predictors include park amenities, the responses are health outcomes, and the groups were determined via k-means clustering using the response variables. I want to determine the direction and magnitude of the relationship between the park amenities and health outcomes and see if that changes between groups.\n\nI think I should fit a regression model for each group and then compare the results. But what would be the difference between doing that and just fitting one model with interaction between the group factor and all the predictors? Would fitting multiple models be an issue because it would require additional p-value adjustment for the multiple models? I understand an interaction term measures the effect of an interaction between group and park amenities, but in layman’s terms, what is the difference between that and fitting a model to each group?\n\nI hope my ramblings make sense lol TYIA!",
        "created_utc": 1677094007,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sample size and measured difference",
        "author": "yes_maybe_no11",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1197a0d/sample_size_and_measured_difference/",
        "text": "Hi. Let's say we conduct an A/A test for testing app.\n\nWe will randomly divide users into two groups, with a 50% probability of each user getting into one or the other of the groups, but both groups will get the exact same app. \n\nThe onboarding conversion rate for these groups turned out to be different, lets say because of seasonality. \n\nMy question is if we add more users to the previous A/A test, will the difference in conversions calculated based on the users in each of the groups increase or decrease? \n\nThanks in advance!",
        "created_utc": 1677090731,
        "upvote_ratio": 1.0
    },
    {
        "title": "Have \"what test do I use for this study\" questions killed this subreddit?",
        "author": "lmericle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1197497/have_what_test_do_i_use_for_this_study_questions/",
        "text": "Interested in discussing moderation decisions. Personally I find much more noise than signal on my homepage lately, and this subreddit is a large contributor. I stay subscribed on the off-chance I learn something from an interesting question, but cannot find the value in it anymore.",
        "created_utc": 1677090371,
        "upvote_ratio": 1.0
    },
    {
        "title": "what test do I use? I'm stuck in a hole, help me 🙈😂",
        "author": "Harbarth_9",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1195s0k/what_test_do_i_use_im_stuck_in_a_hole_help_me/",
        "text": "Have you ever tried to do something fairly simple, like dig a hole, but you spend so long inspecting and questioning the shovel that you cannot workout which end of the shovel goes in the ground? 🙈\n\nI have thought myself round in so many circles that I don't know which test I should be using anymore, please advise! \n\nMy experiment had 3 big tubs, each containing the same substrate. I then grew maggots in the substrate of each tub. Once mature, 3 samples of these maggots were harvested from each tub giving me 9 samples. The rest of the maggots were left in the tub and we're allowed to develop into flies which subsequently laid eggs. these eggs then hatched into maggots and, once mature, another 3 samples were taken from each tub, giving 9 more samples.\n\n\nAll 18 of these samples then had their nutritional values analysed, giving results on all minerals, amino-acids and fatty-acids.\n\nSo I have a dataset of 18 samples. 9 of the samples are replicates of treatment 1 (1st generation) and 9 are replicates from treatment 2(2nd generation). There are then 3 groups, with each containing 3 cross treatment replicates.\n\nUsing the mineral calcium (Ca) as an example, my null hypothesis is:\n\nThe generation (1st or 2nd) of a larval sample makes no difference to the Ca content of that sample. \n\nWhat test do I use to assess this hypothesis?\n\nMy initial thought was a dependent t-test as I am comparing the means of 1st gen to 2nd gen. However, this doesn't incorporate the 3 replicate tubs. As far as a t-test knows each pair of samples either came from their own tubs, or one big one. I have been trying to incorporate these 3 replicate groups within the 9  pre and post replicates for so long that I cannot see the wood for the trees anymore. Which test is most appropriate?",
        "created_utc": 1677087273,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with finding out the excel formula for a question I'm not sure - what is the name of the way to solve the problem",
        "author": "KingDror",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1195gt4/help_with_finding_out_the_excel_formula_for_a/",
        "text": "Hi,  \nI have this problem:  \nI have a bag with 15 balls inside  \n8 green balls  \n7 red balls  \nI draw 5 balls from the bag - one after the other (so the last one matter)  \ngreen ball gives me 1 point  \nthe red ball takes from me 1 point  \nmy point balance can't be negative  \nwhat is the distribution to finish the game with 0, 1,2,3,4, or 5 points?",
        "created_utc": 1677086530,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help",
        "author": "KingDror",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119560o/help/",
        "text": "Hi,  \nI have this problem:  \nI have a bag with 15 balls inside  \n8 green balls  \n7 red balls  \nI draw 5 balls from the bag - one after the other (so the last one matter)  \ngreen ball gives me 1 point  \nthe red ball takes from me 1 point  \nmy point balance can't be negative  \nwhat is the distribution to finish the game with 0, 1,2,3,4, or 5 points?",
        "created_utc": 1677085831,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to divide my groups for a one way between groups ANOVA?",
        "author": "deplorable_word",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1194twh/how_to_divide_my_groups_for_a_one_way_between/",
        "text": "Hello, statisticians!\n\nI’m looking for feedback on how to divide my groups for a one way between groups ANOVA. \n\nHypothesis: that participants who score higher on a measure of social needs will have higher scores on a depression measure. \n\nMy first thought was to divide participants into three groups based on social need (low need, moderate need, and high need) and compare their depression scores in the ANOVA.\n\nOr should I compare the depression scores as one group and the needs scores as another group?\n\nAny guidance greatly appreciated!",
        "created_utc": 1677085044,
        "upvote_ratio": 1.0
    },
    {
        "title": "Nested repeated measures with ordered outcome",
        "author": "CrazyProfessor88",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11938zs/nested_repeated_measures_with_ordered_outcome/",
        "text": "I have a problem that I can't wrap my head around. Couples in one of two settings rate their stress on a ordered 4-category scale at 3 time points (i.e 1 month, 12 months, and 18 months from baseline). The aim is to compare the ratings of the two settings as well as the ratings between gender over time.\n\nIf the outcome was continuous, I would have tried something like a mixed model with fixed effects for time+setting+gender+interactions; random effects for individual nested in couple; an ar1 structure for the repeated measures. Then presented the results as marginal means.\n\nWith the ordinal outcome i am clueless. How to model this efficiently and present the results in a simple manner? Any wise suggestions? \n\nAll input appreciated //CrazyProfessor",
        "created_utc": 1677081382,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] what statistical analysis or statistical tool should i use?",
        "author": "Free_Construction462",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11936o4/q_what_statistical_analysis_or_statistical_tool/",
        "text": " I did a study on coagulation-flocculation in wastewater using different coagulants at different doses. The variables on which I decided the efficiencies of the coagulants are turbidity and absorbance. So, for each coagulant, I have about 5 different doses and for each dose I have several values of turbidity and absorbance. Which tool or analysis would be most appropriate for me to claim, using statistics, that a particular coagulant at a specific dose is the best?",
        "created_utc": 1677081245,
        "upvote_ratio": 1.0
    },
    {
        "title": "What test should I use for this between group comparison?",
        "author": "beatmypete",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1190fzf/what_test_should_i_use_for_this_between_group/",
        "text": "Context: 2 groups, Control and treatment, aiming to reduce the development of delirium, in particular, analysing if the intervention is more effective in reducing delirium in participants with individual risk factors ie, heart failure or cognitive impairment. \n\nIs there a particular test would i need to use to do a between group comparison to measure statistical difference in delirium presence in each risk factor. \nOr do would i just do multiple Chi Square Tests for every risk factor to calculate this?",
        "created_utc": 1677075464,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a way to find the F-value for testing overall significance of a model given just the following",
        "author": "BethStubbs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118yfu5/is_there_a_way_to_find_the_fvalue_for_testing/",
        "text": "Sum sq regression\nSum sq Error\nAnd the number of independent variables\n\nI thought you would need sample size too but that’s all I’ve been given",
        "created_utc": 1677071921,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I calculate incremental growth rate using a logarithm regression analysis over 3 year time blocks/moving averge?",
        "author": "Comfortable-Goose173",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118y8sb/how_can_i_calculate_incremental_growth_rate_using/",
        "text": " I have count data from 1992-2022 and I want to calculate the growth rate of the population over a three year moving average. I want to replicate similar methodology and use natural log regression to calculate the growth rate over each three year period. I am not sure exactly how to do this. I have taken the only methodology listest in the paper and attached the figure. The author is unable to provide anymore detail. is anyone able to help clarify how I can calculate this?\n\n\"**Data analysis:** \n\nThe mean compound annual growth rate was calculated using a three-year sliding average to consider the average 3 calving intervals of SRWs and subsequent cohort structured calving cycles. A simple exponential regression was fitted to the long-term count data and presented with 95% confidence intervals and standard deviation.\n\nFigure caption: Incremental growth rate using a logarithm regression analysis for southern right whales at Head of Bight, South Australia between 2006 and 2016, for female and calf pairs (black-CC) and total adults (red-TA), with 95% error bars.\"",
        "created_utc": 1677071577,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to reason about probability \"conjunction\"?",
        "author": "korkvid",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118xt96/how_to_reason_about_probability_conjunction/",
        "text": "The book I'm going through talks about probabilities as \"fractions of a finite set\". So, for example, it gives you a CSV where reach row is for an individual containing their age and a  flag indicating if they're overweight.\n\nIn the CSV, the fraction of people that are ...  \n \\* children is 0.25  \n \\* overweight is 0.5  \n \\* children and overweight is 0.115\n\nIIRC the formula for probability conjunction is P(A) \\* P(B). Given the stats above, P(&lt;18) \\* P(overweight) should come out to 0.12 (0.25 \\* 0.5 = 0.125.) 0.115 is close to 0.125, but not the same. So, how should I be thinking / reasoning about the conjunction formula and the 0.125 output?\n\nMy other question related to conjunction: Does the conjunction formula still apply if P(A) and P(B) aren't covering the same \"finite set\"? So, for example, the probability that ...  \n \\* a person is a child is 0.25.  \n \\* a rabbit is cream colored is 0.1.  \nPeople and rabbits don't overlap, so does it make sense to do P(person child AND rabbit is cream colored) = P(person is child) \\* P(rabbit is cream colored) = 0.25 \\* 0.1?",
        "created_utc": 1677070817,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hypothesis Testing: I am struggling to understand the given that part of this problem for v=14 P=0.539... and the calculation related to it like why are we take a diff. also there is no explanation of this in the book it just jumps straight to an example problem",
        "author": "Kirito275",
        "url": "https://www.reddit.com/gallery/118uzi6",
        "text": "",
        "created_utc": 1677062397,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Statistics:Hypothesis Testing] I am struggling to understand the given that part of this problem for v=14 P=0.539... and the calculation related to it like why are we take a diff. also there is no explanation of this in the book it just jumps straight to an example problem",
        "author": "Kirito275",
        "url": "https://www.reddit.com/gallery/118uyn0",
        "text": "",
        "created_utc": 1677062308,
        "upvote_ratio": 1.0
    },
    {
        "title": "Who is sexually abused more under 18, boys or girls?",
        "author": "[deleted]",
        "url": "",
        "text": "[removed]\n\n[View Poll](https://www.reddit.com/poll/118qher)",
        "created_utc": 1677045529,
        "upvote_ratio": 1.0
    },
    {
        "title": "About analysis",
        "author": "Hour_Woodpecker_906",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118oubg/about_analysis/",
        "text": "So I'm preparing for my dissertation proposal and I'm yet to make a concrete one\n\nI have Two IV and two DV and a mediating variable. All are continuous\n\nis MANOVA the way to go?",
        "created_utc": 1677040080,
        "upvote_ratio": 1.0
    },
    {
        "title": "Writing down, assessing, and understanding a linear mixed-effects model for a nested experimental design",
        "author": "Weekly-Laugh-5202",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118olqj/writing_down_assessing_and_understanding_a_linear/",
        "text": "I am trying to analyze some data for an experiment that I ran, but I'm having trouble understanding whether I'm doing it correctly.\n\nI began with a population of cells. I split this population into three different tubes. \n\nI measured the length of many cells in the population in the first tube (&gt;5000, &lt;40000). \n\nI treated cells in the second tube with a chemical for a certain period of time, and did not treat the cells in the third tube with anything. I then measured the length of many cells in the population in both the second tube and the third. \n\nI repeated this experiment in the presence of a second chemical that might interact with the effect of the first chemical.\n\nThis whole experiment was performed three times. \n\nI now have length data for all of these different populations.\n\nMy goal is to determine whether or not these different treatments had an effect. Running t-tests on summary statistics gives me different results depending on which statistic I choose, so I've been wading into some more complex analysis that can take into account the whole dataset. So far, my poking around has suggested that I can use a linear mixed-effects model to perform this analysis. I've come up with an equation, but I can't understand it well enough to know if it's correct. \n\nHere is how my data looks:\n\n Length    Repeat    Time    Treatment\n\n\\_\\_\\_\\_\\_\\_    \\_\\_\\_\\_\\_\\_    \\_\\_\\_\\_    \\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n38.737      1        0          0    \n\n37.532      1        0          0    \n\n:            :         :           :    \n\n31.259      3        1          1    \n\n38.339      3        1          1    \n\nLength is my measurable, Repeat refers to which of the three replicates the value belongs to, Time refers to whether the measurement was in the first tube (the initial population) or the second and third tubes, and Treatment refers to whether the cells were in the second (treatment) or third (control) tubes. \n\nHere is the model that I've come up with:\n\n'Length \\~ -1 + Treatment + Time + (1|Repeat)'\n\nIs this appropriate? Should this model be different from how it is, given the experimental design? \n\nRight now this does not take into account the second chemical -- I've fit the lme and then just compared the effect size +/- the second chemical by looking at the number with my eyes. Would it be better to incorporate this second chemical into the model somehow to be able to estimate the interaction between the two more directly (and more importantly, to assess its significance?)\n\nFinally, is there a good resource out there for me to get a handle on interpreting these equations? So far all that I have found is too high-level for me, or too abstracted away from the science that I am familiar with (cell biology) for me to understand.",
        "created_utc": 1677039323,
        "upvote_ratio": 1.0
    },
    {
        "title": "Doubt on MGF",
        "author": "cityboyonbed",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118o13s/doubt_on_mgf/",
        "text": "What is the significance of  t in Mx(t) in moment generating function?",
        "created_utc": 1677037538,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which variables should you put in your multivariate logistic regression analysis?",
        "author": "Big_Category7915",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118jfzo/which_variables_should_you_put_in_your/",
        "text": "Multivariate logistic regression analysis?\n\nI need to find predictors for a certain outcome. I started with Univariate log regression because my outcome is binary\n\nMy question is; do I only put the variables that were significant in the univariate analysis in my multivariate (p value 0.05) or is there another rule that I don’t know off",
        "created_utc": 1677024432,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone help me with statistics??",
        "author": "Big_Category7915",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118i6na/can_someone_help_me_with_statistics/",
        "text": "Multivariate logistic regression analysis?\n\nI need to find predictors for a certain outcome. I started with Univariate log regression because my outcome is binary\n\nMy question is; do I only put the variables that were significant in the univariate analysis in my multivariate (p value 0.05) or is there another rule that I don’t know off",
        "created_utc": 1677021139,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sorry for low quality question. Where can I find the answers of All exercise questions of Blitzstein and Hwang Intro to Probab?",
        "author": "ConfusedAnimal12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118gtbk/sorry_for_low_quality_question_where_can_i_find/",
        "text": "I am self studying Probability and statistics from this book(which is complimentary to stat110 course). \n\nHowever after solving those exercise questions, I found no feedback mechanism to judge myself. I tried to google this but all I found was Selective questions solution manual. I don't need detailed solutions of each and every question, I just need answers to each and every question to check myself.\n\nThank you.",
        "created_utc": 1677017820,
        "upvote_ratio": 1.0
    },
    {
        "title": "Conditional use of Welch's T-test?",
        "author": "claudinbernard",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118g2g2/conditional_use_of_welchs_ttest/",
        "text": "Hello all,\n\nFirstly I am not a statistician so apologies if this seems a dumb question. I have qPCR dataset for 10 primer pairs where I have calculated 2\\^ddCq values, and normalized treatment group (n = 26) onto the controls (n = 11). I compared treatment to control using a T-test with prism, and also performed tests for outliers (Grubb's), normality (Shapiro-Wilk), and equal variance (F-test). In some of my genes, the standard deviations of control vs. treatment group are significantly unequal, so I apply Welch's T-test. I am wondering, since these are measurements of different genes but from the same sample, is it valid to use Welch's T-test for some genes (where variance is unequal) and standard T-test for others (where variance is equal) - or should I apply Welch's correction for all tests even if some have equal variances? I have not seen a clear answer for this in the literature...\n\n&amp;#x200B;\n\nFor example:\n\nGene 1: Difference in means +/- SEM = 0.2287+/-0.0648. **F-test to compare variances, F = 5.302, p = 0.0091 -** unequal variance - use Welch's T-test\n\n&amp;#x200B;\n\nGene 2: Difference in means +/- SEM = 0.001+/- 0.0722. **F-test to compare variances, F = 1.860, p = 0.2838 -**  equal variance - use Standard T-test?\n\n&amp;#x200B;\n\nThanks in advance!",
        "created_utc": 1677016035,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpretation of Regression Analysis with Log AND Root transform",
        "author": "salsa_bear",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118epe5/interpretation_of_regression_analysis_with_log/",
        "text": "Hi,\n\nI'm working on a multilinear regression analysis and since my variables didn't have a normal distribution, I had to transform them. Some variables responded better to log, some better to root.\n\nhttps://preview.redd.it/hfc6d4v4tlja1.png?width=884&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7bb8f864200ba4e15ec2a8eb5d21f5658ffa3009\n\nThe dependent variable is log-price and all the other variables and their transformations are written above. How can I interpret these results? \n\nAlso, despite all the transformations these still seems to be some outliers as can be seen below by std residuals;\n\nhttps://preview.redd.it/1w42bu3htlja1.png?width=744&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2428218212fa6c5c715f45cc392a90a2454597b3\n\nI didn't want to remove the outliers to keep the dataset 'honest'. Would this be a problem? If not, how should I say mention this in my interpretation? Here is more info;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1dsuulz7ulja1.png?width=973&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=131e9a4260d0c26efd6b1a3944a3f6e213ab44c7\n\n&amp;#x200B;\n\nThanks for your help! I'm new to all this!",
        "created_utc": 1677012716,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are some common challenges in scaling machine learning systems?",
        "author": "Nice-Tomorrow2926",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118c3i0/what_are_some_common_challenges_in_scaling/",
        "text": "What are some common challenges in scaling machine learning systems?",
        "created_utc": 1677003468,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Dissertation] [Stata] What can be drawn from this multinominal logistic regression?",
        "author": "paytontayylor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118c24z/dissertation_stata_what_can_be_drawn_from_this/",
        "text": "I’m analysing some data, and I usually use Jamovi, but my supervisor has told me i need to use Stata (which I’ve never used before). I’m already terrible with stats, but I am even more confused using Stata. Please, help me figure out what this means!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xpbvh0qb2lja1.jpg?width=451&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5d4490cc589a9af1b76f3599a1e508f520dfd755",
        "created_utc": 1677003379,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparison of nested cross validation vs human performance for image classification",
        "author": "ThrowRA39384749",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118bjmd/comparison_of_nested_cross_validation_vs_human/",
        "text": "I have trained machine learning models (all the same architecture) for binary classification of images. The datasets of images came from 5 different universities. I performed nested cross validation to train 5 machine learning models, each with a different universities data set being left out of the training data. I then run inference on each model on their left out university dataset to get binary predictions (no data leakage this way).\n\nFor the datasets I also have human assessors (variable number of people per image) perform binary classification. \n\nDoes anyone have recommendations on how to statistically show that the binary classification accuracy of the machine learning models (each predicted on the unseen test data) compares to that of human assessors? \n\nNote: the number of images per university is large, on order of 1000 each. I know no prior information about the distribution of the datasets. \n\nThanks in advance!",
        "created_utc": 1677002143,
        "upvote_ratio": 1.0
    },
    {
        "title": "Calculating adjusted residuals for complex survey data",
        "author": "Ok-Improvement-1612",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118amfj/calculating_adjusted_residuals_for_complex_survey/",
        "text": "Hello,\n\nI'm trying to calculate adjusted residuals for a post-hoc analysis with complex survey data to see what is driving the significant Pearson chi-sq results for a few 2x3 and 3x3 contingency tables. For non-complex data, this is simple enough (𝑂𝑖𝑗 − 𝐸𝑖𝑗/√𝐸𝑖𝑗(1 − 𝑚𝑖/𝑁)(1 − 𝑛𝑗/𝑁)). For complex survey data, I'm a bit stumped. Is it ok to just use the weighted counts to calculate the adjusted residuals? This tends to results in really huge values.\n\nAppreciate your input!",
        "created_utc": 1676999965,
        "upvote_ratio": 1.0
    },
    {
        "title": "Seeking advice regarding PhD applications",
        "author": "Beneficial_World2786",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1189i7p/seeking_advice_regarding_phd_applications/",
        "text": "Hello,\n\nI have applied to several stats (as well as CS) PhD programs in the US this cycle and while I'm yet to hear back from some of the places, the overall signal has been quite underwhelming. I recognize that my profile is far from perfect, though believe I do have some points of substance. I am hoping to seek perspective about where I stand, and what I can do to strengthen my profile for the next cycle if things don't work out this time.\n\nSome salient points about my profile (Apologies beforehand for the length, I'm applying after quite a few years since graduating, and I'm including all that I think is relevant):\n\n\\_ International applicant. University education in the UK (not my home country). Here is a crude guide that converts UK grades to US GPA, if needed: [https://gpacalculator.net/grade-conversion/united-kingdom/](https://gpacalculator.net/grade-conversion/united-kingdom/)\n\n\\_ Undergrad ('13-'16) in maths/ stats/ econ (at a top 5 for maths/ stats UK university). Received a high 2.1 - tended to do very well (90+%) in proof-heavy courses such as real analysis 3, stochastic processes, mathematical statistics and probability theory. Middling grades (60s, 70s) in more applied stats courses. And poor grades in econ/ finance courses (I didn't take too many of these, but nevertheless, enough to bring my overall grade down from a 1st to a 2.1. In retrospect, I really wish I stuck to math/stat courses)\n\n\\_ Masters ('16-'17) in Applied Stats at a top 2 overall UK uni. At the time, I didn't have the clarity about wanting to pursue research, and so made the decision to do a more applied master's despite my proclivity for theory courses. Had a couple of emotionally difficult incidents occurring in quick succession on the personal front in the midst of the academic year - this took a huge toll on my mental health and my grades suffered significantly (received a low 2.1). Given that the program was just a year-long, I barely had the time and bandwidth to take care of my mental well-being whilst also keeping on top of academics. Deferring wasn't really an option due to various reasons including costs.\n\n\\_ Worked at an industrial research/ ML lab for \\~3 years ('18-'21) in my home country thereafter. It was in my last year or so there, that I had the opportunity to do something that truly excited me. Up until the point, I had been engaged in more experimental/ software engg work, however, a project presented, wherein I was allowed to dig into the fundamentals (of an emerging subfield of ML). In the process, I wrote a first-author paper that was accepted at an ICML workshop. The paper delved into the theory of responsible ML and included proving sample complexity/ consistency results for an algorithm, as well as other interesting things such as: characterizing the trade-off incurred by said algorithm between fairness and accuracy, modifying the algorithm to satisfy differential privacy in the presence of sensitive training data. The paper nearly missed the cut at AIStats - though this experience had me charged since it allowed me to recognize my zeal for theoretical research and also gave me a lot of confidence since I wrote/ ideated on the paper largely independently.\n\n\\_ Thereafter (Late '21-present), I joined as a research fellow at one of the top unis in my home country. I started working with a professor with adjacent research interests. I was presented with a problem to solve (entailed bringing about a non-trivial improvement in the convergence rate of a proposed algorithm), and after a fair amount of toiling, I managed to solve it. This was included as one of four algorithmic schemes, consolidated into a paper submitted at JMLR (under review). Of the four algorithms, the three other algorithms (or versions of them) had been previously presented by the co-authors as independent papers at venues like NeurIPS/ ICML in past years. This meant that even though I made a novel contribution to the paper, I was only made 4th/ 6 authors. This is fair, since the bulk of the work was inherited from previous papers - so by no means am I complaining. Just, I'm not sure a 4th author contribution is generally seen as significant - though I would imagine my letter writer would have made clear its significance in their letter. I'm currently working on another interesting problem in the realm of learning theory and hope to churn out substantial results in the next couple of months.\n\n\\_ If it means anything at all, my GRE scores are: 169Q, 166V, 5.0 AWA\n\nI was wondering if I could get some advice on things I could do going forward. I understand my GPAs don't stand out - though I was cautiously optimistic going into this cycle since the distribution of my undergrad scores and my research experience do reflect my proclivity for theory. Nonetheless, I understand that there are many stellar candidates that apply each year. So perhaps I need to apply to 'less ambitious' schools next time.\n\nBeyond that though, what can I do to improve my credentials? How many research papers are likely to offset my middling grades, and at what kind of research groups? I am absolutely willing to put in the hard yards and stretch myself over the next year. If nothing else, my experience thus far have certainly allowed me to identify research themes that excite me, and I certainly have a lot more focus/ conviction now than ever before.\n\nI did my due diligence whilst making applications, in the sense that I applied to a program only if I identified a strong match in research interests. I would be happy to share my SoP/ CV/ research sample with anyone willing to guide me. Thank you for reading!",
        "created_utc": 1676998172,
        "upvote_ratio": 1.0
    },
    {
        "title": "Paired t-test",
        "author": "TomatoTornado97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11873ik/paired_ttest/",
        "text": "Can the hypotheses for paired t-test be for example:\nh0: mean1 &lt;= mean2\nh1: mean1 &gt; mean2\n\nOr should it be:\n\nh0: mean(difference) &lt;= 0\nh1: mean(difference) &gt; 0\n\nBecause if paired t tests are only to find if theres a difference, would it be wrong to compare it to anything other than 0?",
        "created_utc": 1676994865,
        "upvote_ratio": 1.0
    },
    {
        "title": "Correlation with time series",
        "author": "younikorn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1184j6k/correlation_with_time_series/",
        "text": "It’s my first time working with time series data in R and I was wondering what the most appropriate statistical test would be  for my scenario;\n\nI have repeated continuous measurements (14 measurements throughout the year) in a patient cohort as outcome. \nI also have various continuous and categorical sociodemographic and lifestyle factors measured at baseline that i want to test for correlation with the outcome.\nI was wondering how i could correlate these baseline variables with the outcome trends. I’ve thought about adding a timepoint variable and correcting for timepoint and patient ID or using the AUC of the trends as single value outcome for each patient but i was wondering if there is a more appropriate way of tackling this.\n\nThanks in advance!",
        "created_utc": 1676989236,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the best/most difficult test I can use for a non-probability sample?",
        "author": "No_Procedure6855",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1181vhd/what_is_the_bestmost_difficult_test_i_can_use_for/",
        "text": "I’m an undergrad and my dissertation is an online survey which is collecting quantitative data about opinions on a topic, and how these opinions differ across various demographic variables. \n\nDue to this being an undergrad diss, it is practically impossible to collect a probability sample, so instead I have decided to use convenience sampling in order to recruit the most ppts possible.\n\nI’m aware that using a non-probability sample means that I cannot infer my findings to the population, but when analysing the data, is there anything I can use other than descriptive statistics? My professor said there is not much point doing any formal statistical tests I.e. chi-squared to measure association between variables (as it won’t represent the population) - but is there any tests that would be good for a study like this?\nI want to be as thorough as I can with the data I collect.\n\n(I will state that this study has its limitations and explain when I write up, this is only an undergrad diss so findings are not going to be world-changing information, but I want to make it as interesting/applicable/valid/reliable as possible in my analysis)\n\nThanks :)",
        "created_utc": 1676981057,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a primer on the change in mindset when it comes to doing statistics?",
        "author": "VastDragonfruit847",
        "url": "https://www.reddit.com/r/AskStatistics/comments/118162t/is_there_a_primer_on_the_change_in_mindset_when/",
        "text": "The very first Statistics book of my life is \"Statistical Rethinking: A course in Bayesian Statistics\". It has an excellent framework of how to go beyond the analytical bits. It is pretty introductory in nature and yet I feel there are some things that I should know to get more comfortable with the subject.  \n\n\nIt feels foreign as to how simple a \"model\" is and yet it just doesn't feel so right. Maybe it is because I have an engineering background. The author does say that there are many ways to build a bridge. I just wanted to know if there are some ground rules or a different mindset that I should have while going through the book and lectures.  For example, \"Hey in the stats world, we don't do X anymore, you gotta learn to it the Y way\" or \"You did in X way in CS but in Stats we do it the Y way\"?  \n\n\nIt could very well be that I am a little too early to worry about this - On chapter 4 and I'm yet to do linear regression.",
        "created_utc": 1676978533,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I calculate and display risk of an outcome by continuous predictor?",
        "author": "Humanplumber",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11811na/can_i_calculate_and_display_risk_of_an_outcome_by/",
        "text": "I am trying to identify risk factors for a specific outcome seen in some patients after having surgery (let’s call it ‘organ damage’). \n\nI have ran univariate tests and multiple logistic regression to find significant risk factors and determine odds ratios. \n\nOne or the significant risk factors is patient age. The older a patient is, the more likely they are to experience this outcome. \n\nIs there a way that I can calculate and graphically present ‘risk of organ dysfunction’ (Y axis) by age (X axis). \n\nThanks for any help you can provide!",
        "created_utc": 1676978086,
        "upvote_ratio": 1.0
    },
    {
        "title": "can i consider and analyze hours slept as categorical data?",
        "author": "sphindumb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11803j7/can_i_consider_and_analyze_hours_slept_as/",
        "text": "i have a survey about sleep quality and there is a question about hours slept and the choices are less than 6 hours, 7-9 hours, more than 9 hours, is it considered categorical or numerical?",
        "created_utc": 1676974451,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are the chances that a husband who cheats on his wife will marry his mistress?",
        "author": "Negative_Mushroom_69",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117yzo8/what_are_the_chances_that_a_husband_who_cheats_on/",
        "text": "",
        "created_utc": 1676970123,
        "upvote_ratio": 1.0
    },
    {
        "title": "Odds and probabilities - descriptive or predictive analytics?",
        "author": "euphoricrealm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117y3qm/odds_and_probabilities_descriptive_or_predictive/",
        "text": "Might be a silly question but not sure which it falls into. Not a homework question, putting notes together before my next study term.",
        "created_utc": 1676966707,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate probability without calculating the inverse probability first?",
        "author": "savvamadar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117viul/how_to_calculate_probability_without_calculating/",
        "text": "I’m going to use three examples to better illustrate the concept I’m trying to understand.\n\n1) I have a machine that never fails to output a wooden toy duck.\n\nWhat is the probability that a toy duck is not produced?\n\n2) I was convinced by a salesman to add an external piece that colors the toy duck but the addon has a 25% failure rate where the toy duck vanishes.\n\nWhat is the probability that a toy duck is not produced?\n\n3) the same sales man has come around and convinces me again to add another of his addons that will add a rope for pulling the toy duck but it too has a 25% failure rate where the toy duck vanishes.\n\nWhat is the probability that a toy duck is not produced?\n\nFor example 1 - I think everyone could answer without any calculation that the duck production failure rate is 0%.\n\nFor example 2 - anyone who understand percentages could answer that the duck production failure rate is 25%.\n\nFor example 3 - suddenly things become significantly harder. People will either say 50% or will need to calculate the success rate (0.75*0.75 = 0.5624) and then give the inverse of that (1-0.5625 = 0.4375).\n\nMy question is:\nFor the first two examples we don’t need to calculate the success rate to get the failure rate. In the third example I have no idea how someone would get the failure rate of 43.75% without first getting the success rate.\n\nIs it possible to derive the 43.75% without first finding the 56.25%?\n\nSorry if it’s a dumb question - thank you!",
        "created_utc": 1676957357,
        "upvote_ratio": 1.0
    },
    {
        "title": "am i doomed career wise? business analytics major stat minor",
        "author": "OilWorried41",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117ukky/am_i_doomed_career_wise_business_analytics_major/",
        "text": "hello! i have been stressing about this. in the business analytics major (it's a bachelor of science) we do some analytics and also learn python, and in the stats minor it's more normal stats classes (regression, probability, testing, design methods). i am at a school that has a lot of research opportunities. i also started learning R in my own time. all this is to say that i (ideally) want a more stats focused job instead of business analytics and i want to make sure i haven't doomed myself. i want to know if i can still get a more stats-y job with this lineup, or apply for a stats masters. is there anything i should do to give myself more experience and knowledge?",
        "created_utc": 1676954320,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why does the sum of u^2 become n*u?",
        "author": "priceless77",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117u6c3/why_does_the_sum_of_u2_become_nu/",
        "text": "Why does the 3rd term become n\\*u\\^2? I thought it would just be u\\^2 because u is a constant and the expected value of a constant is the constant. \n\nhttps://preview.redd.it/rf4jqkz1sgja1.png?width=1194&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5deaa2606eddf32ed02fbaa607bbc142cf39aa40",
        "created_utc": 1676953098,
        "upvote_ratio": 1.0
    },
    {
        "title": "Appropriate usage of coefficient of variation?",
        "author": "Delicious_Clerk2670",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117t4za/appropriate_usage_of_coefficient_of_variation/",
        "text": "I have 5 variables.\n\n2 are continuous measurements of force and 1 is a discrete count of occurrences of a characteristic. From what I've gathered, its appropriate to use the CV to compare the variation between these 3 groups.\n\nHowever, my last variable (relative length) is a little funky... To calculate relative length, we first measure the length of a feature of a material. We then measure the total length of the material. To calculate relative length, we divide the feature length by the total length of the material. This provides a scaled measure of the length of the feature relative to the length of the material.\n\nIs it appropriate to use the CV of relative length to compare the variability of relative length to force or my discrete variable? I am concerned that since it is already scaled, it might not be appropriate.",
        "created_utc": 1676949943,
        "upvote_ratio": 1.0
    },
    {
        "title": "In hypothesis testing with continuous probability distributions, how does it make sense to have the null hypothesis be an equality, seeing as the probability of any particular value will be precisely zero?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117qlyh/in_hypothesis_testing_with_continuous_probability/",
        "text": "I'm thinking specifically of z tests and t tests for population means.  Both the z and t distributions are continuous and in the context of continuous distributions it's not typically useful to ask about about the probability of a particular value, since it will always just be 0.  So then why is the null hypothesis in a standard z or t test for a mean typically of the form mu = constant?  \n\nI mean, I know the p value isn't the probability the null hypothesis is true, given the evidence, but rather, it's the probability of the evidence given the null hypothesis, or, as it's often stated, \"the probability of getting evidence at least as extreme as we did, given the null hypothesis\".   But it still seems suspect to me to have the null hypothesis be an equation.  I mean, the calculation of p assumes that the null is true and the entire purpose of the test is decide whether the evidence is strong enough to reject the null (although, to be fair, a single hypothesis test probably shouldn't be the sole factor in making any sort of important decision, but should instead be used in conjunction with other statistical tools).",
        "created_utc": 1676942586,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mplus - LCGA - model comparision lrt test",
        "author": "Busy-Kaleidoscope-74",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117qez6/mplus_lcga_model_comparision_lrt_test/",
        "text": " Hi, I ran different LCGA model in Mplus and I use the VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST to guide the choice of the best mode. Results are as follows: 2 trajectories better than 1 trajectory 3 trajectories not significantly better than 2 trajectories 4 trajectories better than 3 trajectories.  Basically, the test shows that a model with 4 trajectories fits the data better than a model with 3 trajectories. However, since the model with 3 trajectories does not fit the data better than a model with 2 trajectories, I think that comparing the model with 4 trajectories with the one with 3 is silly. I would like to compare the model with 4 trajectories with the one with 2.  Mplus does not automatically provide that test. I would like to know if anyone knows if it is possible to obtain a test for comparing model 2 Vs 4. I managed to compute the test by hand, but it is tedious.  Thanks for your help!",
        "created_utc": 1676942044,
        "upvote_ratio": 1.0
    },
    {
        "title": "Law Student here.",
        "author": "Pretty_Soft7294",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117pvbb/law_student_here/",
        "text": "In the case of Commonwealth v. Malone, the court stated that the Defendant had a 60% chance of shooting a bullet -- the court apparently assumed that the defendant twirled the revolver’s cylinder before each of the three times that he pulled the trigger (see footnote 1 of the opinion). Because it was a five-chambered gun, the chances were three out of five that one of those shots would fire the bullet -- is this correct????",
        "created_utc": 1676940523,
        "upvote_ratio": 1.0
    },
    {
        "title": "A quick (or maybe not so quick :) question about dynamic model",
        "author": "Agtwdr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117p54n/a_quick_or_maybe_not_so_quick_question_about/",
        "text": "Dynamic model is new to me. Do we have any dynamic model expert here? If yes, can you let me know if some of the IVs for my model are continuous variables but, say over the course of 2 years (104 weeks), the IVs have data for some weeks (with large weekly variance) and have no data at all for other week, what can be done to apply dynamic model to these IVs? Or dynamic model isn’t the right model for this kind of IVs at all? \n\nThanks in advance!",
        "created_utc": 1676938511,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to use \"greater\" or \"less\" command in Wilcoxon test in R",
        "author": "mstferkaya",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117oxg4/how_to_use_greater_or_less_command_in_wilcoxon/",
        "text": "I'm kind of confused about how to use these codes, since there are not many examples asking directions of tests.\n\n&amp;#x200B;\n\nMy hypothesis is this:\n\nFor Gully Cats, the average territory size at location B is greater than at location A.\n\nhttps://preview.redd.it/gbgn75ednfja1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4d91047f79e8dafcde823cc35a578c9d44158491\n\nAnd I have a dataset for which parametric assumptions are not met. So I'm using Wilcoxon test.\n\n&amp;#x200B;\n\n[LocA stands for Location A, locB stands for Location B.](https://preview.redd.it/xa28i1mfnfja1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=585747a310fe44faa67fb844d22d870d6b3d44fb)\n\nI don't know which code is appropriate for my study. I think the second one is the right code, but it gives me a higher p value. Can you help me?",
        "created_utc": 1676937947,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you guys report a linear mixed effects model with a binary link function?",
        "author": "investm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117ory2/how_do_you_guys_report_a_linear_mixed_effects/",
        "text": "Hello,\n\nI’m in psychology and need to report a linear mixed effects model. From what I’ve read there isn’t much consistency on how to report the mixed effects model and there is even less information on how to do do when you have a binary link function. Does anyone have any experience reporting these? Of course, I should mention the fixed effects/random effects and if my mod is a slower, intercept or both model.\n\nThank you",
        "created_utc": 1676937556,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would the Linear by Linear association test be appropriate for assessing whether there is a significant difference in proportion of time (ie. 2000-2010).",
        "author": "imreadytolearn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117nw5w/would_the_linear_by_linear_association_test_be/",
        "text": "I only use SPSS and know that the Cochrane-Amirage is the best test, but cannot be performed on SPSS. Was just learning about the linear-by-linear association on crosstabs of SPSS and was hoping to know if 1) this is similar to Cochrane-Amirage 2) can be used to assess if there is a difference in proportion over a period of time.",
        "created_utc": 1676935242,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would you use hypothesis testing to detect anomalies in data?",
        "author": "johndatavizwiz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117mh4r/would_you_use_hypothesis_testing_to_detect/",
        "text": "",
        "created_utc": 1676931804,
        "upvote_ratio": 1.0
    },
    {
        "title": "Under Bayesian statistics, what is my estimate of the true proportion of heads under three scenarios…",
        "author": "ragold",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117jctd/under_bayesian_statistics_what_is_my_estimate_of/",
        "text": "1) I flipped an alien coin from another dimension 10 times a week ago and recorded 2 heads. Presently I flip an alien coin from another dimension 5 times and get 3 heads. What is my estimate of the true proportion of heads?\n\n2) Most people have told me alien coins from another dimension (ACFAD) come up heads 50% of the time but some say it’s actually 60% but those people seem a little cuckoo. Presently I flip an ACFAD 5 times and it comes up heads 3 times. What is my estimate of the true proportion of heads?\n\n3) A combination of 1 and 2s priors — the word of mouth probabilities and the 10 flips from a week ago. Presently I flip an ACFAD 5 times and it comes up heads 3 times. What is my estimate of the true proportion of heads?\n\nThanks!",
        "created_utc": 1676924350,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing Cumulative Incidence Curves",
        "author": "UroJetFanClub",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117iupt/comparing_cumulative_incidence_curves/",
        "text": "I have a dataset of 1000 patients and I was analyzing cancer specific outcomes and ultimately realized that in fact most patients were dying from other causes rather than the cancer itself (which is an interesting finding itself). I plotted out the cumulative incidence curves for both deaths from cancer and deaths from other causes and subjectively to appear very different. Is there a statistical test to compare those curves?\n\nIn my reading about competing risks, I’ve come across Fine and Gray models as well as cause specific hazards, but those seemingly focus on evaluating the impact of covariates on the risk of death. \n\nThanks",
        "created_utc": 1676923196,
        "upvote_ratio": 1.0
    },
    {
        "title": "If I have a bunch of 1000 sided dice. How many dice should I use so that when I roll them all at once I get exactly one 4. And what is the chance of getting exactly one 4 when throwing this proper amount of dice.",
        "author": "NIGHTMARESP00N",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117ie17/if_i_have_a_bunch_of_1000_sided_dice_how_many/",
        "text": "Where is the sweet spot. Not enough dice and it wont get a 4, too many dice and I will get more than one 4.",
        "created_utc": 1676922085,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Correlation between multiple variables over time",
        "author": "mhdubs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117ghbl/q_correlation_between_multiple_variables_over_time/",
        "text": "What is the best way to investigate correlation of two (or more) variables over time?\n\nLet's say I have a cohort of patients with disease x, all with different dates of diagnosis and blood tests taken at different times (from which the variables are drawn from). Presumably I'd need to correct for time scale too, which seems like a gargantuan task. I'm trying to illustrate all of my data in a coherent way... any ideas?",
        "created_utc": 1676917661,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help me interpret the statistical significance test in this Cox proportional hazards model survival analysis example?",
        "author": "Ridyot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117flex/help_me_interpret_the_statistical_significance/",
        "text": "I generally seem to have a mental block in interpreting statistical significance tests especially when it comes to p-values. I'm studying survival analysis and am focusing on the Cox Proportional-Hazards Model. I'm going through the examples in [http://www.sthda.com/english/wiki/cox-proportional-hazards-model](http://www.sthda.com/english/wiki/cox-proportional-hazards-model) and in their example for univariate Cox regression, as redacted in the image below, they assert that the ph.karno variable is not statistically significant, while the age/sex/ph.ecog variables are statistically significant, I guess the null hypothesis being that the variable is not associated with the probability of survival. How does one conclude that ph.karno is not significant from the below? Here's the redaction:\n\nhttps://preview.redd.it/d3ohiw9wrdja1.png?width=671&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e8ff5c7298645aa374f1e859957eb4fbe7425bf7",
        "created_utc": 1676915651,
        "upvote_ratio": 1.0
    },
    {
        "title": "Different test statistic from two identical Wilcox tests",
        "author": "Dr_Devilish",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117c8g3/different_test_statistic_from_two_identical/",
        "text": "Hi,  \n\n\nWhy does the value of the test statistic (W) change depending on which group is entered first in the wilcox.test function in R? See picture:  \n\n\nThanks in advance\n\n \n\nhttps://preview.redd.it/7z5obaqiddja1.png?width=1920&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=92f8e185327099fa18fdac5ba9f55f274aa782f6",
        "created_utc": 1676910255,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any recommendations for a good textbook on bayesian networks?",
        "author": "rawrpandasaur",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117bhtc/any_recommendations_for_a_good_textbook_on/",
        "text": "Hello stats nerds!\n\nI'll soon be starting my final dissertation chapter using bayesian networks for ecological risk assessments. Do you have any suggestions for good textbooks in this field?\n\nThank you!",
        "created_utc": 1676909249,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multilevel models: individual vs aggregated outcome variable with one-observation-per-participant fixed effects",
        "author": "MultilevelConfused",
        "url": "https://www.reddit.com/r/AskStatistics/comments/117a8a7/multilevel_models_individual_vs_aggregated/",
        "text": "Hello, I'm new here so please be kind! I couldn't find a similar post on here but if you know of one, feel free to point me that way!\n\n**Basically my question is - how do multilevel/mixed models estimate the effect of a fixed effect with one observation per participant on an outcome variable with multiple observations per participant? And how does this differ from aggregating the outcome variable?**\n\nTo illustrate, I'm confused about the difference in results between the following models:\n\n(1) Speed \\~ TopicKnowledge + Motivation + (1 | Participant ID) + (1 | Text ID)\n\n(2) Average\\_Speed \\~ TopicKnowledge + Motivation + (1 | Text ID)\n\nin which 'Speed' is the speed of reading a paragraph of text in the experiment, with as many observations per participant as there are paragraphs (the number of paragraphs varies between different texts, each participant read only one text). 'Average\\_Speed' is each participants' average speed of reading a paragraph.\n\nI'm trying to model reading speed by topic knowledge and motivation, both of which are obtained via questionnaires and I have one score of each for each participant.\n\n(1 | Participant ID) and (1 | Text ID) are the random intercepts. Participants read different texts, participant id is just a participant indicator.\n\nHow do the models estimate the effect of topic knowledge and motivation in each of these cases? Does the first model somehow aggregate 'Speed' across participants to estimate the effect of TopicKnowledge and Motivation? If so, why do the models result in different results (based on coefficients of TopicKnowledge and Motivation)?\n\nContext - I'm trying to use multilevel models for analysis as part of my psychology PhD. I only have a minor mathematics background, mostly based on psychology stats classes.\n\nThank you so much if you take the time to respond!!",
        "created_utc": 1676907513,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it better to buy one 20$ scratchcard lottery ticket or twenty 1$ ones?",
        "author": "Remarkable_Lie_5060",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1173i02/is_it_better_to_buy_one_20_scratchcard_lottery/",
        "text": " \n\nMe and my friend have a dispute. Please explain me your calculations:\n\n\\-the 20$ one has 1/12,5 chance of winning 50$, and 1 in 10.080.000 of winning the maximum prize of 5 mil\n\n\\-the 1$ one has 1/14,56 chanceof winning 3$ and 1 in 4320000 of winning the maximum prize of 10 k\n\nThe chances of winning the exact amount of the play aren't available, but they are obviously greater than the ones of winning 50$ or 3$ which I wrote above",
        "created_utc": 1676889347,
        "upvote_ratio": 1.0
    },
    {
        "title": "What test to use for a general up/downwards trend from a collectible?",
        "author": "TywinASOIAF",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1173btr/what_test_to_use_for_a_general_updownwards_trend/",
        "text": "Suppose we have data about X collectibles.  Of those collectibles, we know the daily lowest sale price and amount left since the release date. With that data I want to know if there is a relationship between sale price and amount left over the time period from release date to now. How should I approach this?",
        "created_utc": 1676888701,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you determine if enough people were surveyed to make a conclusion about a sub group?",
        "author": "codeyCode",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1171yx1/how_do_you_determine_if_enough_people_were/",
        "text": "If  someone conducted a nationwide survey of a random group and you wanted  to look at a specific sub-group of the larger group of people polled,  how would you determine if enough people from that group participated in  the survey?\n\nFor example, say  there is a survey of thousands of Americans on their living situations,  and you want to look at a certain race (or gender, or socioeconomic  class) and determine the average rent the people in that specific group  pay in rent, compared to everyone else.\n\nHow would you be able to know if enough people of that group participated in the survey?",
        "created_utc": 1676883297,
        "upvote_ratio": 1.0
    },
    {
        "title": "Something I never understood about Bayesian statistics … are priors a posteriori?",
        "author": "ragold",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1171tfu/something_i_never_understood_about_bayesian/",
        "text": "For instance, where do expectations about the distribution of heads in a series of coin flip come from? Observation. Then why are they called priors as if they are derived outside observation?",
        "created_utc": 1676882677,
        "upvote_ratio": 1.0
    },
    {
        "title": "Will we always get statistical significance/ a lower p value with larger and larger sample sizes?",
        "author": "AstralWolfer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1171t9y/will_we_always_get_statistical_significance_a/",
        "text": "Imagine I’m doing a double blind RCT to find out if a treatment is working or not. The gist of it is the treatment group is also receiving the placebo pill as the control group. My contention is that as you increase the sample size to arbitrarily high enough number, you will always achieve statistical significance (p&lt;0.05). Am I right or wrong?\n\nThe idea is that if the effect size is even somewhat off of zero (0.000005) instead of (0.000000000…), the confidence interval lower tail can get short enough such that it doesn’t cross the line of non-signficance",
        "created_utc": 1676882655,
        "upvote_ratio": 1.0
    },
    {
        "title": "Good method to get accurate derivative from a time series that is cut off at too few decimal places?",
        "author": "Opus_723",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1170ad0/good_method_to_get_accurate_derivative_from_a/",
        "text": "I have a data set consisting of time series for the positions of many objects. Due to an oversight, this data was stored with only three decimal places to save disk space.\n\nThe data looks a touch janky if you zoom in due to the truncation but you can very easily eyeball a smooth curve through it.\n\nMy problem is that I need a decent numerical derivative of this data. A simple low order finite difference won't cut it with the truncations. \n\nIt seems very feasible by eye, as the errors are quite small relatively, but I am not a statistician and I was just wondering what technique people with that expertise would recommend in this kind of situation. I know there are tons of different smoothing methods and it's easy to get decision paralysis so I just wanted some guidance as to a starting point that would be appropriate.\n\nI was also curious if knowing that the error in question is only due to truncation gives me any kind of advantage I can leverage compared to data that is just \"noisy\".\n\nThanks!",
        "created_utc": 1676876707,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you keep learning and sharpen your stats knowledge?",
        "author": "Markov_Chain8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116wdoe/how_do_you_keep_learning_and_sharpen_your_stats/",
        "text": " Hello, community. \n\nI'm an Actuary graduate, thus, my background in statistics is mostly theoretical. Most of the statistical methods I reviewed at school were the basics: classical Linear Models and GLMs, classical Time Series Analysis, Classical Survival Analysis and Insurance Statistics. I consider myself a self-taught person and have never had any problem trying to plunge deeper into advanced methods up until now when I want to start learning Mixed Models. Furthermore, I not only would like to learn about them but also about many other useful models, prioritizing their applications to real-world problems. \n\nI was pondering over studying a master, but unfortunately in Mexico, I haven't found any that meets my interest. What would be your suggestions for me to start learning advanced statistics considering I prefer books whose main core are **applications** that go briefly over the theoretical part? Books, Internet Resources, Master degrees, Youtube videos, etc. \n\nThanks!  \n\n\nPd: Nowadays I work at a Media Agency. Marketing Mix Modelling books or resources would be of much help!",
        "created_utc": 1676863237,
        "upvote_ratio": 1.0
    },
    {
        "title": "Basic question: normal distribution",
        "author": "dandruff-free",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116ugvl/basic_question_normal_distribution/",
        "text": "I'm trying to compare two groups risk factor profile.\n\nThe only continuous variable that I have in my data set is age. Sample size is 230.\n\nTo check for normal distribution with age, can I do Shapiro-Wilks with the both groups' ages combined or do I have to do Sharipo-wilks test on each group?",
        "created_utc": 1676857445,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bayesian vs frequentist interpretation",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116u9qn/bayesian_vs_frequentist_interpretation/",
        "text": "What are some advantages (if there are any) of working in a bayesian framework in regards to probability and machine learning as a pose to the classical frequentist framework/ interpretation. Are they actually the same thing? Is one better than the other for machine learning? Personal preferences and why does Bayesian thinking seem to be making a comeback in popularity amongst statisticians and data scientists? Any thoughts and opinions are appreciated.",
        "created_utc": 1676856859,
        "upvote_ratio": 1.0
    },
    {
        "title": "What Stats Test to Use When You Have Multiple Continuous and Categorical Independent and Dependent Variables",
        "author": "ashbringer30",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116s29l/what_stats_test_to_use_when_you_have_multiple/",
        "text": "Hi,\n\nI'm trying to do a stats project on the effect of NFL bye weeks on team performance. I am trying to determine what statistical test to run and I've identified my independent and dependent variables as well as some confounding variables from my dataset. \n\nIndependent: \n\nBye Week (Y/N) \n\n&amp;#x200B;\n\nOther Important Factors That  Go Into Predicting the Dependent Variable Beyond Bye Weeks: \n\nWin % (Ex. 0.4)\n\nOpponent Win % (Ex. 0.6)\n\nHome Field Advantage (Y/N) \n\n&amp;#x200B;\n\nDependent: \n\nYards Scored (Ex. 400)\n\nYards Allowed (Ex. 200) \n\nPoints Scored (Ex. 14) \n\nPoints Allowed (Ex. 7)\n\nResult (W/L) \n\n&amp;#x200B;\n\nIs there a test I can run that takes into account all of these factors? Or do I need to reframe my analysis in some way? Any advice would be appreciated.",
        "created_utc": 1676850957,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are there other alternatives for time-series forecasting when you have much more data behind the simple curve that you're trying to forecast from?",
        "author": "Ridyot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116loju/are_there_other_alternatives_for_timeseries/",
        "text": "I'm studying time-series forecasting methods in R. The data I am working with shows transitions to dead state \"X\" over time; plotted, the data expressed as number of units transitioning on the y-axis and time t+1 on the x-axis, the curve is exponential decay; and expressed as number of units transitioning divided by the starting number of units at time 0 (a cumulative percentage death rate) on the y-axis and time t+1 on the x-axis, the curve shows a logarithmic function as shown in the left plot below. I'm exploring R fable package models for simple forecasting methods (mean, naïve , naïve with drift), exponential smoothing (ETS), and ARIMA models. I'm getting the best results and fits with ETS with damping (Holt's linear method). However the curve I'm working with is very simple and summarized (left plot below), which simply shows cumulative average transitions each month.\n\nIs there a better methodology or models to work with for time-series forecasting, when you have much more information available?\n\nFor example, I can calculate standard deviations for the cumulative transition rates (right plot below), I can draw a histogram of transition rates showing a binomial distribution (a unit either dies or survives), and I have other data for each unit and for each period showing strong correlations with eventual death such as Stage I (some indicia of future dead state X) through Stage IV (almost all end up in dead state X). I'm wondering if there are other time-series forecasting methods that can put more of this other information to work for better forecasting and better simulations.\n\nhttps://preview.redd.it/o5dt91tu57ja1.png?width=1523&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=435900d25a55932b99b408febb46b9c16b0fe13d",
        "created_utc": 1676835075,
        "upvote_ratio": 1.0
    },
    {
        "title": "Inconclusive predictive study on CV?",
        "author": "Expelliarmus30",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116lgcq/inconclusive_predictive_study_on_cv/",
        "text": " \n\nI have been part of a feasibility study that looked at conducting a trial on take home naloxone. Another phase of the study was to create a predictive algorithm to identify opioid overdose users from general population using routine data. However, we couldn't get the results we were looking for and hence concluded that one cant predict people at risk with only routine data in the our study population.\n\nMy question here is, should such inconclusive studies also be added onto my resume? I was responsible for the analysis, so I cleaned, managed the datasets and ran the necessary analytical tests, so there were definitely some skills that I learnt (It was my first project). I am just unsure if I can say I actually created a predictive algorithm but I didn't because it was not feasible",
        "created_utc": 1676834525,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Tests for proportions of a categorical variable between three groups after a kruskal-wallis test",
        "author": "Goddry",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116jpvk/q_tests_for_proportions_of_a_categorical_variable/",
        "text": "The data has just two variables:\ntreatment (3 groups) and outcome (3 different outcomes: empty, half-full and full; ordinal scale).\nThe proportions of the outcomes are already visually different between the groups and a kruskal-wallis test confirms it.\n\nMy question is: what are the post-hoc tests I can do?\nIdeally I'd like to test which of the outcomes differ between the three groups. Do I now just test between any two groups for each of the three outcomes and correct for the multiple testing?",
        "created_utc": 1676830348,
        "upvote_ratio": 1.0
    },
    {
        "title": "LASSO BASSO is freaking confusing!",
        "author": "KingKyrgios11",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116jee8/lasso_basso_is_freaking_confusing/",
        "text": "Hello,   \n\n**I am trying to determine which factors predict patient outcome.** The outcome is 0 = discharged from the hospital alive; 1 = died at the hospital).  So this is binary.  I have 18 possible predictors (plus the intercept).  I use lasso (glmnet package in R).  I followed the R protocol here: [https://www.statology.org/lasso-regression-in-r/](https://www.statology.org/lasso-regression-in-r/) \n\nSo the lasso results are attached as a picture (**Fig 1**) , and I'm trying to understand them.  I think there are 8 predictors plus the intercept that is important, and these 8 predictors result in the lowest mean-squared error, **correct?**\n\n1) **What is the s0 column? Are these the actual coefficients?**\n\n2) The R2- of the training data is reported as 0.74. The best lambda is 0.0129. **What does this tell me?**\n\n3) I decided to then run a binary logistic regression with these eight predictors and I get the following (attached pic, **Fig 2**.). You can see that one of those 8 predictors (LYMPH) is highly NOT significant, and then there are three other predictors that are barely not significant (ADMISSION\\_WARD p =0.10; HOSPITAL\\_STAY p =0.09; dNLR = 0.08).  **Why did the LASSO model include these 4 predictors and the binary logistic regression coefficients show that these 4 are not statistically significant?**\n\n4) Also, **Why are the coefficients for the LASSO model different from the coefficients of the binary logistic equation model?** I'm confused.\n\n5) OK, I will then perform model selection based on BIC, starting off with the original 8 predictors, and then 7 predictors, and then 6 predictors, including global (18 predictors) and null model etc...From the attached **Fig 3**.  Model 3 (k = 7) is the best model as the BIC is the lowest. Age, Pulse oximetry, mechanical vent, days of hospital stay, log of platelets, log of dNLR and intercept.  So I think I will choose model c with 6 predictors + intercept as the best model that predicts outcome.  **Am I doing this right?** \n\n6) If The Hosmer-Lemeshow p-value for the model  = 0.44, the goodness of fit is acceptable, **correct**?\n\n7) The nagelkerke pseudo r2 = 0.84 for the 6 predictors plus intercept. **Why is this R2 different from the R2 of 0.74 from the training data from lasso?**\n\n8) In the binary logistic model with (6 predictors plus an intercept, or k = 7, i.e. model c), every predictor coefficient is statistically significant except for SPO2\\_ADMISSION (p = 0.077). **Why did the model include this predictor if the p-value is &gt; 0.05?**\n\nThank you so much cool cats.",
        "created_utc": 1676829595,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is this sort of \"graded probability\" concept called?",
        "author": "RedditChenjesu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116j9uv/what_is_this_sort_of_graded_probability_concept/",
        "text": "I'm wondering if anyone can provide a reference for this concept.\n\n&amp;#x200B;\n\nLet's say you have a circle in the x-y plane that starts small, and then it grows ever larger, representing all possible vectors of a given magnitude.\n\n&amp;#x200B;\n\nWell, now let's add a statistical element to it. Let's say as the circle grows, starting from small to big, that your probability of drawing a particular set of points or magnitudes from that circle starts small, then grows, then decays after that, with a maximum probability occurring at a specific radius.\n\nSo at radius 0, the \"probability\" is 0. At radius 1 the probability is 0.25, at radius 2 the probability is 0.5, at radius 3 the probability is 0 .2, at radius 4 the probability is 0.1, at radius 5 the probability is 0.05 and so on, decaying towards 0 as you tend towards an infinite radius.\n\n&amp;#x200B;\n\nSame for a growing sphere. Let's say you have a sphere in 3D space that starts infinitely small at the origin, and your probability of drawing a certain set of points from it is also 0. Then let's image the sphere starts growing and growing and growing, and as it grows, the probability of drawing certain points from the sphere grows,but then decays. It decays even as the sphere grows ever larger.\n\n&amp;#x200B;\n\nWhat is this sort of graded probability as a sphere interpolates called? What can I research in reference to this concept?",
        "created_utc": 1676829280,
        "upvote_ratio": 1.0
    },
    {
        "title": "Determining weighted variable",
        "author": "Creative_Tangelo_434",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116hr7o/determining_weighted_variable/",
        "text": " I'm in intro to statistics as a pre-requisite for my nursing bachelors. We recently covered weighted averages, and I understand the math, but only once I'm told which variable is the weighted and which is the value. I asked my prof and he simply told me that \"The weight will either be the actual weight of whatever the problem is detailing (like the nut example) or the credit amount of the courses. The weight of a course is always how many credits it is.\"\n\nThis might be a helpful answer for the weighted average question on the exam (which is about GPA), but not so much if I have to do this again in the future. Healthcare does a fair amount of statistical review, so I don't think that running into weighted averages is unlikely.\n\nDoes anyone have an actual explanation on how I can determine which variable is which?",
        "created_utc": 1676825657,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I study the effect of a \"summary variable\" on Y?",
        "author": "Porlamar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116hhld/how_do_i_study_the_effect_of_a_summary_variable/",
        "text": "In my study, I would like to study the effect of an indicator on Y. Let's say the indicator is a value from -1 to 1 that says how likely you are to be left wing or right wing. The indicator Y is something like mortality/ obesity whatever. I established that there is a clear link (association) between those two variables. Now, I make the hypothesis that the indicator is a summary variable that is determined by other factors such as religiosity, wealth, etc. I also have data for those sub variables. How do I show the importance of those subvariables on the indicator and on the variable Y? A possible conclusion could be something like \"wealth plays an important role in being left or right wing, and has an effect on one's mortality\".\n\nMaybe this is a wrong approach, as I could study directly the effect of wealth on Y. But I would like to involve that summary indicator. \n\nThanks for your help.",
        "created_utc": 1676825064,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone further break down how to transition from \"at least 1\" to \"at least 2,..., at least n...\"?",
        "author": "RedditChenjesu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/116h5jq/can_someone_further_break_down_how_to_transition/",
        "text": "I ask a question similar to this before but I didn't quite grasp the responses.\n\nSometimes our teacher asks \"what is the probability that at least one...\" but I'd like to know a generalization of this, \"what is the probability at least &lt;insert any integer&gt;...\".  \n\n\nAs one example, let's consider 20 students taking a test with 4 possible answers. The probability of getting any particular answer correct is 1/4 ( in the theoretical case). Then we ask \"What is the probability at least one student got the question correct?\", to which I might guess\n\nP(at least one) = 1 - P(none) = 1 - (1/4)\\^(20). Is that correct?  \n\n\nIf that is correct, how can this be generalized to \n\n\"What is the probability at least two students got this question correct?\"\n\nand \n\n\"What is the probability at least &lt;n&gt; (out of 20) students got this question correct?\"",
        "created_utc": 1676824385,
        "upvote_ratio": 1.0
    },
    {
        "title": "E(m) = 12 | p = m/n = 0.06 | E(x) = 200 * 0.06 , is that the right way to Show that its unbiased?",
        "author": "ayhancolak",
        "url": "https://i.redd.it/p6ftxmeeo7ja1.png",
        "text": "",
        "created_utc": 1676823239,
        "upvote_ratio": 1.0
    },
    {
        "title": "Null Hypothesis test for series of weight measurements over time",
        "author": "wisequackisback",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1169wsv/null_hypothesis_test_for_series_of_weight/",
        "text": "I have a series of weight measurements (daily but note: some days missing) which are subject to variation, and am wanting to tell as quickly as possible whether the overall trend is increasing, decreasing, or staying the same. My first instinct was to use a t-test (on the first half of measurements vs the second half), but on reflection I think the requirement that samples be identically distributed is violated here - the whole question is whether or not the distribution is changing over time!\n\nIs there a statistical test I can use to get the probability that the observed weights were drawn from the same distribution, i.e., that that true weight underneath is flat-lined?",
        "created_utc": 1676812156,
        "upvote_ratio": 1.0
    },
    {
        "title": "What method should I use for a dunn test?",
        "author": "BritishMan5",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1166ppe/what_method_should_i_use_for_a_dunn_test/",
        "text": " I've found like 8 different types of methods that give different results, should I use the method that gives me more adjusted p-values &lt;0.05 or a method that gives me more &gt;0.05? \n\nI need to find out what the significant pairwise differences are from four areas (Area A, Area B, Area C, and Area D).\n\nPrior to this, I had done a Kruskall-Wallis test to see if there is significant difference of bird wing size within the four areas, which there is (&lt;0.05).\n\nThis is all done on R.",
        "created_utc": 1676800561,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a methodology for forecasting a time-series that takes the form of a logarithmic function curve that can't decrease over time nor exceed a specified value?",
        "author": "Ridyot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1166fob/is_there_a_methodology_for_forecasting_a/",
        "text": "I have 32 months of data, and I'm trying out different models for testing the forecasting of cumulative monthly unit transitions to dead state \"X\" during months 13-32 as a percentage of the total population of units at time 0, by training from data for months 1-12. The curve plot is the inverse of exponential decay; it's a logarithmic curve that climbs up and can never exceed 100% and can only climb up or flatten out; it can't curl down over time since the dead never return. Not all beginning units die off, only a portion. \n\nIn the below image I plot out simulated paths for months 13-32 using the \"naïve\" (NAIVE) forecasting function from the fable time-series package in R without any interval constraints applied nor transformations. The heavy black line for months 13-32 represents actual data for those months, the fine colored lines are simulation paths. I need a model or transformation that prevents a simulation path from ever curving down, and that prevents a simulation path from exceeding a value of 1 (100%). Please, any recommendations for a time-series model approach or transformation for addressing this very real-world example?\n\nhttps://preview.redd.it/vrz7s5i664ja1.png?width=750&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=20cbd836c2c430d4756fc904119ef621fa020048",
        "created_utc": 1676799463,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculated expected number of trials with multiple desired outcomes?",
        "author": "Flutter_Mane",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1163gx1/how_to_calculated_expected_number_of_trials_with/",
        "text": "Not sure what the correct terminology for this is haha.\n\nSo for example if I wanted to find out how many times I need to roll a dice to roll all 6 numbers on average, I can solve this intuitively. First roll will always produce a new outcome (1), then it would take 1.2 rolls to get the second, 1.5 for the third etc. yielding 1 + 1.2 + 1.5 + 2 + 3 + 6 = 14.7 rolls.\n\nHowever, I am unsure how to do this when the outcomes aren't equally weighted. Like for example if I was to say the odds of rolling each number were:\n\n1 - 10%\n\n2 - 30%\n\n3 - 20%\n\n4 - 20%\n\n5 - 15%\n\n6 - 5%\n\nI am unsure how to approach this (I could technically solve it with an exhaustive tree diagram of outcomes - but the actual question I want to answer is much more complicated than a dice).\n\nWhat is the best way to tackle these problems?",
        "created_utc": 1676787839,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multiple Comparisons",
        "author": "ConstantCheesecake37",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11611af/multiple_comparisons/",
        "text": "Hello all, I wanted to understand multiple comparisons better. I have a study that has 4 groups based on BMI (underweight, normal, overweight, obese), with 4 different time-to-event outcomes (death, hypertension, diabetes, stroke). I included multiple different tests (ANOVA/chi-square, multivariate time-to-event model). My questions are, how does one calculate the number of comparison groups for this study (for Bonferroni test)? Is it based on the number of groups, outcomes, or both? Also, do you use this adjusted p-value throughout the study, or just for certain tests (ANOVA) - as the multivariable model has many variables (it is based on the entire cohort and not stratified by any group), some with multiple groups, it does not seem logical to use an adjusted p-value built on the grouping of one variable (BMI) to determine the significance level of the grouping of another covariable (gender). Thanks!",
        "created_utc": 1676779402,
        "upvote_ratio": 1.0
    },
    {
        "title": "Am I right to use z-scores and hypothesis test this way to check if a hardware event was an anomaly?",
        "author": "johndatavizwiz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115vg11/am_i_right_to_use_zscores_and_hypothesis_test/",
        "text": "Hey everyone, I bet you know better if this is the right approach:\n\nBackground: I have hardware logs from two systems. First one tells me about a complete crash. Second one monitors devices connected to the first one, monitors traffic, and tells me about an error occurrence in this system in form of an \"error\" event timestamp. I know that one Thursday in May last year the first system had two crashes in one day. I checked the second system logs from that day and I saw unusual error timestamp error - usually I have about 22 error events during an hour, but for an hour before the crash of the first system i had more than twice that much, about 54.\n\nI want to know if this was a significant anomaly in statistical sense - like how little probability is that this was just a regular bump in data and is this a good idea to monitor my second system for such bumps to discover crashes earlier. Is the number of events an hour before the crash significantly big?\n\nMy idea was to (in steps):\n\n1.  Compute a bootstrap sample from the second system from days when there was no crash in the first system. For example, if the crash was on Thursday at 15pm and I noticed increased number of error events for an hour before the crash - then I would get the data from each other day between 14pm - 15pm, join them in one long list, then resample them 5000 times and with each resample calculate the average number of error events. I would then get the bootstrap distribution of the non-anomalous behavior. Am I right here? would this be a right way to calculate the bootstrap dist?\n\n2.  Calculate the z-score for the anomalous window: I would take the average number of error events from the bootstrap distribution, subtract from it the average number of events from the anomalous 1 hour window and divide it by the standard deviation of the bootstrap distribution?\n\n3. Here's my problem: I understand that then I need to actually formulate my hypotheses. Which one of those two ideas is correct: \n\n\t1. H0: the average of errors in 1 hour window between 14pm and 15 pm is the same as the average of the bootstrap distribution and equals 25 (the same as the mean of the bootstrap dist) H1:  the average of errors in 1 hour window between 14pm and 15 pm is bigger than the  average of the bootstrap distribution and equals 54 (the same as the found anomaly)\n\n\t2. H0: the usual average of errors (from bootstrap) is less than the anomalous average of errors, so X&lt; 54 H1: the usual average of errors (from bootstrap) is equal to the  the anomalous average of errors, so X= 54\n\n4. Then I think I should choose the alpha and calculate the p-values. I'm a natural pessimist, so I'm choosing alpha=0.05. Also, since this is a right-tailed test (option 1 from previous point) I'm calculating: 1 - norm.cdf(z\\_score, loc=0, scale=1).\n\n5. I'm assesing if p-value is less/equal to alpha (p\\_value &lt;= alpha) and if it is, then I reject the null hypothesis\n\n6. Should I also check confidence intervals? would they be more useful here? Maybe it's a better idea to use the quantiles of the bootstrap distribution and check if my p-value is in their range?\n\n&amp;#x200B;\n\nPlease help me get my head around this.",
        "created_utc": 1676763105,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the distribution of averages always a bell curve?",
        "author": "RedditChenjesu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115v0ej/is_the_distribution_of_averages_always_a_bell/",
        "text": "For instance, lets consider a very up and down function, like cosine(x) for x from -10 to 10.\n\nIs the distribution of averages of samples taken from this function a bell curve?  \nIf yes, then more generally, is the distribution of averages over any probability distribution always a bell curve?",
        "created_utc": 1676761943,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sufficient Statistics Factorization Theorem Intuition",
        "author": "oomydoomy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115ucl6/sufficient_statistics_factorization_theorem/",
        "text": "Could anyone explain the intuition behind the factorization theorem, which is used to find a sufficient statistic for a parameter? I understand how to apply it, but can't quite wrap my head around why it works.",
        "created_utc": 1676760213,
        "upvote_ratio": 1.0
    },
    {
        "title": "Covariance with missing values",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115tqw7/covariance_with_missing_values/",
        "text": "How should I calculate the covariance between two variables if there is missing data at certain locations for both variables. Are they completely ignored or factored into the formula some way?",
        "created_utc": 1676758658,
        "upvote_ratio": 1.0
    },
    {
        "title": "When does one consider alternative \"sizes\"?",
        "author": "RedditChenjesu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115s39c/when_does_one_consider_alternative_sizes/",
        "text": "For calculating the standard deviation, why does one rely on the sum of squares instead of other metrics of distance, like for instance, the fourth root of the sum of powers of 4 or the logarithm of a sum of exponentials and so on? What's so important specifically about squares and square roots?",
        "created_utc": 1676755436,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does a uniform prior distribution necessarily mean that the posterior is uniform in approximate Bayesian computation?",
        "author": "mdk9000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115kelt/does_a_uniform_prior_distribution_necessarily/",
        "text": "Hi everyone,\n\nI have a small side project at work where I'm exploring ways to fit structural models to light microscopy data. I'm currently looking into the feasibility of using approximate Bayesian computation (ABC) to get posterior distributions over the models' parameters. For a set of parameter values, I can generate simulated images and compare them to real images from the microscope because the physics of image formation is well understood. ABC appears appropriate because, from my understanding, it's useful when you can't explicitly write down a likelihood function. In my case, I don't know how to construct a likelihood from the real and simulated images, but I do know several ways to compute a distance measure between two images. ABC requires such a distance measure.\n\nMy question is: if all of my priors on the model parameters are uniform distributions with upper and lower bounds, then does ABC always produce posterior distributions that are approximately uniform with domain sizes that are less than or equal to the sizes of the domainsnof the prior distributions? I think that this must be true because the subset of model parameters that are accepted as belonging to the posterior will be uniformly and randomly chosen over a subdomain of each corresponding parameter.\n\nAdditionally, is there a better solution than ABC for this problem? I'd really like to have distributions over parameter values if possible, not just a best-fit set of values.\n\nThanks all, and sorry if I'm not clear or misused some jargon. I don't have a background in stats.\n\nhttps://en.m.wikipedia.org/wiki/Approximate_Bayesian_computation",
        "created_utc": 1676741235,
        "upvote_ratio": 1.0
    },
    {
        "title": "I've 4 values of x1 x2 and 4 values of y.",
        "author": "Turbulent-Beyond-781",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115i8p9/ive_4_values_of_x1_x2_and_4_values_of_y/",
        "text": "I've to make linear regression model. \nI did weight equal to X inverse Y. And then predicted Y as X * Weight.\nWhy my predicted Y and given Y are different???",
        "created_utc": 1676735341,
        "upvote_ratio": 1.0
    },
    {
        "title": "how do I conduct time lag multiple regression analysis?",
        "author": "greenyouth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115fs3n/how_do_i_conduct_time_lag_multiple_regression/",
        "text": "I have collected 2 waves of survey data and I want to do multiple regression. How can I go about this?",
        "created_utc": 1676728320,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate winner propability of a two sport teams in a playoff series based on your own propabilities of single match outcome?",
        "author": "petteri519",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115ebhy/how_to_calculate_winner_propability_of_a_two/",
        "text": "Lets say that i estimate that team A wins 60% of time against Team B. And the teams play Best-of-7 series (4 wins are required to qualify).",
        "created_utc": 1676723547,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can a non-significant result be a Type 1 error?",
        "author": "AstralWolfer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115djg8/how_can_a_nonsignificant_result_be_a_type_1_error/",
        "text": "I got the following excerpt from this link: [https://replicationindex.com/2021/01/15/men-are-created-equal-p-values-are-not](https://replicationindex.com/2021/01/15/men-are-created-equal-p-values-are-not/?amp)\n\n&gt;The false discovery rate is the percentage of significant results that are false positives or type-I errors. The false discovery rate is often confused with alpha, the long-run probability of making a type-I error. The significance criterion ensures that no more than 5% of significant and non-significant results are false positives. When we test 4,000 false hypotheses (i.e., the null-hypothesis is true) were are not going to have more than 5% (4,000 \\* .05 = 200) false positive results. This is true in general and it is true in this example. However, when only significant results are published, it is easy to make the mistake to assume that no more than 5% of the published 200 results are false positives. This would be wrong because the 200 were selected to be significant and they are all false positives.  \nThe false discovery rate is the percentage of significant results that are false positives. It no longer matters whether non-significant results are published or not. We are only concerned with the population of p-values that are below .05 (z &gt; 1.96)\n\nThe poster says that FDR is often conflated with alpha. They say that alpha ensures that no more than 5% of significant and non-significant results are false positives (Type 1 error). This implies that some false positives consist of non-significant results. Type 1 error is defined as the mistaken rejection of an actually true null hypothesis.\n\nMy question is: How is it possible to commit a Type 1 error on non-significant results? Doesn’t the definition of a Type 1 error necessarily preclude it to apply ONLY to significant results? What is the distinction between FDR and Alpha that the poster is trying to draw then?",
        "created_utc": 1676720627,
        "upvote_ratio": 1.0
    },
    {
        "title": "Textbook for deeper statistics study?",
        "author": "Victorgab",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115baq5/textbook_for_deeper_statistics_study/",
        "text": "Hi, I am a medical intern trying to improve my statistics knowledge for academic purposes. I already know the basics  (student's t, anova, chi squared) and I have a good math level, but I feel like I know concepts \"on the surface\". The thing is: textbooks are either \"hey let's do some \"child level maths\" or \"take some random matrices for you to figure out\". Is there anything in-between? Thank you",
        "created_utc": 1676711580,
        "upvote_ratio": 1.0
    },
    {
        "title": "Propagating (non-gaussian) uncertainty",
        "author": "ilrazziatore",
        "url": "https://www.reddit.com/r/AskStatistics/comments/115a25c/propagating_nongaussian_uncertainty/",
        "text": "i posted this on crossvalidated but i got no answer, i hope you can help me solve this doubt :\n\n&amp;#x200B;\n\ni was reading this blog post [Propagating (non-gaussian) uncertainty](https://cosmiccoding.com.au/tutorials/propagating/) on how to show region confidences of fitted models.\n\nThe author uses random samples generated by mcmc samplers to produce different realizations of the same models for each values of the input variable x. then he find the 95 and 58% percentiles for each value x. In this way he is able to create the confidence band.\n\nMy question is:\n\nif we consider calibrated a forecaster whose predictive cumulative distribution match the empirical cumulative distribution(like  defined in [https://arxiv.org/pdf/1807.00263.pdf](https://arxiv.org/pdf/1807.00263.pdf)),\n\nis the confidence region found in this way calibrated w.r.t. the marginal distribution of the target variable?\n\nMy thoughts were that since as the number of samples goes to infinity the target variable distribution moves toward the marginal distribution, the confidence bands are calculated wrt it, but i don't know if i am wrong or not.",
        "created_utc": 1676706565,
        "upvote_ratio": 1.0
    },
    {
        "title": "Response Distribution in Sample size calculation",
        "author": "Mayazara",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1155lht/response_distribution_in_sample_size_calculation/",
        "text": "Why response distribution is kept 50 while using an online sample size calculator? What is a response distribution and how it affects sample size?",
        "created_utc": 1676690559,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why doesn't one calculate the p-value with the ratio of incidence rates among the exposed and unexposed?",
        "author": "ProposalLeast3057",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1153dod/why_doesnt_one_calculate_the_pvalue_with_the/",
        "text": "",
        "created_utc": 1676683614,
        "upvote_ratio": 1.0
    },
    {
        "title": "When using a Dunn test, which method should I use?",
        "author": "BritishMan5",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1152k66/when_using_a_dunn_test_which_method_should_i_use/",
        "text": "I've found like 8 different types of methods that give different results, should I use the method that gives me more adjusted p-values &lt;0.05 or a method that gives me more &gt;0.05?",
        "created_utc": 1676681220,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probability that all 4 of a kind are in one half of a split deck",
        "author": "HydrA-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1150tbg/probability_that_all_4_of_a_kind_are_in_one_half/",
        "text": "Hey guys. Huge argument in my family. We can’t agree what the probability is that 4 aces are all in the same half of a split deck. I initially said 50% but people are mad at me and they can’t explain why because they are stupid themselves. Please help us our simpleminded genes and keep peace in the family",
        "created_utc": 1676676299,
        "upvote_ratio": 1.0
    },
    {
        "title": "Response distribution in raosoft calculator",
        "author": "Mayazara",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114w2j6/response_distribution_in_raosoft_calculator/",
        "text": "\nWhat should be the value of response distribution kept in sample size calculations from any online software e.g raosoft calculator?",
        "created_utc": 1676664070,
        "upvote_ratio": 1.0
    }
]