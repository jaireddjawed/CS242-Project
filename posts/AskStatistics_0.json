[
    {
        "title": "Are there any flaws with the statistical methods used for this study? It doesn’t seem of high quality, but I wish to confirm.",
        "author": "Shining_Silver_Star",
        "url": "https://journals.sagepub.com/doi/pdf/10.1177/147470491501300114",
        "text": "",
        "created_utc": 1679017223,
        "upvote_ratio": 1.0
    },
    {
        "title": "Had 2 blood panels &amp; urine tested @urgent care. Doc says they’ve come out perfect, she’s never seen this in 20yrs of medicine. Nothing flagged as too high or low. Now I’ve a statistic question.",
        "author": "PassTheMayo1989",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11tcea9/had_2_blood_panels_urine_tested_urgent_care_doc/",
        "text": "\nI asked the doctor if she was kidding me and she said she wasn’t. I had mentioned the blood test results on another sub and a person told me there’s got to be more to it than that. \n\nMy blood &amp; urine results can’t really be so unique - because by definition they’re within normal range - ie, not extraordinary. The individual blood elements tested for in the complete blood count &amp; complete metabolic panel are many. \n\nI thought the significant observation was that taken altogether it’s uncommon for so many individual things to come out unflagged. Enough so that the doctor has no recollection of seeing it in 20 years, unless she was pulling my leg for some reason. \n\nWhat say thee?",
        "created_utc": 1679014385,
        "upvote_ratio": 1.0
    },
    {
        "title": "Repeated Measures? Maybe?",
        "author": "VesperJDR",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11tbf12/repeated_measures_maybe/",
        "text": "Hi folks,\n\nHypothetic scenario here. Say I am looking at hair density in some animal in relation to prolonged cold exposure during development. I collect data by looking at the density of follicles in a square centimeter in a general location (e.g. upper back). Because the animal has a lot of surface area, I want to take multiple measurements per individual. Say, four square centimeter observations per individual. Those four observations would be linked in some way statistically, since they are from the same biological replicate. Is there a way to use repeated measures ANOVA to correct for this? Or is that only for matched observations pre / post or control vs experiment? \n\nThanks for any help!",
        "created_utc": 1679012606,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mirroring data",
        "author": "Spike_der_Spiegel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11tbalh/mirroring_data/",
        "text": "I have a question, but sure how to phrase it.\n\nSo I've got some data. The dependent variable is the margin of victory in a game. The independent variables are all functions of differences between the two teams (e.g. Team A's height minus Team B's height or w/e).\n\nShould I, when doing a linear regression, include each game event twice from the perspective of each time. That is, if a game ended 10-5 for Team A against Team B include one pair (+5, HeightA-HeightB) and then another (-5, HeightB-HeightA). Will it make a difference? If it does make a difference (It does so far) does that mean I'm doing something wrong?",
        "created_utc": 1679012424,
        "upvote_ratio": 1.0
    },
    {
        "title": "How best to order significance tests between box plots in a time series?",
        "author": "Typical_Ecologist268",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11t9n0o/how_best_to_order_significance_tests_between_box/",
        "text": "If I have a time series of box plots, each represents the distribution of ecological trait **x** for a particular period of time **a** through **e**, is it better to do significance testing in pairs like \n\n1) a-b, a-c, a-d, a-e  \nor like this  \n2) a-b, b-c, c-d, d-e  \n\n\nMy goal is to visualize general trends in the increase/decrease of the ecological trait in question over the given time periods. I'm leaning toward the first option making the most sense as it's easy to compare period a and period e (beginning and end) to see if the change is significant. Thoughts?",
        "created_utc": 1679008485,
        "upvote_ratio": 1.0
    },
    {
        "title": "insight on quantitative analysis from survey data for a college research project",
        "author": "Codeheff12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11t7895/insight_on_quantitative_analysis_from_survey_data/",
        "text": "I know this is extremely novice but looking for some guidance here. In one of my survey questions, individuals could identify whether they have been displaced. they are also asked to identify negative health outcomes they have experienced and are given a list of choices (mental health, obesity, etc.) i am trying to correlate displacement and mental health.   \n\n\nI have access to SPSS just looking for someone to point me in the right direction as my stats knowledge is extremely basic.",
        "created_utc": 1679002991,
        "upvote_ratio": 1.0
    },
    {
        "title": "Determining a sample size for an AB test",
        "author": "software_noob",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11t733s/determining_a_sample_size_for_an_ab_test/",
        "text": "I'm trying to measure the impact of an intervention. For example, I'm going to take 2 groups of runners and give one group an energy drink and the other group water to see how much effect there is, if any from the energy drink.\n\nI want to determine the sample size I need to use. How many runners I need in each group to have what level of confidence in the results. I've read a bunch of guides online about power analysis and statistical significance tests, but the info doesn't seem applicable to my problem.\n\nFor example, the formula asks for the two mean values and variances of the two groups. How can I know that before I choose a sample size because how can I even run the test without knowing how many runners I need?\n\nIt also asks for things like variance in the two groups. I do have some data I can use to determine mean and variance of my population of runners before I run this test. And I know I can pick some values for desired statistical power like 0.8 and guess at a desired effect size I'm looking to study. How can I translate this into an answer to the question \"how many runners do I need in each group?\"",
        "created_utc": 1679002649,
        "upvote_ratio": 1.0
    },
    {
        "title": "Conditions for logistic regression",
        "author": "Zarick_Knight",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11t4ftq/conditions_for_logistic_regression/",
        "text": "In ordinary least squares, the conditions for making inferences are (1) independence (2) normality (3) mean zero and (4) constant variance.\n\nWhat are the conditions for making inferences in logistic regression? And how do you check them?",
        "created_utc": 1678996545,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I interpret Ordinal regression statistics",
        "author": "Silent-Thund3r",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11t3l7e/how_do_i_interpret_ordinal_regression_statistics/",
        "text": "Hi there, I'm currently an ordinal regression where the X-variable are 6 different age categories and the Y-Variable is 4 different levels of knowledge, ranging from \"I Know nothing\" to \"I Know a Great Amount\". I ran the [model](https://docs.google.com/document/d/1uM8mmygMhz47ickgxrLENMbN-viJzasR9E4xx5PpRsA/edit?usp=sharing) and I got some coefficients for all categories, from 1 to 2, from 2 to 3 and from 3 to 4. I'm unsure as to what the coefficients exactly mean. I know how to interpret normal regression, but I'm unsure with ordinal.\n\n&amp;#x200B;\n\nThe hyperlink goes to a google doc which has a screenshot of the model.",
        "created_utc": 1678994547,
        "upvote_ratio": 1.0
    },
    {
        "title": "Applied vs. Theoretical Jobs (PhD)",
        "author": "_Kazak_dog_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11t0jpk/applied_vs_theoretical_jobs_phd/",
        "text": " \n\nHey all,\n\nI'm currently deciding between PhD programs, not in stats but similar to stats (think econometrics, biostats, data science, etc). One program is set up to be focused entirely on applied/empirical stats. While the coursework will obviously involve some degree of rigor/theory, the research is definitely about using stats in an applied manner. The other program is more theoretical, and I'd be doing research on advancement to statistical methods, rather than just using stats to uncover interesting findings.\n\nWhile my background is more applied and I am a little more nervous about excelling in a theoretical program, I think I'd ultimately be happy doing either forms of research. My question is therefore on what the non-academic job market looks like for either specialty. I love research, and I'd love to continue doing research, but my goal is really just to continue doing stats. I love stats, it's the thing that gets my out of bed every morning! I'd prefer not to end up in an industry data science role where I'm either building out pipelines or finding the mean (I do that now and it's getting old!). So, **do there seem to be more jobs focused on empirical stats or advancing methods?** All the jobs I've been able to find that look like they actually involve stats seem to be quant roles, and for that I imagine I'd have to go to the theoretical program (it's also a much higher ranked program). But I could be wrong!\n\nWould really appreciate any feedback on this.\n\nDoubt it matters but my focus area is on causal inference.\n\n**td;dr Are job prospects better for theoretical or applied statistics?**",
        "created_utc": 1678987739,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need help running statistical analysis for grip strength.",
        "author": "NoImHim1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sylbh/need_help_running_statistical_analysis_for_grip/",
        "text": "\n\n\tGrip strength: Neutral\tGrip: Flex\tGrip: Ext\n\nF\t14.7\t10.3\t10.7\n\nF\t20.3\t10\t13.3\n\nF\t22.67\t12\t15\n\nM\t41.73\t25.13\t19.87\n\nF\t19.3\t13\t14.3\n\nF\t25.3\t15.3\t19.3\n\nM\t23.3\t26.3\t18\n\nAverage \t23.9\t16.00428571\t15.78142857\n\nthis is the data, I need to find the significance between each point, based on the length-tension relationship. I'm unable to access R and downloaded the wrong plug-in on excel, any help would be great. Thank you.",
        "created_utc": 1678983484,
        "upvote_ratio": 1.0
    },
    {
        "title": "Regression with multiple categorical variables",
        "author": "hash-brown3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sw59i/regression_with_multiple_categorical_variables/",
        "text": "Hi all, I'm working on a project for my Stats class where we had to find out own dataset, ask a research question, and use R to answer it. I am working with a dataset where each variable is categorical. Because all the predictor variables I am working with are categorical, I used the factor function to create buckets for each one (and I also did the same for the response variable with 0=No and 1=Yes). I want to see which predictor variables are most significant in predicting whether or not someone will respond \"Yes\" to the response variable. \n\nThe response variable is the willingness to try a certain drug and the predictors are all demographic and social things like awareness that the drug existed, age/sexual orientation/religion/education level. There are 250 observations. \n\nHow should I approach this? I tried using a binary regression using the glm() function but I did not get any significant predictors. I am currently trying to use the likelihood ratio test but I'm not sure what to set as the full model. Should it be every variable from the dataset (about 20 variables?). How do I know which bin/bucket from each categorical variable is most significant? How do I know that the model is predicting \"Yes\" responses? In the LRT I can't use categorical variables for the response, so I turned my yes/no response variable into 0s and 1s. I feel like I am doing this very wrong.",
        "created_utc": 1678977908,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does two periods automatically make the data panel in a regression?",
        "author": "need2feelbetter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sviz7/does_two_periods_automatically_make_the_data/",
        "text": "We're observing the effect of an event that happened in 2020 to the stock prices of 2021. Initially we thought this was cross sectional but now we think otherwise.\n\nAny help would be appreciated.",
        "created_utc": 1678976461,
        "upvote_ratio": 1.0
    },
    {
        "title": "Books for market research surveys",
        "author": "kingkongjaffa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sttfn/books_for_market_research_surveys/",
        "text": "Hi all I am looking for book recommendations for statical methods in the area of market research surveys. \n\nI really want to get a first principles to cutting edge understanding (academic papers are appreciated as well)\n\nMost of the work I have seen is basically from sawtooth software research and their white papers. I would like to get a more direct academic background on the subjects.\n\n\nThings I am interested in:\n\nMax diff analysis \n\nConjoint analysis \n\nTechniques for evaluating survey outliers (to measure respondent quality and consistentcy in the sample)\n\nTechniques to weight and otherwise generate representative data from s sample.\n\nGeneral stats I could apply to sampled data (think multiple choice questions etc., likert scales ). \n\n\nThanks!",
        "created_utc": 1678972419,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing results from repeated surveys with overlapping samples",
        "author": "JudgeJudyJr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sqbtj/comparing_results_from_repeated_surveys_with/",
        "text": "Could someone please direct me to a comprehensible research paper on the factors that must be considered while comparing responses to an identical survey conducted annually with around 50 percent repeat participants. Thanks so much!",
        "created_utc": 1678962916,
        "upvote_ratio": 1.0
    },
    {
        "title": "Correct test to use for sum scores",
        "author": "cloverfart",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11spe5w/correct_test_to_use_for_sum_scores/",
        "text": "Hello everyone!  \nI have a question regarding the correct test to use on sum scores. \n\nI have formed sum scores from different variables regarding various socio-political attitudes and want to compare them to different other things, like preferred music genres (dichotomy) or the interest in subsidized housing (likert).  \n\n\nNow I dont know what tests to use in SPSS. For the music genres I used Eta (because its nominal), but now I dont know whether to use H-test (Kruskal Wallis) or a linear regression.  \n\n\nThanks alot in advance, our professor wants us to do a full blown statistical study without having learned about multivariate analysis or anything. He made a \"dossier\" with \"all the info we need\" (according to him) but its rather a mess.",
        "created_utc": 1678959816,
        "upvote_ratio": 1.0
    },
    {
        "title": "What statistical test I should do?",
        "author": "Archer387",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11snnf4/what_statistical_test_i_should_do/",
        "text": "**This is for the number of genes in a bacterial genome**\n\n&amp;#x200B;\n\nMy sample  - only 1 data- show a value of A.\n\nOther reference samples (5 samples) show a value of B, C, D, E, and F respectively.\n\n&amp;#x200B;\n\nI want to know the value of A is significantly higher or lower compare to other reference sample.\n\nI tried using the T-TEST in excel but It would not let me if my value is only A. So what I did is I put A 2 times and compared it to the other 5 reference samples.",
        "created_utc": 1678953421,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing Two Independent Diagnostic Tests",
        "author": "ZuC18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sni6b/comparing_two_independent_diagnostic_tests/",
        "text": "I have the following example data wherein the patients undergoing diagnostic test1 is different/independent from the patients undergoing diagnostic test2:\n\nTest1 TP=100\n\nTest1 FP=50\n\nTest1 FN=80\n\nTest1 TN=200\n\n&amp;#x200B;\n\nTest2 TP=40\n\nTest2 FP=10\n\nTest2 FN=70\n\nTest2 TN=50\n\n&amp;#x200B;\n\nTest1 yields a sensitivity of 0.5556 or 55.6% and specificity of 0.8000 or 80.0%\n\nTest2 yields a sensitivity of 0.3636 or 36.4% and specificity of 0.8333 or 83.3%\n\n&amp;#x200B;\n\n1. What statistical test can I use to conclude which diagnostic test has better sensitivity? specificity?\n2. Materials I'm seeing says to use Chi-square test or Fisher test but I only know these two tests to check for independence/relationship and not really to compare sensitivities and specificities.\n3. Is there also a way to statistically compare PPV? NPV? Odds ratios?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThank you!",
        "created_utc": 1678952872,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question Regarding Measuring Effect of Supplements",
        "author": "TheAlternativeStudy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sjrvz/question_regarding_measuring_effect_of_supplements/",
        "text": "Apologies in advance for a beginner statistics question that is likely more simple arithmetic than stats. However, it is important to me that I measure my data properly so I wanted to confirm.\n\nOk so I am trying to determine if a singular intervention (taking a particular supplement, only one supplement in question) causes a negative, positive effect, or no/neutral effect.\n\nMy plan was to have the effect measured by -1 (negative), 0 (neutral/no effect, 1 (positive effect) and then average that by dividing n number of days where I measured the effects.\n\nWould this give me a correlation/representation of that supplements average effect on me? Is there a better way of doing this? In the future if I were to create a method of measuring effect on a scale that utilized decimals based on the quality of the effect, would you consider the results to be stronger?",
        "created_utc": 1678939925,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing three different measures' performance within a model?",
        "author": "Apprehensive_Yam_567",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sff7r/comparing_three_different_measures_performance/",
        "text": "I'm comparing how well three different measures of one construct perform within the model I use for my research. Within the model, the construct I am investigating is a moderator variable. The model is comprised of two predictors, two mediators, and one outcome variable, with the moderator affecting the predictor -&gt; mediator path. \n\nTwo of the three measures have subscales that I'd also be interested in investigating. \n\nWhat is the best way to analyze this? I want to preserve as much power as possible and be computationally efficient as I'm working in R.",
        "created_utc": 1678928599,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I find out if there is statistical difference between two sets of time-series data. More info in comments",
        "author": "Ridaleneas",
        "url": "https://i.redd.it/stnstzu3dzna1.png",
        "text": "",
        "created_utc": 1678919867,
        "upvote_ratio": 1.0
    },
    {
        "title": "ANSI Z1-4 Tables vs Probability Mass Functions (Hypergeometric/Binomial) for Product Sampling Sizes",
        "author": "ccg5058",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11sas94/ansi_z14_tables_vs_probability_mass_functions/",
        "text": " \n\nI have only studied stats on a limited level in school. I got a job where i have to multitask and learn QA sort of on my own. I have been using the ANSI Z1-4 inspection tables to determine sample sizes for various quality levels, but never really understood where the choices come from. The other day it dawned on me that i could use the hypergeometric or binomial distribution to estimate my chances of finding a defect in a batch.\n\nSay i have a lot of 1000 peices. The tables for gen inspection 1 say to use sample size H or 50 pieces. According to my ANSI chart 50 pieces would give me an AQL of 0.25, which to me i have interpreted this to mean that no more than 2.5 defective peices will slip by the random inspection of 50 pieces in a lot of 1000.\n\nHowever if i plug the following into the hypergeometric series (0 sample successes, 50 sample size, 2.5 population successes, 1000 population size) i get a probability of 0.902 of zero successes (or in other words i am interpreting this to mean that there is only about a 10% chance that i flag the lot as bad if 2.5 out of 1000 peices are bad).\n\nSo where do these ANSI numbers come from? There are also very small sample sizes you can choose in \"special inspection\" and they basically also tell me that they have a very low percent chance of finding defects unless there are a huge qty of them.\n\nI know there is appendages and explanations to the ANSI tables in the resources but it goes over my head. Could anyone explain in laymans terms why the sample size is so much smaller than what a hypergeometric distribution would tell you to sample?\n\nCan anyone help me? I find it helpful to be able to explain to a customer why you might not find a 10000 piece population with 40 defects with a 20 piece sampling plan for example.",
        "created_utc": 1678917669,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sum of binary responses as outcome- what model to use?",
        "author": "tchaikswhore",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11s9abx/sum_of_binary_responses_as_outcome_what_model_to/",
        "text": "I am examining an outcome that is the score from a survey. There are 8 y/n questions and their score is the number they said yes to. So the outcome is a discrete number from 0 to 8. \n\nI wanted to run a multivariable regression model with this sum as the dependent variable. I originally thought that because of its discrete distribution, ordinal regression would be better than linear. Now I am thinking perhaps a poisson or negative binomial model is more appropriate, but I haven’t used these before so I’m unsure. Any thoughts?",
        "created_utc": 1678914387,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mean or Standard Deviation for Headcount Data",
        "author": "bensterrrrr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11s8zyh/mean_or_standard_deviation_for_headcount_data/",
        "text": "Hi all. I am trying to identify popular hours/days/months for the library I work at using headcount data we collect. So far, I've successfully made pivot tables that consolidate all of our data into a useful format, but now I am wondering what the most accurate measure would be useful for determining our busiest times. As of now, I'm simply going off averages of the hours/days/months, but would it more accurate to use standard deviation for this situation? Should standard deviation even be used in this instance?",
        "created_utc": 1678913780,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about outliers and summary statistics for a histogram",
        "author": "Test_account010101",
        "url": "https://i.redd.it/38jstqi810oa1.jpg",
        "text": "",
        "created_utc": 1678909941,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting coefficients of beta regression",
        "author": "0xthales",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11s6zma/interpreting_coefficients_of_beta_regression/",
        "text": "I have implemented a beta regression and am a little confused on how I should interpret the coefficients of my model. For context, both my independent variables and dependent variable are expressed in percentage form, ranging from \\[0, 1\\]. The only exception is one independent variable which takes the binary value of 0 or 1. Also, I used a logit link. Does anybody mind sharing how I could interpret the coefficients here in this beta regression? I've never worked with a dataset like this before; any help would be appreciated!",
        "created_utc": 1678909417,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Suggestions on Approaching Enrollment Projections?",
        "author": "hamta_ball",
        "url": "/r/statistics/comments/11s3e1b/q_suggestions_on_approaching_enrollment/",
        "text": "",
        "created_utc": 1678908863,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about data entry outliers",
        "author": "AidenTheRoombaDog",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11s2ghu/question_about_data_entry_outliers/",
        "text": "Hi, all! First off, thanks for those who helped with my question regarding ARIMA vs other ITS modeling methods. I finally caved and am using SAS and making a lot more progress!\n\nThis is probably a relatively simple question, so my apologies up front! My study is evaluating antibiotic prescriptions before and after an intervention.\n\nI am working on my pre and post intervention groups comparison chart and wanting to include a comparison of means of the duration of therapy in days. The issue is there are obvious errors on some of these prescription entries with clinicians ordering into the 100’s and even 1000’s of days. My concern is that by eliminating these entries that I might bias it against providers who are less computer savy and more prone to make errors when ordering prescriptions? It also eliminates many more pre-intervention prescriptions, which would make sense, less familiar with the EMR when introduced several years ago = more likely to make errors.\n\nFrom what I can tell, it would be the difference between a statistically significant difference in the mean and a no difference. When I use SPSS to identify unusual cases, it identifies anything with greater than 108 days as unusual. Is that an acceptable way to determine a cut off of what to eliminate? Obviously concerned if I personally set what's acceptable versus unacceptable duration that it definitely inputs some bias. \n\nMy other thought was instead of eliminating outliers and using a t-test to compare means between the two groups, I could instead use a Mann Whitney U to compare medians (and leave the extremes in)?",
        "created_utc": 1678899910,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the max value of skewness that allows to use mean as central tendency indicator instead of median?",
        "author": "d00pska",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11s0sdw/what_is_the_max_value_of_skewness_that_allows_to/",
        "text": "Could you please tell me how much the data can be skewed and the mean as an indicator of central tendency can still be used? I can't find anything online, but I heard that there is a rule of thumb. Do you know what it is? Could you provide a source? Thanks!",
        "created_utc": 1678896335,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] What is the best way to keep a time series consistent when adding new sources of data?",
        "author": "Gohigas",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rzim0/q_what_is_the_best_way_to_keep_a_time_series/",
        "text": " I am working on a project that has been carried out for three years. It involves the analysis of relative changes in vehicle flows using the data obtained by n number of sensors. That is, the first day in the time series is equivalent to 100%, at this moment, for instance, the flow increased by 20% compared to that first day (so 120% is the current value).\n\nDuring all this time, the number of sensors remained constant; however, their number will be increased in the short term, thus increasing the coverage area. My question is, what is the best way to maintain consistency between the work done so far with respect to what will be done with the incorporation of these new sensors? Of course you can take the new absolute value (with n+ sensors) with the value of 120%, and record the fluctuations from that point onwards. But I would like to know if there are more sophisticated techniques that address this specific problem, or something similar, that you recommend I investigate. Maybe I think about imputing the values ​​that those sensors would have had if they had been installed from the beginning.\n\nThank you very much, I've been following this subreddit for a long time and I find it super useful. This is my first post here, I'm from a non-English speaking country so I apologize in advance if I made any grammatical mistakes.",
        "created_utc": 1678893647,
        "upvote_ratio": 1.0
    },
    {
        "title": "Power for an association in a random effects model?",
        "author": "foogeeman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rynq1/power_for_an_association_in_a_random_effects_model/",
        "text": " I often deal with clustered data, with individuals nested within some group, which are analyzed using a random effects model. I've done lots of power analyses for treatment effects in such situations, looking at the power to detect an impact on some outcome.\n\nNow I need to do a power analyses for an association in a random effects model, to determine how large a coefficient must be in order to detect it with a given sample size, significance, and power. I would regress some continuous outcome on a continuous measure of interest, with some additional covariates, and with random means at the cluster level. \n\nI can't find any guidance on how to do such a power analysis, as everything I'm finding looks at power for a binary indicator representing treatment.\n\nDo you know of any resources for power calcs for associations in a random effects model?",
        "created_utc": 1678891821,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do i calculate the probability of each points",
        "author": "Cyphex555",
        "url": "https://i.redd.it/4gzchlwajyna1.jpg",
        "text": "I am trying to create a game and i need to know that if lets say starting point is x and it goes to first line and drops back the player gets +1 point, if it goes to second line and drops back player looses -1 point, if it goes to third line and drops back thr player looses -2 points, if it gets to 4th line and either drops back or goes through player gets +2\n\nThank you",
        "created_utc": 1678891804,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I perform a Hosmer Lemeshow test on a model fit using train() in the caret package?",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rxs8s/how_do_i_perform_a_hosmer_lemeshow_test_on_a/",
        "text": "",
        "created_utc": 1678889958,
        "upvote_ratio": 1.0
    },
    {
        "title": "Evaluating Unobserved Confounding with Logistic Regression",
        "author": "TK-710",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rwrko/evaluating_unobserved_confounding_with_logistic/",
        "text": "https://preview.redd.it/oev3m1cjpwna1.jpg?width=1500&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a2e8a368c9882e9417c679aacf9422468aae52f4\n\nHello,\n\nI was hoping to get your thoughts on dealing with potential unobserved confounding in an observational study.\n\nContext.  We want to know if an exposure variable (X; binary variable) affects  the risk of an outcome (Y; binary variable). It's neither feasible nor  ethical to randomize X. We're also controlling for about a dozen a  priori covariates (a mix of binary, multinomial, and continuous  variables) using logistic regression. Now we want to evaluate the risk  of unobserved confounding given that we can't randomize.\n\nWhat  I've done so far. I have simulated potential confounding variables (Z -  assumed to be normal with M = 0 and SD = 1, since I don't have an a  priori intuition for what properties this variable should have) by  varying the degree to which it correlates with X and Y, rerunning the  logistic regression, and then storing the coefficients and p values. The  image I've attached shows the results of the simulation. The X axis  shows the Pearson correlation between X and Z, the Y-axis is the Pearson  correlation between Y and Z, and the color is the p value from the  logistic regression with that combination of correlations for XZ and Yz.  The green region in the middle shows where the coefficient for X is  significant, the red region shows where it's not significant, and the  green region in the upper left/lower fight shows where the coefficient  is significant but has changed signs.\n\nThe  plan is to then use domain knowledge and prior studies to determine  whether it is likely that the sort of confounders/correlations that  would change our inference are out there.\n\nIs this a reasonable approach or have I missed something important? Thanks!",
        "created_utc": 1678887699,
        "upvote_ratio": 1.0
    },
    {
        "title": "Evaluating Unobserved Confounding with Logistic Regression",
        "author": "TK-710",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rwp21/evaluating_unobserved_confounding_with_logistic/",
        "text": "Hello,\n\nI was hoping to get your thoughts on dealing with potential unobserved confounding in an observational study.\n\nContext. We want to know if an exposure variable (X; binary variable) affects the risk of an outcome (Y; binary variable). It's neither feasible nor ethical to randomize X. We're also controlling for about a dozen a priori covariates (a mix of binary, multinomial, and continuous variables) using logistic regression. Now we want to evaluate the risk of unobserved confounding given that we can't randomize.\n\nWhat I've done so far. I have simulated potential confounding variables (Z - assumed to be normal with M = 0 and SD = 1, since I don't have an a priori intuition for what properties this variable should have) by varying the degree to which it correlates with X and Y, rerunning the logistic regression, and then storing the coefficients and p values. The image I've attached shows the results of the simulation. The X axis shows the Pearson correlation between X and Z, the Y-axis is the Pearson correlation between Y and Z, and the color is the p value from the logistic regression with that combination of correlations for XZ and Yz. The green region in the middle shows where the coefficient for X is significant, the red region shows where it's not significant, and the green region in the upper left/lower fight shows where the coefficient is significant but has changed signs. \n\nThe plan is to then use domain knowledge and prior studies to determine whether it is likely that the sort of confounders/correlations that would change our inference are out there.\n\nIs this a reasonable approach or have I missed something important? Thanks!",
        "created_utc": 1678887549,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mplus - Can we use multiple imputation with latent class growth analysis?",
        "author": "majorcatlover",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rwoh7/mplus_can_we_use_multiple_imputation_with_latent/",
        "text": "I am thinking about using multiple imputation to deal with missing data and then run a latent class growth analysis, but I am not sure if there is any way of pooling the results of the multiple latent class growth models and whether that even makes sense. I think coming up with the number of latent trajectories seems feasible, but is it possible to pool class membership so I can afterwards do further analyses checking the influence of covariates?",
        "created_utc": 1678887512,
        "upvote_ratio": 1.0
    },
    {
        "title": "Career Advice",
        "author": "Crafty_Locksmith8289",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rti6b/career_advice/",
        "text": "I am a public policy professional with a background in economics(albeit with very little quantitative background). I did study some econometrics in school(the Wooldridge textbook mostly), however I never got to use it in a practical context. I wanted to know that what set of quantitative skillsets I should build to further my career. I see terms like statistical analysis, forecasting, econometrics, data modelling thrown out in job descriptions but I don't know what those entail. Could anyone guide me on what books or courses should I take and what knowledge areas I should focus on in econometrics to built my quantitative skillset.",
        "created_utc": 1678879390,
        "upvote_ratio": 1.0
    },
    {
        "title": "GLM: understanding relationship between E(Y|X) and the link function",
        "author": "achsoNchaos",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rrgpp/glm_understanding_relationship_between_eyx_and/",
        "text": "I'm trying to understand what exactly Y is in the definition for GLM's from wikipedia:\n\n&amp;#x200B;\n\n[https:\\/\\/en.wikipedia.org\\/wiki\\/Generalized\\_linear\\_model](https://preview.redd.it/a1qviuo4gvna1.png?width=1286&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=938712b3b31627debc230586d0cc9d75ae82d455)\n\nIt says about Y: \" each outcome **Y** of the [dependent variables](https://en.wikipedia.org/wiki/Dependent_variable) is assumed to be generated from a particular [distribution](https://en.wikipedia.org/wiki/Probability_distribution) in an [exponential family](https://en.wikipedia.org/wiki/Exponential_family)\"\n\nyet what exactly models Y? As far as I understood is X the explanatory variable and eta is a basic linear regression model (yet I'm a little confused why the intercept is missing). Why does E(Y|X) relates to the link function g as stated in 3.?\n\n&amp;#x200B;\n\nI'd really appreciate explanations or links that explain on this topic a little clearer.",
        "created_utc": 1678872880,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Am i allowed to do a normallity test with discrtely displayed data?",
        "author": "Fragrant-Speed-2573",
        "url": "https://www.reddit.com/gallery/11rqpkb",
        "text": "",
        "created_utc": 1678870372,
        "upvote_ratio": 1.0
    },
    {
        "title": "scatterplot (determining linear relationship)",
        "author": "bellydaddy553",
        "url": "https://i.redd.it/r22akxt54wna1.jpg",
        "text": "",
        "created_utc": 1678862497,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical Analysis: How would I do an ANOVA test on the effects temperature have on polar bears?",
        "author": "Budget-Educator2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rmc08/statistical_analysis_how_would_i_do_an_anova_test/",
        "text": "Statistical Analysis: How would I do an ANOVA test on the effects temperature have on polar bears?\n\nIf I were comparing the number of polar bears found in two different time periods, i thought i would need to do a t-test, but i was told that it has to be an ANOVA. Can anyone explain why please?",
        "created_utc": 1678855461,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Comparing completion rates of 4 assessments",
        "author": "Significant_Visit411",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rlzs7/q_comparing_completion_rates_of_4_assessments/",
        "text": "I am trying to compare completion rates of 4 online assessments, A1, A2, B1 and B2. A2 is the modified version of A1, B2 is the modified version of B1. There are 15,000 participants assigned to one of the four assignments and the status variables of either Completed or Not Completed are available for all of them. Probably dumb questions.\n\n1. Could I combine A1 and A2, and combine B1 and B2, and then do the T-test for comparing completiom rates of A and B? (Ideally we would like to compare A vs B). Or ANOVA is more recommended?\n\n2. A1 has 5 times more sample size compared to other assessments (this was assigned more frequently). Would this difference in sample size be a problem? If so, could I randomly sample from A1 to reduce the sample size so that all 4 assessments have the similar sample sizes?",
        "created_utc": 1678854404,
        "upvote_ratio": 1.0
    },
    {
        "title": "Zero Inflated ARMA",
        "author": "tib_13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rljem/zero_inflated_arma/",
        "text": "I am currently working on a dataset that suffers from zero inflation. Moreover it being a time indexed requires to be dealt with ARMA (or ARIMA etc) models. [Here](https://arxiv.org/pdf/2004.10732.pdf) is a link to the theoretical aspect of the model. Can anyone help me with the coding aspect? (Some packages or source codes will be helpful)",
        "created_utc": 1678853039,
        "upvote_ratio": 1.0
    },
    {
        "title": "Moderation analysis with a nominal/dichotomous moderating variable? (Jamovi medmod)",
        "author": "EwoksAreAwesome",
        "url": "https://i.redd.it/qnqzk9m86rna1.jpg",
        "text": "",
        "created_utc": 1678820666,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using dependent variable as class variable for glm",
        "author": "shr_yay",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rcss7/using_dependent_variable_as_class_variable_for_glm/",
        "text": "Is it a bad idea to use dependent variable as class variable for linear regression? Can you please guide me to some articles that explain why? Appreciate any help..",
        "created_utc": 1678815520,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is this kind of graph called and how do I go about making one?",
        "author": "Jeff2900",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rcae0/what_is_this_kind_of_graph_called_and_how_do_i_go/",
        "text": "I need to make a graph that looks like this for 2 sets of data. Can I make this using excel?",
        "created_utc": 1678814421,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any social statistics academic article that utilizes multiple regression?",
        "author": "sociojeje",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rbx0g/any_social_statistics_academic_article_that/",
        "text": "Hi, I'm looking for a sample social statistics article that utilizes multiple linear regression in a very simple way so that students who took a basic statistics class can understand how it is used in a real research situation. Thank you very much!",
        "created_utc": 1678813608,
        "upvote_ratio": 1.0
    },
    {
        "title": "Book help for quasi-experimental research design",
        "author": "ScreamnMonkey8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11rbepj/book_help_for_quasiexperimental_research_design/",
        "text": "Trying to find some good books on quasi-experimental research design. Any help would be greatly appreciated!",
        "created_utc": 1678812522,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help! Anyone familiar with API calls to PHE fingertips",
        "author": "Appropriate-Brain813",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11raon5/help_anyone_familiar_with_api_calls_to_phe/",
        "text": "Hi All,\n\nI am trying to extract the % people over 65+ for Oxfordshire CCG. In particular I want to separate the trends by sex. \n\nI have tried the API builder data/tends by gender API call but when I plug the link into excel it just gives me the row headers and not the data itself? I have tried following the help guide but it seems I'm missing a step.\n\nTia",
        "created_utc": 1678810916,
        "upvote_ratio": 1.0
    },
    {
        "title": "In what order should I learn the statistics topics?",
        "author": "rr-0729",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11r7lcv/in_what_order_should_i_learn_the_statistics_topics/",
        "text": "There are lots of topics in statistics like regression analysis, ANOVA, time series analysis, etc. What are the important topics in statistics and what order should I study them in? What topics are prerequisite knowledge for other topics? I already know probability theory and linear algebra.",
        "created_utc": 1678804098,
        "upvote_ratio": 1.0
    },
    {
        "title": "Questions re Multivariate test and two-way ANOVA",
        "author": "el_staso",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11r481q/questions_re_multivariate_test_and_twoway_anova/",
        "text": "Hi, could you please help me with the following questions?\n\n&amp;#x200B;\n\nGiven I want to test if average pageviews per session (not normally distributed) depends on channel and device:\n\n\\* shall I use two-way ANOVA or shall I use some different test?\n\n\\* shall I keep the number of observations for each variant equal?\n\n&amp;#x200B;\n\nThank you in advance.",
        "created_utc": 1678795229,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical analysis of data post intervention",
        "author": "Candid-Role-5484",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11r3lk4/statistical_analysis_of_data_post_intervention/",
        "text": "I have carried out a study assessing the accuracy of contouring the prostate (i.e. drawing around the prostate gland) pre and post guideline intervention.\n\n16 observers were asked to contour the prostate on three MRI scans (MRI1, MRI2 and MRI3). A STAPLE (ground truth contour) was creating using a contouring software for each MRI scans using the contours. Each contour drawn was compared to the STAPLE and variety of interobserver variability metrics (DICE, average surface distance) were provided.\n\nGuidelines were then provided to the same 16 observers. I asked them to contour the prostate gland on two new MRI scans (MRI4 and MRI5) and duplicated one MRI scan that was used in part one of the study (MRI3) on three MRI scans.\n\nI now have the following data for each scan Volumes of each contour from the 16 observers DICE scores for each contour Average surface distance\n\nI would like to see whether interobserver variability has improved.\n\nI could do a t test of the DICE scores pre and post guideline intervention. Should I do a paired t test for the MRI 3 values and an independent t test for the other two scans?\n\nAlternatively, a kappa statistic for interrater agreement is something I've seen in other similar studies. Could I do the kappa statistic for each scan (for eg. for volumes). How do I go about doing this?",
        "created_utc": 1678793220,
        "upvote_ratio": 1.0
    },
    {
        "title": "A statistical question",
        "author": "SmOothOperatOR123456",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qyxt3/a_statistical_question/",
        "text": "Assume for every trade i take there is 70% probability of winning and 30% of losing. Risk to reward is 1:1 and i risk 10% of my account on every trade, what is the probability i can lose whole account(100%)?\n\nIs there a method of calculating this?\n\nThanks for the help",
        "created_utc": 1678776116,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need help understanding this percentage of scores falling below a z of 2.56",
        "author": "Icyyrada",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qyu3x/need_help_understanding_this_percentage_of_scores/",
        "text": "",
        "created_utc": 1678775743,
        "upvote_ratio": 1.0
    },
    {
        "title": "Repeated Measures ANOVA with JMP Pro",
        "author": "noworries_uwu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qy9n2/repeated_measures_anova_with_jmp_pro/",
        "text": "Hello, I'm currently analyzing antibody data with Repeated Measures ANOVA and have run into problems. I built a model with age, gender, vaccine background, and around 7 genetic polymorphisms. When I do multiple comparisons afterwards, I get the error \"all pairwise comparisons mean-mean scatterplot cannot be shown because confidence intervals cannot be computed jmp.\" I'm not sure how to solve this. Does anybody know what the problem is? Someone suggested something about the degrees of freedom running out but I genuinely do not understand. Appreciate your help!",
        "created_utc": 1678773784,
        "upvote_ratio": 1.0
    },
    {
        "title": "Effect size and test design",
        "author": "mgltt",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qvqo3/effect_size_and_test_design/",
        "text": "Hello,\n\nNo idea if this is where I can get this question answered, but I'd be very grateful if someone could point me in the right direction.\n\nI'm trying to understand a statement by a psychometrician on the use of effect size to judge the difference between test results (as in academic test results, like standardised tests that students would do). The statement was that using percentages to compare test results don't take into account the difficulty of the test (I understand that); effect size gets around that problem.\n\nMy question is : how would using effect size allow us to compare the test results at two different times whilst accounting for the difficulty of the test?\n\nThanks in advance!",
        "created_utc": 1678765701,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is percentage of variance explained with R^2 or with Beta in regression?",
        "author": "InvestmentCautious54",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qv56c/is_percentage_of_variance_explained_with_r2_or/",
        "text": "I'm analyzing data with my colleague and they have much more experience with this than I do. They're using the Beta column to say SCS\\_sumscore explains 61% of the variance and an additional .07% with GLTEQ\\_TLS.\n\nIn Model Summary they focused on std error of estimate to say GLTEQ\\_TLS explains .01% more of depression in this sample than SCS by itself. Again, I didn't think this was something we looked at.\n\n**My question:** I thought R\\^2 explained variance percentage? And that it would really be 39%?\n\nI've looked this up and everything I find insinuates that R\\^2 is used unless I am completely missing something.  \n\n\nhttps://preview.redd.it/kc7rgc15dmna1.png?width=741&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=97d2c504018d14e3c82b6fe16fb307066ccf9b28\n\nhttps://preview.redd.it/2kg32jw2dmna1.png?width=706&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8045445e6c9b06ed042ae8b13fa5ef5090802ad4",
        "created_utc": 1678763986,
        "upvote_ratio": 1.0
    },
    {
        "title": "Prediction with grouped data (scale data by group?)",
        "author": "ParlyWhites",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qsgqz/prediction_with_grouped_data_scale_data_by_group/",
        "text": "I have an assignment at work that is driving me a bit crazy that I need some help with. I can't divulge too much about the details of the project, given that the data and work is proprietary, so I will do my best to describe the data as best as I can given my limitations. \n\nI have a regression prediction problem that has data from the last 3 years, approximately 300 observations for each year. The distribution for the outcome for each year is clearly different, in that the range of values vary significantly, but all are roughly Gaussian and overlap. Furthermore, I believe that all of the features I've developed are likely to have a similar but not exactly the same relationship with the outcome across years (this pans out in the training data). I'm not satisfied with my out-of-sample MAE though, and I'm pretty sure that the differences in ranges are causing problems in my predictions.\n\nWhat I\"m wondering is if it is reasonable to standardize the values by group to get all of the values on the same scale and then reverse that transform by year when calculating MAE? \n\nFor further context, this model will be deployed for future prediction. If i am to do this scaling I would wait for about 20 observations, retrain the model with the new year's scaled data and transform parameters (e.g. the new years mean and sd) and then deploy it. \n\nUnder normal circumstances I would also try a multilevel model with a random effect for year, but this model is deployed in excel by my coworker, where he takes the model coefficients and plugs values in. It's truly not ideal but he \"has a system\" and spreadsheet made for this work and I'm relatively new so don't want to blow his work up. \n\nThanks for the help!",
        "created_utc": 1678756887,
        "upvote_ratio": 1.0
    },
    {
        "title": "Covariance question: how to simplify cov(a + b + c, a + d + e) into a sum of covariances (cov(b,d) + cov(b,e) + cov(c,d) + cov(c,e)) ?",
        "author": "lousyguest",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qrmhh/covariance_question_how_to_simplify_cova_b_c_a_d/",
        "text": "Sorry I don't even know how to google this. I've encountered this type of simplification of covariance of two sums before and I don't know what's going on. It doesn't seem to immediately follow from variance or covariance rules off the top of my head so I'm just wondering what the invisible steps are. Thanks!",
        "created_utc": 1678754807,
        "upvote_ratio": 1.0
    },
    {
        "title": "Ratio, nominal, ordinal, interval",
        "author": "TwistIndependent3289",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qr0ov/ratio_nominal_ordinal_interval/",
        "text": "What would be gpas? (0.0 - 4.0)",
        "created_utc": 1678753311,
        "upvote_ratio": 1.0
    },
    {
        "title": "QUESTION! What is the difference between + and * in ANOVA",
        "author": "M1SH0GA",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ql3x1/question_what_is_the_difference_between_and_in/",
        "text": "Hi! I am having trouble remembering what my code actually tells me since I wrote it a few months ago! Please help! My code reads \n\nb1&lt;-lm(basket$Mass\\~basket$Round\\*basket$Treatment)\n\nb2&lt;-lm(basket$Mass\\~basket$Round+basket$Treatment)",
        "created_utc": 1678740197,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multivariable regression: how to account for uncertainty in estimated variables",
        "author": "jesushitlerchrist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qi53r/multivariable_regression_how_to_account_for/",
        "text": "Hey everybody!\n\nI'm looking for help on how to account for the uncertainty in the point estimates that I'm using as variables to build a multivariable regression model. \n\nI have the 90% confidence intervals for each estimate, but I don't know how to \"plug in\" that uncertainty to be appropriately reflected in my analysis. \n\nIn short, I'm trying estimate the correlation between two variables using publicly-available U.S. CDC/Census Bureau data, with several other estimates used as controls:\n\n&gt;Dependent variable = county-level Labor Force Participation rates\n\n&gt;Independent variable = county-level Diabetes prevalence\n\n\n&gt;Control variables: county-level Poverty Rates, Median Educational Attainment, Disability Rates, etc etc...\n\nMy dataset basically looks like:\n\n&gt;County 1: Diabetes prevalence = 10.0% (+/- 2.0%); Labor Force Participation = 60% (+/- 5%); Poverty rate = 20% (+/- 3%) ... ...\n\nand so on for 100+ counties.\n\nWhat method(s) could I use to account for the uncertainty in the ~1000 county-level estimates I'm using to estimate the correlation between my two main variables of interest? \n\nIf anyone could point me in the right general direction, I would be very grateful. I have Stata but I'm willing to explore other tools if needed.\n\nThank you!",
        "created_utc": 1678733809,
        "upvote_ratio": 1.0
    },
    {
        "title": "The central limit theorem as a test of normality?",
        "author": "Life_Sprinkles_7958",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11qfjj7/the_central_limit_theorem_as_a_test_of_normality/",
        "text": "Hi everyone, I was wondering if it is true that you can assume normality in your data when you do an ANOVA on groups with n &gt; 30? I know the distribution of the mean converges to a normal distribution when your sample is larger than 30, however, I was wondering whether I was applying it correctly as I never saw it ''in action\". I thought I could apply it when using ANOVA and paired sample t tests as it uses sample means.",
        "created_utc": 1678727884,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can statistics help me here?",
        "author": "JG00220011",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11q643d/can_statistics_help_me_here/",
        "text": "Hello friends,\n\n&amp;#x200B;\n\nI am trying to calculate if the difference between the means of two samples is statistically significant but have a couple of **issues preventing me from using the t-test** (I think?) for null hypotheses: \"there is no statistically significant difference in satisfaction between group a and group b\"\n\n&amp;#x200B;\n\nMy population is 6500, **sample size is 400**. Each row in this sample is a person. Each person has given a likert score for \"satisfaction\" (I've converted these to -2, -1, 0, 1, and 2) for a question. Each person also has a number of binary \"flags\" against their name - if they do or do not have a certain qualification, if they are or are not over the age of 25 etc.\n\n&amp;#x200B;\n\nMy two **issues** are:\n\n* Some of the binary flags generate **unequally sized groups; a 20 vs 380** split for example.\n* Some of the answers to likert score questions are **non-normally distributed** \\- 65% extremely satisfied, down to 2% extremely dissatisfied.\n\n&amp;#x200B;\n\nAny help or guidance would be really appreciated - thank you :)",
        "created_utc": 1678703586,
        "upvote_ratio": 1.0
    },
    {
        "title": "Making a linear regression model in R but unsure if interpreting it correctly",
        "author": "apb1412",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11q133u/making_a_linear_regression_model_in_r_but_unsure/",
        "text": "I am trying to make a linear regression model in R to predict basketball games. I will be predicting the victory margin of the game, so if it is positive the first team wins, and negative the second team wins. All of the variables i am using in my lm() model are differences between the two teams (team 1 - team 2) and I have variables such as OffensiveEfficiencyDifference, FTPercentDifference, etc.\n\n&amp;#x200B;\n\nI run my model and get an equation such as VictoryMargin \\~ 8.88 + 0.453(difference.x) - 22.8(difference.y) + 4.3(difference.x). I plugged in a few games and it seems right on first glance but some of the numbers seemed off. When I switch team one and team two I would expect to get the same absolute value of victory margin but one negative and one positive, since it is the same two teams but just flipping which is being subtracted but I dont. For my test case I got like 11.342 and 6.343. But when I take away the intercept of 8.88 for both cases,  i do get the same value just with one negative and one positive. Can someone help me understand how I am misinterpreting the model?\n\n&amp;#x200B;\n\nThank you!!",
        "created_utc": 1678685150,
        "upvote_ratio": 1.0
    },
    {
        "title": "Supplemental Resources for Comp Stats Grad Level Course? [Q]",
        "author": "throwaway69xx420",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pygm3/supplemental_resources_for_comp_stats_grad_level/",
        "text": "Hi all,\n\nI am getting my butt kicked in my graduate level course for comp stats that uses the book Computational Statistics by Givens and Hoeting. For those who have used this book or gone through a similar course... Does anyone have any resources they used to help succeed in this course/material? We do everything in R. \n\nReally only covered optimization methods and that's enough for me to cry myself to sleep and question my ability to do anything stats related. My coding is decent, but the content itself is very abstract to me.\n\nThank you!",
        "created_utc": 1678677039,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sample Survey",
        "author": "Calybugstew",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pxhhn/sample_survey/",
        "text": " Will someone please explain to me what design unbiased and model unbiased means? I can't find an actual definition for either as they relate to sample surveys. I only see them mentioned in relation to estimators or population means.",
        "created_utc": 1678674354,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Interpretation of regression model with &gt;2 categorical predictors",
        "author": "permanenthouseguest",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ptden/q_interpretation_of_regression_model_with_2/",
        "text": "I have a model with a categorical predictor which has 5 levels. These 5 levels are equal in the sense that they are not ranked. I understand that intercept is the expected mean for the reference group, and each slope is the difference between the group to which the slope belongs to and the reference group.\n\nIf the coefficients are interpreted relative to the reference category, how is this useful in understanding the effects of the categories on the outcome variable?\n\nI end up with 4 comparisons, category 1 vs category 2, category 1 vs category 3, and category 1 vs category 4. But what I really want is to see how the 5 categories differ in their effects on the outcome variable, not just each category relative to 1 category.\n\nDoes this question make sense? Is there a more appropriate method for what I'm trying to find out? Or is there something I'm missing in the interpretation?\n\nWould appreciate any help, thanks so much!",
        "created_utc": 1678663489,
        "upvote_ratio": 1.0
    },
    {
        "title": "Kruskal Wallis test",
        "author": "Fair_Lawfulness_8875",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ps0w7/kruskal_wallis_test/",
        "text": "Kruskal Wallis data\n\nFirst, apologies for what may be a stupid question, I'm studying stats and I'm trying to work something out. I want to run a kruskal Wallis test on a CSO (central statists office) dataset. The data is in an excel file and consists of rows (geographical regions) and columns (numbers of people who fall into a specific category, eg number of people who classify their health as \"very good\"). The columns are grouped in themes, eg 5 columns representing health, from very good to very bad, or 10 columns representing educational attainment. Any time I look at the dataset, I come away with totals, eg total number of people in a region who describe their health as 'very good'. Am I right in assuming I CANNOT use totals in a kruskal Wallis test? If so, could you explain how to extract the proper type of data for a kruskal Wallis test, or even if it's possible?",
        "created_utc": 1678660252,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question About Hypothesis Testing",
        "author": "Plane-Stand6689",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ppf60/question_about_hypothesis_testing/",
        "text": "Hey all, if my null hypothesis states that the average student GPA = 3.43, and the alternative there means that GPA does not = 3.43, and I fail to reject the null (test statistic lower than critical value), what can I conclude? Is it that the sample hasn't provided enough information to conclude that average GPA doesn't equal 3.43? or that the average GPA DOES equal 3.43? or something else? Thanks",
        "created_utc": 1678654151,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hypothesis Testing Question and what it means to fail to reject the null",
        "author": "TestStriking4861",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ppc9z/hypothesis_testing_question_and_what_it_means_to/",
        "text": "Hey all, if my null hypothesis states that average student GPA =3.43, and the alternative therefore means GPA != 3.43, and I fail to reject the null hypothesis because my test statistic is lower than my critical value, what can I conclude? Is it that the sample hasn’t provided enough information to conclude that average GPA doesn’t equal 3.43? or that the average GPA DOES equal 3.43?",
        "created_utc": 1678653938,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you run a regression model where the observations' independent variables are only comparable to other observations within their group?",
        "author": "spicysnake333",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pj8vo/can_you_run_a_regression_model_where_the/",
        "text": "I am curious about whether you can make statistical inferences when you have nested data, and the way your independent variable is measured makes it only comparable within its own group.\n\nFor example (this is a hypothetical) Let's say you want to know the relationship between a town's investment in recreation and the level of physical fitness of the people in the town. Your towns are nested within states, and each state has a different way of allocating public funds for things like recreation to towns and each state classifies recreation a bit differently. So the level of recreation funding in Town A in State 1 includes Town 1's spending on, say the local public track, but the level of funding in Town B in State 2 does not include spending on running tracks. We can see the problem here: If we simply regress fitness on investment in recreation, then we could just be capturing the effect of different expenditure classification schemes. Spending on recreation is only comparable within states, but we don't really have enough observations to do separate regressions for each state.\n\nSo instead of measuring gross per capita recreation spending in each town, let's say we measure the amount of revenue that each town spends on recreation per capita as a percentage the entire state's spending on recreation. For simplicity's sake, let's say every town is the same size and has the same characteristics (although you could obviously use population weights if this weren't the case). Let's also say that you have reasonable evidence that cross-state variation in recreation spending [according to a common definition of recreation] is not very large; that is, most states spend a similar amount and that the variation is mostly at the substate level. So a town in State 1 may have higher gross spending but a lower percentage than a town in State 2, since State A has a more expansive definition of recreation.\n\nCould you run a random effects, fixed effects, or mixed model estimating the relationship between recreation investment as a percentage of total state rec. investment and overall fitness? I understand that you lose some insights, since you're no longer looking at gross spending. But my question stands: Is it possible to make inferences about the relationship between fitness and recreation spending with independent variables that are only comparable within the group they're nested in, or is it kind of impossible to draw any conclusions?",
        "created_utc": 1678639772,
        "upvote_ratio": 1.0
    },
    {
        "title": "Could you help me understand this xkcd comics?",
        "author": "act-and-grant-w",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ph2a9/could_you_help_me_understand_this_xkcd_comics/",
        "text": "Perhaps you already know it: [https://www.explainxkcd.com/wiki/index.php/882:\\_Significant](https://www.explainxkcd.com/wiki/index.php/882:_Significant)\n\nThe explanation is that there is a 1 in 20 chance that the result is due to chance, and incidentally there are 20 tests and just one that suggests a link with cancer, so the latter is likely due to chance.\n\nWhat I don't understand is why I should take into account the other tests if they refer to different types of jelly beans. Shouldn't they be all 20 about green jelly beans, to say that the only positive result is likely a statistical fluke?\n\nPerhaps it is the green dye that is carcinogenic. The other 19 tests don't tell me anything about it, because they don't analyze the effects of its consumption - only those of different types of colored jelly beans. \n\nWhat am I missing?",
        "created_utc": 1678634498,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does repeated measures ANOVA work in this situation?",
        "author": "LeyLineSkeptic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pguyw/does_repeated_measures_anova_work_in_this/",
        "text": "I’m a bit of a beginner to statistics, so please have patience with me if this is a silly question or if I’m thinking about it the wrong way or something!\n\nAll right, so I will be doing a repeated measures study, to be specific there will be one group for which the dependent variable will be measured at several different levels of the independent variable. There is one IV, which is continuous, and one DV, also continuous.\n\nIs repeated measures ANOVA valid in this situation? It seems appropriate from some of what I’ve read, but then some other sources seem to say that DV must be ordered or nominal for this model. Is that true, or have I just stumbled upon bad information?\n\nA second question as well: if repeated measures ANOVA really is the way to go, then is there any method to determine the minimum necessary sample size and number of measures?\n\nThanks for your help!",
        "created_utc": 1678634001,
        "upvote_ratio": 1.0
    },
    {
        "title": "can I use statsmodels package without installing pandas (python)",
        "author": "NextGenGoldSilver",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11peqhv/can_i_use_statsmodels_package_without_installing/",
        "text": "[removed]",
        "created_utc": 1678628245,
        "upvote_ratio": 1.0
    },
    {
        "title": "why is n-1 used instead of N for sample of a population?",
        "author": "Vongola___Decimo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pemoh/why_is_n1_used_instead_of_n_for_sample_of_a/",
        "text": "",
        "created_utc": 1678627923,
        "upvote_ratio": 1.0
    },
    {
        "title": "ANOVA table",
        "author": "Anastasia97ld",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pdr1x/anova_table/",
        "text": "Hello:-)\n\nI have done a multiple regression analysis and the ANOVA table is showing 1 less total participant and I do not know why, any suggestions?",
        "created_utc": 1678625233,
        "upvote_ratio": 1.0
    },
    {
        "title": "SAMPLE SIZE CALCULATION FOR FINITE POPULATION",
        "author": "gundanm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pdr1y/sample_size_calculation_for_finite_population/",
        "text": "",
        "created_utc": 1678625233,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical significance",
        "author": "chicasparagus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pddk6/statistical_significance/",
        "text": "I see that you guys have debated p value over and over, but my question is even more basic, since it’s my first time doing this. Hope you guys don’t mind.\n\nPerforming ANOVA and getting a result that’s statistically significant does not mean anything about the data other than we can treat the difference as real. Am I right?\n\nHypothetical situation:\nThere was statistical significance in the difference between group A and Group B where group A reported higher ratings. But we don’t use statistical significance to claim that the results of group A were *significantly* higher (as in using statistical significance to say the difference in ratings are more pronounced) Right? \n\n\nSorry for the very basic question, I just need to know I’m not going crazy because I’m reading papers which are confusing me where they are using statistical significance to claim a certain group reported significantly higher results.",
        "created_utc": 1678624025,
        "upvote_ratio": 1.0
    },
    {
        "title": "How Difficult is it to Get into a MS Statistics Program?",
        "author": "poopchute96",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11pb2qd/how_difficult_is_it_to_get_into_a_ms_statistics/",
        "text": "l've been working as a data analyst for the past 4 years but I want to get into more data science roles. I got my bachelors in math/statistics in 2019. My major gpa was like a 3.6 but my overall gpa was a 3.3. I also barely spoke to my professors so l'm sure my letters of recommendation are going to be weak. I think I'll be able to write a decent statement of purpose. I'm only applying to Texas a&amp;m because it's really the only one I can afford (instate/online). My friend told me grad school is difficult to get into this year, even for masters programs, because everyone is doing it now. Is this true? I wasn't worried before because thought masters programs were generally easy to get into because you pay for it yourself.\nBut now I'm wondering, with my grades and letters of recommendation, will even have a chance?",
        "created_utc": 1678615952,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate the probability of rolling matching dice?",
        "author": "Rogue_Walrus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11p8jqr/how_to_calculate_the_probability_of_rolling/",
        "text": "Hello!\n\nI'm trying to design a new dice system for a table-top roleplaying game, but the statistics for the dice system I'm interested in are proving a bit over my (and ChatGPT's) head. \n\nIn the game, the player will roll a number of ten-sided dice. Every dice that matches the value of at least one other dice will count as a \"match\". If a player rolls 8 dice with values {1, 2, 3, 3, 3, 4, 4, 5}, they will have 5 total \"matches\" because there are three 3s and two 4s. I've been able to intuit that if the player rolls only 2 dice, they would expect to have a 90% chance of no matches, and a 10% chance of 2 matches (one pair of matching dice). \n\nI'm expecting the relationship of # of Dice rolled to Probability of Getting \"n\" matches to be a bell curve, where there will be some # of Dice that maxes out your chance of getting \"n\" number of matches. At some point I expect more dice to mean a lower chance of getting \"n\" matches -- for example, if you rolled 12 dice it would be impossible to get less than 4 matches. \n\nDo y'all have any tips for how I can calculate more complex dice probability with this type of \"matching\" dice?\n\nThank you!",
        "created_utc": 1678606396,
        "upvote_ratio": 1.0
    },
    {
        "title": "Could someone clarify this stat section in a bio paper for me",
        "author": "EstablishmentSure632",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11p6c8u/could_someone_clarify_this_stat_section_in_a_bio/",
        "text": "it says \"Three independent experiments were performed using 3 biological samples in triplicate. One representative data (n = 9) were analysed by unpaired T-test or analysis of Variance (ANOVA).\n\nIm confused if there are 3 biological samples in each experiment and each sample has triplicates =&gt; 9 total samples in one experiment (and 27 for 3 independent experiments)\n\nOr\n\nThe 3 biological samples ARE the triplicates so 3 samples each experiment, 9 in total for 3.\n\nIt’s even more confusing with the last line saying “one representative data (n=9)”. Could someone please clarify?",
        "created_utc": 1678598921,
        "upvote_ratio": 1.0
    },
    {
        "title": "The multiple testing problem",
        "author": "Adventurous_Edge22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11p3g09/the_multiple_testing_problem/",
        "text": "[removed]",
        "created_utc": 1678590136,
        "upvote_ratio": 1.0
    },
    {
        "title": "Major in stats and neuro with a minor in cs or major in stats with a double minor in cs and neuro or major in stats with a minor in cs",
        "author": "speedoflight_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11p2n2p/major_in_stats_and_neuro_with_a_minor_in_cs_or/",
        "text": "I'm planning to major in stats and I was thinking of doing a combined honours in neuroscience, but I don't know if that will be too much. I want to go into computational research. Lots of faculty in stats are into bioinformatics and so there's lots of opportunities there. There are also computational neuro labs and CS courses that focus on AI and deep learning. I just don't know if I'm better off learning the computational side of things and volunteering in various labs such as some that work with mammalian models and applying that to a stats degree and research in machine learning.",
        "created_utc": 1678587916,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multiple vs Single Predictor Variables for GLM Pairwise Comparisons",
        "author": "AwwwFiddlesticks",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11p1fmq/multiple_vs_single_predictor_variables_for_glm/",
        "text": "I am running binomial GLMs in R to determine whether species presence on a hydrophone is different between seasons (i.e. spring, summer, fall, and winter) and photoperiods (i.e. day, night, dawn, and dusk). I understand that I can use the 'multcomp' or 'emmeans' packages to conduct pairwise comparisons, but am unsure whether I should run separate GLMs for Presence ~ Season and Presence ~ Photoperiod or should include them in the same model as Presence ~ Photoperiod + Season?\n\nUsing anova(), both photoperiod and season have significant effects on presence in both modelling approaches. However, my pairwise comparisons results using emmeans are different enough to effect significance depending on whether I model the predictors together or separately. \n\nAre there any justifications for using two models with a single predictor vs a single model with multiple predictors if my goal is conducting pairwise comparisons for each predictor?\n\nThanks!",
        "created_utc": 1678584531,
        "upvote_ratio": 1.0
    },
    {
        "title": "Finding the right statistical test for my research",
        "author": "Zetpill",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ozbj3/finding_the_right_statistical_test_for_my_research/",
        "text": "Hi there,\n\nI'm trying to find a relation between business size (number of employees) and their engagement towards certain brand in a business-to-business setting. The hypothesis is that smaller businesses are more likely to develop engagement towards certain brands than bigger businesses.\n\nI've measured brand engagement with multiple statements to which respondents answered with a 7 point Likert Scale, so the numerical values range between 1 and 7 with 2 decimals.\n\nBusiness size is the independent variable and was asked with a number entry, and most responded with having just 1 employee (24%), while only a couple here and there go in the hundreds.\n\nI tried a simple linear regression, but the R Square is only 0.021, and in a scatterplot with brand engagement (1-7) in the y axis and business size ranging from 1 to 1000 in the x axis, almost all dots are jumbled together near 1 on the x axis.\n\nI'm VERY MUCH a beginner, and I'm wondering if there would be a better way to find a correlation between the two variables, or if this one is good as is.\n\nThanks in advance",
        "created_utc": 1678578980,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can a variable have a positive correlation, but negative coefficient when running a multiple regression?",
        "author": "jbr2811",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11oxkif/how_can_a_variable_have_a_positive_correlation/",
        "text": "I’m kind of new to regression models. I’m noticing some variables of a multiple regression I’m running have a positive correlation when using excels correl function, but the coefficient is negative when running the linear regression. How can that be?",
        "created_utc": 1678574734,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help structure my linear mixed effects model",
        "author": "ESharer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11owxi5/help_structure_my_linear_mixed_effects_model/",
        "text": "Nesting models hurts my head.\n\n&amp;#x200B;\n\nI want to identify if there is an effect of 1) age, 2) task condition in an eye tracking task on a longitudinal sample.\n\n&amp;#x200B;\n\nThere are 8 trials with half a pixelated and unpixelated version of 4 videos in the eye tracking task\n\nTask Variables: TrialOrder (Dummy: 0 - 8), Content (Dummy: 0 - 4), Social/Nonsocial (Dummy: 0,1)\n\nInfant came in at different ages over 1 - 6 visits spanning 2 - 50 months\n\nWe had about 250 infants with 750 recordings (\\~ 200 with 500 recording after data quality assurance)\n\nParticipant Variable: Age(Continuous), SexM (Dummy: 0,1)\n\nEach recording had an associated data quality measure to ensure effects are not spuriously acocunted for by noise or missing data etc\n\nData Quality: %ValidData (Continuous), Accuracy (Continuous), Precision (Continuous)\n\n&amp;#x200B;\n\nMy proposed repeated measures...\n\n&amp;#x200B;\n\nGiven it is repeated measures I believe this is required, plus it provides indepedent intercepts for individual\n\nSaccade Amplitude \\~ Age + SexM + TrialOrder + Content + Social/Nonsocial + %ValidData + Accuracy + Precision + (1 | ID) \n\nand if I think participants might have different different slopes\n\nSaccade Amplitude \\~ Age + SexM + TrialOrder + Content + Social/Nonsocial + %ValidData + Accuracy + Precision + (age | ID) \n\n&amp;#x200B;\n\nNow how do I integrate the multiple trials within a recording?\n\n&amp;#x200B;\n\nSaccade Amplitude \\~ Age + SexM + TrialOrder + Content + Social/Nonsocial + %ValidData + Accuracy + Precision + (age | ID)  + (1 | trialorder)\n\n&amp;#x200B;\n\nor... nesting... \n\n&amp;#x200B;\n\nPlease help.",
        "created_utc": 1678573192,
        "upvote_ratio": 1.0
    },
    {
        "title": "Proportion Question (Easy)",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ouinh/proportion_question_easy/",
        "text": "I think this is a easy question I just can't think of the best way to model it. Let's say \"p\" percent of a certain population of size \"n\" has a dog at home. If I go up to random people and ask them if they have a dog at home how can I mathematically describe the expected length of the streaks of people that I will encounter that do not have a dog?",
        "created_utc": 1678567308,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hypothetical/conceptual ridge regression question about the regularization term.",
        "author": "perturbedisturbed",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11orsa2/hypotheticalconceptual_ridge_regression_question/",
        "text": "For the ridge regression loss function, we have: min wrt B {1/2 || (XB-y) ||^2 + (k/2)|| B ||2 }. But what if instead of B, we had y? Would the gradient of that loss function then just give us the OLS solution, since the second term DND on B anymore?",
        "created_utc": 1678560652,
        "upvote_ratio": 1.0
    },
    {
        "title": "What kind of statistical analysis can I do to see the relationship between continuous and ordinal data",
        "author": "Silent-Thund3r",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11opp18/what_kind_of_statistical_analysis_can_i_do_to_see/",
        "text": "Hi there, I wanted to ask what kind of tests I can do to see if there is a relationship between 2 variables, where the Y is an ordinal dataset, whereas the X is continuous. I'm analyzing the relationship between income and knowledge on a certain idea, where the answers vary from \"I Know nothing\" to \"I Know a Great Amount\".",
        "created_utc": 1678555527,
        "upvote_ratio": 1.0
    },
    {
        "title": "Federal Statisticians/Data Scientists how much technical work do you get to do?",
        "author": "blumenbloomin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ooqll/federal_statisticiansdata_scientists_how_much/",
        "text": "Previous stat contractor, now a fed at a different agency in an analyst role. As a contractor I did a lot of stat computation and modeling. As a fed analyst I don't but it seems the statisticians and data scientists in our division don't either, they just rely heavily on contractors. We do a lot of hand-waving big-talking here and it's leaving me feeling kind of empty. \n\nI'm wanting to eventually move into a more technical role (I took the analyst job to get into a fed role, but I'm a statistician by training) and I'm wondering if any federal stat/data folks get to do challenging work, unlike at my agency, or if it's all farmed out basically everywhere. I miss being technical, dislike the vague grandiose talk about the work we do but do love the fed benefits.",
        "created_utc": 1678553143,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the set of scores with a standard deviation of 0 normally distributed?",
        "author": "Strict_History7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11omrgu/is_the_set_of_scores_with_a_standard_deviation_of/",
        "text": "",
        "created_utc": 1678548105,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to induce non-proportionality of the risks in a Cox model?",
        "author": "hawkeyeninefive",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ojp83/how_to_induce_nonproportionality_of_the_risks_in/",
        "text": "For a thesis in survival analysis, I'm studying what happens when the assumption of proportional risks in a cox model is not respected and, consequently, which models to use in this case. The problem is that in my dataset out of twenty variables none violates this assumption.  I've been working for over a month on data cleansing and various exploratory and non-parametric analyses, so I don't want to have to start over on a new dataset.  I need at least one variable, even if it were only one, to violate the proportionality of risks assumption.  Is there a way to induce this?",
        "created_utc": 1678539835,
        "upvote_ratio": 1.0
    },
    {
        "title": "multiple linear regression with uncertainty on input data",
        "author": "Polareggo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ohsn6/multiple_linear_regression_with_uncertainty_on/",
        "text": "hello all\n\nI am trying to perform multiple linear regression using statsmodels.OLS (in python); ie my goal is to fit a set of measured data points to a linear combination of two or more predefined sets of values. The measured data has a measurement error on it, but I can't find anywhere how to include this uncertainty in the model, and I need to do so in order to have correct errors on the regression coefficients. Is there any way to do this?",
        "created_utc": 1678533923,
        "upvote_ratio": 1.0
    },
    {
        "title": "Causal Inference Research",
        "author": "repigyou",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ob5j6/causal_inference_research/",
        "text": "Hello all, \n\n&amp;#x200B;\n\nI was wondering what research in causal inference is like? I am having trouble thinking about ways people approach causal inference research programs because my understanding is limited and I just think of causal inference as correlation does not mean causation, so we try to do that through controlling for a bunch of variables. and a RCT is the only way to infer true causality. \n\n&amp;#x200B;\n\nThanks!",
        "created_utc": 1678510498,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I use the standard error of a regression beta to calculate the cumulative probability of beta being less than a certain value?",
        "author": "Nixon_was_framed",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11oa3xs/can_i_use_the_standard_error_of_a_regression_beta/",
        "text": "I've run a regression and got the following [output](https://i.imgur.com/NYBtxek.jpg): \n\nBeta Coefficient: 1.62\n\nStandard Error: .45\n\nI want to know the probability that the slope of the relationship is less than or equal to the regression beta + 1 standard error. Can I simply treat it like a standard normal distribution where P (Z &lt;= 1) = 0.84134 ?",
        "created_utc": 1678507143,
        "upvote_ratio": 1.0
    },
    {
        "title": "What would the chances be that someone recognizes a persons picture?",
        "author": "narwaffles",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11oa2hy/what_would_the_chances_be_that_someone_recognizes/",
        "text": "If someone posts pictures of themself on reddit on a sub with a certain amount of subscribers, what would the chances be that someone they know sees it? Would posting to a sub with, say, 500k subscribers be the same as posting to 2 subs with 250k subscribers? Also I’m wondering if it’s linear, like would posting to a sub with 500k subscribers be double the chance as posting to 2 subs with 250k subscribers?",
        "created_utc": 1678507004,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I skip learning Discrete R.V, and learn Continious R.V right away?",
        "author": "Ffflovin20",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11o6tzg/can_i_skip_learning_discrete_rv_and_learn/",
        "text": "I'm short of time at the moment, will there be a significant impact on my understanding of Continuous R.V. if I don't have a solid grasp of Discrete R.V.?",
        "created_utc": 1678497383,
        "upvote_ratio": 1.0
    }
]