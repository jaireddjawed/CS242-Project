[
    {
        "title": "Is there a good way to use regression analysis to simultaneously predict the X and Y spatial coordinates of an object?",
        "author": "Astrobrony",
        "url": "https://old.reddit.com/r/statistics/comments/8swhy2/is_there_a_good_way_to_use_regression_analysis_to/",
        "text": "",
        "created_utc": 1529623024,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a way in STATA how to consider variance-covariance matrix?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8suy1x/is_there_a_way_in_stata_how_to_consider/",
        "text": "[deleted]",
        "created_utc": 1529610624,
        "upvote_ratio": ""
    },
    {
        "title": "A statistical test for comparing pie chart-type data",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sujxy/a_statistical_test_for_comparing_pie_charttype/",
        "text": "[deleted]",
        "created_utc": 1529607642,
        "upvote_ratio": ""
    },
    {
        "title": "I think I swam too far out. How do I \"relearn\" statistics?",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sslkv/i_think_i_swam_too_far_out_how_do_i_relearn/",
        "text": "I have been trained in classical statistics for most of my university career. Some courses were rather applied, a few more theoretical. I got increasingly curious about statistics and wanted to learn about bayesian methods. However, no such courses are offered in my departement so that I read most of \"statistical rethinking\" which I found excelent. But more often than not, I hit a wall. As we speak, my computer is running NUTS for the better part of an hour on a bayesian model I fitted in R via the brms package. I do not know why. I opened the case studies from RStan to learn more about bayesian methods [here and imediatly was lost.](https://betanalpha.github.io/assets/case_studies/gp_part1/part1.html)  I tend to just scan over expressions like: *π*(*y*,*x*∣*θ*,*ϕ*)=*π*(*y*∣*x*,*θ*,*ϕ*)*π*(*x*∣*θ*,*ϕ*). I do not know what an infinite-dimensional hilbert space or what an  *exponentiated quadratic* kernel *k*(*x*1,*x*2)=*α*2exp(−(*x*1−*x*2)2*ρ*2) is supposed to mean. Most of the time when there is a proof in statistical texts I cannot make sense of it most of the time.\n\nI am not asking for help for these specific problems. These are just points of reference to illustrate my trouble with the underlying mathematical concepts. Any time I want to study or learn something by myself I encounter this problem (except for cookbook like approaches where I am told to poke a package). Now, I  bought a book on probability theory and another rather bad one for \"mathematics for the social sciences\" (They introduce set-theory by asking you to write down the elements of the set of every country in the US) and another book on proofs. Taken together, these are 1500 pages. To me, it seems somewhat unrealistic that I'll manage to go through all of it to build a foundation. I guess my question boils down to \"what are the milestones I should aim for to have a better footing in statistics?\" ",
        "created_utc": 1529593532,
        "upvote_ratio": ""
    },
    {
        "title": "Testing for multicollinearity",
        "author": "AncientNetwork",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sqfv0/testing_for_multicollinearity/",
        "text": "**What are pairwise correlation coefficient, condition index and variance inflation factors doing?** \n\n**I can calculate them and see where they fit in the rules of thumb - but I struggle to understand what they are actually doing?**",
        "created_utc": 1529572860,
        "upvote_ratio": ""
    },
    {
        "title": "Rating/algorithm for weighing a number of values as a total",
        "author": "WithFullForce",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8spsb0/ratingalgorithm_for_weighing_a_number_of_values/",
        "text": "Sorry if this is a rather basic question. It was nearly 15 years ago I had a stats class for my engineering degree.\n\nI have a list of different items, every item has 5 characteristics \n (A-E), ranked 1-5. Snag here being that characteristic E has more value than the rest and in a sense enables the others to a degree.\n\nIf I wanted to extract a total rating from each item's characteristic what would be a good algorithm for this? \n\nI'm currently using ((A+B+C+D)*0.5)+E but that's me fumbling in the dark and the 0.5 value feels completely arbitrary (just doubling the value of E basically).",
        "created_utc": 1529564532,
        "upvote_ratio": ""
    },
    {
        "title": "is there a difference between expectation maximization for gaussian mixtures and expectation maximization for other problem formulations?",
        "author": "cheekyyucker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sp4q1/is_there_a_difference_between_expectation/",
        "text": "for gaussian mixtures, EM makes sense to me. I want to find the probability for my parameters with respect to each data point, and then i want to assign data points to their respective most probable parameters, and then i want to recalculate my parameters.\n\n\nIt seems like the problem formulations related to hidden markov models or multinomial, or even binomial parameters are much more complex. Does the above process of getting posteriors, assigning points, and updating parameters apply to all usages of expectation maximization, or am i missing something due to my inexperience?",
        "created_utc": 1529556853,
        "upvote_ratio": ""
    },
    {
        "title": "Question about p-value and R-squared for correlations.",
        "author": "Brittster0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8snru1/question_about_pvalue_and_rsquared_for/",
        "text": "I have a feeling this is a stupid question, but I'm having trouble finding the answer anywhere online:\n\nI have run all pairwise correlations for a large dataset (using R, psych package). I'm getting to know my data and I  realized that for every relationship where the p-value (of the corr.test) is 0.05, the R-squared value is 0.7744. This seems to be true throughout the dataset. For example, in every relationship where the p-value is 0.04, the R-squared value is 0.7921.\n\nI would consider myself a beginner in stats. I am learning R and statistics to support my thesis research in another field. This relationship ( if it is one?) is not something I remember learning about.\n\nMy question: Is this normal, or is it a sign that I have done something incorrectly?\n\nThanks in advance!",
        "created_utc": 1529543946,
        "upvote_ratio": ""
    },
    {
        "title": "preference: covariance or correlation?",
        "author": "ConwayPuder",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8snowo/preference_covariance_or_correlation/",
        "text": "In the context of presentations, when do you prefer covariance over correlation coefficients?  Lay audiences are obviously more familiar with the idea of correlation, but is it worth talking about covariance?",
        "created_utc": 1529543166,
        "upvote_ratio": ""
    },
    {
        "title": "What is the mathematical concept I need to answer this question?: what are the chances that 4 random people are born in 4 different seasons?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8snbul/what_is_the_mathematical_concept_i_need_to_answer/",
        "text": "[deleted]",
        "created_utc": 1529539929,
        "upvote_ratio": ""
    },
    {
        "title": "Looking to conduct a brief interview",
        "author": "kmagyoyo95",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8smuzx/looking_to_conduct_a_brief_interview/",
        "text": "One of my tasks for a midterm project is to interview a professional in a field I’m interested in (Data science). \n\nIf any of you would be willing to take some time out of your week to answer a few questions for me, I would greatly appreciate it. \n\nThe questions from the assignment are:\n\n1. What is your job title?\n\n2. What type of education would I need to get into this field?\n\n3. What have been the biggest changes in the technology you use in your work? \n\n4. What salary range could I expect starting out?\n\n5. What are some of the current technology-related issues/challenges/problems in the field today?\n\n6. What's changing in the field? What should someone coming into it need to know in, say, 3 years from now?\n\nThanks for looking, and if this is in the wrong place I apologize and will remove it ",
        "created_utc": 1529535746,
        "upvote_ratio": ""
    },
    {
        "title": "How much of a change would adding options to an election have?",
        "author": "Diabeto_13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sm94r/how_much_of_a_change_would_adding_options_to_an/",
        "text": "So I have created an election for a government youth program. Voters are supposed to pick 7 out of 14 candidates on the ballot to fill multiple seats for a position. How much would picking 9 out of 14 change the results? The total number of voters is around 250.",
        "created_utc": 1529530766,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating the median without the raw data.",
        "author": "Marrx",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8skttj/estimating_the_median_without_the_raw_data/",
        "text": "Hello, it's been several years since I've taken a statistics class, so I'm pretty rusty.\n\nI'm trying to calculate the medians for several sets of data. I don't have access to the actual raw data for each set, but I do have the max, mean, min, and # of data points. The sets of data tend to have outliers, so I am trying to estimate a more accurate average. Each set tends to have between 20-50 data points, although some are as few as 5.\n\nIs there anyway to do this? Or am I working with too little information here? I appreciate the help, thanks.",
        "created_utc": 1529520182,
        "upvote_ratio": ""
    },
    {
        "title": "Can you do algebra on regression equations?",
        "author": "ringraham",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sknen/can_you_do_algebra_on_regression_equations/",
        "text": "This might be a silly question, but I was thinking about this earlier, and I got stumped. Do the rules of algebra apply to regression equations? My gut instinct says yes, as it should just be solving for a different variable, but my primary experience with regressions has been with my Intro to Statistics and my Applied Econometrics classes, where we only did OLS. Solving for a different variable would give either a negative output, or a subtractive error term, which violates Gauss-Markov. Can anyone give me some guidance?",
        "created_utc": 1529518866,
        "upvote_ratio": ""
    },
    {
        "title": "Calculate a more reasonable sampling size for future trials",
        "author": "BottomOfTheBarrel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sk41l/calculate_a_more_reasonable_sampling_size_for/",
        "text": "I've run several trials where I have a sample size of 1200. It is a huge labor cost analyzing all these samples, and I think unnecessary. I think I can get the same conclusions with a smaller sample size, but i'm not sure what number to choose or how to support that decision. Is there a way to look at my old data with 1200 samples, and like use the variance to calculate a smaller sample size which leads to the same distribution? I can calculate this for each of my old trials, then average the sample size it dictates and choose that...\n\nThank you in advance for your assistance.",
        "created_utc": 1529515027,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics for a binary classification",
        "author": "drsxr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sjlwz/statistics_for_a_binary_classification/",
        "text": "Hi there - I'd like to get a 2nd opinion as I don't have ready access to a statistician, and I need a sanity check.\n\nI'm running a bunch of deep learning classifiers.  They are all binary - on two classes (+'ve and -'ve whatever).  I'm looking at the effects of sample size on output. \n\nI have four datasets.   The first dataset has 125 positive examples, and 125 negative examples.  The second 250/250, the third 500/500, and the fourth 1000/1000. \n\nI have a validation set of 200 examples and a hold-out test set of 200 examples for final accuracy.  I am not including these points in the error calculation.\n\nI'm running the algorithm multiple times, and I'm going to get the same answer each time.  Therefore, the output is actually a mean.  I can use Standard Error of the Mean,  1/sqrt(N) to give me an error.  \n\nThe question is this: should I use N=125,250,500,1000 for this because it is a binary class problem, or is N more correctly 250,500,1000,2000 because I am counting all samples.  I vote the latter.  Anyone disagree? \n\nWould anyone include the N in the validation and test sets either?  While the validation set influences the dataset, it is more of an indirect variable.  And the test set N is clearly excluded.  Would anyone differ with that?\n\nThanks. ",
        "created_utc": 1529511389,
        "upvote_ratio": ""
    },
    {
        "title": "Conditional distributions",
        "author": "statrowaway",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sh6po/conditional_distributions/",
        "text": "P(Y|X=x)=xCy (0.6)^y (1-0.6)^x-y\n\nDoes this mean that\n\nY|X ~bin(0.6,X) ? or however I write it? \n\nHow can I find the marginal distribution for Y? Is it somehow possible to find the marginal of Y with the information given ? I suppose if I knew the marginal of x then I could have found the joint pdf of X and Y and just summed over x to get the marginal of y.\n\n--------------------------------------------------------------------------\n\nAnother question:\n\nIf I am interested in finding the PDF of X+Y\n\nX got PDF f(x), CDF F(X) \n\nY got PDF f(y), CDF F(y)\n\nP(X+Y=t)\n\ncondition on Y.\n\nP(X+Y=t)=sum_y P(X+Y=t|Y=y)P(Y=y)\n\nIs it even possible to progress from here if I don't know whether or not X and Y are independent? What is the interpretation of the expression P(X+Y=t|Y=y) here? It seems to be some sort of conditional distribution. I am getting really confused about the X+Y=t to the left of the bar. I'm not used to these type of expressions, only conditional distributions of the form Y|X (or Y|X=x).",
        "created_utc": 1529489999,
        "upvote_ratio": ""
    },
    {
        "title": "F-test with samples that have errors",
        "author": "OmegaNaughtEquals1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sd4tg/ftest_with_samples_that_have_errors/",
        "text": "I have two experiments run under slightly different conditions. I make N measurements of the same quantity from each run. Each measurement has an independent, normally-distributed error attached to it. I want to know if the means of the two samples are the same. If there were no errors, I would just use the F-test and get a p-value using the CDF (as explained [here](https://stackoverflow.com/questions/21494141/how-do-i-do-a-f-test-in-python)). However, I want to take the errors into account. Is there a test statistic like the F-test that can do this?",
        "created_utc": 1529447385,
        "upvote_ratio": ""
    },
    {
        "title": "Linear regression aftee BoxCox transformation",
        "author": "mvaa12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sbrph/linear_regression_aftee_boxcox_transformation/",
        "text": "Hey everyone, I have a question. I had to make some linear regression of data, it was heteroscedatic model so I had to do some transformation to make it homoscedastic. I did Box Cox transformation of dependent variable and made a model. My question is how to interpret the results? \nSo, if Y is transformed variable, I have a equation like this:\nY=0.05x1+0.3x2 + epsilon\n\nThank you very much for help. :)",
        "created_utc": 1529437002,
        "upvote_ratio": ""
    },
    {
        "title": "Sorry - probably a simple question",
        "author": "paiute",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sabbr/sorry_probably_a_simple_question/",
        "text": "I have three measurements. The highest value minus the smallest value is the range. What do I call the range divided by the average value?",
        "created_utc": 1529426364,
        "upvote_ratio": ""
    },
    {
        "title": "Brainstorming a Model for Targeting Voters",
        "author": "VanillaNillaBaby",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8sa8jx/brainstorming_a_model_for_targeting_voters/",
        "text": "This is my first post in this Subreddit. If I should post the following elsewhere, or if I am violating any guidelines, feel free to point me in the right direction.\n\nI am looking to build a model that can help a congressional campaign in optimally targeting people in a certain region as potential voters. The data we would use for this campaign includes individual-based information on demographics (sex, age, race, residence, etc.), voting history from the past 20 years (only whether or not they voted), donation history, and previous contact with our campaign.\n\nThe upcoming election is an open primary, meaning that we are not limited to those registered in a particular primary. Additionally, this primary is party-specific, meaning that we are only concerned with rival candidates within the same party.\n\nI wanted to first hear if this community had any loose ideas on where to begin in constructing a model for targeting potential voters. If I need to be more specific, please let me know!\n\nTo clarify my background, I recently graduated with a Masters degree in statistics, though most of my classes emphasized the more mathematical aspects of the field. I know I have an interest in calculating probabilistic scores for each individual; maybe a variant on logistic regression would be suitable for this problem. However, as the model is meant to choose which people to which we reach out, I was thinking that some sort of decision tree would work better. I am open to any and all suggestions!",
        "created_utc": 1529425791,
        "upvote_ratio": ""
    },
    {
        "title": "Regression / Scatter plots - line through averages (?)",
        "author": "freedamanan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s82e9/regression_scatter_plots_line_through_averages/",
        "text": "**Question** here : https://i.imgur.com/DqZyuVz.png\n\nI'm not too sure how to go about answering this.\n\nWe have that the slope of the regression line for IQ ~ Income is \n\n    r * SD[income] / SD[IQ]\n\nWhich is \n\n    0.5 * 45,000 / 15 = 1500\n\nBut this is the line through the original scatter plot, rather than the line\nthrough the plot of the averages. \n\n********\n\n#### How am I supposed to compute the line through the plot of the averages? \n\n********\n\n# Working ( attempt )\n\n*My idea here is to compute the value of the regression given for a value of\n85,000 and 95,000, and from these two points on the graph compute the slope of\nthe line through them*\n\n********\n\nIf I use the given information which corresponds to the scatter plot then I\nwould say that, for a shift of ~ 0.11 SD's below the average income I'll have\n\n    r*0.11 = ~ 0.056 SD's \n\nbelow the average IQ, which would be \n\n    0.056 * 15 = ~ 0.83,\n\nand then the IQ here would be 99.16.\n\nSo that for the mid point of the interval [80,90) I would have the point\n\n85,000 (income) -&gt; 99 (IQ)\n\nThis process can be repeated for the midpoint of the following interval, on [90, 100)\n\nI have an increase of 0.11 SD's *above* the average. And this will give (reusing\nthe previous values) an IQ of 100 + 0.83 ~ 101.\n\nSo that for the mid point of the interval [90, 100) I have\n\n95,000 (income) -&gt; 101 (IQ)\n\nThis gives two points and the slope can be computed for these as\n\n(101 - 99) / (95000 - 85000) = 1/5000 \n\nSo that I would say that it's closest to 1/6000, from the options given.\n\n******************************\n\nWhether or not I said that there's not an appropriate option would depend on\nwhether or not I was happy to accept a difference of 1/1000 I guess.... Which\nI'm not sure about (is there a convention?)\n",
        "created_utc": 1529407299,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for heteroscedasticity.",
        "author": "Juanfra21",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s6qwl/looking_for_heteroscedasticity/",
        "text": "I'm doing a multiple regression analysis and I'm currently looking for Heteroscedasticity, so I made a Predicted Value vs Standardized Residuals plot, and i got [this](https://i.imgur.com/YBzwMvR.png). In classes i've never seen a pattern like this, so i can't really say if there's heteroscedasticity or not.  \n\nI think this pattern is caused because when we asked people to give marks to ceartain aspects of a service to estimate how much they valued it, they tended to answer with whole numbers, despite the fact that they could answer with decimal numbers also, but then again I'm not really sure if it's because of that.\n\nI was also thinking of running the White test, but sadly I can't figure out how to do it in SPSS.\n\nAny help would be very appreciated!\n\n",
        "created_utc": 1529390660,
        "upvote_ratio": ""
    },
    {
        "title": "What is the gamma function in relation to the student's t distribution?",
        "author": "MILFtonFriedman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s610v/what_is_the_gamma_function_in_relation_to_the/",
        "text": "What are the inputs of the gamma function?",
        "created_utc": 1529382386,
        "upvote_ratio": ""
    },
    {
        "title": "I can't remember what to use this equation for. Please help!",
        "author": "desiboy05",
        "url": "https://i.redd.it/5lv08kynmv411.jpg",
        "text": "",
        "created_utc": 1529378753,
        "upvote_ratio": ""
    },
    {
        "title": "Some samples have greater weight than others. How can I represent this?",
        "author": "qazzquimby",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s4x3q/some_samples_have_greater_weight_than_others_how/",
        "text": "Idea is to track win rates of characters in a game. Characters can be changed during a round, so samples are weighted by how much the character was played. A character played 10&amp;#37; of the round it won would have its win rate increased less than one that was played 100&amp;#37; of the round, probably through simple multiplication.\n\nI'm looking to find a confidence interval of a character's win rate. I know how to do this with unweighted samples, but have no idea how with weighted samples.\n\nI can't duplicate samples  based on weight, since the same distribution with more samples leads to greater confidence.\n\nWould I just calculate the mean and standard deviation with the weights? Would my number of samples be the sum of the weights? I haven't found anything on this.\n\nEdit: Thank you very much all of you. That was fast and thorough, and very helpful.",
        "created_utc": 1529371667,
        "upvote_ratio": ""
    },
    {
        "title": "How to compare pie charts?",
        "author": "baby_taxi_driver",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s36eu/how_to_compare_pie_charts/",
        "text": "Let's say I have several pie charts (lets call them C1, C2, C3 and C4). They are each broken into 3 categories (A,B and C) with different percentages. Is there a statistical method that helps describe how similar/different the pie charts are? If someone could also explain why that particular statistical method is applied, that would also be great. Math isn't my strong point and the I struggled with the bit on the sidebar. ",
        "created_utc": 1529356798,
        "upvote_ratio": ""
    },
    {
        "title": "I don't understand odds ratio and confidence intervals",
        "author": "fastguy824",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s2twr/i_dont_understand_odds_ratio_and_confidence/",
        "text": "So here is a simplified format of my research: For each a category of CHADS score = 0, CHADS = 1, and CHADS = 2 or more, I tracked the \\[treatment &amp;#37;\\] in both \\[Cardiology specialty\\] or \\[Other physician specialty\\].\n\nFor example, let's say for a CHADS score of 2 or more, \\[Cardiology specialty\\] would prescribe treatment 30&amp;#37; of the time while \\[Other physician specialty\\] would prescribe treatment only 20&amp;#37; of the time.\n\nThe **odds ratio is 0.58 while the 95\\% CI is (0.34-0.97)**. So the CI seems pretty narrow, which means high precision. It does not include 1, so that means it is statistically significant (not sure why though). What does the odds ratio of 0.58 in this case mean? I've read how  \"OR&lt;1 means that exposure associated with lower odds of outcome.\" But I am confused on how to describe that based on my study. That there is a (1-0.58 = 0.42) 42&amp;#37; less chance that....?\n\nEDIT: I believe this may be an **\"adjusted odds ratio\" as we controlled for variables such as age, gender, ethnicity, etc**.",
        "created_utc": 1529354173,
        "upvote_ratio": ""
    },
    {
        "title": "Analyzing employee data about ethics in their workplace, What statistical tools would you suggest?",
        "author": "Jsreb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s2mjo/analyzing_employee_data_about_ethics_in_their/",
        "text": "It has been awhile since I've taken a statistics course and have only used it with a narrow focus ever since.  I am tasked with a scenario that is forcing me to revisit statistical tools I haven't touched in awhile.  Hopefully someone can steer me in the right direction.\n\n&amp;nbsp;\n\nHere is the information I have to review:\n\n* Data came from one location. N=200\n* The request for data was sent to 240 employees.  Data was collected anonymously.\n* There are 16 metrics to measure employees' perception of ethics in their workplace (i.e. Integrity, Idealism, Amorality, etc.)\n* The metrics are measured on a scale from 1-100.\n* Based on the scale, I have the mean, median, mode, std. deviation, high, low, and percentiles (25, 50, 75).\n* [For each metric, I've been provided a graph to understand the distribution of the data.](https://imgur.com/oFHDKMl)\n* Not all data is normal.\n* Some metrics, by definition, are related but I have not measured a statistical correlation yet.\n\n&amp;nbsp;\n\nBased on the data I have, what tools would you suggest I use to identify problems and opportunities within the organization and develop a recommendation for changes that can be made within the organization?  \n\n&amp;nbsp;\n\nIf additional data/information would be useful, what else should I try to retrieve?",
        "created_utc": 1529352654,
        "upvote_ratio": ""
    },
    {
        "title": "[Book Recommendation] Please help me pick a good book on introductory probability theory",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s0h9g/book_recommendation_please_help_me_pick_a_good/",
        "text": "[deleted]",
        "created_utc": 1529336411,
        "upvote_ratio": ""
    },
    {
        "title": "How do payouts in prediction markets work?",
        "author": "RickAndMorty101Years",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8s072w/how_do_payouts_in_prediction_markets_work/",
        "text": "I've been reading about \"prediction markets\" where you put a probability on an event occurring and you'll win money if your prediction is closer to the outcome than the average of the other guesses. But how does this work exactly?\n\nFor example, imagine there is a three person prediction market. A bets $10 on a 10% chance. B bets $20 on a 20% chance. C bets $30 on a 30% chance. The event does not happen. So how is the money then distributed?\n\nA obviously \"won\" but how much money do they get since they bet less money?",
        "created_utc": 1529334244,
        "upvote_ratio": ""
    },
    {
        "title": "\"Significant\" numbers and their probability",
        "author": "maxcap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rz9fb/significant_numbers_and_their_probability/",
        "text": "A recent (at times unsavory) dialogue in /r/peloton is making me second guess myself. Could somebody chime in and put my mind at rest?\n\nHere's the link:\n\nhttps://www.reddit.com/r/peloton/comments/8rjfgs/chris_froome_drug_defence_suffers_blow_after/e0s7b6g/\n\nFor context: Chris Froome, a pro cyclist, is awaiting the outcome of a failed drugs test, where the threshold for a substance called salbutamol is urine output of 1000 ng/ml. Froome had 2000 ng/ml in his urine, exactly double the limit.\n\nThe OP remarked how unlikely/suspicious it was that Froome's level was exactly twice the threshold amount.\n\nI - and others - said it was no more likely than any other level.\n\nWho is right?",
        "created_utc": 1529326382,
        "upvote_ratio": ""
    },
    {
        "title": "Need help understanding discrepancy of sample size (n) in odds ratio analysis",
        "author": "JazzOctober",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rz771/need_help_understanding_discrepancy_of_sample/",
        "text": "[immigrant adults substance use disorder](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4211337/)\n\nHello! I need help understanding table 2 and table 3 (odds ratio and adjusted odds ratio between reference group of native US adults to first-gen immigrants and second-gen immigrants).\n\nIn the lit, the native US adults is 15,733 (as mentioned in methodology section) but table 3 has different number of 13,077. Also, I think table 3 has a typo because I think it meant to say that it is comparing US natives to second gen immigrants (and not first-gen). \n\nCan someone explain to me why the n is different in table 3 (n=13,077) when table 2 and methodology section explains that the n should be 15,733? \n\nI suspect that it has to do with the covariate adjustment but I’m not sure how they calculated this new n. \n\nThank you! ",
        "created_utc": 1529325775,
        "upvote_ratio": ""
    },
    {
        "title": "Help interpreting results of a chi square test",
        "author": "Roxy123456Q",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ryetj/help_interpreting_results_of_a_chi_square_test/",
        "text": "I would like to analyze the age at menarche between survivors and controls. Unfortunately, due to the way the questionnaire was structured, I have the following groups: (1)ages below 8, (2) ages 8,9,10,11, (3) ages 12,13,14 and (4) ages 15+.   \n\n\nI found that if I want to analyze if there is a difference in terms of age at menarche (first menses), the chi square test is a suitable test. I've ran the test and got a significant result, but I don't really know how to interpet this with the abovementioned variables. For reference, age at menarche &lt;8 years and &gt;15 years are considered abnormal.  \n\n\nhttps://i.redd.it/4d798j3mhq411.png\n\nHere is a few things I've noticed:\n\n* The p value is 0.044, so the difference is significant. But what difference? I don't know what this is referring to.\n* No participants (survivors or controls) fell into category 1.\n* In category 4 (abnormal category): more survivors fell into this category than what was expected (count &gt; expected count).\n\nWould it be better to recode the variable into a dichotomous variable with the following ctaegories (1) abnormal age at menarche or (2) normal age at menarchy. Or would this not make a difference?\n\nIdeally, I would want to say something like 'survivors reported an abnormal age at menarche more frequently than controls', but I don't know if this is possible based on the results of the chi square test.   \n\n\nFurthermore, I don't understand why the Cramer's V test revealed a value of 0.073 (since it's close to 1, it's considered a strong relationship), when the p value is actually barely significant.\n\nThanks in advance!",
        "created_utc": 1529317101,
        "upvote_ratio": ""
    },
    {
        "title": "Examples of regression not making sense.",
        "author": "freedamanan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rybxr/examples_of_regression_not_making_sense/",
        "text": "\n\nI have a text that gives an example of someone using regression in a nonsensical\nway.\n\nHere's the example that they give : https://i.imgur.com/8klHU7C.png\n\nI'd like to read some other basic examples about regression just not making\nsense like this though. Not because the relationship is non-linear, but just\nbecause things haven't been understood properly.\n\nI'm not sure how to better phrase that, because I don't understand enough myself. Which makes searching for it a bit tricky too.\n\nThanks\n",
        "created_utc": 1529316143,
        "upvote_ratio": ""
    },
    {
        "title": "Need some help figuring out how to go about calculating significance for my Masters",
        "author": "YajSernal",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rv7af/need_some_help_figuring_out_how_to_go_about/",
        "text": "Hi!\n\nSo I'm finishing up my thesis for a masters and I'm unsure how to approach calculating significance for my results.\n\nFor clarity, I'm calculating changes in expression of different proteins through qpcr and each trial was run 12x.\n\nI have a mean value for each trial, however I'm not sure how to go about getting either a Z or T score  (I'm a beginner with stats). Would I compare these means against the control group (which is always normalized to 1) or would I need to research to determine what the \"expected change\" is?\n\nAny advice would be greatly appreciated!",
        "created_utc": 1529280524,
        "upvote_ratio": ""
    },
    {
        "title": "Conduct a test at the alpha=.01",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ruoc5/conduct_a_test_at_the_alpha01/",
        "text": "[deleted]",
        "created_utc": 1529275364,
        "upvote_ratio": ""
    },
    {
        "title": "Pooled vs. Unpooled [First Year Uni. Stats]",
        "author": "jfg902",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rt10w/pooled_vs_unpooled_first_year_uni_stats/",
        "text": "Can someone explain to me what the difference is between a pooled and an unpooled two sample test is? How do I tell the difference when I am given a question (that to me seems very similar?)",
        "created_utc": 1529260720,
        "upvote_ratio": ""
    },
    {
        "title": "Conditional probability question: Can someone confirm if my answers are right?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rsyy3/conditional_probability_question_can_someone/",
        "text": "[deleted]",
        "created_utc": 1529260209,
        "upvote_ratio": ""
    },
    {
        "title": "Regression Discontinuity (RD) Design at the individual level",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rsab9/regression_discontinuity_rd_design_at_the/",
        "text": "[deleted]",
        "created_utc": 1529254344,
        "upvote_ratio": ""
    },
    {
        "title": "Time Series forecasting with just 2 data points as reference.",
        "author": "Scarface05",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rqqd7/time_series_forecasting_with_just_2_data_points/",
        "text": "Hi,\nI have two data points( a data point from the year 2001 and 2011). Are there any reliable forecasting methods for such a short time series?\n\nThank you.",
        "created_utc": 1529238793,
        "upvote_ratio": ""
    },
    {
        "title": "Type I and type II error",
        "author": "SnowHow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rqjmq/type_i_and_type_ii_error/",
        "text": "Hey guys, I wanted to ask you why do we call type I error size of the test and type II error the power of the test. Is there some reason for these names  - size and power?\nThanks!",
        "created_utc": 1529236382,
        "upvote_ratio": ""
    },
    {
        "title": "Stats help",
        "author": "StatesmanRye",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rkmzx/stats_help/",
        "text": "1: A clinical trial evaluated a new compound designed to improve wound healing in trauma patients. The new compound was compared against a placebo. After treatment for 5 days with the new compound or placebo, the extent of wound healing was measured and the data are shown in Table 7-45. Is there a difference in the extent of wound healing by treatment? Run the appropriate test at a 5&amp;#37; level of significance (Hint: Are treatment and the percent of wound healing independent?)\n\nhttps://i.redd.it/blynpdb98e411.png\n\nHelp, I don't even know what formula to use!? How to get started? Or what my next steps would be!Not looking for an answer, just guidance. ",
        "created_utc": 1529168077,
        "upvote_ratio": ""
    },
    {
        "title": "Can I compare effect size of regression and correlation?!",
        "author": "sassafrasfly76",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rjqpv/can_i_compare_effect_size_of_regression_and/",
        "text": "Trying to analyse correlational study. Have supporting material that used a regression not correlation. Is r the same for both. I apologise if this is an obvious question. ",
        "created_utc": 1529159918,
        "upvote_ratio": ""
    },
    {
        "title": "Start my first “real” data job next week - need some advice.",
        "author": "zetaphi938",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rjgck/start_my_first_real_data_job_next_week_need_some/",
        "text": "So, I start as a data manager for a small school system on Monday and I want to get some advice from pros who work in educational statistics. Any good resources for research design? What challenges have you run into? Any advice helps, thanks! ",
        "created_utc": 1529156994,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing two chi square values",
        "author": "tal_spi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rjf8f/comparing_two_chi_square_values/",
        "text": "Hi\n\nI have bunch of nominal variables and i want to to test them with chi square, separately by gender.\n\nHow can i test the differences between the chi square values between male and female?\n\nThanks",
        "created_utc": 1529156633,
        "upvote_ratio": ""
    },
    {
        "title": "Simple question: What simple free graph tool should I use?",
        "author": "Longr33n",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rieh6/simple_question_what_simple_free_graph_tool/",
        "text": "Hey, I am trying to graph my grades through school and highlight moments etc, and I want to have the dates at the bottom (horizontal axis iirc) and grade, but sorted where it goes like this:\n0-5-4-3-2-1.\n\nIt is because in my country, 1 is the best and 5 the worst, and I dont want 5s to be at the top of the graph, as the top should be the best grades.\n\nI tried excel, but it nearly gave me stroke because I just can't understand the program.\n\nAny advice on my listed problems and also advice on the program?\n\nThanks for any answers!",
        "created_utc": 1529142575,
        "upvote_ratio": ""
    },
    {
        "title": "Logistic regression: less than 50% predicting accuracy but \"significant\" p-values",
        "author": "Fatlark",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rh0f6/logistic_regression_less_than_50_predicting/",
        "text": "Hi,\n\nI am trying to find whether my classifier is significant. I don't understand how significance is calculated for this (I am using a package to calculate it). Because of this, I am comparing the results for the classifier I care about to another which shouldn't be significant (a second classifier which has 50&amp;#37; accuracy). I am finding that the second classifier, even though it can't predict with &gt;50&amp;#37; the \"p-value\" associated with it is less than 0.05. What gives?\n\nAlso, another classifier I have can predict with 55&amp;#37; accuracy and a positive coefficient for label A (as opposed to label B). However, when I compare the means between A and B, B has an insignificantly higher mean than A. This also worries me. I can imagine that strong outliers could skew the mean, but even when I throw out the highest and lowest 10&amp;#37;, B still has a higher mean. !?!\n\nFurthermore, when I put in random values for my only feature, I seem to get p-values lower than 0.05 more than 12&amp;#37; of the time!?!?\n\nThanks",
        "created_utc": 1529122665,
        "upvote_ratio": ""
    },
    {
        "title": "Logit Regression: Should I include 'Negative results' in the analysis?",
        "author": "MeneerPuffy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rd4w6/logit_regression_should_i_include_negative/",
        "text": "I have a data set about military interventions from different countries in civil wars. It contains a list of civil wars with 4 possible outcomes.\n\n    1. US intervention\n    2. Soviet Intervention (its a cold war data set) \n    3. Intervention by 'other'\n    4. No outside intervention\n\nThe data set contains, per war, a lot of variables on the nation that is suffering the war (GDP, political system, location, etc)\n\nI am running Logit regressions to find out which factors influence intervention from one of the 2 major cold war factions.\n(For example, is the 'democracy score' significant for the United States, or does is the fact that the country is fighting a war of independence statistically significant for the Soviet Union?)\n\nHowever, I was wondering that if I analyzed, for example, US interventions whether or not I should include / exclude Non-US interventions.\n\nWhen looking at previous research, I see researchers do both. In one article the researcher excluded all Non-US interventions, reasoning that he was only interested factors that influenced US interventions.\n\nAnother article ran the regressions on a single data set that included all nations. Neither really went into the 'why' of their decision, leaving me to wonder whether their results were distorted.\n\nWouldn't excluding 'NON-US' interventions create 'false positives'? (As including cases where US intervention did not happen might reveal them to be just a fluke). In many cases the N is somewhat small (below 120), so I am a bid paranoid about things that might distort the results.\n\nWhen running regressions I've also made sure to include 'negative' results in order to get a more complete / realistic model, and I would assume that would apply in this situation as well, am I missing something?",
        "created_utc": 1529086707,
        "upvote_ratio": ""
    },
    {
        "title": "interpretation of logit coefficient (binary independent variable)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rcw0c/interpretation_of_logit_coefficient_binary/",
        "text": "[deleted]",
        "created_utc": 1529084812,
        "upvote_ratio": ""
    },
    {
        "title": "Help!",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8rbk5i/help/",
        "text": "[deleted]",
        "created_utc": 1529074527,
        "upvote_ratio": ""
    },
    {
        "title": "Anyone have any idea on how to solve this? Absolutely cluesless atm",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/learnmath/comments/8r8dru/college_statistics_chebyshev_inequality_to/",
        "text": "[deleted]",
        "created_utc": 1529041958,
        "upvote_ratio": ""
    },
    {
        "title": "Women consistently report less sexual partners and less sexual encounters. Is that even possible?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r81h9/women_consistently_report_less_sexual_partners/",
        "text": "[deleted]",
        "created_utc": 1529034885,
        "upvote_ratio": ""
    },
    {
        "title": "Factor Mixture Model vs Gaussian Mixture Models",
        "author": "Pocshi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r7td5/factor_mixture_model_vs_gaussian_mixture_models/",
        "text": "I cannot seem to understand the difference between the two. I have been looking into factor mixture models but it seems to be used by very few people compared to Gaussian mixture models.\n\nI'm hoping to understand what are the differences and why people seemingly prefer GMM.\n\nThanks for all your help!",
        "created_utc": 1529032456,
        "upvote_ratio": ""
    },
    {
        "title": "What are some common measures of longitudinal intra-individual variability in X, assuming that X is decreasing for each individual over time at a different rate?",
        "author": "sublimesam",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r6gwh/what_are_some_common_measures_of_longitudinal/",
        "text": "I have a population of individuals with repeated measurements for lung function over time (on average, 4-10 measurements over 10-20 years). Almost everyone's lung function declines over time in adulthood, but to varying degrees. I can describe this slope, but am looking for ways to describe variation around the slope (i.e. who declines in a very stable/predictable way and whose measurements bounce all over the place). \n\nI know that one measure of relative variability is the coefficient of variation, or the ratio of the standard deviation to the mean. However, it occurs to me that this measure would just be correlated to the magnitude of an individual's decline, and not necessarily a good measure of how much variation there is around the slope. ",
        "created_utc": 1529019297,
        "upvote_ratio": ""
    },
    {
        "title": "Is statistics on it's way to be automated? I see projects such as Automatic Statistician on the rise?",
        "author": "angelface101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r63hs/is_statistics_on_its_way_to_be_automated_i_see/",
        "text": "",
        "created_utc": 1529016008,
        "upvote_ratio": ""
    },
    {
        "title": "\"Linear Regressions aren't the Philosopher's Stone\"",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r49gz/linear_regressions_arent_the_philosophers_stone/",
        "text": "[deleted]",
        "created_utc": 1529001878,
        "upvote_ratio": ""
    },
    {
        "title": "Creating evenly matched teams in my office's table football",
        "author": "lystellion",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r446o/creating_evenly_matched_teams_in_my_offices_table/",
        "text": "We play table football at my office. And with the World Cup coming, we'd like to set up a tournament. It would be good to come up with some balanced teams. \n\nWe play doubles, so it's always 2 people vs 2 other people. You can either be in defence or attack for your team. I've recorded my past 50\\* matches (under initials AB) and logged all of these variables, plus the final score, in this spreadsheet: [https://docs.google.com/spreadsheets/d/1IygRzpUsfYz5NqwdEOnodXLxCjZ2kG6n\\-LVpExcX44E/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1IygRzpUsfYz5NqwdEOnodXLxCjZ2kG6n-LVpExcX44E/edit?usp=sharing) \n\nEach person is represented by their initials, eg MM or AX or whatever. (My own performances can be very poor, as you can see ...) \n\nI'd like to figure out:\n\n1) How can we best create evenly balanced teams?\n\n2) What are the relative strengths/weaknesses of everyone in attack and defence? \n\nI figure that for a really rough estimate, you could just see what someone's winning percentage is across all the matches. But there must be something that controls properly for all variables. It's not much to win if you're paired with the strongest player in the office. And it's a good result to lose 10\\-8 if you're a weaker player and playing a really strong pair.\n\n\\*I know this dataset is super small, but I'm just trying to get something vaguely indicative. I'm constantly adding to this so even if it would take months or years to get some good data, it would be nice to have a model to plug results into.",
        "created_utc": 1529000757,
        "upvote_ratio": ""
    },
    {
        "title": "Does the predictor variable goes first when describing an association?",
        "author": "MoraHJ",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r41l6/does_the_predictor_variable_goes_first_when/",
        "text": "Hi all, I have a simple question that is causing me a headache. \n\nI conducted negative binomial regression model to assess the association between X (predictor variable) and Y (response variable). When I am describing the results, I am not sure if I should name the predictor variable first (e.g. We found a positive association between X and Y) or name the response variable first (e.g. We found a positive association between Y and X), or if this really doesn't matter.\n\nSo, what do you think is the correct way?",
        "created_utc": 1529000163,
        "upvote_ratio": ""
    },
    {
        "title": "Why does my linear regression fail according to this data?",
        "author": "Moose_117",
        "url": "https://i.redd.it/b684mxmq00411.png",
        "text": "",
        "created_utc": 1528996058,
        "upvote_ratio": ""
    },
    {
        "title": "Weibull plots",
        "author": "Hoefsdavid9701",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r33qq/weibull_plots/",
        "text": "Hi everyone,\n\nI am doing a summer internship for a semiconductor manufacturing company and they are wanting me to create Weibull plots/distributions for them. Before now I have never heard of this and have been doing a lot of research online. I am having some trouble understanding it though. Could any of you help explain to my why someone would use a Weibull plot and in what occasions you should use one? \n\nThanks for any help",
        "created_utc": 1528992834,
        "upvote_ratio": ""
    },
    {
        "title": "Determine Sample Size for a Pass/Fail audit",
        "author": "Kickassness",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r2oeg/determine_sample_size_for_a_passfail_audit/",
        "text": "I've been searching for a good sample size for a quality audit, where unfortunately, the team doesn't know what to exactly look for. The audit consists of looking at unused case files to ensure the staff as a whole is properly filling out information correctly. I assume if there is any error in the file, it is rejected. \n\nI am currently looking at Acceptance Sampling and am wondering if this is the best technique for this audit. \n\nWould something like this be appropriate? \nhttp://garmentstech.com/full-aql-acceptable-quality-level-inspection-level-2-5/\n\nConcerning the number of cases, we have an mean of 1650, median of 1566 and a population standard deviation of 198.94",
        "created_utc": 1528989611,
        "upvote_ratio": ""
    },
    {
        "title": "Should I study statistics? [in Europe/EU]",
        "author": "MateBubalo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r2ldc/should_i_study_statistics_in_europeeu/",
        "text": "Hi, so I'm in 3rd(11th) grade in a Croatian high school that's focused on computer science/engineering. I like and am decent at programming(even though we barely learn any since the curiculum is dated af) but I don't think I want to study it and I don't want it as a career path. The engineering/technical classes we have I'm mostly mediocre at (Bs and Cs) so studying anything like EEng or Mech.Eng is completely out of the question. So the only thing I can see myself studying is mathematics, statistics, economics, analytics or something of the sort. \n\nI was always a natural at math up until last year when my math avg fell from an A to a C because we went from algebra and arithmetics to entirely trigonometry,geometry and vectors. So I think studying mathematics is also pretty much not an option. I also love problem solving and I'm the best at worded math tasks.\n\nPhysics I've always been terrible at and I find it completely uninteresting.\n\nAlso here's my grade averages for each year:\n\nfirst year: 4.5/5 \nMath was an A \n\nSecond year 3.9/5\nMath was a B\n\nThird year 4.1/5\nMath was a C\n\nSo if you've bothered to read all of this pointless text, here are my questions:\n\n1. Should I study statistics if I'm moderately interested in it?\n\n\n2. Are my grades sufficient to get into any decent university in Europe?\n\n3. Does anyone have any university recommendations that I should look into (anywhere except the US but preferably EU)?\n\n4. How does application to universities outside of your country work? Can anyone share some experiences/tips?\n\n\nSorry if this I come off as a dumbass I have no idea how applying to college works.\nAlso this might be terribly formatted since I'm on mobile.\n\n\n",
        "created_utc": 1528988966,
        "upvote_ratio": ""
    },
    {
        "title": "Is a house number nominal or ordinal data?",
        "author": "abu_rakan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r1mgq/is_a_house_number_nominal_or_ordinal_data/",
        "text": "",
        "created_utc": 1528980985,
        "upvote_ratio": ""
    },
    {
        "title": "With a probability of 1/6 beings true and 5/6 being false why is the probability of 2 things being true higher than 1?",
        "author": "greenking2000",
        "url": "https://i.imgur.com/rVfYoEg.jpg",
        "text": "",
        "created_utc": 1528977580,
        "upvote_ratio": ""
    },
    {
        "title": "Was the statistical risk justified?",
        "author": "Zorak6",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r17pc/was_the_statistical_risk_justified/",
        "text": "There is a plot point in a story I've read that hinges on someone being confident they can win a game they've fixed.\n\nThey take a huge risk for relatively little reward (like betting a house to potentially win a car).\n\nIt is presented as a 50-50 chance game, but is in fact a 2/3 to 1/3 chance in favor of the game fixer.  So he has a 1/3 chance of losing the game and losing everything for a relatively small gain.\n\nHere is the thing though..  the competitors will play until someone wins 10 times.  Whoever wins 10 times first, wins the bet and claims the others prize.\n\nEverything tells me that the amount of games is irrelevant to the stated statistics.  As it's total games and not winning games in a row or anything like that, there should still be a 1/3 chance that the opponent would win 10 games first.\n\nThe game fixer saw this bet as practically a sure thing.  He was willing to risk everything for a small gain.  If I am correct in my assessment, I can only conclude it was a flaw in the writing of the story or in the character himself.\n\nAm I wrong?  Does the chance of winning 10 times first lower the probability that the game fixer would lose to less than 1/3rd?  Or was this a flawed decision on the part of the fixer? ",
        "created_utc": 1528977046,
        "upvote_ratio": ""
    },
    {
        "title": "Basic Stata questions",
        "author": "needinganswersldn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r0i8a/basic_stata_questions/",
        "text": "Hi all,\n\nBasic questions, but can't seem to figure these out. Grateful for any thoughts on tackling these.\n\n1.  I have a variable which shows when people have logged-in to different parts of a website. This is just a list of empty cells and timestamps (which I think are strings). I'd like to see which section is most popular. I think the way to do this is to create a new var for each section, and replace the timestamp with a '1' if one is there, and a 0 if nothing is there. However, I can't figure out how you replace any value (i.e. the timestamp) with a 1. Usually all I've done is replace different values (e.g. x=1, y=2, z=3) but not any value! What do I add to the below code to make this work?\n\ngen newvar=.\nreplace newvar=1 if oldvar=(? - this is the timestamp)\nreplace newvar=0 if oldvar=. (? - I think this is correct)\n\n2. Once I've done the above, I'd like to sum the above to show the number who have logged into two particular sections. Is there a way to include an AND statement when creating a new var? E.g. sumvar=1 if newvar1=1 AND newvar2=1?\n\n3. Finally, we'll have timestamps for the first entry into the site and the user's last entry. I'd like to create two new vars: one for users who returned 7 days following their first entry, and one for users who returned 14 days after their first entry. Is there a way of doing this, using the timestamps? I think these are string values, so I'll need them to destring first..\n\nThanks all! Any help you can offer would be much appreciated.",
        "created_utc": 1528968913,
        "upvote_ratio": ""
    },
    {
        "title": "Am I wrong in assuming that the bayesian estimate has to converge to the true mean?",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8r061m/am_i_wrong_in_assuming_that_the_bayesian_estimate/",
        "text": "I wrote a small simulation in R wherein I simulate two slightly  different groups that are normally distributed. I then simulated a  series of ten identical experiments where 25 new participants are  sampled for each group for each experiment.\n\nI start with a simple bayesian model with a noniformative prior for the effect of group on Y. Then:\n\n* I take the posterior mean and sd of the model\n* construct a new prior from the posterior mean and sd\n* feed this prior into the model for the next experiment\n\nand so on and so on. basically, the posterior of experiment t\\-1 becomes the prior for experiment t.\n\nMy intuition would suggest that over time, the estimate should slowly  converge towards the true mean of the population. However, this is not  what happens. Am I missing something?\n\nBasically, what I wanted to simulate is this: \"Will the estimate become better over time even if measurement techniques are unreliable, effects are small, and n is small simply by studying the effect over and over again?\"\n\nHere is the R\\-Code in question, I added some comments:\n\nEDIT: The problem has been fixed. The code below is the update version wherein changes are shown with a #&amp;#37;&amp;#37;&amp;#37;&amp;#37; frame\n\n    #########################################################################################\n    # loading the libraries\n    #########################################################################################\n    \n    if (!require(tidyverse)) install.packages('tidyverse')\n    #if (!require(cowplot)) install.packages('cowplot')   # nicer plots\n    #if (!require(beepr)) install.packages('beepr')       # sends a \"beep\" useful for slow machines like mine\n    if (!require(tidyverse)) install.packages('tidyverse')  \n    if (!require(brms)) install.packages('brms')\n    library(tidyverse)\n    library(cowplot)\n    library(brms)\n    library(beepr)\n    \n    #########################################################################################\n    # Simulating Data\n    #########################################################################################\n    \n    # difference between means of two groups\n    difference &lt;- 0.2\n    \n    # variation within group\n    variation &lt;- 1\n    \n    # number of experiments\n    experiments &lt;- 10\n    \n    # samples per experiment\n    n &lt;- 25\n    \n    #------------------------------------------------------------------------------------\n    #simulation of new data for the two groups\n    #------------------------------------------------------------------------------------\n    \n    mysample &lt;- c(rnorm(n,0,variation),rnorm(n,difference,variation))\n    df &lt;- data.frame(mysample,group=rep(c(0,1),each=n))\n    \n    #------------------------------------------------------------------------------------\n    #variables reserved for the for loop\n    #------------------------------------------------------------------------------------\n    \n    mofs &lt;- rep(NA,experiments)\n    mypmean &lt;- rep(NA,experiments)\n    mypsd &lt;- rep(NA,experiments)\n    \n    # dimensions of the plot\n    minva &lt;- difference-(variation*3)\n    maxva &lt;- difference+(variation*3)\n    \n    \n    #########################################################################################\n    # Kickstart the Model\n    #########################################################################################\n    \n    # We first need to kickstart the model with some rather broad prior:\n    bmodel1 &lt;- brm(formula=mysample~group,\n                   prior=c(\n                     set_prior(\"normal(0,50)\",class=\"Intercept\"),\n                     set_prior(\"normal(0,50)\",class=\"b\",coef=\"group\"),\n                     set_prior(\"normal(0,50)\",class=\"sigma\")\n                     ),\n                   data = df,\n                   warmup = 200, iter = 1000, chains = 4,\n                   control = list(adapt_delta = 0.95),\n                   cores = getOption(\"mc.cores\", 4L))\n    \n    \n    #saveRDS(bmodel1,\"testing.rds\")\n    #bmodel1 &lt;- readRDS(\"testing.rds\")\n    \n    \n    #########################################################################################\n    # Extract the posterior summary\n    #########################################################################################\n    \n    # extract posterior\n    param &lt;- posterior_summary(bmodel1)\n    #create new priors from the posterior estimates\n    \n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    # Priors for intercept, sigma, and groups\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    newpriorInt &lt;- paste0(\"normal(\",param[[1]],\",\",param[[1,2]],\")\")\n    newpriorG &lt;- paste0(\"normal(\",param[[2,1]],\",\",param[[2,2]],\")\")\n    newpriorSig &lt;- paste0(\"normal(\",param[[3,1]],\",\",param[[3,2]],\")\")\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    \n    #########################################################################################\n    # Run the specified number of experiments\n    #########################################################################################\n    \n    for (i in c(1:experiments)) {\n    \n    # two groups with mean difference as specified and a variation as specified. combined as a data frame\n     mysample &lt;- c(rnorm(n,0,variation),rnorm(n,difference,variation))\n     # mysample &lt;- c(rep(0,n),rep(difference,n))\n      df &lt;- data.frame(mysample,group=rep(c(0,1),each=n))\n      \n    # the mean difference between the groups of the sample \"mofs\" is saved just to see how it varied in each experiment  \n      mofs[i] &lt;- mean(df$mysample[which(df$group==1)]-\n                      df$mysample[which(df$group==0)])\n      \n      \n    #------------------------------------------------------------------------------------\n    #The kickstarted model is updated with new data AND a new prior (the posterior of the experiment before)\n    #------------------------------------------------------------------------------------\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    # the updated model is now recompiled for each experiments. It takes ages but it works\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    \n      tmp &lt;- update(bmodel1,prior=c(\n          set_prior(newpriorInt,class=\"Intercept\"),\n          set_prior(newpriorG,class=\"b\",coef=\"group\"),\n          set_prior(newpriorSig,class=\"sigma\")\n        ),newdata = df)\n    \n    \n      \n    #------------------------------------------------------------------------------------\n    # create a new prior from the posterior\n    #------------------------------------------------------------------------------------\n      param &lt;- posterior_summary(tmp)\n    \n      newpriorInt &lt;- paste0(\"normal(\",param[[1]],\",\",param[[1,2]],\")\")\n      newpriorG &lt;- paste0(\"normal(\",param[[2,1]],\",\",param[[2,2]],\")\")\n      newpriorSig &lt;- paste0(\"normal(\",param[[3,1]],\",\",param[[3,2]],\")\")\n      \n      # save the estimated difference between both groups  and the sd\n      mypmean[i] &lt;- param[[2]]\n      mypsd[i] &lt;- param[[3]]\n    }\n    \n    \n    #####################################################################################\n    # Plot the results\n    ####################################################################################\n    \n    df &lt;- data.frame(estimatedDif=mypmean,\n                     index=c(1:experiments),\n                     sd=mypsd,\n                     sampledDif=mofs)\n    \n    \n    x11()\n    ggplot(df,aes(index,estimatedDif))+\n      geom_point(aes(index,sampledDif),alpha=1,color=\"lightblue\",shape=1)+\n      geom_point(alpha=1)+\n      ylim(c(minva,maxva))+\n      geom_hline(aes(yintercept=difference),color=\"green\")+\n      ggtitle(paste0(\"Sampled Mean (blue) and Estimated Mean (black) for \",experiments,\" Experiments and n=\",n))\n    \n    x11()\n    ggplot(df,aes(index,estimatedDif))+\n      geom_smooth(method = \"loess\")+\n      geom_hline(aes(yintercept=difference),color=\"green\")\n    \n    beep()\n\n[\\(OLD\\)Here is the plot after 300 Experiments with n=25 and a difference of 0.2 with sd 1:](https://i.redd.it/0ri9gxbj0z311.png)\n\nUPDATE: IT WORKS:\n\nhttps://i.redd.it/ts1q9159xz311.png\n\nThanks to StephensSRMMartin I included a prior for the intercept and sigma as well. An thanks to rutiene I discovered that the \"update()\" function simply did not incooperated the new priors . I updated the new code above and marked the changes with #&amp;#37;&amp;#37;&amp;#37;&amp;#37;&amp;#37;&amp;#37;&amp;#37;&amp;#37;&amp;#37;&amp;#37;.",
        "created_utc": 1528964337,
        "upvote_ratio": ""
    },
    {
        "title": "How to sort items according to their weight + frequency?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qznto/how_to_sort_items_according_to_their_weight/",
        "text": "[deleted]",
        "created_utc": 1528958017,
        "upvote_ratio": ""
    },
    {
        "title": "Grouping categories together within a categorical variable",
        "author": "gasolinecanman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qvn6u/grouping_categories_together_within_a_categorical/",
        "text": "Hi all,\n\nI have a categorical variable that I want to include in a regression model, however, this variable has over 500 categories.\n\nIs there any statistical method I can use to group categories together if they perform in a similar manner to each other?\n\nThanks.",
        "created_utc": 1528920870,
        "upvote_ratio": ""
    },
    {
        "title": "Hypothetical question (Time series statistics)",
        "author": "zzzzz94",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qoue5/hypothetical_question_time_series_statistics/",
        "text": "This is an applied question.\n\nSuppose you are a statistician for the state prison system, where there are a number of prisons (virtually identical to each other on the surface) and they have safety incidents, things like stabbings, escapes, etc. You find most obey a white noise process from week to week but one follows an AR process. What could be some adequate explanations for this? Is it possible that the prison that follows the AR process is not taking adequate counter measures during periods of conflict, for example? What are some other explanations why this could be the case?\n\nAlso a second question, suppose the true time series models of all the prisons were identical. How could you combine the data from all to get more data points to estimate a single model in R? (As in, say you only have 10 weeks of data, too short to get a good model estimate with a single prison's data, but if you combine 10 prisons you have 100 observations)\n\nNot a homework question",
        "created_utc": 1528856550,
        "upvote_ratio": ""
    },
    {
        "title": "How to determine if a result is statistically different from the mean",
        "author": "ikeepthingsinmyrump",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qopkp/how_to_determine_if_a_result_is_statistically/",
        "text": "I'm trying to determine if a particular result is statistically different from the mean of all results.  For example, if 100 people pick a random number between 1 and 1,000 and 5 of them pick the same number while the rest pick different numbers, how would I determine if that is a statistically significant result?  I suppose the \"mean\" I am speaking of is the frequency of a particular number being picked, so in this case close to zero.\n\nI apologize if I'm using incorrect nomenclature here, that may be one of the reasons my google-fu has failed so far.  I really appreciate any help I can get, even if it's just a link to somewhere I can read up on it!",
        "created_utc": 1528855321,
        "upvote_ratio": ""
    },
    {
        "title": "Need help with a model/statistical test (couldn't find answr in IDRE page in sidebar)",
        "author": "MCLennon93",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qnr02/need_help_with_a_modelstatistical_test_couldnt/",
        "text": "Hi /r/AskStatistics, apologies if this is the wrong subreddit to post in/please let me know what a more appropriate place to post would be.\n\nI have a large longitudinal dataset and I'm trying to figure out what the proper test is to model and analyze my data. I have ~800 subjects with varying number of timepoints, and time interval between each timepoint is variable. Each subject has a survey score at each timepoint (I’ll call this measure A). Additionally, there is another measure of which each subject only has one value (I’ll call this measure B). Is there a proper test to see if change in measure A over time correlates with measure B? I was thinking it might be some kind of linear mixed model because of the longitudinal data and different number of timepoints for each subject, but I wasn’t sure. Thanks!",
        "created_utc": 1528846544,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing racial, employment, and other multi-level independent variables in single test.",
        "author": "Makewayfordarkhelmet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qndkh/comparing_racial_employment_and_other_multilevel/",
        "text": "Running some descriptive stats \\[race, employment, martial status, etc.) to compare two non\\-paired populations. Currently each data category are formatted in a single independent variables with multiple levels. Race, for example, is 0 for white, 1 for black, 2 for asian, etc. Normally I would create dummy/binary variables for each level and run the appropriate chi/f test to compare the proportions for the control and intervention group. However, I was wondering if there is a test where I can just run the single variable with multiple levels across both groups? Said differently, is there a test that will return CI andsig. values for each 2x2 contingency even if the data are say 2x6?  When I tried to do a chi square test in R for white, for example (chisquare.test(mydata$X\\[mydata$X==0\\],mydata$y0 where X is race and y denotes a binary for control/intervention. ",
        "created_utc": 1528843311,
        "upvote_ratio": ""
    },
    {
        "title": "In statistical inference, why are the sampling distributions for mean and variance different?",
        "author": "TangoCJuliet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qnaz5/in_statistical_inference_why_are_the_sampling/",
        "text": "I'm trying to gain some understanding of concepts that I've memorized by rote.\n\nIn statistical inference when we don't know the true population parameters, the sampling distribution of sample mean is t\\-distribution, and sampling distribution of sample variance is the chi\\-squared distribution. \n\nNow I understand that this follows the assumption that the population parameter is normally distributed. Here are my questions: \n\n1. I get that we use the t\\-distribution when we don't know the true variance, but why does the t\\-distribution equal the normal distribution when there are 30 degrees of freedom? \n2. Why is the sampling distribution of sample variance a chi\\-squared distribution? Does this have something to do with having to square the standard error to calculate variance, and the standard error is assumed to be normal? (and chi\\-squared being the square of the normal distribution)\n\nThank you for any help",
        "created_utc": 1528842690,
        "upvote_ratio": ""
    },
    {
        "title": "confidence intervals",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qmch7/confidence_intervals/",
        "text": "[deleted]",
        "created_utc": 1528835347,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating population ratios from a survey?",
        "author": "KidzKlub",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qljqi/estimating_population_ratios_from_a_survey/",
        "text": "I am looking to calculate the Male Marriageable Pool Index as described by William Julius Wilson. There are many variations, but the simplest one is [#Employed men/#All women]. Usually this is grouped by race, age groups, education level, etc.\n\n[Brookings did a calculation](https://www.brookings.edu/wp-content/uploads/2015/09/Sawhill-922001.png), and the source is listed as \"Author's tabulations of 2012 American Community Survey IPUMS Data.\" My question is: can you assume that ratios in a survey like this (nationally representative) accurately reflect ratios in the population? For example, I am using the ASEC supplement of the Current Population Survey and found 181 cases of employed African American males between 20-24 in 1978, and 506 cases of African American females between 20-24 in 1978. Can I fairly conclude that the Male Marriageable Pool Index for this subgroup in the population is [181/506] = .358? It feels like I am only measuring the ratio with regards to people who responded to the survey, but if the survey is supposed to be nationally representative, then it should apply to the population right? \n\nI know that the survey population is \"non-institutionalized Americans\" which significantly affects things for African American males, but since institutionalized males are not typically considered part of the marriageable pool I don't think this should be a problem for me.\n\n",
        "created_utc": 1528829586,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone explain this statistical joke to me? Also isn't 10 just too smaller sample for a statistical test?",
        "author": "stupidredditwebsite",
        "url": "https://imgur.com/a/Jk7o6Xv",
        "text": "",
        "created_utc": 1528792815,
        "upvote_ratio": ""
    },
    {
        "title": "Chi-Squared - which factors are significant?",
        "author": "Trek7553",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qfh5j/chisquared_which_factors_are_significant/",
        "text": "I have a dataset with two columns: US State, and a binary outcome variable. Based on the guide from the sidebar, I think I want to use chi-squared. Using that test, I can tell that there is a difference among states in general. However, I want to know which states specifically have a higher or lower proportion of positive outcomes compared to all other states. Ultimately, I want to make a bar chart of all the states and highlight the ones that are significantly different from the overall mean. Do I need to repeat the test for each state against all others? I am using R, but general stats guidance is fine and I can sort out the code.",
        "created_utc": 1528771695,
        "upvote_ratio": ""
    },
    {
        "title": "How to explain the significance of MA and AR terms in an ARMA model?",
        "author": "zzzzz94",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qf9uf/how_to_explain_the_significance_of_ma_and_ar/",
        "text": "How would you explain them intuitively to someone with no training in statistics? Like if you get a model based off sales data, how would you explain it? (In the title I do not mean statistical significance)",
        "created_utc": 1528769911,
        "upvote_ratio": ""
    },
    {
        "title": "Best way to implement A/B testing?",
        "author": "ohai123456789",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qeheu/best_way_to_implement_ab_testing/",
        "text": "I want to A/B test two separate back\\-end recommendation logic (A and B) but I'm not sure which method works best (from a statistical standpoint). Here's how my application works:\n\n1. User goes to my application.\n2. Application recommends 5 widgets to purchase (using either Logic A or Logic B)\n3. User can choose to either buy or not buy.\n4. User can use this application over and over again.\n\nI narrowed down to 2 methods of A/B testing.\n\n* The first method is to randomly divide the users up into bucket A and bucket B. Bucket A will contain Logic A and bucket B will contain Logic B. **Example:** If Bob gets bucketed to Bucket A then Bob will always get Logic A.\n* The second method is to randomly serve up Logic A 50&amp;#37; of the time and Logic B 50&amp;#37; of the time to the users. **Example:** Bob could get Logic A the first time he visits the application. But then he could just as well get Logic B the second/third/fourth time he visits the application.\n\nIn both cases, I can keep track of which logic the user books or doesn't book from. Is method 1 or 2 better to see which logic works best?",
        "created_utc": 1528762971,
        "upvote_ratio": ""
    },
    {
        "title": "How to report correlation and partial correlation-help needed",
        "author": "sassafrasfly76",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qd0tw/how_to_report_correlation_and_partial/",
        "text": "I have conflicting information. Do we usu report on % (r2/pr2) or just r/pr?",
        "created_utc": 1528750940,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate the probability of getting a chi^2 of 10.4 from 4 data points?",
        "author": "rightside24",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8qbpcw/how_to_calculate_the_probability_of_getting_a/",
        "text": "This is NOT a homework question but for personal research.",
        "created_utc": 1528741656,
        "upvote_ratio": ""
    },
    {
        "title": "Error of normalization",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q92dk/error_of_normalization/",
        "text": "[deleted]",
        "created_utc": 1528719381,
        "upvote_ratio": ""
    },
    {
        "title": "I'm I thinking of Markov Chain?",
        "author": "whitebarrys",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q8pyr/im_i_thinking_of_markov_chain/",
        "text": "AM I THINKING OF MARKOV CHAIN? (Damn auto correct)\n\nI'm trying to find out the name of a event. Here is what I remember of it: Say you are going to flip a coin, so you start out, but at a certain point you get a bunch of tails in sequence even tho it's a 50/50 chance of getting heads and tails. Is there a name for this?\n\nHead = 0\nTail = X\n\nX00X0X00XX0X0X00**XXXXXXX**0XX00X0X00",
        "created_utc": 1528715764,
        "upvote_ratio": ""
    },
    {
        "title": "Conversion of data from 1 to -1 to 1 to 0.",
        "author": "Grantmitch1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q8i4m/conversion_of_data_from_1_to_1_to_1_to_0/",
        "text": "Hey guys,\n\nI was just wondering if there was a method of converting data that runs from 1 to -1 to 1 to 0? \n\nTo explain a little more what I mean: \n\nLet us suppose I am interested in voter attitudes towards Donald Trump (let's choose a contemporary issue). I could rank these as 1 depicting very favourable, 0 as neutral, and -1 as very negative. \n\nLet us suppose that the statistical method I wish to use requires these codes to run from 1 to 0. Is there a way for me to covert it so that:\n\n1 - very favourable\n0.5 neutral\n0 - very negative\n\nAny thoughts would be most welcome. \n\n",
        "created_utc": 1528713249,
        "upvote_ratio": ""
    },
    {
        "title": "Do these authors properly use statistics? Is a sample size of 5, 10 patients enough to make any useful quantifications or fit a model?",
        "author": "qdcm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q7j2u/do_these_authors_properly_use_statistics_is_a/",
        "text": "Please see the abstract and Figures 5, 6 of [this proton radiotherapy article](https://www.redjournal.org/article/S0360-3016(09\\)03634-7/fulltext).\\*\n\nIs this science statistically meaningful? Is that a proper use of a two-tailed paired t-test in Figure 5? What is being conveyed by these p values?\n\nI think this study is good for showing feasibility but does not establish a reliable model.\n\n\\* Gensheimer MF, Yock TI, Liebsch NJ, et al. IN VIVO PROTON BEAM RANGE VERIFICATION USING SPINE MRI CHANGES. Int. J. Radiation Oncology Biol. Phys. 2010 78(1):268-75.",
        "created_utc": 1528700650,
        "upvote_ratio": ""
    },
    {
        "title": "Do I need random sampling or simple random sampling?",
        "author": "chengxuyuan11",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q6z0p/do_i_need_random_sampling_or_simple_random/",
        "text": "I need to estimate the accuracy of a classifier which works independently on an object. I can only use 10% of the population to go through classifier due to high cost. I have 2 approaches to choose this 10%. \n\nApproach 1: randomly choose 10% of the population without replacement. \nApproach 2: I evenly divide the population into 10 subgroups, and randomly pick one subgroup. (This approach will be much cheaper if I have to estimate the accuracy of different versions of the same classifier to improve.)\n\nI understand that an object has the same probability of being selected in the above 2 approaches, which means both approaches are random sampling. But approach 2 is not simple random sampling. \n\nMy question is: can I use approach 2 to estimate the accuracy of the population? Do the samples need to be selected via simple random sampling to be statistically valid? \n\nThank you!",
        "created_utc": 1528694099,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating proportion of variance explained in non linear models",
        "author": "Consistent_Ruin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q674d/calculating_proportion_of_variance_explained_in/",
        "text": "I have a nonlinear model with 2 predictor variables.  Is it possible to calculate what percent of variance is explained by each of these predictor variables?",
        "created_utc": 1528686137,
        "upvote_ratio": ""
    },
    {
        "title": "Curious about what distribution for a type of problem",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q5ipd/curious_about_what_distribution_for_a_type_of/",
        "text": "[deleted]",
        "created_utc": 1528679596,
        "upvote_ratio": ""
    },
    {
        "title": "Operations between two random variables",
        "author": "ztnq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q4513/operations_between_two_random_variables/",
        "text": "I am curious about operations between 2 random variables.(Not hw, just curiosity and trying to learn edge cases before the exam) The other day I asked my professor and he told me the following\n\nP(X+Y|Y)=P(X)\n\nE[X+Y|Y]=E[X|Y]+E[Y](my mistake earlier)\n\nP(X|X)=P(X)\n\nP(X|X=x)=P(X=x)?\n\n\nThough I don't specifically understand the intuition or derivation of those properties.\n\nWhat about these ones? \nP(X+Y|Y=y)=?\n\nP(XY|Y)\n\nP(X,Y|Y)\n\nP(XY|Y=y)\n\nP(Y|Y=y)\n\nE[X+Y|Y=y]\n\n\n\n\n\nE[XY|Y]\n\nE[XY|Y=y]\nE[X,Y|Y)\nE[Y|Y=y]\nor any general operation between X and Y, from xor to division,how do I approach it when conditioned on Y or Y=y? Is there one rule for all?\n\nThanks in advance",
        "created_utc": 1528666970,
        "upvote_ratio": ""
    },
    {
        "title": "MGF of aX + b",
        "author": "statrowaway",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q3pl5/mgf_of_ax_b/",
        "text": "the mgf of the random variable X is\n\nM_x (t)=E(e^(tx))\n\nHow can I find the mgf of the variable (aX + b)? \n\nI guess my actual problem is; why does what I am doing here specifically actually work: http://mathb.in/25920 ? Specifically talking about going from line 2 to line 3.\n\nif mathbin didnt work here is picture: https://gyazo.com/e874c8a8e51bfa5dbcaa2b5f2ac9d6cc\n\n\nI suppose the more general way of doing a problem like this would be in the following way:\n\nhttp://mathb.in/25925\n\nif mathbin didnt work here is picture: https://gyazo.com/6144e7e8f32dc0bea44b144db8859b69\n\n\nWhy did the first method even work? what I am doing going from line 2 to line 3 seems incredibly faulty? Don't I need the pmf of Y? i.e the pmf of Y=aX+b to do it in that way?  ",
        "created_utc": 1528663329,
        "upvote_ratio": ""
    },
    {
        "title": "Formulas?",
        "author": "5k1rm15h",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q30k4/formulas/",
        "text": "Are there some sources of formulas you use as a go-to?\n\nI've been studying sampling recently and while looking online for confirmation or a second perspective, I've noticed that not a lot comes up outside of relatively simple equations.\n\n&amp;nbsp;\n\nA lot of the population estimates feel fairly intuitive to me but I might have trouble explaining to someone how to calculate the standard error of a population ratio from a clustered sample, for example.\n\n&amp;nbsp;\n\nAre there any good books or websites that specialise in listing formulas beyond the basics that you use as a reference?",
        "created_utc": 1528657425,
        "upvote_ratio": ""
    },
    {
        "title": "How to choose limits of integrals when working on joint distributions? Need help with general stragety",
        "author": "electric_tore",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q0v6m/how_to_choose_limits_of_integrals_when_working_on/",
        "text": "the pdf of f(x,y)\n    \n     f(x,y) = x + y     0 &lt; x &lt; y &lt; 2\n    \nMy trouble is figuring out what my limits should be when I analyze this pdf.\n\nFor instance.\n\nCalculating the area I use the limits\n  \n    0 &lt; x &lt; y and 0 &lt; y &lt; 2\n\nCalculating E(X) and E(Y) I use\n\n     0 &lt; x &lt; 2 for E(X) and 0 &lt; y &lt; 2 for E(Y)\n\nHowever, E(XY) is not calulated using the same limits. Not even the ones I used for the area. They are\n\n    0 &lt; x &lt; 2 and x &lt; y &lt; 2\n\nCalculating the marginal distributions yield limits\n\n       for fx x &lt; y &lt; 2\n       for fy 0 &lt; x &lt; y\n\nI am struggling finding a strategy to use the correct limits for the different calculations. \n\nThis is so confusing :S\n\nAny tips? \n  ",
        "created_utc": 1528638212,
        "upvote_ratio": ""
    },
    {
        "title": "unbiased estimator?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8q04ju/unbiased_estimator/",
        "text": "[deleted]",
        "created_utc": 1528628773,
        "upvote_ratio": ""
    },
    {
        "title": "Need help finding which test to run for this data. I looked at the link in community info, but I am new to stats and math in general and I need some additional help.",
        "author": "werewarbler",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8pxtz5/need_help_finding_which_test_to_run_for_this_data/",
        "text": "Originally I thought I should run a 2SampT-Test, but the values I’m getting don’t seem right.  Also i dont think this problem meets all requirements necessary to run this test.  For example, we already know the sample standard deviation, and and mean salaries.  \n\nThe National Association of Colleges (NAC) reports mean starting salaries for college graduates in various disciplines. NAC reported that the mean salary for 2014 engineering graduates was $62,719, whereas the mean salary for 2013 engineering graduates was $62,535. Assume that these statistics were drawn from independent samples of size 300, each with a sample standard deviation of $8,500. Test whether there was a significant increase in the salary of engineering graduates from 2013 to 2014, using a 10% level of significance.\na. State the null and alternative hypotheses \n𝐻0:\n𝐻1:\n\nb. What calculator test will you use? List the requirements that must be met to use\nthis test, and indicate whether the conditions are met in this problem.\n\nc. Run the calculator test and obtain the P-value.\n",
        "created_utc": 1528597068,
        "upvote_ratio": ""
    },
    {
        "title": "Does this Table Indicate that there is a Negative Correlation between RWA and Harm–care and how Strong of One?",
        "author": "[deleted]",
        "url": "https://i.redd.it/dlwczyr8u2311.png",
        "text": "[deleted]",
        "created_utc": 1528594606,
        "upvote_ratio": ""
    },
    {
        "title": "Where did this number come from?",
        "author": "5k1rm15h",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8px4vo/where_did_this_number_come_from/",
        "text": "Can anyone tell me where the number highlighted in red in the image - 5,005.6 - came from? There are a couple of possibilities I can think of from the information given.\n\n[; \\widehat{SE} \\left( \\bar{\\bar{x}}_{clu} \\right) = \\left( \\frac{10}{5005.6 \\left( .10 \\right) \\sqrt{3}} \\right) \\sqrt{4} \\sqrt{\\frac{50056-708}{50056}} ;]\n\nhttps://i.imgur.com/OXmBNdE.jpg\n\nhttps://i.imgur.com/FLIxYIV.jpg\n\nhttps://i.imgur.com/x9TP3un.jpg\n\n`M`= number of clusters in the population, `m` = the number of clusters sampled, and [; f_2 ;] is the 2nd-stage sampling frequency: the probability of a sample within `m` being selected, specifically within that `m`",
        "created_utc": 1528590022,
        "upvote_ratio": ""
    },
    {
        "title": "I just can’t figure out how to solve this due to brain fog. Any help would be much much appreciated &lt;3",
        "author": "[deleted]",
        "url": "https://i.redd.it/wklb1ewvg2311.jpg",
        "text": "[deleted]",
        "created_utc": 1528589840,
        "upvote_ratio": ""
    },
    {
        "title": "Need help with a simulation/modeling problem. Much appreciated!",
        "author": "JohnnyCaggz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8puc5m/need_help_with_a_simulationmodeling_problem_much/",
        "text": "We have to create a simulation to maximize revenue for an imaginary gym. The framework is that there's a gym that has 1500 that people come on a daily basis, 1000 of which come between 12-6. Men use the club on average for 60 minutes with SD of 10 mins, and women use it on average for 55 minutes with SD of 15 mins. The ratio of male to female attendance is 60 to 40 on average. The issue is that there's a capacity of 150, during the peak hours the gym is going above max capacity and causing members to leave. \n\nWe are proposing a time limit on gym members during the peak hours. I am trying to calculate the likelihood of capacity going over 150 based on different time limits (70 minutes - 30 minutes). I am not how to approach calculating the probabilities.\n\nThe easy thing to say would be 54 minutes, because at 54 minutes the average per hour is 150. Although that seems naive. Any help is appreciated.",
        "created_utc": 1528565146,
        "upvote_ratio": ""
    }
]