[
    {
        "title": "Currently using KR20 to check for consistency, would changing how the final score is calculated make it not a good indicator for reliability?",
        "author": "IComeFromAnotherLand",
        "url": "https://www.reddit.com/r/AskStatistics/comments/863ggq/currently_using_kr20_to_check_for_consistency/",
        "text": "I have a test that comprises of 128 multiple choice questions separated into 2 sections (section A 80, section B 48), we're planning on having the two sections contribute to 50% to their final mark going forward.\n\njust wondering if it would throw out the calculation of KR20 enough that it would make it incorrect to use, as it would affect the variance calculation step.\n\nCurrently planning to use KR20 separately between the 2 sections and using that when discussing reliability of the test, since i'm not sure about the above.",
        "created_utc": 1521649734,
        "upvote_ratio": ""
    },
    {
        "title": "Suggestions for how to analyze these data?",
        "author": "sozialwissenschaft97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/863985/suggestions_for_how_to_analyze_these_data/",
        "text": "I am looking at the relationship between globalization (independent variable) and media liberalization (dependent variable). I have data on these variables since 1970. I understand that the logic thing to do would be time series analysis. I don't have the time or experience to do this kind of analysis. Does anyone have any suggestions for a  simple way to analyze these data? Thanks.",
        "created_utc": 1521648295,
        "upvote_ratio": ""
    },
    {
        "title": "Finding joint probability density function of 2 dependent random variables",
        "author": "BeautifulPreparation",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85zzvw/finding_joint_probability_density_function_of_2/",
        "text": "I need help getting started on the following question:\nhttps://imgur.com/a/lsNMt\n\nI think the answer can be can be calculated with the following formula: https://imgur.com/a/3pPsC\n\nbut I'm not sure how to find f(x,w) which is the joint probability density function.",
        "created_utc": 1521613652,
        "upvote_ratio": ""
    },
    {
        "title": "Big amount of cash into bank account in small amount of time. Buying/Selling employee purchases. 25k+",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85y4yj/big_amount_of_cash_into_bank_account_in_small/",
        "text": "[deleted]",
        "created_utc": 1521593996,
        "upvote_ratio": ""
    },
    {
        "title": "R's lm.fit function",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85y2bl/rs_lmfit_function/",
        "text": "[deleted]",
        "created_utc": 1521593364,
        "upvote_ratio": ""
    },
    {
        "title": "Suggested books or sources on gambling, taking losses, risk management, or psychology on these topics?",
        "author": "cartmichael",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85xshq/suggested_books_or_sources_on_gambling_taking/",
        "text": "",
        "created_utc": 1521590990,
        "upvote_ratio": ""
    },
    {
        "title": "Just curious about a probability problem: How to fill 200 unique positions with two independent subsets containing 100 distinct positions each.",
        "author": "Sophroniskos",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85xnzl/just_curious_about_a_probability_problem_how_to/",
        "text": "Assume there is a set of 200 unique positions to fill. You have two subsets, each containing 100 distinct positions (i.e. there are no repetitions) that are totally independent from each other. This means that you can fill 100/200 of the positions with the first subset. The second set, however, will contain a lot of positions that are already filled, thus resulting in unfilled positions (probably). \nIt's not hard to compute the probability of having all positions filled or filling a specific position. But how do I calculate an expected value of how many positions I can expect to be filled by two sets? How about three sets?",
        "created_utc": 1521589913,
        "upvote_ratio": ""
    },
    {
        "title": "Variance of Sample mean of an MA(1) process",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85wn3k/variance_of_sample_mean_of_an_ma1_process/",
        "text": "[deleted]",
        "created_utc": 1521581978,
        "upvote_ratio": ""
    },
    {
        "title": "Rules of thumb for PCA.",
        "author": "TheRealBeakerboy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85v1vu/rules_of_thumb_for_pca/",
        "text": "I have an addin for MS Excel that performs a PCA on a range of data and plots the results. This add-in no longer works, and is no longer supported by the developer, so I'm working on doing it myself \"manually\".  \n\nSo far, I can tell that it is taking the data, and creating the correlation matrix. Then it is finding the eigenvalues and eigenvectors of that correlation.  \n\nMy first question is regarding the eigenvalues. The add-in displays the eigenvalue for each component, but the values appear to be the eigenvalue that I have calculated, multiplied by the number of data points, and divided by the number of variables. Is there a reason the correlation matrix might be multiplied by this scalar before calculating the eigenvalues?  \n\nThe add-in is choosing to use only a subset of the total number of components; the ones that are used all have an eigenvalue greater than 1 (after multiplying by the above scalar). Is this a general rule of thumb, to only choose components with eigenvalue&gt;1?  The breakoff point also is where the cumulative r^2 is 99% and the cumulative Q^2 is 90%. Are these more meaningful places to determine which components are best to keep? What exactly is Q^2 ?\n\nFinally, the add-in calculates the Loading Factors. I used the procedure outlined by [NIST](https://www.itl.nist.gov/div898/handbook/pmc/section5/pmc552.htm). I verified that the final Score produced by the addin is, in fact, the data multiplied by the loading matrix that the addin displays. However, the loading matrix that it produces is almost exactly the eigenvector matrix, but with some of the columns having the reversed sign. Oddly, the sum of the squares of each column in this loading matrix are all 1, while my calculated loading matrix does not have sumsq=1.\n\nTo produce a loading factor matrix from a subset of the total number of components, I assume I multipy the eigenvector matrix, using only the columns of interest, by the L^-1/2 matrix, produced from the eigenvalues of interest. Correct?\n\nThanks for any help!",
        "created_utc": 1521570307,
        "upvote_ratio": ""
    },
    {
        "title": "Help with statistic assignment. Dont need help with questions, have questions for some charts I need to make.",
        "author": "im2old_4this",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85umi9/help_with_statistic_assignment_dont_need_help/",
        "text": "I NEED YOUR HELP!  So I'm in a stats class right now (working on my nursing degree) and we have a project that requires asking a number of questions.  If any that read this and have a couple minutes would be willing to answer it would be extremely appreciated.  Feel free to just reply to this post.  There's just a few questions altogether that I will be turning in to charts, graphs and such.  \n\n1.  Are you married or do you live with a significant other? \n\n2.  What color are your eyes? Choices are: Blue, Green, Hazel, Amber, Brown.\n\n3.  Do you own your home, or rent?\n\n4.  Do you have a job? (For the sake of this assignment, job meaning something that brings    income. I completely agree stay at home parents work a TON as I was one for both my kids)\n\n5.  How many hours do you work weekly? (Hours not including overtime)\n\n6.  How many hours (if any) do you work overtime weekly on average?\n\n7.  How many hours per day are you home? (on average, including sleeping time)\n\n8.  How many hours of sleep do you get per night? (on average)\n\n9.  How long have you lived in your current residence? \n\n10.  How old are you?\n\nAnswers should be straight forward, for the data I’ll be charting I mostly need yes/no answers or specific numbers.  My answers would be:\n\n1.\tYes\n2.\tBlue\n3.\tOwn\n4.\tYes\n5.\t32 Hours\n6.\t4 hours\n7.\t12 hours\n8.\t6 hours\n9.\t10 months\n10.\t35 years\n",
        "created_utc": 1521567277,
        "upvote_ratio": ""
    },
    {
        "title": "Predicting Potential Outcomes of a Clinical Trial",
        "author": "JesusDied",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85u2yy/predicting_potential_outcomes_of_a_clinical_trial/",
        "text": "Hi! So I'm trying to build a model for figuring out if a drug is going to pass a clinical trial based on past data. I have past data for the drug and placebo for the primary endpoint, which includes, mean score, SD, SEM, 95% CI, and p-value. I also have the population size for the new study. I know based on this I can plug in different values of SEM and mean scores into a t-test with the new population size to see where I would get good p-values. My question is though is there anything more that I can do with this data? Is there a better way of estimating SEM based on old data? Thanks so much!",
        "created_utc": 1521563438,
        "upvote_ratio": ""
    },
    {
        "title": "trying to find a semi-partial correlation using SEM",
        "author": "anat-and-mika",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85rqnq/trying_to_find_a_semipartial_correlation_using_sem/",
        "text": "hi! im trying to test whether a variable (x1) would predict another variable (y) better when controlling for the variance of a third variable (x2) but only from x1 (and not from y). so i basicaly want to see if x1's beta is better (higher?) in a model where x1 predicts y alone than when controlled for x2. my advisor dosent want me to use a basic regression because as he understands, SEM cleans more error variance and thus makes the measures more reliable (i personally dont know much about SEM). so my question is how do i go about it? can i compare a model (a) in which i insert all 3 variables, to a model (b) in which i somehow tell the program to clean x2 from x1 and see which has a better fit? help would be much appreciated!! \nthanks :)\n\n",
        "created_utc": 1521542654,
        "upvote_ratio": ""
    },
    {
        "title": "If I open and close 100 doors randomly, how many doors will I need to open before I can be pretty sure I've opened all of them at least once?",
        "author": "mandragara",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85qfe0/if_i_open_and_close_100_doors_randomly_how_many/",
        "text": "This is for a bit of code I'm writing, not homework.\n\nThe doors thing is just an analogy. In my code I have a 3D lattice, with points lying at integer coordinates within the lattice (1,4,10 | 23,42,43 etc). For reasons related to what my code does, it randomly samples lattice points. I want to make sure (more or less) that it samples every lattice point at least once.",
        "created_utc": 1521524838,
        "upvote_ratio": ""
    },
    {
        "title": "Causal inferences from longitudinal data?",
        "author": "EmpiricalPancake",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85ovxq/causal_inferences_from_longitudinal_data/",
        "text": "So obviously you can’t PROVE causality, but I have a longitudinal within-persons data set. I have a hypothesis that the direction of a relationship is the opposite of what everyone thinks it is (or can be, it might be reciprocal). I ran the model both ways in its simplest form as a hierarchical linear model in R using the lmer function. My level two variable is the identifier for each participant. Participants responded at up to 9 time points (measurements being the level 1 variable).\n\nMy question is, if I find that the model with V1 predicting V2 is a better fit, using BIC and AIC, than the model with V2 predicting V1, is this evidence that my predicted direction is likely or is this insufficient to make this claim, and if so, is there any way to use longitudinal data to make this assertion? Both variables were measured at all time points.\n\nFor reference, my syntax is:\nM1 &lt;- lmer(V1 ~ V2 + (1|participantID), data = data, REML = FALSE)\n\nM2 &lt;- lmer(V2 ~ V1 + (1|participantID), data = data, REML = FALSE)",
        "created_utc": 1521508608,
        "upvote_ratio": ""
    },
    {
        "title": "Correlation Coefficient. How to account for missing data (non-election years)? GNI / Capita Growth (%)",
        "author": "LeBouffonTriste",
        "url": "https://i.redd.it/fafns7cjgtm01.png",
        "text": "",
        "created_utc": 1521507306,
        "upvote_ratio": ""
    },
    {
        "title": "Analyzing ordinal data sets",
        "author": "Slubgob123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85og21/analyzing_ordinal_data_sets/",
        "text": "It's been quite some time since I've looked at stats, and after a brief review, I'm not sure how to start with this problem.\n\nI have three years of ratings for bands playing a festival, Likert-style. So, ordinal data. The ratings for two of the years are 1 through 5, and the last year is also 1 through 5, but also allowing for .5s. Each year continues around 170 ratings.\n\nThere are two questions I'm trying to answer. First, within one year is there a significant difference between two people's ratings? (Despite my best efforts, I've only ever managed to get one other person to do a significant number of ratings).\n\nSecond is whether or not there is significant differences from year to year for one person -- i.e. is this year better than last year?\n\nThanks in advance for the help!",
        "created_utc": 1521504817,
        "upvote_ratio": ""
    },
    {
        "title": "whats test for 2x3x2 design",
        "author": "Pieyoup",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85npxx/whats_test_for_2x3x2_design/",
        "text": "Hi Guys, \n\nI'm stuck on my experimental design. In the design I have two conditions. (a) Word cue and (b) a picture cue. \nIn both I will be using red wine, tabacco and anise as cues. Afterwards the participants in my study will be divided into two groups depending on their reaction to the cue. \n\nSo I was wondering if my design is a 2x3x2 (between subjects) design? Should I use anova to analyse it? Also are there maybe statistical tests to see if there are no differences between my cues? \n\nThanks in advance! ",
        "created_utc": 1521498753,
        "upvote_ratio": ""
    },
    {
        "title": "Distributions of birthdays in digits of pi",
        "author": "GaboGobbo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85mf9a/distributions_of_birthdays_in_digits_of_pi/",
        "text": "Is the distribution of the position of the first occurrence of finite sets of digits of equal length in an infinite series of random digits a Poisson distribution? With pi day last week, my family looked at where in the digits of pi our month/day/year birthdays first appeared. My brother thought it was interesting that his came in the forst 2500 digits, whereas mine mas well past 2 million, but I pointed out that as he was born in a single digit month on a single digit day (ie 6 total digits), we should expect his birthday to come earlier than mine, with a two digit month and two digit day. So the distribution of all birthdays in pi should be trimodal, right? 6 digit Bdays, then lots of 7 digit ones, and some 8 digits. But for any one of those three subsets, is the distribution a Poisson?",
        "created_utc": 1521488924,
        "upvote_ratio": ""
    },
    {
        "title": "How to treat student grade data under different treatments?",
        "author": "xy1001",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85lw14/how_to_treat_student_grade_data_under_different/",
        "text": "I have 2 populations of students that completed a term under different treatments (one population completed an extracurricular assignment and the other did not). I want to pull out if there is a difference in their grades for a course between the 2 populations with various subcategories (freshman, sophomore, male/female, first gen.) The grades for the course are broken down into the normal A, A-, B+ etc. Do I start by comparing the 2 groups for each grade? Would I use an ANOVA for this? I'm just sort of lost as to how to pull out the differences, if there are any... ",
        "created_utc": 1521484963,
        "upvote_ratio": ""
    },
    {
        "title": "Standard error of normalized data",
        "author": "CamembertM",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85l89n/standard_error_of_normalized_data/",
        "text": "First off, sorry if I missed a post that already covered this.\n\nAnyhow, I have trouble finding out what to do with my standard error (s.e.) and standard deviation (s) after my normalisation procedure.\n\nI have a data set, let's call it A and I need to correct it with a reference B as follows: A/B.\nI have the s and the s.e. of both A and B, but what are they for the new value (let's call it A*)\n",
        "created_utc": 1521480160,
        "upvote_ratio": ""
    },
    {
        "title": "Likert Scale Statistics",
        "author": "peaches124810",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85l019/likert_scale_statistics/",
        "text": "So this was also posted to homework help and then I saw this subreddit but I have no idea how to cross post things. So, sorry about that. Anyways, I have two different likert scale surveys I performed for a public health class. I have been running around in circles online trying to figure out what analysis I should perform on the data to produce meaningful results. I just have no idea what analysis to do or where to start with the data. Survey One includes 10 questions and has 42 participants. It has the categories: strongly disagree, disagree, neutral, agree, strongly agree, and Don't Know with numbers 1,2,3,4,5,0 assigned to those categories, respectively. Survey one has 8 questions, 42 participants, and has the categories never, rarely,sometimes, often with numbers 1,2,3,and 4 assigned to them respectively. Does anyone have experience with Likert Scales and their analysis? I would so appreciate any insight you all can give me.",
        "created_utc": 1521478478,
        "upvote_ratio": ""
    },
    {
        "title": "What does this equation do?",
        "author": "Bong112",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85iopa/what_does_this_equation_do/",
        "text": "I've been looking over some code that was given to me by a friend. Unfortunately, she is out of reach at the moment. Her notes state that it was used to normalise the data.\n\nVariable_norm &lt;- (variable - mean(variable))/sd(variable)\n\nAny idea what this calculates?\n\nThanks for any reponses.",
        "created_utc": 1521458095,
        "upvote_ratio": ""
    },
    {
        "title": "I’m an idiot and I’ve been trying to explain our pay stubs to my coworkers.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85ik89/im_an_idiot_and_ive_been_trying_to_explain_our/",
        "text": "[deleted]",
        "created_utc": 1521456614,
        "upvote_ratio": ""
    },
    {
        "title": "Canadian Hockey Championship Statistic",
        "author": "whyUsayDat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85i3cp/canadian_hockey_championship_statistic/",
        "text": "I have a (hopefully) quick statistics question. There have been 6-8 Canadian hockey teams who have not won the Stanley cup from 1994-2017. There is only 1 winner per year. What are the odds that a Canadian team should have won over this time frame? Conversely, what are the odds that they don't win once?\n\nThis is not homework. I am a depressed fan of hockey.\n\nSince there are various numbers of teams over years I have provided that data here. \n\n* 1994, 26 teams, 8 Canadian teams.\n* 1995, 26 teams, 8 Canadian\n* 1996, 26 teams, 7 Canadian\n* 1997, 26 teams, 6 Canadian\n* 1998, 26 teams, 6 Canadian\n* 1999, 27 teams, 6 Canadian\n* 2000, 28 teams, 6 Canadian\n* 2001, 30 teams, 6 Canadian\n* 2002-2011 same as above line.\n* 2012, 30 teams, 7 Canadian\n* 2013-2017 same as above line.\n\nThank you for your time.",
        "created_utc": 1521450247,
        "upvote_ratio": ""
    },
    {
        "title": "Appropriate statistical test for the following analysis?",
        "author": "gonadbarbarian101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85fugv/appropriate_statistical_test_for_the_following/",
        "text": "Hi,\n\nI want to know what statistical test to use to test the validity of when I compare the recovery rates after surgery between different hospitals. I think I should use ANOVA, but I am unsure. \n\nI should add my hypothesis is that recovery rates differ significanytly between the different hospitals.\n\nThanks in advance",
        "created_utc": 1521422587,
        "upvote_ratio": ""
    },
    {
        "title": "Test for significance between slopes and intercepts",
        "author": "gg10001",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85fu9g/test_for_significance_between_slopes_and/",
        "text": "If I am given two sets of data and plot a linear trendline for each (so I have a slope and intercept for data set 1, and a slope and intercept for data set 2), is it possible to test for a difference between each of the slopes or each of the intercepts? ",
        "created_utc": 1521422532,
        "upvote_ratio": ""
    },
    {
        "title": "How many predictors to use for a logistic regression?",
        "author": "nothing_spatial",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85fi4a/how_many_predictors_to_use_for_a_logistic/",
        "text": "Hello,\n\nI have a dataset with a sample of 250,000.  I plan on fitting a logistic regression using about 20 independent variables, to get a sense of how they relate to the response variable.  My colleague said she would never use that many variables in a model because it's unstable, but won't elaborate (this isn't specific to this analysis, but as a generalization).  \n\nMy question: is there any validity to this? She wants to run variable selection to narrow down how many predictors to include in the model.  I found the [One in ten rule](https://en.wikipedia.org/wiki/One_in_ten_rule) on wikipedia, but wanted to ask around to get more opinions.\n\nAs a follow-up, how do you gauge when you've included too many variables in your model?  \n\nThanks!",
        "created_utc": 1521419200,
        "upvote_ratio": ""
    },
    {
        "title": "What % above the average is a strong significant difference?",
        "author": "gravitydriven",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85f818/what_above_the_average_is_a_strong_significant/",
        "text": "I have a group of arrows all pointing various directions. An equal distribution histogram would have 16.6% of the arrows in each of the 60 degree wide bins. I've also grouped them into 30 degree bins, which would have an equal distribution of 8.3%. My question is what % above or below this equal distribution would be considered strong? The sample size is over 15000, and my confidence in the measurements themselves is over 95%. I understand that this is a relatively simple question but I've had a lot of trouble finding an answer. Would standard deviation work here? How would that be calculated?",
        "created_utc": 1521416532,
        "upvote_ratio": ""
    },
    {
        "title": "Understanding terminology behind factorial replication",
        "author": "canadaface",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85etwh/understanding_terminology_behind_factorial/",
        "text": "This is slightly homework based, but I am trying to take it to another level. \n\nThe question asks to examine a survey with 2^6 possible combinations running a 1/4 replicate. I have to alias appropriate effects by considering 4-factor interactions only and to list the four sets of possible combinations I could use in the survey and how I can choose between them.\n\nI am not sure what the last part is asking of me. What \"combinations\" might it be referring to? At this point I have found a set of defining contrasts and aliased the effects.\n\nThanks in advance.\n",
        "created_utc": 1521412878,
        "upvote_ratio": ""
    },
    {
        "title": "Question regarding how to approach data analysis - pretest/posttest design",
        "author": "NeoKnife",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85ek8s/question_regarding_how_to_approach_data_analysis/",
        "text": "I’ve been really struggling trying to determine the best statistical analysis for a study I’ve been working on. Now that I have my data I’m not sure of what I had planned is ideal.\n\nBasically, it’s a pre-post test design with an IV of two levels (treatment and control) and pre-post test scores for TWO separate measures (each measuring a different thing related to student success). At first i was thinking a manova, but now I’m not sure.\n\nJust to give you more info, basically I tested to see how well an intervention worked to increase student conceptual understanding of conservation of matter. Understanding of the topic was the first pre and post test assessment. I also measured student logical thinking ability as a second DV, since research says that it is highly correlated to conceptual understanding of conservation. Therefore, I administered a pre and post test for logical thinking ability as well. Both measures are interval scales.\n\nMy issue resides in how to handle analysis including these two measures in a pretest-posttest design. In education, ANCOVAs are generally performed which allow you to control for the pre test scores by inserting it as a covariate. How would you suggest i go about it if I pursue the MANOVA Route? Just compute a gain difference score for each measure as the two DVs or try to conduct a MANCOVA?\n\nThanks so much.",
        "created_utc": 1521410488,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating parameters from a non-linear model with uncertainties in both X and Y, plus dealing with multicollinearity",
        "author": "just_a_grad_student",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85dfrx/estimating_parameters_from_a_nonlinear_model_with/",
        "text": "tl;dr - what is the best way to robustly estimate parameters and their uncertainties from a non-linear model given data, when (1) the data have uncertainties in both the dependent and independent variables, (2) the dependent variables have multicollinearity problem, and (3) a lot of data points are \"uninformative\"?\n\nhttps://stats.stackexchange.com/questions/335182/estimating-parameters-from-a-non-linear-model-with-uncertainties-in-both-x-and-y",
        "created_utc": 1521400770,
        "upvote_ratio": ""
    },
    {
        "title": "If corr(x,y)!=0, and corr(x,z)!=0, is it necessarily the case that corr(y,z)!=0?",
        "author": "turdfurguson",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85cpuf/if_corrxy0_and_corrxz0_is_it_necessarily_the_case/",
        "text": "As the title says - suppose that x and y are correlated, and x and z are correlated.  Is it necessarily the case that y and z will be correlated too?  My gut says that this is not necessarily true, but I cannot think of a counterexample.  Thank you!",
        "created_utc": 1521394578,
        "upvote_ratio": ""
    },
    {
        "title": "Static Analysis",
        "author": "sozialwissenschaft97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85bx0r/static_analysis/",
        "text": "What exactly is a static analysis? Is this an alternative to time series analysis (i.e., picking certain points form time series to analyze)?",
        "created_utc": 1521387204,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical Analysis midterm help...",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85b7lq/statistical_analysis_midterm_help/",
        "text": "[deleted]",
        "created_utc": 1521379736,
        "upvote_ratio": ""
    },
    {
        "title": "Repeated Measures ANOVA for method comparison",
        "author": "CalzzOne",
        "url": "https://www.reddit.com/r/AskStatistics/comments/85aqfb/repeated_measures_anova_for_method_comparison/",
        "text": "Hi I have 20 patients whose mandible was measured 3 times by 3 different methods, none of which is a gold-standard. I am attempting 3 pairwise comparisons using Bland-Altman analysis (method 1 vs method 2, M2 vs M3 and M1 vs M3) which include 3 T tests for paired samples. Do I have to replace these T tests with RM-ANOVA and a post-hoc test for pairwise comparisons? I have never seen ANOVA used in such a way and the software (NCSS) does not correct for multiple comparisons. Thanks!",
        "created_utc": 1521373190,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate the standard error of a particular predictor within a regression model",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8587je/how_to_calculate_the_standard_error_of_a/",
        "text": "\nIf i have something like this : \nhttps://i.imgur.com/hagwq9J.png\n\n\nAnd I want to calculate the SE by hand for `prices$LivingArea`, how would that\nbe done?\n",
        "created_utc": 1521335997,
        "upvote_ratio": ""
    },
    {
        "title": "R vs MiniTab output",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8582cp/r_vs_minitab_output/",
        "text": "I don't have MiniTab, I have R. \n\nI have this print out from MiniTab https://i.imgur.com/A1MMlv0.png , and the text says that \n\n**p-value\n&lt; 0.001, reject H 0 , there is overwhelming\nevidence that the model is useful**\n\nThe version I have from R is this https://i.imgur.com/hagwq9J.png\n\nI'm wondering where I should read the P value off from the R output? \n\nthanks",
        "created_utc": 1521334377,
        "upvote_ratio": ""
    },
    {
        "title": "Predicting the number of applications for a university?",
        "author": "frenchwonk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/856sp0/predicting_the_number_of_applications_for_a/",
        "text": "Hello.  I work for a small college.   We are currently in applications seasons.  It is very helpful for us to be able to predict the total number of applications for this year, based on current trends.  \n\nCan you help me think through some ways to predict the total number of applications?  \n\nHere is the data that we have.  For the last 3 years, we have application data for each date from September - April.   This consists of how many applications per day we received that day.  (e.g. Day 10: 20 applications, Day 11: 15 applications...Day 155:20).   We also have this data for this year up until yesterday, so we know how many applications total we have so far this year, and also this year's average application rate.    I would like to predict the totals about 45 days in the future.\n\nI have tried a few techniques to predict.  One technique I tried is to do a regression using days as my predictor variable, and daily application number as my predicted variable.   This seems okay, but I won't know until we get the total.\n\nOne thing I would like to do is incorporate past years analysis, but I can't figure out how.   Each year we have a pretty similar application trend - application rates start out slowly, pick up in November, slow down again, and then pick up again in Feb - March (so the rate changes).  This seems fairly consistent from year to year.   What is different is the average application rate from year to year.   For whatever reason, we are more popular some years, and less popular other years, and this affects our average applications per day.\n\nIs there anyway to use past years data, and then incorporate this year's application rate?  Or any other ideas that you have for predicting, I would be happy to help.  Many thanks!",
        "created_utc": 1521321865,
        "upvote_ratio": ""
    },
    {
        "title": "Probability calculation for estimating the likelihood of money laundering",
        "author": "UnretiredGymnast",
        "url": "https://www.reddit.com/r/AskStatistics/comments/856pcg/probability_calculation_for_estimating_the/",
        "text": "I came across this analysis purporting a high likelihood that a particular set of transactions is related to money laundering.\n\nhttps://gist.github.com/RyanCavanaugh/a42ee7f8c4edb540c1b303cf7a7d26e2\n\nHaving read some of the comments at the bottom of the page, the analysis just doesn't seem like it demonstrates what it claims to, but it's hard for me to verbalize why.\n\nCan any of you more experienced statisticians give it a quick skim and determine if the conclusion follows from the results or if it's seriously misinterpreting things?",
        "created_utc": 1521320930,
        "upvote_ratio": ""
    },
    {
        "title": "ANOVA testing without normal distribution help",
        "author": "smorement",
        "url": "https://www.reddit.com/r/AskStatistics/comments/855cv7/anova_testing_without_normal_distribution_help/",
        "text": "Hey guys, I'm unsure on what stats test to use for a project I'm working on.\n\nI want to investigate whether or not my variable (temperature) changes over time. I have 11 days of data, each day there is 32 temperature values.\n\nI've conducted distribution tests (including a Shapiro-Wilk test) and the data does not seem to follow a normal distribution, log distribution or even a poisson distribution.\nI was trying to do a repeated measures ANOVA test but it works under the assumption the data is normally distributed...\nI tried it anyway but not sure whether it's valid. It also failed the test for sphericity but I used a Greenhouse-Geisser correction to account for that so hopefully that's ok.\n\nI'm just confused as to whether or not the ANOVA test is valid if my data is abnormally distributed. I can't find anywhere online on what to do.\n\nI want to investigate changes over time but really lost, any help would be greatly appreciated!",
        "created_utc": 1521308916,
        "upvote_ratio": ""
    },
    {
        "title": "Why is the notation for central and regular moments the same? How do I tell which is being referenced?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8542sf/why_is_the_notation_for_central_and_regular/",
        "text": "[deleted]",
        "created_utc": 1521297211,
        "upvote_ratio": ""
    },
    {
        "title": "quick question about some algebra",
        "author": "mathstudent137",
        "url": "https://www.reddit.com/r/AskStatistics/comments/853p69/quick_question_about_some_algebra/",
        "text": "https://gyazo.com/d7f564f497e24407b71002205247177b does the n's come from the arrows I painted in? So the sum from i=1 to n and sum of j=1 to n of (Xi - X(bar)^2 is just the sum of i=1 to n of n(Xi - X(bar)^2 where the n here comes from the sum of j=1 to n? How come? (forgot the math behind it, is the sum from 1 to n off 1 just n)? Also what about the last thing they have there. Not able to see how they get S^2",
        "created_utc": 1521293070,
        "upvote_ratio": ""
    },
    {
        "title": "Need help can somebody please help me will venmo you $$",
        "author": "[deleted]",
        "url": "https://i.redd.it/r6j6yir349m01.jpg",
        "text": "[deleted]",
        "created_utc": 1521260882,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate the probability of winning March Madness pool?",
        "author": "ItsUnderSocr8tes",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8507fa/how_to_calculate_the_probability_of_winning_march/",
        "text": "How could one calculate the probability of one person winning a march madness pool with more than 2 participants, as the tournament progresses?\n\nIn a march madness pool, participants predict the results of basketball games in the NCAA basketball tournament, and get points based on the number of correct predictions.  A person wins the pool as a result of the most correct predictions, with weighting given to each round of the tournament.  \n\nIf the probability of two teams winning a game was simplified to a 50/50 coin toss, the pool could be simplified as a coin toss tournament.  As the tournament progresses, it is known how many points each participant has already won, and as a result of teams being eliminated from the tournament, it is known how many opportunities to win points each participant still has. \n\nUsing this information, how could one calculate the probability that each participant may win the pool?  I imagine it could be calculated based on the intersection of probability curves, but with multiple curves, I'm not sure how to approach this.  Could someone give me some insight into appropriate methods?\n\n",
        "created_utc": 1521246873,
        "upvote_ratio": ""
    },
    {
        "title": "Which test to use? [HELP]",
        "author": "stageT",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84zf8q/which_test_to_use_help/",
        "text": "After years in research, reading other papers, reading mines, I am still never 100% sure if me and others do the correct tests. Usually it some of another kind of ANOVA. But the pattern is always the same.\n\nFor example:\nWe test a game, where you throw a dart - and you get a score.\nYou have 4 dart types (d1,d2,d3,d4).\nYou have 3 input devices to throw darts (mouse, touchpad, joystick).\n\nWe will test 12 participants and we have 12 conditions (3 input devices * 4 dart types). For each condition the user will do 10 trials (throw 10 darts per condition, by that we collect 10 scores for each condition). So we will collect 1440 data entries (12 participants * 12 conditions * 10 trials).\n\nThe questions are always:\n- which dart scores the best?\n- which input device scores the best?\n- which combination of input device and dart type is the best?\n\nBy the trick is in the details:\n- How does a row of data look like in a statistical program (e.g. SPSS). Probably all 1440 data would need to be in there, or? But in what kind of formatting, how to deal with that in SPSS? Often we see people also averaging the trials, and pasting conditions into SPSS, is this correct at all?\n- How to deal with 12x4x3? Often it can be even more 12x4x3x2.\n- Do repeated-measures, mixed, multi-way, three-way, or what kind of ANOVA?\n- How to reason with this 12 participants * within * within * within X 10 trials, tests? \n\nPlease help.",
        "created_utc": 1521239431,
        "upvote_ratio": ""
    },
    {
        "title": "How to handle error-in variables and measurements calculated from them?",
        "author": "stryken10123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84ye1n/how_to_handle_errorin_variables_and_measurements/",
        "text": "I am trying to teach myself statistics. I am running into a road bump with error-in variables and measurements calculated from them. I will give an example to clarify my problem.\n\nSuppose you are interested some metric S. To attain S, you must measure x, y, and z, perform a calculation of X*Y/Z. In an singular group, how do you handle the measurement error in x, y, and z?\n\nSecond, you devise an experiment with multiple conditions which you will measure x, y, and z for S. How would you compare S between groups to determine significant differences?\n\nLastly, what area of statistics would this be related so that I can do more reading on the subject?\n\nThank you\n",
        "created_utc": 1521230867,
        "upvote_ratio": ""
    },
    {
        "title": "Box-Cox Transformation, lambda = 1",
        "author": "tryinamath",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84yc0b/boxcox_transformation_lambda_1/",
        "text": "If the optimal lambda value given is 1, does this mean that the original model does not have lack of fit in the response? \n\nAlso, is there any way to compare a model with a transformed response to a model with an non-transformed response to test for LOF? I know if two models are nested you can use an ANOVA to compare the two models for LOF but you wouldn't be able to use that in this case.",
        "created_utc": 1521230435,
        "upvote_ratio": ""
    },
    {
        "title": "Mixed Model Design for Work",
        "author": "BigWetFlippers",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84yb5e/mixed_model_design_for_work/",
        "text": "For work I have been asked to design an experiment testing the effects of a rewards program on customer sales.  I have two questions concerning the design which I could really use help with.\n\nA.\tMy initial thought was to design a mixed effects model with a control group who will not receive rewards points and a treatment group who will receive rewards points.   Sales would be measured at two time periods, before and after the treatment.  I chose a mixed model in order to control for the random effect (Customers) since they are measured multiple times.  Is this the correct approach to be taking?\n\nB.\tMy second question, assuming I have the concept of a mixed model design correct, is about sample size.  In total we sell to about 500 customers.  There is not a lot of will however to sample a large percentage of these customers do to the potential exposure of the program.  How do I go about determining the minimum sample size while still retaining reasonable power?  Alternatively, are there any established methods to deal with small sample sizes?\n\nAny suggestions/advise would be greatly appreciated.",
        "created_utc": 1521230238,
        "upvote_ratio": ""
    },
    {
        "title": "Regression model with 300 binary variables, many of which are correlated",
        "author": "elchucknorris300",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84xwws/regression_model_with_300_binary_variables_many/",
        "text": "I'm trying to model the effect of disease conditions on health care cost, but I have 100's of conditions I need to model, and they are often correlated with one another.  I have a pretty large data set with millions of people-years.  My priorities are the following:\n\n1)  Create relative weights for each condition, i.e. Condition A results in twice the healthcare costs as Condition B\n\n2)  Create a model where I can add up the dependent cost variable from each condition to predict a person's total costs.\n\nI was planning on just pushing all these condition variables through a linear regression model with cost as the dependent variable.  Is there a better way I can do this? ",
        "created_utc": 1521226977,
        "upvote_ratio": ""
    },
    {
        "title": "Sample Size for Paired T-Test",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84xetl/sample_size_for_paired_ttest/",
        "text": "[deleted]",
        "created_utc": 1521223160,
        "upvote_ratio": ""
    },
    {
        "title": "Understanding the difference between a z-score and test parameter",
        "author": "onzie9",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84xayg/understanding_the_difference_between_a_zscore_and/",
        "text": "Preface: this is not a homework problem.  Sadly, I'm actually supposed to be teaching this material, and I'm stuck here. These numbers are totals from an actual classroom exercise.\n\nSuppose I flip a fair coin 2660 times and get heads 1181 times.  Since this is a binomial random variable, the standard deviation is sqrt(2660(.5)(1-.5)) = 25.79, so the z-score is -.002, which gives essentially a 50% chance of this (or fewer heads) occurring. This seems fine to me.\n\nConsider the problem now as a hypothesis test.  Assume that the coin is fair, so p0 = .5.  For my test, I flip it 2660 times and get a proportion of 1181/2660 = .444.  The test statistic is (.444-.5)/sqrt(.5(1-.5)/2660) = -5.78.  This test statistic gives a P-value of essentially 0, which is obviously less than any significance level. So I have to reject that the coin is fair, even though I had a ~50% chance of flipping about 1181 heads.\n\nClearly, I am greatly misunderstanding something very simple, but I just can't seem to wrap my head around it on a Friday afternoon. Can anyone point out the boneheaded thing I'm doing?",
        "created_utc": 1521222288,
        "upvote_ratio": ""
    },
    {
        "title": "Assessing consensus among observers",
        "author": "sennrion",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84wtqg/assessing_consensus_among_observers/",
        "text": "Hey everyone, I have a question hoping someone can help. For a new project I'm starting 3 to 6 different people will be quantifying mice behaviour (they will watch videos of mice, and have to record how much time the mice spent doing X behaviour). I want to determine consensus among these different observers. How should I do it?\n\nA bit more information: Let's say the behaviour I want them to register is 'exploration'. Mice can explore either object A or object B. I want to know two if all the observer decided the mice explore object A or object B around the same amount of time (compared to the observers, ie. observer 1 said mice explored 5s, obs 2 5.3s, obs 3 4.9s, etc).\n\nBasically, I do novel object recognition experiments, and I want to know if all people involved is on the same page regarding quantification.\n\n\nThanks!",
        "created_utc": 1521218477,
        "upvote_ratio": ""
    },
    {
        "title": "Hi, what models can I use to analyze scattered points in a box?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84wkom/hi_what_models_can_i_use_to_analyze_scattered/",
        "text": "[deleted]",
        "created_utc": 1521216561,
        "upvote_ratio": ""
    },
    {
        "title": "Model reduction",
        "author": "arbsoutter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84vywl/model_reduction/",
        "text": "So I was wondering if I could get some help. I have a model currently with multiple predictors however, I want to reduce this model so only significant ones remain.\n\nCurrently I use backwards regression based on AIC values. However, a problem I have at the moment is a few of my regressions violate normality and homoscedacity. The way I tend to address this is through bootstrapping, however, you can't use backward regression on a boostrapped model.\n\nI've been reading that stepwise is a terrible method anyway. But the question I have then is what is the alternative way to reduce the number of predictors in a model, so that I'm not fitting meaningless variables?",
        "created_utc": 1521211739,
        "upvote_ratio": ""
    },
    {
        "title": "How to compute Z- scores for stock prices?",
        "author": "pkzz1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84vhc0/how_to_compute_z_scores_for_stock_prices/",
        "text": "I am new to statistics, was a little confused on how to compute z scores for stock prices.\n\n S&amp;P 500 stocks\nMean = - 38.5 percent\nStandard deviation = 20 percent\n\nFor computing Z score using the below formula :\nZ = (X - Mean)/Standard Deviation\n\nSo for the probability of stocks that gained in that financial year is :\n0 is the initial gain value\n\nZ = (0 - (-38.5))/20 = 1.925, \nZ score table : 0.027114679, i.e. 2.7%\n\na) What is the probability that the S&amp;P stocks have a gain of more than 10%?\nb) What is the probability that the S&amp;p stocks have a loss of more than 50%?\n\n\nAgain I am still learning to correct me if I am wrong.",
        "created_utc": 1521207638,
        "upvote_ratio": ""
    },
    {
        "title": "Hypothesis Test on significant difference",
        "author": "iyong829",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84uyf5/hypothesis_test_on_significant_difference/",
        "text": "Q:\nMedical test was conducted to learn about druf resistance. Of 142 cases tested in NJ, 9 were found drug resistant. of 268 cases tested in NY, 5 were found drug resistant. do these data suggest a statistically significant difference between the proportions of drug resistant cases in two states? Use a 0.02 level of significance. Whatd the p value and conclusion?\n\nCan you guys help me solve this?",
        "created_utc": 1521202447,
        "upvote_ratio": ""
    },
    {
        "title": "Is this a Unimodal or bimodal distribution? I know there is a peak at 4, which in strict meaning make the distribution bimodal, but I was wondering if it is too small and so should be considered negligible. if it is not, should we consider this distribution multimodal as we have a third peak at 10?",
        "author": "Ammar-1992",
        "url": "https://i.redd.it/3f89reh0r2m01.jpg",
        "text": "",
        "created_utc": 1521183913,
        "upvote_ratio": ""
    },
    {
        "title": "Is this Unimodal or bimodal distribution?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84tix4/is_this_unimodal_or_bimodal_distribution/",
        "text": "[deleted]",
        "created_utc": 1521182998,
        "upvote_ratio": ""
    },
    {
        "title": "Is this distribution unimodal or bimodal?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84tgm6/is_this_distribution_unimodal_or_bimodal/",
        "text": "[deleted]",
        "created_utc": 1521182081,
        "upvote_ratio": ""
    },
    {
        "title": "Win rates and risk-reward ratios in trading",
        "author": "The-Solomon-Grundy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84shqf/win_rates_and_riskreward_ratios_in_trading/",
        "text": "Apologies if this question isn't appropriate for this sub. If you could redirect me to another place that'd be great.\n\nLet's say a trader has two trading strategies:\n\nStrat A risks 2% each trade to gain 4% and has a win-rate of 40%.\n\nStrat B risks 4% each trade to gain 8% and has a win-rate of 40%.\n\nSo Strat A and B have the same risk-reward ratio of 2 (4/2 and 8/4).\n\n**Why then does Strat B outperform Strat A over the same number of trades?**\n\n100 trades on Strat A:\n\nGains +4% 40/100 times, loses -2% 60/100 times\n\n= (4 * 40) - (2 * 60) = (160) - (120) = 40% gain\n\n100 trades on Strat B:\n\nGains +8% 40/100 times, loses -4% 60/100 times\n\n= (8 * 40) - (4 * 60) = (320) - (240) = 80% gain\n\nMy understanding was that strategies with identical win-rates and risk-reward ratios should (in theory) perform identically over the same number of trades. Obviously this is not true. Could anyone here help me to understand why? Thank you.",
        "created_utc": 1521170478,
        "upvote_ratio": ""
    },
    {
        "title": "Logistic regression tests done are statistically significant, but odds ratio are very low. Is this ok?",
        "author": "farali",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84s3po/logistic_regression_tests_done_are_statistically/",
        "text": "I performed two separate bivariate logistic models (between one dependent variable [dichotomous] and one independent variable [continuous]), and while both were confirmed to be statistically significant, I couldn't help but notice the findings are very low numbers. When I probed its odd ratio, it was 1% and 10% higher odds for the two tests respectively. \n\nCan something be statistically significant yet not have meaningful findings? And if so, what is the cutoff for something to be not meaningful (like for the odds ratio, what percent cut off is it safe to say it is not meaningful)?   ",
        "created_utc": 1521166477,
        "upvote_ratio": ""
    },
    {
        "title": "So stuck, which statistical test??",
        "author": "daisysneal",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84r7mn/so_stuck_which_statistical_test/",
        "text": "I'm doing an intervention study. Everyone does an implicit attitudes test (as a baseline measure), then takes one of four conditions (the control, or one of the three interventions), everyone is then retested with implicit attitudes test and also takes an explicit attitudes test. I want to see 1. if there is a difference in implicit attitudes within each group from time A to time B.. 2. if there is a difference in those differences between the four groups... 3. if there is a difference in explicit attitudes between the four groups. Sorry if this is worded in a really confusing way, i'm confusing myself too! ",
        "created_utc": 1521158033,
        "upvote_ratio": ""
    },
    {
        "title": "How important is histogram \"binning\"? How do you decide on the number/width of bins to use in a histogram?",
        "author": "fib_11235813",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84qzih/how_important_is_histogram_binning_how_do_you/",
        "text": "For a project in physics, I have some histograms showing a distribution of kinematic variables (e.g. momentum) for which I have been asked to \"determine the binning\" that should be used.\n\nIs this just as simple as just tweaking the number and width of the bins by trial and error to see what looks prettiest, or is there a proper/rigorous way that a scientist **should** do this? Any advice is greatly appreciated!",
        "created_utc": 1521156041,
        "upvote_ratio": ""
    },
    {
        "title": "Meta-Analysis: Help interpreting heterogeneity",
        "author": "xblushpink",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84q87g/metaanalysis_help_interpreting_heterogeneity/",
        "text": "I'm conducted a meta-analysis and have used a random effects model - what is the difference between I-squared and Tau-squared? Which do I need to pay attention to? I think it's I-squared.\n\nWith respect to I-squared, my meta-analysis has very high heterogeneity which I have investigated by subgrouping my data to see what may be causing it.\n\nMy meta-analysis involves looking at symptoms of a disease, and there are multiple scales used to assess the severity of the disease. I found when I excluded one particular scale my heterogeneity went from high to 0%. My questions are:\n\n- Does that mean that one scale was the cause of all my heterogeneity? I have since done another meta-analysis using all my data aside from the papers that used that one scale and I-squared=0%. Do I need to investigate other potential sources of heterogeneity or is that it?\n\n- How do I correctly interpret this finding? Is something along the lines of saying that it suggests the scale is inconsistently applied and interpreted in assessing the severity of the disease correct? Or have I misinterpreted heterogeneity?\n\nThank you!!",
        "created_utc": 1521149877,
        "upvote_ratio": ""
    },
    {
        "title": "Want to improve cross selling product recommendations on website. How should I go about it?",
        "author": "gutterandstars",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84opol/want_to_improve_cross_selling_product/",
        "text": "I have about three years worth of e-commerce transaction data which has the date, transaction id along with the type/quantity of products purchased in that particular transaction....      \nWe have already received guidance from a product team on what to show as cross selling during checkout but I want to challenge this and see if there is a way to validate/disprove this.       \nHow should I proceed?      \nThx.",
        "created_utc": 1521138299,
        "upvote_ratio": ""
    },
    {
        "title": "A priori power analysis for a 2x2x2 design?",
        "author": "gundedun",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84lgie/a_priori_power_analysis_for_a_2x2x2_design/",
        "text": "I have to determine required sample size via apriori power analysis for a study I will be conducting. The design of the study is 2(within)x(2(within) x 2(between). To achieve 80% power with a small or medium effect how should I calculate the sample size in GPower or a related medium? Which test would be the correct one to select? ",
        "created_utc": 1521109881,
        "upvote_ratio": ""
    },
    {
        "title": "Stats help for thesis",
        "author": "Zydrarc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84k9ft/stats_help_for_thesis/",
        "text": "Hi \n\nIm struggling to decide on which test/s to use for my thesis in Biology. In my thesis, I'm exploring the possibility of controlling an invasive species of mussels by the use of natural predators.\n\nIn the experiment in question, I tested if the predator had a preference towards small or large mussels. I gathered how many mussels of each size class were eaten, size of predators and the time taken to eat the mussels.\n\nExample of data gathered:\n\n\nCarapace Lenght (mm)| Carapace Width (mm)| Weight (g) | Small mussels eaten | large mussels eaten |total mussels eaten |\n---|---|----|----|----|----|----|----\n51|39|65.15|3\t|0\t|3 |\n55\t|42\t|69.12\t|4\t|1\t|5\n\n\nI was thinking of using a Shapiro-Wilk test to test for normality and possibly an independent sample T test or Mann-Whitney U-test depending if the data is normal.\n\nIs there any other way to go about this? I'm not that great with statistics so any help would be very much appreciated. Thanks in advance",
        "created_utc": 1521093337,
        "upvote_ratio": ""
    },
    {
        "title": "Would using a large number of online forums as data be representative of the general population?",
        "author": "redditpirateroberts",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84jyka/would_using_a_large_number_of_online_forums_as/",
        "text": "My friend and I are in a heated argument. He contends that using a large number of online forums as data is valid in the sense that conclusions from the data would be representative of the general population. In this specific case, the data is from 3000 yahoo groups. Specially, it is data regarding the most common fetishes. I believe that the people who contribute to online forums, in this case specifically studying fetishes, would be far from a random sample of the general population. Does anyone have any insight or research that would lend credence to either of our view points? ",
        "created_utc": 1521089486,
        "upvote_ratio": ""
    },
    {
        "title": "Probably an easy question for this sub (TINV ..df?)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84jglm/probably_an_easy_question_for_this_sub_tinv_df/",
        "text": "[deleted]",
        "created_utc": 1521083916,
        "upvote_ratio": ""
    },
    {
        "title": "Pre- and Post- Survey Analyses",
        "author": "get_me_outta_Indiana",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84ikvm/pre_and_post_survey_analyses/",
        "text": "I am conducting analyses over two surveys distributed to sample groups with unequal response rates.  The surveys consisted of the following:\n\n\nPre-Experience Survey\n\n4 Demographics questions\n\n6 Yes/No questions with prompts \"please explain why/why not\"\n\n2 short answer questions (\"What do you hope to learn?\" and \"What do you think about topic X?\")\n\n17 Items utilizing Likert-scale responses\n\n-----------------------------------------------------------------------------------------------\n\n\nPost-Experience Survey\n\n4 Demographic Questions\n\n1 Multiple Choice Question\n\n2 Yes/No (no explanation)\n\n10 Likert-scale responses\n\n5 Yes/No questions with prompts \"please explain why/why not\"\n\n\nThis survey was distributed with subject anonymity maintained, so we cannot conduct an exact pairing of before/after by individual.  Further, aside from the demographic questions and three Y/N questions (w/ explanation prompts), the questions to both tests are different.  \n\n\nMy questions are these: \n\n1.)  What comparisons What kind of comparisons can I do between these surveys on the repeated questions?\n\n2.)  What ways can the other questions be compared/visualized best?\n\n\nThanks, all.  I did not design these surveys or else I would have done so much differently.  If anyone else has dealt with similar issues, I invite your insights!",
        "created_utc": 1521075318,
        "upvote_ratio": ""
    },
    {
        "title": "When analyzing coefficients of a linear probability model, how do we deal with negative values and values over 1?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84idg1/when_analyzing_coefficients_of_a_linear/",
        "text": "DV dichotomous and normalized. The combined value of some coefficients + constant is either far below negative or far above 1. Can we treat negative values as close to 0 and values above 1 as close to 1? I know there are problems with LPM, and frankly I'd rather use logit for my model, but my professor is requiring that we use LPM. Thanks.",
        "created_utc": 1521073413,
        "upvote_ratio": ""
    },
    {
        "title": "Low sample size, three different instruments, instructor recommended a student multiply sample size by 3... huh?",
        "author": "finishbamn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84i20r/low_sample_size_three_different_instruments/",
        "text": "An instructor advised a student with a low sample size, in a study using three scales, to multiply the sample by three since 3 different instruments were completed thus making a sample size of 45 now 135 with a smaller effect.  Can this be done? Has it been done? Can you make one subject, 3 subjects? I need receipts:)",
        "created_utc": 1521070546,
        "upvote_ratio": ""
    },
    {
        "title": "Question about Bayesian gamma inference",
        "author": "richard_sympson",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84hd3o/question_about_bayesian_gamma_inference/",
        "text": "I'm using a conjugate prior setup to solve for [both (unknown) parameters](https://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions) of a gamma distribution likelihood function (described in a bit more detail [here](https://www.johndcook.com/CompendiumOfConjugatePriors.pdf)).  The second link does not seem to have properly specified some equations but I think I have reached some posterior marginal distributions for the \"a\" and \"b\" (shape and rate) parameters.\n\nI can use a Monte Carlo method to obtain a simulated (a,b){1:N} sample of the parameters, and also from those pairs further simulate (x){1:N}, where \"x\" form a posterior predictive distribution.\n\nHere's my problem: I can calculate, say, the 95% quantile from (x){1:N) easily enough, and I (think I) can also obtain a distribution of the 95% quantile using each simulated pair (a,b) and then plugging 0.95 into the inverse cumulative function for the corresponding gamma distribution.  I then obtain a singular 95%ile value, from the posterior predictive distribution, and what seems to me to be the posterior distribution for the 95%ile itself.\n\nHowever, the singular value from the PPD is very noticeably biased high (or the posterior distribution of the 95%ile is itself \"low\").  I cannot figure out why.  It's not an issue of too few MC samples.  The bias grows with quantile size (it's greater for the 99%ile, etc.).\n\nThe bias starts to disappear with larger data sample sizes (and is exacerbated with low sample size), which indicates to me that somehow the prior is causing this difference.  However, why this would be is not clear to me.  Since the singular value and the simulated 95%ile values are derived from the same information, prior and posterior, I'd not expect there to be such a large difference.\n\nAny pointers on where things are going wrong?  Or where they're going right, and my interpretation/understanding of why is off?",
        "created_utc": 1521064828,
        "upvote_ratio": ""
    },
    {
        "title": "Using \"given\" in R modeling",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84e9iw/using_given_in_r_modeling/",
        "text": "[deleted]",
        "created_utc": 1521041503,
        "upvote_ratio": ""
    },
    {
        "title": "VAR model/Granger Causality",
        "author": "TheWiibo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84e3jk/var_modelgranger_causality/",
        "text": "Hello,  \nI'm running a Granger causality test in STATA (i have 2 variables named close and i24) but to do so i first need to run a VAR model. My problem is that, as far as i know, in order to run the VAR I need my two variables to be of the same order of integration. This is not my case since close is I(1) and i24 is I(0). My question is:  \nShould I difference i24 so i can run the model or can I run the model as it is or my variables cannot be put into a VAR because they are not of the same order of integration?  \nAny answer (or reference material that can help me)will be appreciated.  \nThanks",
        "created_utc": 1521040191,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating the average height in a population.",
        "author": "statsboy321",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84bqla/estimating_the_average_height_in_a_population/",
        "text": "https://i.imgur.com/cGYKOkD.png\n\nHi, this is NOT homework. I just want to know how where they get 1.96 from. \n\n    P(-1.96 &lt; Z &lt; 1.96) = .95\n\non the z table i see that 1.96 is 0.4750 and that 0.4750 + 0.4750 is .95, but I don't know where they get 1.96 from.",
        "created_utc": 1521015186,
        "upvote_ratio": ""
    },
    {
        "title": "Can you prove the null?",
        "author": "radai120507",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84ba14/can_you_prove_the_null/",
        "text": "So let's say I have a research hypothesis of something like \"People who graduate to high school are less likely than others to go to prison.\" In this case, the null hypothesis would be that there's no difference between people who graduate and other groups, right? So say my test comes back with no significant values, does this mean I've proven people who graduate high school are not less likely than others to go to prison? Or does it mean that I've proven nothing? ",
        "created_utc": 1521008899,
        "upvote_ratio": ""
    },
    {
        "title": "Questions about logistics regression",
        "author": "zoe_root",
        "url": "https://www.reddit.com/r/AskStatistics/comments/84a1yl/questions_about_logistics_regression/",
        "text": "Can anyone tell me how to build a binary logistics regression using 2 variables with three groups x two groups respectively and using three reference groups? For example, first variable with 3 groups like low, med, high education; second variable with 2 groups like single and nuclear family and using single family as reference group for each education level to predict success in the future. (i.e. among the high education, reference group is high education and single family) How can I do that? Thanks. ",
        "created_utc": 1520995614,
        "upvote_ratio": ""
    },
    {
        "title": "Stats for Business Financial Statements?",
        "author": "zensoma",
        "url": "https://www.reddit.com/r/AskStatistics/comments/849cfs/stats_for_business_financial_statements/",
        "text": "So, a little back story. Recent graduate, new job and a passion for business statistics. Access to all the financial data from the beginning of time. \n\nI'm responsible for forecasting our financial statements, and I've got a modest model built, but I want to send something a little extra to my boss, in the form of some statistical analysis on our past data. \n\nWhat kind of analysis can be done and is useful in a small business? ",
        "created_utc": 1520988861,
        "upvote_ratio": ""
    },
    {
        "title": "Need help analysing these sales figures",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/848oq2/need_help_analysing_these_sales_figures/",
        "text": "[deleted]",
        "created_utc": 1520982998,
        "upvote_ratio": ""
    },
    {
        "title": "Question about independent variable. I'm pretty sure there's only one level, can anyone confirm?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/847zzk/question_about_independent_variable_im_pretty/",
        "text": "[deleted]",
        "created_utc": 1520977614,
        "upvote_ratio": ""
    },
    {
        "title": "How to describe and test datasets stemming from multivariate components?",
        "author": "stryken10123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/846x89/how_to_describe_and_test_datasets_stemming_from/",
        "text": "As the title describes, I am unsure how to handle calculated numbers. This is not a homework question; just my curiosity. I will try to be as simple as possible.\n\nSuppose you are a student looking to disprove the first law of thermodynamics; matter cannot be created or destroyed. So, you devise an experiment with the Ideal Gas Law (PV = nRT or n = (PV)/(RT). You have a suspicion that you can destroy matter with enough pressure. \n\nP = Pressure, V = Volume, n = number of molecules, R = a constant, T = temperature.\n\nYou design an experiment to vary only pressure and calculate the number of particles. You measure P, V, and T (as R is a known constant). Then, you calculate the `n` afterwards. However, since you are limited by measuring tools, you take multiple measure for each pressure condition you have created and obtain a relational dataset for each variable.\n\nHo: Pressure cannot destroy matter\nHa: Pressure can destroy matter. \n\nSo, to my original questions...\n\nHow should this data look? Should n be normally distributed? Should we consider the distribution of the variables that compose `n`? How would we test this data for a statistical difference between pressure groups?\n\nI understand that this is a silly example but these are the sorts of questions that I have trouble with when looking at data that has been calculated from multiple variables. Thank you for your input.\n",
        "created_utc": 1520968979,
        "upvote_ratio": ""
    },
    {
        "title": "Should I use a z test or a test and what are the populations?",
        "author": "millystelescope96",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8465ca/should_i_use_a_z_test_or_a_test_and_what_are_the/",
        "text": "Imagine that researchers are interested to know whether relaxation training decreases the number of headaches a person experiences. Then imagine that they randomly assign 30 patients who frequently experience headaches to a control group or a relaxation training group and note the change in the number of headaches each group reports from the week before training to the week after training. \n\nI think I should use a z-test since the population is 30. I think the population is frequency of headaches and the comparison distribution is a distribution of means. Idk if I'm correct. Help?",
        "created_utc": 1520963351,
        "upvote_ratio": ""
    },
    {
        "title": "How many jellybeans in this snifter glass?",
        "author": "[deleted]",
        "url": "https://i.redd.it/kpyzblh2ujl01.jpg",
        "text": "[deleted]",
        "created_utc": 1520954841,
        "upvote_ratio": ""
    },
    {
        "title": "How to quantify significant trends?",
        "author": "westmc9th",
        "url": "https://www.reddit.com/r/AskStatistics/comments/844faj/how_to_quantify_significant_trends/",
        "text": "I am redoing a paper for a class and one of the major comments I received was to \"quantify your significant trends\". Just for reference, I am doing Mann-Kendall analysis and I already included my signficiance levels in  the figure. What are they wanting to be included in the figure? Sen's slope? Tau?",
        "created_utc": 1520950259,
        "upvote_ratio": ""
    },
    {
        "title": "Not sure what type of chart I am looking at.",
        "author": "ayeandone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/844561/not_sure_what_type_of_chart_i_am_looking_at/",
        "text": "It's similar to a scatter plot but not exactly. The chart is: https://imgur.com/a/dpXrX\n\nX and Y axis are independent variables, and then the plotted value is a dependent variable. ",
        "created_utc": 1520947794,
        "upvote_ratio": ""
    },
    {
        "title": "Normal or Uniform?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/843sle/normal_or_uniform/",
        "text": "First r/AskStatisitics post, and novice statistical observer here.\n\nI work in a production environment and I am working to determine the mean and standard-deviation of the time it takes for a conveyor to reach a particular station. The cart on the conveyor will be released as soon as the user finishes their operation, and pushes a button. \n\n- The user's input is effectively random where the user doesnt always start the task immediately, and the task lengths may vary from simple to time-consuming. \n\n- The conveyor has a \"catch\" every 10 feet, and moves at a constant speed. Assume the conveyor moves 1ft/second; If the user pushes the button to release the carrier, and the catch is 2 feet away, then the carrier will start moving in 2 seconds.\n\nSince travel post-catch is easy to calculate, my question is; if the users' triggering of the carrier is truly random, then do I have a normal distribution of \"time till catch\" or is it uniform?\n\nThanks in advance.",
        "created_utc": 1520944570,
        "upvote_ratio": ""
    },
    {
        "title": "What to do with binary data?",
        "author": "stolethesun",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8439pc/what_to_do_with_binary_data/",
        "text": "My DV is binary (yes or no). I was considering using Chi-square but my design causes some difficulties with that. I have two categorical IV's (both with two levels) one is repeated measures and the other is independent. Any ideas how I could analyse this? ",
        "created_utc": 1520938731,
        "upvote_ratio": ""
    },
    {
        "title": "Proportional odds regression/ordered logistic regression - Odds Ratio HELP. How do I interpret? I have an idea, please PM so I can check if I'm right? Also posted on r/statistics",
        "author": "hellangel_",
        "url": "https://i.redd.it/qxnaynsxdil01.png",
        "text": "",
        "created_utc": 1520937325,
        "upvote_ratio": ""
    },
    {
        "title": "Disentangling and estimating between-day and between-time variability in a longitudinal study",
        "author": "COOLSerdash",
        "url": "https://www.reddit.com/r/AskStatistics/comments/842pef/disentangling_and_estimating_betweenday_and/",
        "text": "[Disclaimer: This questions was also posted on [Cross Validated](https://stats.stackexchange.com/q/333139/21054)]\n\nIn a longitudinal study, the hand grip strength of subjects is measured. Each subject is measured 7 times (T1−T7):\n\n* T1−T6 are measurements at **different days and different but fixed hours** (at 7, 10, 13, 16, 20, 21 hours).\n* T7 is measured at a **different day but at the same time** as T6.\n\nAt each time, one measurement per subject is taken so that there are 7 data points for each participant.\n\nAs I understand it, the measurements at T6 and T7 capture the between-day variability whereas the measurements at T1−T6 capture the between-day and the between-time variability.\n\nQuestion: Is it possible to disentangle the between-day and the between-time variability with these data and if so, how?\n\nI thought of using a mixed model or a repeated measures ANOVA but I'm unsure how to set up the model to estimate the two sources of variability separately (if it is possible at all). Thanks for your time.",
        "created_utc": 1520931463,
        "upvote_ratio": ""
    },
    {
        "title": "Block design experiment?",
        "author": "punaisetpimpulat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/842li3/block_design_experiment/",
        "text": "I'm planning to carry out some experiments and I have a few restrictions I need to live with. I would like to know if I'm on the right track here or if there's a better way to design these experiments.\n\n\n**Basics**\n\nFirst of all, the main variable of interest is concentration of substance A and different concentrations of are expected to produce a different result in the end. Because of hardware limitations, I can only run 6 experiments at one time. The concentration range of interest goes from 0 to 100 mg/l and going higher than that is not useful. Chemical A is a really nice since it's always the same and it requires no special procedures. I could buy some A 10 years from now and it would be essentially the same as the one I have today.\n\n\n**Variables that are hard to control**\n\nHowever, the experiment also requires chemicals L and K, but they are the a bit tricky. Both chemicals are complicated mixtures and their exact characteristics depend on a myriad of variables. Sadly, they can not be preserved either, so stockpiling them isn't an option. I just need to collect a sample of L and K from our production and deal with the variations in them. Fortunately though, they are abundant and easily available to me.\n\n\n**Carrying out the experiment**\n\nEvery flask will always have the same concentration of L and K and an interesting concentration of A. Could be like 0, 20, 40, 60, 80 and 100 mg/l. The flasks will be mixed on my 6 flask mixer for a few days and the results will be measured. Let's call that response variable I, if someone needs to know. Running those experiments will take a couple of days, so by the time I'm ready for more, the chemicals L and K have already changed so much that I need to harvest a fresh batch. So every time I start another batch of 6 experiments, I'll have the same A as last time but slightly different L and K.\n\n\n**Here are the options I have considered so far**\n\n**Scheme 1.** Three concentrations of A + replicates = 6 flasks. The trouble is, my three concentrations of A might not be that great, so later I'll have to run another experiment like this, but with different concentrations. I might have chosen concentrations 0, 50 and 100 mg/l, but perhaps something important happens at 60 mg/l and I have no data about it. If I run another batch of, let's say 0, 40 and 60 mg/l, I will, of course, have a new batch of L and K, which will affect the results. So, comparing the first and second runs could get messy.\n\n**Scheme 2.** Six concentrations of A + no replicates, using chemicals L1 and K1. Later I'll replicate the same experiment with L2 and K2. Perhaps I'll also run a third batch with L3 and K3.\n\nIs one scheme better than the other? Are there other ways of running such experiments?\n\nThanks.",
        "created_utc": 1520929967,
        "upvote_ratio": ""
    },
    {
        "title": "integrals, kernel's",
        "author": "mathstudent137",
        "url": "https://www.reddit.com/r/AskStatistics/comments/841c3m/integrals_kernels/",
        "text": "So I don't really understand the stuff that is going on here: https://gyazo.com/e7018bd74db7cee1ca3a7a647066bd93 So I understand the Γ(a-1) stuff which comes from the kernal of gamma, but shouldn't the b there be \"by\"? Also how are they rewriting Γ(a-1) to get the cancellations requires to end up with 1 / (a-1)b? Wondering the same about EY^2\n\nHow is this integral 1 in this one: https://gyazo.com/d1a09f98b41f76438c2bcb1355ddd48f , where does that come from? ",
        "created_utc": 1520913608,
        "upvote_ratio": ""
    },
    {
        "title": "What test should you use for comparing just two data points?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8415po/what_test_should_you_use_for_comparing_just_two/",
        "text": "[deleted]",
        "created_utc": 1520911656,
        "upvote_ratio": ""
    },
    {
        "title": "Analysing Likert data — when to use regression vs one-sample T test?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8410xi/analysing_likert_data_when_to_use_regression_vs/",
        "text": "[deleted]",
        "created_utc": 1520910256,
        "upvote_ratio": ""
    },
    {
        "title": "Model fit metrics that incorporate quantitied noise knowledge",
        "author": "edwwsw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/840tpg/model_fit_metrics_that_incorporate_quantitied/",
        "text": "I have a multi variable regression  fit for some Spectra data.  Each spectra contains absorption info at different wave lengths.  These absorption patterns are used  to determine which compounds may be present. The spectra data is structured in that it has well define peaks in certain wave bands.   I can quantity the noise in the sample spectra data to say +- some wave number .  The noise is typically do to hardware and sensors.  I would like to incorporate the noise  knowledge into the metrics that measure the quality of the fit.  Like some modified p-value or r squared which are adjusted for noise knowledge.\n \nAs an example of the problem I'm trying to compensate for.  I  have a model that is straight line except for one peak.  The r squared between the model and sample looks \"bad\" because the linear part of the model is being compared against noise.  The peak alone fits very well.\n\n\nAny suggestions?",
        "created_utc": 1520908242,
        "upvote_ratio": ""
    },
    {
        "title": "Sensitivity, specificity, and the interaction between the two",
        "author": "ImperialViribus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/83zzb0/sensitivity_specificity_and_the_interaction/",
        "text": "Suppose I have an instrument designed to measure the prevalence of a disease in the general population, and that this instrument has been determined to have 100% sensitivity and 90% specificity; is it the case that this instrument will tend towards overestimating the prevalence of the disease? \n\nSimilarly, if this instrument were to have 90% sensitivity and 100% specificity, would it be the case that the instrument underestimates the prevalence of the disease?\n\nI'm just not entirely confident about my understanding of the interaction between statistical sensitivity and specificity, so thanks for any assistance.",
        "created_utc": 1520900446,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics in Text Mining",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/83zcm2/statistics_in_text_mining/",
        "text": "I have been given a text mining project for my MSc Statistics dissertation. I have to examine vocabulary, semantics and syntax over time in a specific database of texts. Obviously this project requires a large statistical component, but most of the texts I have looked at so far don’t have much statistical content, and I am sure you can find a way to use PCA, FA, clustering, LDA, etc. I am still in the early stages, but the field appears vast, and I feel I am moving too slowly despite my efforts. My questions are:\n\n1) What, if any, are the classic/standard texts in this field? Preferably with statistical content.\n\n2) Are there any particular authors with journal publications I should be exploring?\n\n3) Are there less obvious fields I should be looking into for resources?\n\nThanks in advance.",
        "created_utc": 1520895009,
        "upvote_ratio": ""
    },
    {
        "title": "What is an example of an event for which frequentist probability doesn't apply?",
        "author": "UnderwaterDialect",
        "url": "https://www.reddit.com/r/AskStatistics/comments/83z0no/what_is_an_example_of_an_event_for_which/",
        "text": "I'm on the hunt for an example to illustrate the difference between frequentist and subjective Bayesian probability. In particular, I'd like a type of event for which frequentist probability doesn't make sense.\n\nThis is the example I have: What is the probability that it will rain on March 14th, 2018 in London UK?\n\nAs far as I know, there can be no long-run relative frequency for this event. One could, of course, approximate this by asking \"The probability it will rain on any March 14th in London UK\", but as far as I can tell that is different.\n\nDoes this example make sense? Is there a better one?",
        "created_utc": 1520892438,
        "upvote_ratio": ""
    },
    {
        "title": "Randomization Check",
        "author": "tryinamath",
        "url": "https://www.reddit.com/r/AskStatistics/comments/83y73m/randomization_check/",
        "text": "Is there a test that could verify that patients were randomly assigned to a treatment level in a clinical trial? \n\nFor example, you have a 150 person study with three treatment levels: placebo, drug1 and drug2. The assumption is that the 150 subjects were randomly assigned to each of the three treatments, with 50 receiving each treatment. Do we just blindly accept that they were randomized to these treatments, or is there some kind of test that could verify that they indeed were randomized? Haven't had much luck searching google.",
        "created_utc": 1520886253,
        "upvote_ratio": ""
    },
    {
        "title": "I need help interpreting this data. I see there is not much significance but struggling with writing the analysis..ahhh..help please.",
        "author": "[deleted]",
        "url": "https://i.redd.it/17jm4hxu4dl01.jpg",
        "text": "[deleted]",
        "created_utc": 1520873788,
        "upvote_ratio": ""
    }
]