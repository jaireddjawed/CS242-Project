[
    {
        "title": "Complex model and ideas on what tests to use and how to procees appreciated",
        "author": "Optimal-Part-7182",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fb7kk/complex_model_and_ideas_on_what_tests_to_use_and/",
        "text": "Hey together,\n\nI have a question regarding analysing data with SPSS for my masther thesis (I posted a question already two days ago but formulated it a bit misleading). \n\nThe data was already collected and now I try to figure out ways on how to analyze it. The data was collected using an online questionnair with 4 randomized groups\n\nBackground:\n\nThe main objective is to analyze the impact of a campaign on behavioral intention with a focus on two factors moderating it. Additionally, it is assumed, that the campaign can trigger reactance, which mediates the effect on the the two mediators, and that this reactance can be moderated by adding additional information. It is also assumed, that a second moderator, peoples perceived cause will moderate the effect of the campaign on the two factors.\n\nOverall the model looks like follows - I am currently a bit lost on what tests to use and how to proceed. \n\nIs it allowed/possible to seperate the analysis in two steps, first handling the Mediators 3 and 4 as DVs to estimate the effects of the DVs and mediator 1 and moderator 2?\n\n&amp;#x200B;\n\nAppreciate every tip/shared experience.\n\nhttps://preview.redd.it/5jjboyc7ttca1.png?width=1219&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6aece60368539c9b39ad6a70527974ff42b041a2",
        "created_utc": 1674058371,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it possible to compute average margins effect for categorical variables?",
        "author": "Holiday_Snow_2734",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fb0w2/is_it_possible_to_compute_average_margins_effect/",
        "text": "Thanks for reading. Is it possible to compute average margins effect for a logit model only with categorical variables? Both DV and IV is categorical.",
        "created_utc": 1674057940,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Can anyone help me?",
        "author": "Dedadok",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fa54w/q_can_anyone_help_me/",
        "text": "I am currently doing research but statistics is not really my cup of tea and sadly I haven't been able to find anyone who could help me, so I really hope some reddithero could help me out. I am comparing two groups (say A and B) with a certain characteristic. Both groups undergo the same surgical procedure and I want to see how they differ in several categorical binary outcomes. I used chi-squared tests to do this. They turned out not to differ from each other, which actually is a positive thing. I also compared the groups on several preoperative characteristics, both continuous and categorical. This I figured out, but I do have a few questions that I hope anyone can help me with.\n\nDoes it add anything to do a binary logistic regression analysis to control for confounders affecting the outcomes if the groups do not differ at baseline? Since I do not know whether this is the case, I tried univariable regression analyses on a few possible confounders, however non of them turned out to be significant. If it were to be useful to do a regression analysis, would I still execute a multivariable analysis even though the univariable analyses did not even come close to significance?\n\nSecondly, I used certain criteria to divide the participants into groups. However, currently there are newer and more strict criteria available and both sets of criteria are widely used. I decided to analyze the groups using these criteria as well, to show that the outcomes would stay roughly the same when using the newer more strict criteria. However, I do not know whether purely describing this and descriptively comparing them would be enough? And what would I call this? As this does not seem to be a subgroup analysis, but I don't feel it would fit the label of a sensitivity analysis either.\n\nThanks a lot in advance!",
        "created_utc": 1674055813,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about cross validation",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10f8uym/question_about_cross_validation/",
        "text": "Hi all,\n\n&amp;#x200B;\n\nI have a logistic regression where I used leave one out cross validation to find the cv MSE for my model. I also want to report the AUROC for this model but I'm not sure if I should report the AUROC for a model trained on the full data or have a train/test set?",
        "created_utc": 1674052568,
        "upvote_ratio": 1.0
    },
    {
        "title": "Selecting from a uniform random distribution with low number of samples but narrow distribution",
        "author": "fuzzybear3965",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10f8j42/selecting_from_a_uniform_random_distribution_with/",
        "text": "I have a problem at work where I want to ensure that m of N instances of a service will have some feature enabled. I want them to do this without coordination. Every deployment of this service is allowed to have slightly different values of m but I'd like to keep that domain of values relatively small (narrow distribution). In my case N \\~ 30 and m = 5. For my specific case, a solution with 99% probability of m = \\[4,6\\] (closed at both ends) would be acceptable.\n\nMy first approach was to select a number uniformly at random between 0 and 1. If the value was greater than m/N then the service would have the feature enabled. In practice, this is a bad solution because sometimes 3 instances have the feature enabled but other times 10 services have the feature enabled. My next approach was to have each service draw k times from the same distribution and if the value chosen was bigger than (m/N)\\^(1/k) for all k draws then the feature would have the service enabled (I hoped this would tighten the distribution but I performed no analysis to justify this). This doesn't seem to work any better.\n\nIf my only resource is a uniform number generator ( \\[0,1\\] ) then what is a solution to ensure that, to within an arbitrarily high probability, I can ensure that the number of services with this feature enabled will lie inside of an interval \\[m-t, m+t\\] for some t&lt;=m, m+t &lt; N?",
        "created_utc": 1674051686,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any publicly available dental data sets for research?",
        "author": "GogaReborn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10f60x3/any_publicly_available_dental_data_sets_for/",
        "text": "Hi, I'm sorry if this isn't the right forum for this, I couldn't think of a better one. I'm looking for a publicly available dental data set for research into dental golden proportion, the data requires measures of teeth widths of people and some sociodemographic characteristics. \n\nI can't find a publicly available data set, if you know any or know any authority where we can request for a data set for research purposes kindly help me out. thanks",
        "created_utc": 1674044290,
        "upvote_ratio": 1.0
    },
    {
        "title": "Asking for help regarding Independent Sample T-tests",
        "author": "ShiroNeko_96",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10f5hjr/asking_for_help_regarding_independent_sample/",
        "text": "What does a negative t value mean and how does it explain your data?",
        "created_utc": 1674042600,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it appropriate to use a linear mixed model to compare 3 group's means at a single timepoint?",
        "author": "Lil_LempelZiv",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ew2d5/is_it_appropriate_to_use_a_linear_mixed_model_to/",
        "text": "Or would a one-way ANOVA be more appropriate?",
        "created_utc": 1674010759,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you lie about numbers?",
        "author": "HeyItsEricc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10es0iu/how_do_you_lie_about_numbers/",
        "text": "Hi so I'm a senior doing a project for my AP Psych class right now, and I got this prompt for a \"teach me\" lesson for the class, around 10 minutes long. I've thought a few things myself, like abusing people's lack of general mathematical intuition (ex: birthday paradox, 2 heads from 4 coins being 35%, survivorship bias, etc.), and just basic stats vocab, like what a percentile is, but I'm kind of struggling to come up with actual ones. Like people talk about how politicians always lie abt stats, but I'm wondering how they actually pull it off (w/o just straight up saying nonfactual numbers).\n\nAny ideas on how to create chaos with numbers are appreciated.",
        "created_utc": 1674000117,
        "upvote_ratio": 1.0
    },
    {
        "title": "Do I need to standarize data before making Q-Q plots?",
        "author": "muskagap2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10en0s5/do_i_need_to_standarize_data_before_making_qq/",
        "text": "Hello everyone. I have a quite huge dataset with many numerical columns. I need to check if there are normally distributed or not, that's why I want to create Q-Q plots. I did it in Python like this:\n```\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n    \nnum_cols = df.select_dtypes(include=np.number)\ncols = num_cols.columns.tolist()\ndf_sample = df.sample(n=5000)\n    \nfig, axes = plt.subplots(4, 5, figsize=(15,12), layout = 'constrained')\nfor col, axs in zip(cols, axes.flat):\n    sm.qqplot(df_sample[col], line='45', ax = axs)\n    \nplt.show()\n```\nThe issue is that Q-Q plots look weird. For example, theoretical quantiles range from 0 to 100 or for some columns from 0 to 3000. It's not between -4 to 4. And even though histogram for variable X looks like normal distribution then the corresponding Q-Q plot looks totally strange (e.g. points don't follow 45-degree line, they are formed in a vertical line).\n\nMy question is - do I need to standarize all my columns (mean=0, std=1) and then make Q-Q plots or not? Now, without z-score standarization those plots look pointless.",
        "created_utc": 1673988404,
        "upvote_ratio": 1.0
    },
    {
        "title": "Data Structure Question for ICC in SPSS",
        "author": "ScreamnMonkey8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ek8dv/data_structure_question_for_icc_in_spss/",
        "text": "I am doing some research and have my data structured in a 'typical' repeated measures format (i.e., a row represents a participant and a column is a DV). As part of my research, participants will perform three 100m dashes. Now each participant was recorded by the same rater, but there were multiple raters. Clarification example: Participant 1 was measured by tester 1. Participant 2 by tester 2 etc. In essence I just have their times and not who recorded each participant. \n\nI was hoping to perform some conservative ICC calculations in which I would utilize the ICC (1,k); I was hoping to examine how well each tester correlated with each other. The ICC (1,k) seemed to be most appropriate, I used an article that discusses possible ICC combinations. Looking on the web about running ICC on SPSS shows data structured as participant by tester and not DV. \n\nMy question is, should I trust my ICC output files? I believe that my interpretation is the value I get (e.g., 0.84) would in this case reflect how consistant the participant performed to themselves and not between different raters. This is because my structure is participant by dv.\n\nHopefully this makes sense and not some random person rambling on.",
        "created_utc": 1673981946,
        "upvote_ratio": 1.0
    },
    {
        "title": "What method do I use to select relevant features/variables for my data?",
        "author": "GuybrushManwood",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ej58d/what_method_do_i_use_to_select_relevant/",
        "text": "Suppose I have a bunch of labeled data and each observation has many features (`x_1` to `x_n`) e.g.:\n\n```\nset.seed(42)\nn &lt;- 10\n\ntibble(\n  x_1 = rnorm(n), \n  x_n = rnorm(n), \n  label = sapply(round(runif(n) * 26), function(x) LETTERS[x])\n)\n```\n\nI know that some of the features/variables (`x_1` to `x_n`) will determine the label, while other's are completely or almost completely irrelevant. What method would I use to identify the relevant features/variables that determine the label?\n\nThanks!",
        "created_utc": 1673979358,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dimensionality reduction or feature selection to identify relevant features for labeled data?",
        "author": "BigClockEnthusiast",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10eipex/dimensionality_reduction_or_feature_selection_to/",
        "text": "[removed]",
        "created_utc": 1673978368,
        "upvote_ratio": 1.0
    },
    {
        "title": "Obtaining Mahalanobis Distance between high dimensional data point and high dimensional cluster",
        "author": "GuybrushManwood",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10eedua/obtaining_mahalanobis_distance_between_high/",
        "text": "Assume I have a data point in high dimensional space `x` (vector with `500` dimensions) and a cluster of data points `y_1` to `y_n` (matrix, `n * 500`), can I use Mahalanobis Distance to estimate the distance between `x1` and the cluster?\n\nIf so, how could I achieve this in Python or R? Thanks a bunch",
        "created_utc": 1673967748,
        "upvote_ratio": 1.0
    },
    {
        "title": "Aren’t the confidence interval and training error doing the same thing?",
        "author": "MaBrowser",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ee8vl/arent_the_confidence_interval_and_training_error/",
        "text": "Let’s say I have a bivariate dataset (X,Y) and force a linear regression through it.\n\nI use bootstrapping to generate the confidence interval. On each resample I get a fresh new equation for the linear model, outputting a slightly different Y for each X. The difference between this bootstrapped Y and the actual observed Y (the residual), is effectively what’s defining my confidence interval. I do this for a number of iterations, and plot say the 95% percentiles for all X. I get an output like so:\n\n[https://i.stack.imgur.com/dETL2m.png](https://i.stack.imgur.com/dETL2m.png)\n\nTo obtain the train (MSE) error, I plot the linear regression through the dataset, calculate the square of the residuals and take the average.\n\nI’m really struggling to see then what is the difference between the confidence interval and train error? Both are expressing a margin of error - granted the former uses the residuals, whereas the training error uses the square of the residuals, but other than that is there some difference I’m missing?\n\nI also get that the confidence interval is expressed in a nice curve over the series of the data, but this could easily be wrapped up into a single number like the train (MSE) error if I wanted to. Or likewise I could calculate the train error for each X and plot a nice curve too.\n\nThis is really confusing me and just wondering why we do both.\n\nThank you",
        "created_utc": 1673967418,
        "upvote_ratio": 1.0
    },
    {
        "title": "Order statistic - unifrom",
        "author": "ilsapo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10edt9i/order_statistic_unifrom/",
        "text": "Hi, I hope this question will be ok.  \nwe have x\\_1,x\\_2....x\\_n i.d.d random varibles \\~ U(0,1)  \nwe have the order statistics x\\_(1)&lt;=x\\_(2)....&lt;=x\\_(n)  \n\n\nand I was asked to calculate E \\[ x\\_(1) \\] and E \\[ x\\_(2) \\]  \nI solved this question using integration and found  \nE \\[ x\\_(1) \\] = 1/(n+1)  \nE \\[ x\\_(n) \\] = n\\* 1/(n+1)  \n\n\nI was wondering if it was possible to solve this question without Integrating, and just using intution and the order statistics definition  \nits just when I look at the results, I kind of feel like I should have been able to \"guess\" the answer",
        "created_utc": 1673966285,
        "upvote_ratio": 1.0
    },
    {
        "title": "two random Uniform varibles P(x&lt;y)",
        "author": "ilsapo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10eb9oi/two_random_uniform_varibles_pxy/",
        "text": "I have two random varibles i.i.d  x,y \\~U(0,1)  \n\n\nand I wanted to calculate P(x&lt; 2 y)  \nnow I kind of feel it should be 1/4 but I find it hard to explain why  \n\n\nso in similar sene I fell like P( X &lt; Y) should be 1/2 but dont know how to explain it,  \nand the quesiton P( X&lt; 2y)  \nfeel like we just \"streched\" the possible values of y by 2, so the porabilty is \\* 1/2",
        "created_utc": 1673959108,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are these types of graphs called and how to read them?",
        "author": "GodBod69",
        "url": "https://i.redd.it/aodghjpxkmca1.png",
        "text": "",
        "created_utc": 1673952742,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are these types of graphs and called and how to read them?",
        "author": "GodBod69",
        "url": "https://i.redd.it/8j7c51q1kmca1.png",
        "text": "",
        "created_utc": 1673952443,
        "upvote_ratio": 1.0
    },
    {
        "title": "Normally distribute a value over time",
        "author": "j3r_bear",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10e4xed/normally_distribute_a_value_over_time/",
        "text": "I don’t know if it’s the wine failing me or the brain, but I’m stumped trying to consider how I would create a normal distribution of a value over a timeframe.  \n\nPE: I have estimated a job will last 100 labor hours.  The job will start on 1/1/2023 and it will end on 6/30/2023 (181 days including end date).  How would I best set this problem up to normally distribute the labor hours across the 181 days?  \n\nEdit: It’s definitely the wine AND the brain failing me.",
        "created_utc": 1673936355,
        "upvote_ratio": 1.0
    },
    {
        "title": "REGRESSION CONFUSION",
        "author": "Hypnotic_Juice",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10e2fbx/regression_confusion/",
        "text": "Hello, I am a grade 12 student that is in dire need of your help.\n\nI am doing a research about microcontroller sensor that senses LPG  gas.\n\nBasically, I have measured the PPM (partspermillion) of LPG that the sensors can sniff out from various distance \n\nSo my IVs are 1) Distance from sensor and 2) PPM Ejected. On the other hand the DV is PPM Detected.\n\nI wanted to do MLR to predict the PPM Detected with further distance and larger ejection of PPM relative to my experiment. However, the PPM Ejected data that I have collected is constant regardless of distance. How can i do MLR with this kind of stuff?\n\n\nTHANKS IN ADVANCE FOR RESPONSES",
        "created_utc": 1673928607,
        "upvote_ratio": 1.0
    },
    {
        "title": "SAS Studio 3.8 vs desktop 9.4 install",
        "author": "lapispimpernel",
        "url": "/r/sas/comments/10dx2i9/sas_studio_38_vs_desktop_94_install/",
        "text": "",
        "created_utc": 1673914125,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multifactorial test for when a factor has effect under some circumstances but not others?",
        "author": "user_--",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dwfas/multifactorial_test_for_when_a_factor_has_effect/",
        "text": "I'm testing how route of administration (Route A/Route B) impacts drug response in mice (dependent variable). In addition to the route as a factor, I also tested sex and dose size as factors, giving 8 groups (each combination of Route A/Route B, male/female, low dose/high dose).\n\nIt is VERY obvious that Route A and Route B give different outcomes for the Female-Low and Female-High groups. For all the other groups, it's not obvious whether the route makes a difference.\n\nI want to run a statistical test to quantify the significance of these differences. I'm told the correct way to do this is a multi-factor ANOVA with 3 factors. However, when I do this, it returns a p-value for each factor, and the p-value for route is not significant.\n\nIs the significance of the effect of route in the Female-Low and Female-High groups getting \"obscured\" in the p-value somehow by the lack of route effect in all the other groups? I'd like to report that route has an effect in these two groups and not the others, but if I report this p-value, it will suggest route just has no effect under any circumstances. What should I do?",
        "created_utc": 1673912468,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I present something with multiple categories",
        "author": "OppositeTree4619",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10duozt/how_do_i_present_something_with_multiple/",
        "text": "Hello, I was wondering what software I could maybe use to help me with some data. For example if I had collected data from a big group of individuals and I wanted to look at a lot of variables at the same time, such as age bracket, gender etc. Not sure if this is an appropriate questions, I have never had to do something similiar before. I don't know if I could put it all on excel or do multiple tables or something. Thank you.",
        "created_utc": 1673908410,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I check a linear relationship when doing a linear regression with an independent categorical variable and a continuous depenent variable?",
        "author": "guileus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dujxb/how_can_i_check_a_linear_relationship_when_doing/",
        "text": "If I'm not wrong, both variables need to have a linear relationship, but when producing a scatterplot of the relationship between both I can see no linearity, as my independent variable is categorical (so all observations are either at the 0 value for the independent variable or the 1 value).  \nHow can I check this assumption?",
        "created_utc": 1673908095,
        "upvote_ratio": 1.0
    },
    {
        "title": "Another question about Wordle",
        "author": "MrTralfaz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dtvm9/another_question_about_wordle/",
        "text": "I'm a statistics bonehead so maybe someone can explain what I am missing.  The website [measuremywang.com/](https://measuremywang.com/) claims to calculate your average guess score in the Wordle game.  My current word scores with Wordle are:\n\n \n\n1:0  \n2:14  \n3:71  \n4:99  \n5:56  \n6:18\n\n7:10 (losses).\n\nThe measuremywang average score I'm given is 4.09\n\nNow my non-statistician gut tells me that if my 1,2 and 3 guesses add up to more than my 5,6 and 7 guesses (85 vs. 84), my average should be ***under*** 4.0.  Am I missing something?  Is my gut using flawed logic?  Are the 7 guess scores given more weight than the others?  Is the website's calculation wrong?",
        "created_utc": 1673906635,
        "upvote_ratio": 1.0
    },
    {
        "title": "I get two complete different correlation coefficients on two variables depending on wether i first difference them or not",
        "author": "doing20thingsatatime",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dt630/i_get_two_complete_different_correlation/",
        "text": "The two variables are gold prices and core inflation.  \n\nFirst differenced inflation (% change) and gold prices give me a correlation of 33%.  \nFirst differenced inflation (% change) and first differenced gold prices give me a correlation of only 2%.\n\nThis basically means that a change in inflation has a greater impact on gold prices (33%), than on the change of gold prices (2%).  \n\nAm i right?  \n\nI'm quite confused here and shocked. I was always been told to take returns because they're stationary, without really understanding why. Now i do get that returns are by nature first difference of prices in percentage of previous price, which is what makes them stationary.  \n\nBut what is the rationale, layman, explanation here?  \n\nTo me intuitively we should be getting the same relationships whether working with plain prices or change of these prices.",
        "created_utc": 1673905252,
        "upvote_ratio": 1.0
    },
    {
        "title": "[QUESTION] GLM",
        "author": "Effective_Thought_91",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dqmoj/question_glm/",
        "text": "Hi there. \n\nI'm a linguistics students interested in stats. I'm planning an experiment and its results could be analysed through a multinomial logistic regression model but I've run into a question. I'll exemplify my question with a simple hypothetical example. \n\nPretend I want to predict whether a product is cheap or expensive based on color (blue, red, yellow) and size (big, small, medium). Is it a problem for my model if all the yellow products in my sample are big? Not all big products from my sample are yellow though. \n\nIf it's a problem for my glm, is there a way to overcome it? \n\n&amp;#x200B;\n\nThank you so much.",
        "created_utc": 1673901023,
        "upvote_ratio": 1.0
    },
    {
        "title": "Random variables:Expectation and Variance",
        "author": "ArmEmergency2946",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dic3t/random_variablesexpectation_and_variance/",
        "text": "Hi! Can you help me with these 2 exercises? I will be really thankful!  \n1. If for a random variable ξ we have that Εξ=2 and Dξ=9, and n=ξ-3/3, then Еn and Dn are?  \n\n\n2. The expectation and variance of a random variable ξ are 1 and 2, respectively. What is the expectation of 2ξ?  \n\\-&gt; in this problem I came up with an answer that rounds to 2, but I'm not sure it's correct because I'm not sure about the solution  \n\n\nTHANKS IN ADVANCE!",
        "created_utc": 1673885195,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for New Learnings",
        "author": "Data_Nerd1979",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dhtuu/looking_for_new_learnings/",
        "text": "[removed]",
        "created_utc": 1673884009,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mediation analysis in R",
        "author": "DrNeerajB",
        "url": "https://m.facebook.com/story.php?story_fbid=pfbid0TwSjEedHpNZcGjCpgmvEHzoY5SAg4QP8vaJUdMYx21PnwzCSN4tmG4xAzgf3i9HDl&amp;id=100061233984738&amp;mibextid=Nif5oz",
        "text": "",
        "created_utc": 1673883074,
        "upvote_ratio": 1.0
    },
    {
        "title": "Tool/set of templates to translate conceptual model into a manual for SPSS?",
        "author": "Optimal-Part-7182",
        "url": "/r/spss/comments/10dgqqr/toolset_of_templates_to_translate_conceptual/",
        "text": "",
        "created_utc": 1673882458,
        "upvote_ratio": 1.0
    },
    {
        "title": "Intraclass correlation",
        "author": "garbagemonsta",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dfsfj/intraclass_correlation/",
        "text": "Hello, I am wondering if it is appropriate to run ICC on survey data in which employees in an organization are responding to an organizational culture survey? I want to know if there is weak or strong agreement **within** departments regarding perceptions of culture. If this test is appropriate, I'm assuming I would use two-way mixed and absolute agreement, is this correct?",
        "created_utc": 1673879013,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about a article from Meta Analytics regarding Long-Run experiments.",
        "author": "MobileOk3170",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10debf2/question_about_a_article_from_meta_analytics/",
        "text": "This is the [link](https://medium.com/meta-analytics/estimating-the-long-run-value-we-give-to-our-users-through-experiment-meta-analysis-6ddb9073b29b) to Meta Analytics article. I have 2 questions.\n\n1. It seems the paragraph is suggesting, I can do linear regression to obtain the coefficients for each experiment. Did I misunderstood it? It seems to make more sense I can treat each experiment as a data point with **Y** as output, and **x's** as input and perform linear regression.\n\n&gt;Next,  in each of the experiments we measure how much composition has shifted,  which will naturally shift a bit as this is what our experiments are  changing. If you see 10% more friend content, you will see a bit less of  other content as a result — it’s not just one type of composition that  shifts but multiple. In terms of notation, assume the first experiment  showed *x₁₁%* more friend content (where the first 1  in the variable stands for first content type = friend content, the  second 1 stands for the first experiment), *x₂₁*% more group content, *x₃₁*% more other content, the second one showed *x₁₂*%, *x₂₂*%, *x₃₂*% and so on, where naturally some *x*’s could be negative. Now we can run a simple linear regression on the data aggregated up at the % per treatment level:  \n&gt;  \n&gt;*Yᵢ \\~ coeff₁\\*x₁ᵢ + coeff₂\\*x₂ᵢ \\* …. \\* coeffₖ\\*xₖᵢ,*\n\n2. The paragraph seems to suggesting given a Y score, having 10x more **type 1** content as **type 2**, implies each piece of **type 1** content worth 1/10 less compared to **type 2** content. I don't understand how it is necessary true.\n\nIf I have 10x more type 1 content, it will only contribute 10x more to the score. Unless, somehow the gain on **type 1** content is somehow fixed?\n\nCan some one make up 1 or 2 simple examples that the numbers will work out?\n\n&gt; Also note that if we compare the coefficients, for instance if *coeff*₁/*coeff*₂ = 2, that means 1% of content of type 1 is roughly on average 2x as valuable toward outcome *Yᵢ*  as 1% of content of type 2. Notice, however, that this is not in  absolutes. E.g. if the average user has 10x as many posts to see of type  1 as type 2, then actually on average each piece of content of type 2  is roughly 10/2=5 times as valuable as content of type 1. Translating  units into absolutes like this then allows you to derive a weighted  metric expressed in terms of absolutes that can be used to evaluate any  standard new experiment that the team runs. \n\nI've also drawn up some diagrams to help my understandings, in case anyone's interested.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/zva1qga1oeca1.png?width=1868&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f6b1de5ee790f37e29d13936c95c9e15c3aa1a2c",
        "created_utc": 1673875007,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question on method for anonymity/pseudoanonymity and 'tracking'",
        "author": "Rendelf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dcuvq/question_on_method_for_anonymitypseudoanonymity/",
        "text": "Hello folks. Looking for a nudge in the right direction here and I hope you can help. \n\nI am not any kind of an expert, but I need to move a project in the right direction (ahead of expert consultations) so seeking advice! \n\n**Here's a quick definition of what I need to do:** \n\nA survey of 1k - 5k individuals about to engage in a particular event. The survey is of protected characteristics, which will anonymous and used for research about the demographics of people who attend that event. \n\nFine so far. However, at the event are a number of cheese sandwiches. The project aim is to consider stats on the demographic differences between those individuals who find a sandwich and those who do not. \n\nBecause of the nature of the data, anonymity is desirable. \n\n**The main options as I see them are:** \n\n1. Survey ahead of the event and pseudonymise the data. Those who get the sandwiches can report as much to the surveyors using some sort of tag or other key and the data can be updated. Certain individuals in the surveying org could in theory identify the respondents.  \n2. Survey after the event - data can be fully anonymous at source. The downside here is that the response rate among sandwichless people will be a lot lower. \n\n**My question to you all:** \n\nIs there some obvious proven system/approach to this kind of problem which I should be looking at? \n\n&amp;#x200B;\n\n Many thanks!",
        "created_utc": 1673870598,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for a test to compare results of different evaluation methods. Basically something like an ANOVA for n = 1.",
        "author": "charliechappy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10dawx0/looking_for_a_test_to_compare_results_of/",
        "text": "Hi there,\nIn writing this, I noticed how it's becoming somwhat of a wall-of-text so I'll provide a short and a long version of the question below.\n\n**SHORT:**\nI'm currently working on a project where we are analysing short transient signals for multiple elements of interest and calculate their intensity ratios.\nThe analytical community mostly relies on 3 different methods to evaluate such signals:\n1. point by point evaluation\n2. linear regression\n3. signal integration\nAll of which have their distinct up- and down-sides.\nFor that reason I was planning to apply all three (and more) methods on my data and then use the one with the \"best\" result (in terms of combined measurement uncertainty).\nHowever, there is no general \"best method\".\nIs there a way where I can test my results of all methods to see if they agree within a given confidence interval? I can only evaluate the results of a singular measurement at a time, because they are considered to be unique.\n\n**LONGish:**\nDue to their different evaluation approaches, some measurements are \"best\" evaluated by one method over another. But how can i be certain that the method i want to apply is valid?\ni.e.: \nPoint-by-point evaluation does not apply any weights to the results, resulting in a higher contribution of low signal intensities to the overall result. This brings along a higher contribution of bias and uncertainty from blank.\nIntegration and linear regression have some \"built in\" weighting towards higher signal intensities but are more prone to outlier events during measurement.\nI assume that I can use ANOVA for the evaluation of reference material samples because there i'm more or less taking random samples from a larger entity. \nHowever, I believe that I can't do that for real field samples, as they are to be considered unique and cannot be measured more than once. They are _very_ small samples and everything is consumed during a measurement so *n* would always be equal to 1.\nIs there a way to test my results to see if they \"agree\" on a result?\n\nThank you for your help!",
        "created_utc": 1673863801,
        "upvote_ratio": 1.0
    },
    {
        "title": "Did I use the correct statistical treatment?",
        "author": "shayyzUwU",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10d7kp3/did_i_use_the_correct_statistical_treatment/",
        "text": "Hi! I'm currently working on my senior year research paper, I wanted to find out the quality of my final output and its acceptability. I conducted a survey using a questionnaire and used a 5-point likert scale for quality, so its options were excellent, very good, ... etc. Then I computed for the weighted mean of each category as to know the quality for each. \n\nTo test for the acceptability of the final output, is it correct that I computed for the average of all the computed weighted mean, then I used a different 5-point likert scale to interpret it, the one where the options were acceptable, slightly acceptable, ... etc.? \n\nAny suggestions on other applicable statistical treatment/s for this?",
        "created_utc": 1673851615,
        "upvote_ratio": 1.0
    },
    {
        "title": "T.test question",
        "author": "daddybuilders",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10d5ng2/ttest_question/",
        "text": "If I collect pilot data on a sample of 30-40 people from a population, and then stratify them…\n\n\n1. Is hypothesis testing meaningful if there are only 15-20 in each group? What if there was, say…30 in one group and 5 in the other? Or, would would any results be so low power they’re meaningless when it comes to deciding on whether it’s worth the money to conduct a larger study?\n\n2. If I wanted to test for normality (I know that’s a whole other issue and frowned upon), is something like the Shapiro-Wilk test used on the whole data set or on each group separately after stratification?",
        "created_utc": 1673845339,
        "upvote_ratio": 1.0
    },
    {
        "title": "What’s up with my inconsistent power?",
        "author": "theraprofessor13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10d0y7t/whats_up_with_my_inconsistent_power/",
        "text": "",
        "created_utc": 1673831634,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multiple independent variables, but interested in the effect of one - what test?",
        "author": "user_--",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10czw76/multiple_independent_variables_but_interested_in/",
        "text": "I'd like to know if using different routes of administration of a drug gives different therapeutic effect. I tried both Route A and Route B on male mice with a low dose. I would just use a two-tailed t test to see if the groups have a statistically significant difference.\n\n\nBut I also want to see if the route makes a difference in female mice, and for high doses. In the end, I have 8 experimental groups (each combination of Route A/Route B, male/female, low dose/high dose).\n\n\nWill people get mad at me if I just do a t test between each Route A/Route B pair to see whether the route makes a difference for that combination of sex and dose? Should I do something fancier like some type of ANOVA to compare all 8 groups at once? If so, why? Thanks!",
        "created_utc": 1673828827,
        "upvote_ratio": 1.0
    },
    {
        "title": "Where to obtain data",
        "author": "alander760",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10cyr4m/where_to_obtain_data/",
        "text": "I’m currently a business analytics &amp; IT major at university and have really been enjoying my statistics classes, and want to keep learning on my own. \n\nI learn best by applying my knowledge to (realistic) data sets and experimenting hands on . Is there a website to generate realistic data or one that provides real data for free?",
        "created_utc": 1673825961,
        "upvote_ratio": 1.0
    },
    {
        "title": "K-means Clustering",
        "author": "SandPaperCatheter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10cwi2j/kmeans_clustering/",
        "text": "The goal of k-means clustering is stated as a process that minimizes the within cluster variation in each of k clusters. Would setting k = the number of observations mean that there is zero within cluster variation and be optimal? There would be zero knowledge gained from this for exploring the unsupervised method.  Is there something wrong with how I am interpreting the goal of k-means clustering?",
        "created_utc": 1673820462,
        "upvote_ratio": 1.0
    },
    {
        "title": "Post hoc power analysis two way ANCOVA",
        "author": "Christophmm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10cuu6s/post_hoc_power_analysis_two_way_ancova/",
        "text": "Help guys, it’s been a while since I’ve dealt with data! Sooo I have the following problem: \nI have 2 independent variables, each with three levels (no, irregular, regular). Additionally, two covariates are included in the Ancova. Lastly, the outcome variables are naturally all continuous. As I have 3 different outcome variables I wanted to test, I’ve run 3 different Ancovas - all with the same IVs and covariates, only differing by the outcome variables. The main analysis, no problemo, all done. \nNow I’m stuck at the power calculation, to be specific: How do I calculate the power? I’m really confused on what I should put into g.power, how often I gotta do a power analysis and whether I’m even doing the right thing using g.power. Might be important to say, that the sample size was pre-established and therefore the option I have is to do a post-hoc power analysis. Additionally, Ancova is the chosen the way to go - even if other designs might be more fitting. Maybe someone can help me, would be very appreciated!",
        "created_utc": 1673816498,
        "upvote_ratio": 1.0
    },
    {
        "title": "What goes in the Verifications section of a JASA/statistics article?",
        "author": "keithreid-sfw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ctvja/what_goes_in_the_verifications_section_of_a/",
        "text": "I want to be submitting to JASA, my Prof likes them. \n\nI downloaded the LaTeX for JASA and there is a Verification section after the methods. \n\nI will be reading a few of their papers to get the style. \n\nAs well as that experiential learning I like a rule of thumb. What goes in there? I am used to medical science and have no familiarity with the term.",
        "created_utc": 1673814259,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is G*Power a valid measure in two-way between subjects unbalanced design?",
        "author": "Few-Cloud-3646",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ct1i1/is_gpower_a_valid_measure_in_twoway_between/",
        "text": "1 IV is has different sample sizes (177/35). Is G*Power a valid measure of total sample size or does it assume a balanced design?\nThanks :)",
        "created_utc": 1673812281,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would I use a random effects regression model for this analysis?",
        "author": "deplorable_word",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10csvka/would_i_use_a_random_effects_regression_model_for/",
        "text": "I have one group, with three scores per individual, and I want to know the effect of each score on the other two\n\nIf not random effects regression, what you you suggest?",
        "created_utc": 1673811883,
        "upvote_ratio": 1.0
    },
    {
        "title": "In Borges library of Babel, what is the longest hex length?",
        "author": "zPrimeCoupling",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10crfoz/in_borges_library_of_babel_what_is_the_longest/",
        "text": "",
        "created_utc": 1673808477,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you cluster tabular categorical data without knowing the number of expected clusters?",
        "author": "o-rka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10cnc3s/how_do_you_cluster_tabular_categorical_data/",
        "text": "I have 4 categories:\n * no alignment\n * aligns on left and aligns on right\n * aligns on left and not on right\n * aligns on right and not on left\n\nWhat’s the best way to calculate a distance matrix for hierarchical clustering?\n\nMy thoughts were to do one hot encoding and then jaccard distance.  \n\nIs that the best way?\n\nWhat other approaches do I have?",
        "created_utc": 1673798619,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do we calculate the center tendency in excel?",
        "author": "Omar_Linguistics009",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ckjtz/how_do_we_calculate_the_center_tendency_in_excel/",
        "text": "",
        "created_utc": 1673791146,
        "upvote_ratio": 1.0
    },
    {
        "title": "Weight score",
        "author": "Zestyclose_Base_6256",
        "url": "https://www.reddit.com/gallery/10cixdl",
        "text": "I have data for 4 categories with weight for each category. For overall rank scores..what is better - weighted score on percentile rank or weighted score on rank numbers(this will be highest score will get lowest rank). Total data size is 1000 rows. Attached is sample data\nMy question is why are two overall scores gives different rankings and which ranking is better -weighted score rank on percentile rank or weighted score rank on rank numbers",
        "created_utc": 1673786461,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with PROCESS moderation analysis assumptions",
        "author": "Acrobatic-Ice1411",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10chmvk/help_with_process_moderation_analysis_assumptions/",
        "text": "Hi! :) I am conducting moderation analyses using PROCESS, and am trying to check if my data fits the assumption of no multicolinearity. To check this, do I just do a normal multiregression analysis, enter the independent variable, and the moderator variable as independent variables, the dependent variable as the dependent variable, and then compare the VIFs? Or do I also add in an interaction term of the IV\\*MV in the independent variable box?  \nIf it's the latter, for the output, how do I interpret this? Should I be checking the VIFs of only the IV, or the VIF of the interaction factor? What about if the interaction factor VIF indicates strong multi-colinearity, but the IV and MV VIFs are fine?  \n\n\nCan I still run the analysis if there is some multicollinearity? I saw a video that said this is not the most important assumption in moderation analyses.\n\nThanks so much for your help!",
        "created_utc": 1673782326,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need help deciding between a degree in 'data science and decisions' and a degree in 'actuarial studies'.",
        "author": "Joketai",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10cg7e6/i_need_help_deciding_between_a_degree_in_data/",
        "text": " \n\nI am based in Sydney, Australia and have received offers for both data science and actuarial studies at UNSW. Torn between the two as I don't know how hard actuarial studies actually is compared to data science.\n\nI'm looking to still have a social life at university and I have heard that actuarial studies exams are very difficult. On top of that, even after completing an actuarial studies degree I heard that there are still exams that must be taken. Does this mean that even after completing my degree, I will be unable to find a decent paying job in comparison to data science in the long run if I choose not to complete any exams? For example if after I complete my actuarial studies degree I will most likely work as a actuarial analyst, is this a better option than becoming a data scientist.\n\nIt just feels as though if I go into a degree in data science and decisions at least I have a proper job standing as a data scientist or a data engineer, while actuarial studies doesn't have a proper support as even after I have completed the degree I still have to spend more time studying for exams. I think it is important to note that my actuarial studies degree is a double degree which will take 4 years while my data science and decisions degree is 3 years but I can choose to do a masters for an extra 2 more years. So at the end, it seems I can get full qualifications as a data scientist after 5 years, whilst my actuarial studies even after 4 years still is shaky since I don't even know how long it will take me to complete my exams and become a full-fledged actuary.\n\nAnother dilemma I have between the two is that a degree in actuarial studies is more well known and data science is still a growing degree so I feel as though I am taking a risk going into data science where the degree seems to be no different from a software engineer in programming or a data analytics person dealing with data.\n\nAnother question I have is, say that I do go into my double degree with actuarial studies. Since the course of actuarial studies seems already difficult, would a double degree be too much to handle or would it be better to stick with the single degree. Is the benefit really worth it of getting dual qualifications at the cost of stress on my mental health? If the benefit is worth it does anyone have a recommended double degree to go with actuarial studies? The options are actuarial studies with a double degree in: computer science, commerce, information systems, science, economics, science(advanced mathematics)(honours). It is important to note that the last one science(advanced mathematics)(honours) takes 5 years to complete the double degree.\n\nThis is a long list of questions so I'm sorry in advance.\n\nIf anyone has any advice for me it is greatly appreciated. Thank you very much in advance as well.",
        "created_utc": 1673777138,
        "upvote_ratio": 1.0
    },
    {
        "title": "hypothesis test: very small sample size, two groups, data not normally distributed, yet same variances",
        "author": "achsoNchaos",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10cg1wk/hypothesis_test_very_small_sample_size_two_groups/",
        "text": "Hi there,\n\nI have samples from two groups, each containing 17 samples.\n\n    group1 = [6.2, 7.1, 1.5, 2,3 , 2, 1.5, 6.1, 2.4, 2.3, 12.4, 1.8, 5.3, 3.1, 9.4, 2.3, 4.1]\n\n    group2 = [2.3, 2.1, 1.4, 2.0, 8.7, 2.2, 3.1, 4.2, 3.6, 2.5, 3.1, 6.2, 12.1, 3.9, 2.2, 1.2 ,3.4]\n\nLet $X$ be the underlying distribution of group 1, and $Y$ from group 2\nMy goal is to perform a plausible hypothesis test to check if the true means of those two distributions are equal, so if $\\mu_X = \\mu_Y$.\n\nMy attempts so far:\n- testing for normality: Shapiro-Wilk test, Kolmogorov–Smirnov test, D’Agostino-Pearson’s K² test rejected the null hypothesis that $X$ and/or $Y$ are normally distributed (I used python's scipy.stats module)\n- conducted a f-test to test if the variances of both samples are equal. The null hypothesis (variances are equal) can be kept to significance level 0.05\n\n(I hope those attempts are plausible, if you spot a logical flaw, I'd be very glad if you let me know.)\n\nMy question: Which hypothesis test would be most plausible assuming that $X$ and $Y$ are actually not normally distributed yet have the same variance? Especially considering the very small sample size.",
        "created_utc": 1673776578,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical test to use to find if the difference b/w 2 or more groups is significant for continuous data?",
        "author": "101coder101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10cfbda/which_statistical_test_to_use_to_find_if_the/",
        "text": "My data is in the following form:\n\n|*text*|*text\\_score*|*group\\_label*|\n|:-|:-|:-|\n|*Hello World!*|0.5|A|\n|Hi Tom|0.6|B|\n|....|....|....|\n|Goodbye.|0.1|A|\n\n*text\\_score* is a continuous variable that lies in the range \\[0,1\\] which is computed from the *text* field. All of the entries is divided between 2 groups : Group A &amp; B.\n\n1. What hypothesis test should I be using to discern if the difference in mean *text\\_score* b/w the two groups is significant?\n2. Which test to use for more than 2 groups?",
        "created_utc": 1673773722,
        "upvote_ratio": 1.0
    },
    {
        "title": "one categorical variable and a binary outcome test for significance?",
        "author": "thevow14",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bwgwv/one_categorical_variable_and_a_binary_outcome/",
        "text": "I have been asked to find out if the difference between frequencies/counts is significant but not sure which test would let me find out. I thought it would be chi-squared but there is only one category - group 1. Then I thought maybe I need to calculate the test statistic and find the critical value from a chi-squared distribution table with 1 degree of freedom, and at a significance level of 0.05. Is this the way to go about it?\n\ne.g. \n\nGroup 1    \n\nPass  4587\n\nFail 13589",
        "created_utc": 1673723175,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do people make money with statistics? I have heard how people use statistics in stock markets or something to make money? How does that work? or something like spread betting in forex? Where can i read more about these things?",
        "author": "Suspicious-Tea-6914",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bv3xh/how_do_people_make_money_with_statistics_i_have/",
        "text": "I have heard how people use statistics in stock markets or something to make money? How does that work? or something like **spread betting** in forex? Where can i read more about these things?",
        "created_utc": 1673719933,
        "upvote_ratio": 1.0
    },
    {
        "title": "EFA or CFA?",
        "author": "TodeSCurls",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bucai/efa_or_cfa/",
        "text": "Hello all,\n\n&amp;#x200B;\n\nFor my master's thesis, I am investigating the adoption factors of technologies in sports. \n\nFor this, I have developed a research model that partially extends a widely used theoretical model (study 1).  \n\nThe extension is based on an EFA done in another study (study 2). \n\n&amp;#x200B;\n\nI have now added the items of the EFA (study 2) to the factors of the widely used theory (in the respective constructs of study 1). \n\n&amp;#x200B;\n\nDo I, therefore, have to calculate an exploratory factor analysis or can I directly figure a confirmatory factor analysis (CFA)? I´m using AMOS for SEM.",
        "created_utc": 1673718059,
        "upvote_ratio": 1.0
    },
    {
        "title": "In simple linear regression, there is only one degree of freedom for the explained SS. Does that mean that choosing a single data-point places a constraint on the rest (n-1)?",
        "author": "praxe0n",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bsgkj/in_simple_linear_regression_there_is_only_one/",
        "text": "For SLR, the SST has n-1 degrees of freedom, which makes sense because you are estimating the mean. For SSE, it has n-2 degrees of freedom due to both the slope and the mean (to calculate the intercept).\n\nTherefore, because SST = SSR + SSE,\n\ndf(SSR) = df(SST) - df(SSE) = (n-1) - (n-2) = 1\n\nThat makes sense to me, but my question stems from what I understand about degrees of freedom; that is (if I understand it correctly), the number of data-points that are free to vary such that the value of the statistic remains unchanged.\n\nSo is it true then, that given a value for a single data-point, all other values for the rest can be determined just by knowing the parameters?\n\nIf we had for example, n=1000, it seems counter-intuitive to me that picking one single value will determine the rest of the 999.",
        "created_utc": 1673713462,
        "upvote_ratio": 1.0
    },
    {
        "title": "What kind of missing data do I have?",
        "author": "KennyBassett",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bri4z/what_kind_of_missing_data_do_i_have/",
        "text": "In my current project, I am collecting data on certain entities from multiple websites across the internet. Some websites provide their own data for as many entities as they possibly can, and some only provide data on something like a \"top ten\" subset of entities. Let's call these type A and type B websites or data sources.\n\nIf a data source of type A is missing data on an entity, how could I classify that missing data (MNAR, MAR, MCAR) and why?\n\nWhat about if an entity is missing from type B's limited subset?",
        "created_utc": 1673711099,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you please ELI5 why Beta is negative on this simple example",
        "author": "Rokets",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10br6mh/can_you_please_eli5_why_beta_is_negative_on_this/",
        "text": "I don't understand why **Beta** for this example, at least according to my calculations **is negative -2.38269184**.\n\nI was using the SLOPE function on Google Sheets. I also double-checked with COVARIANCE.S/VAR.S.\n\nBenchmark: 5.70%, 4.83%, 3.21%, 1.41%, 0.70%, 0.73%\nStock: 25.37%, 5.55%, -7.45%, 1.64%, 17.56%, 49.03%\n\nI find it weird because each day benchmark rises, the stock also has a positive uptrend.  Only on day 3, the trend was reversed between the stock and the benchmark.\n\n*Please note that I'm a newbie and just found out about Beta a few days ago.*\n\nThank you in advance.",
        "created_utc": 1673710292,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it possible to do a one-way ANOVA on binary data?",
        "author": "NoMaybe8778",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bpqz3/is_it_possible_to_do_a_oneway_anova_on_binary_data/",
        "text": "Hi everyone! I hope someone can help enlighten me, as I to this day, am still confused if I used the right statistical analysis in a experimental project I did in medical school. \n\nWe had to place six electrodes on a subjects' forearm, and test wether there was any difference in the perceptual characteristics of electrical stimuli. So for each electrode, the subject was asked to describe the sensation of electrical stimuli as either \"blunt\" og \"sharp\". Afterwards we converted the data to binary numerals (0 for blunt, 1 for sharp) and did a one-way anova for all six electrodes. \n\nI could not help but wonder the following: \n\n* First of all, I had a very small sample size of only 13 people.\n* Second, you need to have a normal distribution of data in order to do ANOVA, but as far as I understand, it is not possible to assess the normal distribution of data when it is binary numerals? \n* Lastly, would it have been better to perhaps do a chi-squared test? Or something completely different? \n\nI look forward to hearing everyones ideas, as it to this day still haunts me 😂 My project counselor was not very helpful at all, and did not have the time to properly explain it to me.",
        "created_utc": 1673706600,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question on 2 x 3 mixed anova and independent samples t test",
        "author": "greenteaaalemonade",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10borwo/question_on_2_x_3_mixed_anova_and_independent/",
        "text": "I’m working on a project which looks at the differences in the frequency of parental talk by child gender (between subjects factor, 2 levels) and category of talk (within subjects factor, 3 levels). There is no main effect of child gender - which I interpret as no effect of child gender on overall frequency of parental talk (irregardless of category, please correct me if I’m wrong!). \n\nResults do show a significant effect of category of talk, but not a significant interaction effect between child gender and category. However, I’m interested in examining whether there are gender differences within each category of talk, would that mean I would have to conduct independent samples t test in this case? I’m confused about how to interpret the non-significant interaction effect.",
        "created_utc": 1673703899,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probability Question",
        "author": "Playful_Part4029",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10boebo/probability_question/",
        "text": "A deck of 78 cards is randomly split into three piles. One card is then pulled from each pile. There are now three cards on the table. Each card is then assigned spot 1, spot 2 or spot 3 on a board game. What is the probability that the card pulled from pile 1 ends up on spot 1.",
        "created_utc": 1673702750,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help me understand this Manhattan plot's y-axis. the y-axis values marked 5, 10 and 15 are in reference to their negative log10 of a given p-value, could someone show me what '5' on the y-axis is in a p-value?",
        "author": "laowaiH",
        "url": "https://i.redd.it/icdbx5uqc0ca1.jpg",
        "text": "",
        "created_utc": 1673701811,
        "upvote_ratio": 1.0
    },
    {
        "title": "STATISTICAL TREATMENT OF DATA",
        "author": "acne_27",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bn265/statistical_treatment_of_data/",
        "text": "Hi! I’m a Marine Biology student. I’m doing a coral assessment inside a marine protected area using the Underwater Photo Transect method and CPCe software, and i’m only using 3 transects with 50 photographs pero transect. How do I use my data for statistical analysis? I really need help with this one guys, so I hope you could give some insights.",
        "created_utc": 1673698370,
        "upvote_ratio": 1.0
    },
    {
        "title": "Wilcoxon Signed-Rank test",
        "author": "KonradasM",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bl2iq/wilcoxon_signedrank_test/",
        "text": "Hi there,\n\nhopefully you can help me out. I have conducted a pilot medical study where I was writing down specific measurements of 10 participants during three time stages (pre, during and post-treatment). It seems that **Wilcoxon Signed-Rank test** should be used to understand whether there was any effect on the treatment, however, when I check for distribution, it seems that data is rather normally distributed which is not ok as I am excluding one of the assumptions mentioned in the reference article (for reference I am looking here [https://www.statstest.com/wilcoxon-signed-rank-test/](https://www.statstest.com/wilcoxon-signed-rank-test/)). Should I remain with Signed-Rank test or choose another statistical method? Thank you very much in advance.",
        "created_utc": 1673690968,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should i use spearman or Pearson correlation?",
        "author": "Otherwise_Gas8940",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10bjcg6/should_i_use_spearman_or_pearson_correlation/",
        "text": "I wanted to measure the relationship between National Identity and Psychological Well-being, which statistical treatment should i use?",
        "created_utc": 1673684306,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probability that murder rates per capita are higher in areas where the population primarily voted for Biden?",
        "author": "DevelopmentPatient62",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10b9q49/probability_that_murder_rates_per_capita_are/",
        "text": "I'm sorry if this is the wrong place to post this. It is fundamentally about probability, but it's based on statistics, so hopefully this isn't too inappropriate.   \n\n\nI'm having a disagreement with someone, and I'm trying to figure out if I'm wrong. This person shared information indicating that the top 20 cities with the highest murder rates. The vast majority favored Biden. Could someone reasonably conclude that it's probable that murder rates are higher in areas that favor Biden across the whole US based on this information? The person then asserted (but hasn't yet given sufficient data) that the murders in these areas account for 10% of the murders in the US. Could you do it based on this additional information given it is true? Are there any equations I can learn about to think through all of this and work it out?",
        "created_utc": 1673654349,
        "upvote_ratio": 1.0
    },
    {
        "title": "Unemployment duration linear probability, probit or exponential",
        "author": "metricscupbird",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10b3rho/unemployment_duration_linear_probability_probit/",
        "text": "I am supposed to estimate the duration of the unemployment. Which one would be a better linear probability model, probit, or exponential? Why?",
        "created_utc": 1673639652,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you compute the constant when it is not indicated in the regression table?",
        "author": "Middle_Background495",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10b01vx/how_do_you_compute_the_constant_when_it_is_not/",
        "text": "I understand that the coefficient of the variable in the regression is the slope. How do I use it to find the intercept?",
        "created_utc": 1673630590,
        "upvote_ratio": 1.0
    },
    {
        "title": "Time series newbie",
        "author": "Actual_Plant_862",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10au2wl/time_series_newbie/",
        "text": "Hi all, I'm new to learning about time series and I am hoping to get some help.\n\nEffectively, I've read about multiple different types of time series modelling but I don't really understand which is applicable when.\nI think that different ones can be run as a sort of cross-validation and then you can decide the best one using standard variance.\n\nI'm trying to apply this to a self-study project and this is what my data looks like:\n6 years of quarterly data\nThere is a trend but little to no seasonality\n\nAny help would be appreciated.",
        "created_utc": 1673615133,
        "upvote_ratio": 1.0
    },
    {
        "title": "Assumptions for moderation analysis?",
        "author": "Acrobatic-Ice1411",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10amhj5/assumptions_for_moderation_analysis/",
        "text": "I am trying to do moderation analyses but don't quite understand how the assumptions for moderation analyses differ to those for multilinear regression. I.e. how the assumptions apply to the moderator variable? I'm so confused, could someone please explain? Thanks so much :)",
        "created_utc": 1673588367,
        "upvote_ratio": 1.0
    },
    {
        "title": "What can I do with 4 years worth of habit tracking data?",
        "author": "sampat97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ajilo/what_can_i_do_with_4_years_worth_of_habit/",
        "text": "I have been consistently keeping a bullet journaling habit for the past 4 years. It has a section where I track habits that I am trying to cultivate or let go. Say for example, in the month of January I am tracking my soda usage; everyday where I don't have soda I put a mark next to that day this way at the end of the month I have the number of days where I didn't have soda. \n\nAs far as the data is concerned, all i can think of is a simple visualisation to see if there are any consistent patterns of peaks or valleys across the years. Any suggestion is welcome.",
        "created_utc": 1673579518,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probability of winning a raffle with multiple winners, but a participant can only draw once.",
        "author": "ayyystunna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10adir3/probability_of_winning_a_raffle_with_multiple/",
        "text": "What is the probability of winning a raffle with multiple winners (multiple prizes)? You can purchase many entries, but you can only win one time. \n\nFor this example let’s say you have 50 entries, 1000 entries overall and 25 winners. \n\nIs there a formula I can use to estimate the probability of winning?",
        "created_utc": 1673564075,
        "upvote_ratio": 1.0
    },
    {
        "title": "Could someone please help me implement a linear mixed model using statsmodel?",
        "author": "lifelifebalance",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10acxre/could_someone_please_help_me_implement_a_linear/",
        "text": "I have been trying to implement a linear mixed model for a while now and I'm not really having much luck finding information online. The docs have limited information and examples for my specific use-case.\n\nI have this dataframe:\n\n|subject|delta|day\\_of\\_week|\n|:-|:-|:-|\n|A|30|0|\n|C|\\-20|3|\n|Y|\\-2|0|\n|S|10|4|\n|A|\\-5|1|\n\n&amp;#x200B;\n\nAnd I am trying to train a linear mixed model using the statsmodel library with this code:\n\n    lmm = smf.mixedlm(\n            \"delta ~ subject\", \n            metricsDf, \n            groups=metricsDf[\"day_of_week\"], \n            re_formula=\"~subject\"\n          ).fit()\n\nbut when I look at the random effects parameters this is the output:\n\n    {\n     0: \n        Group   -0.299498\n        dtype: float64, \n     1: Group    1.679702\n        dtype: float64, \n     2: Group   -1.500649\n        dtype: float64, \n     3: Group   -0.789772\n        dtype: float64, \n     4: Group    0.962137\n        dtype: float64, \n     6: Group   -0.05192\n        dtype: float64\n    }\n\nBut I was expecting there to be random intercepts and random slopes included, not just random intercepts. \n\nMy guess is that it has to do with \"subject\" being a categorical variable but I am having a hard time finding useful information for this library. The docs have not helped me in this case.\n\nDoes anyone have any advice for how I could get this to work ?",
        "created_utc": 1673562666,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hello guys, I need some help with 2 exercises.",
        "author": "Conscious-Ad-8948",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10abgkx/hello_guys_i_need_some_help_with_2_exercises/",
        "text": "I have some exercises, ive completed 8 out of 10 of them but i need help with given exercises: \n\nExercise 9. Consider the following data ( in cm)\t\t\t\t\t\t\t\t\t\n \t \t \t \t \t \t \t \t\t\n172\t180\t191\t167\t156\t180\t188\t170\t156\t171\n \t \t \t \t \t \t \t \t\t\n \t \t \t \t \t \t \t \t\t\n \t \t \t \t \t \t \t \t\t\nOrganize the data in a Frequency distribution and find:\t \t \t \t \t \t \t\t\t\n \t \t \t \t \t \t \t \t\t\nMean\t \t \t \t \t \t \t \t\t\nMedian\t \t \t \t \t \t \t \t\t\nVariance\t \t \t \t \t \t \t \t\t\nStand Deviation\t \t \t \t \t \t \t \t\t\nMin\t \t \t \t \t \t \t \t\t\nMax\t \t \t \t \t \t \t \t\t\n \t \t \t \t \t \t \t \t\t\nExercise 10\t \t \t \t \t \t \t \t\t\n \t \t \t \t \t \t \t \t\t\nFind the value of the correlation coefficient from the following table:\t \t \t \t \t \t\t\t\t\n \t \t \t \t \t \t \t \t\t\nSubject\tWeight\tHeight\t \t \t \t \t \t\t\n1\t55\t66\t \t \t \t \t \t\t\n2\t21\t65\t \t \t \t \t \t\t\n3\t44\t79\t \t \t \t \t \t\t\n4\t42\t28\t \t \t \t \t \t\t\n5\t57\t87\t \t \t \t \t \t\t\n6\t59\t81\t \t \t \t \t \t\t\n \t \t \t \t \t \t \t \t\t\n \t \t \t \t \t \t \t \t\t\nCodeviance(X,Y)\t \t \t \t \t \t \t \t\t\nr coefficient\t \t \t \t \t \t \t \t\t\nRepresent the data with a plot\t \t \t\n\nI can send an excel file if needed. Please help",
        "created_utc": 1673559301,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it fair to put a question on finding the 4th and second moment of the Rayleigh distribution via integration when you're not covering probability and are discussing \"convergence in distribution\"?",
        "author": "ProposalLeast3057",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10a6zuw/is_it_fair_to_put_a_question_on_finding_the_4th/",
        "text": "",
        "created_utc": 1673548704,
        "upvote_ratio": 1.0
    },
    {
        "title": "Factor analysis and t-test",
        "author": "Appropriate-Poem4728",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10a6vm7/factor_analysis_and_ttest/",
        "text": "I run factor analysis in likert data and i took 3 new scale variables with positive and negative values (save as variables in spss...). When i used one of them to run a t-test with a categorical variable (3 levels) i was obliged to compare opposite means. Which is bigger? The positive or negative one?",
        "created_utc": 1673548413,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does a statistics BS require Excel?",
        "author": "Lrenosila",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10a5t18/does_a_statistics_bs_require_excel/",
        "text": "Hi, I hope this is the right place to ask this. I have a new coworker whose resume claims they have degree in statistics from a well regarded university (top 50 in the US). I was surprised to find that they are not proficient in basic excel functions. I’m not even talking about intermediate stuff like vlookups or pivot tables. They don’t know how to use filters, highlight a column, or delete a row. Is it possible for a student to actually graduate with a statistics degree without knowledge of basic Excel?",
        "created_utc": 1673545836,
        "upvote_ratio": 1.0
    },
    {
        "title": "Ultimate guide for biostats in the R",
        "author": "bhatsofbob",
        "url": "https://youtu.be/GDrEwz8r8Xc",
        "text": "",
        "created_utc": 1673534884,
        "upvote_ratio": 1.0
    },
    {
        "title": "How are interesting statistics in cricket created ? Is this manually created by a statistician or automatically generated? Does anyone have an insight on the data models or statistical methods used by ICC ?",
        "author": "RstarPhoneix",
        "url": "/r/Cricket/comments/109ubtw/how_are_interesting_statistics_in_cricket_created/",
        "text": "",
        "created_utc": 1673511604,
        "upvote_ratio": 1.0
    },
    {
        "title": "T-test or chi-square test if only using aggregate data",
        "author": "imreadytolearn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109s97m/ttest_or_chisquare_test_if_only_using_aggregate/",
        "text": "I ran a t-test comparing the proportion of whites and minorites applying into graduate school. My data is only in aggregate so I am confused whether to do an independent t-test (since im comparing two group proportion) or a chi-test (but unsure I can do this if I only have aggregate data as seen in the picture). \n\nMy other thought was should I just subtract the proportions of whites in 2018- 2010. and then do the same thing in minorities and then compared these \"differences\" between both groups.\n\nAny insight would be appreciated",
        "created_utc": 1673504085,
        "upvote_ratio": 1.0
    },
    {
        "title": "variation in weights of bottles filled with individual pieces of known weights",
        "author": "EducatorActual4484",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109q7d4/variation_in_weights_of_bottles_filled_with/",
        "text": "OK so here's the problem: \n\nDuring vitamin bottling process, each bottle is required to be filled with at least 60 pieces.  Bottles are filled by weight, not by counting out pieces individually, so an estimate of the average piece weight is found before each run.  The target weight for filling each bottle is then set based on this average piece weight × [60 + n]  (with n being number of pieces over the minimum 60 requirement to shoot for).\n\nAssuming we had a good estimate for how much piece weights vary for any given batch of vitamins, how would you determine the lowest value of n that still allows an acceptable number of bottles underfilled?  Assuming a pretty low tolerance for underfilled bottles, like at most 1 of every 5000 bottles can be underfilled. \n\nAlso theres a weight inspection step, which has an accuracy of +/- 2% of total weight.  Number of bottles allowed to be rejected during inspection step for being underfilled is 5 for every 100 bottles gone through inspection.  \n\nFinally, assume weights of pieces at any given time is completely random.",
        "created_utc": 1673497651,
        "upvote_ratio": 1.0
    },
    {
        "title": "Super bowl squares- positive expected value?",
        "author": "Solid-Diet-794",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109kj04/super_bowl_squares_positive_expected_value/",
        "text": "My work is doing the traditional super bowl squares game that is popular for the super bowl, but instead of for just the super bowl it is for every game.  Once you pick a square, you stay there for every game, and there are new numbers for every game.  There are 100 squares.\n\nThe payout structure increases each round of the playoffs.  There are 4 rounds in the nfl playoffs: wild card round (6 games), divisional (4), championship (2), and super bowl (1).  For every quarter your numbers win, you get a payout.  I forgot exactly how much each round pays per quarter, but in the first round you get your buy in back, and it goes up from there (super bowl is around 200+ per quarter from what I remember.)\n\nWhat would be a way to calculate the expected possible value of buying a square?  Since there are 52 quarters (13x4), and you only need to hit one time to break even, I’m wondering if it has a positive expected value.  Obviously, some games your numbers will be bad, diminishing your odds for that game, but it is also possible you can get good numbers for any game.  And with 13 games, you should be able to get “good numbers” maybe a quarter of the time (3-4 games).  My instinct is telling me it’s possible that the EV is positive.\n\nSo, does anyone know or care to try to figure out a method of calculation?   Thanks.",
        "created_utc": 1673482218,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does this output for random_effects from a mixedlm model using statsmodel mean?",
        "author": "lifelifebalance",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109jntk/what_does_this_output_for_random_effects_from_a/",
        "text": "I am using this code to create a mixed model:\n\n    lmm = smf.mixedlm(\n            \"delta ~ C(subject)\",   \n            subjectDf, \n            groups=subjectDf[\"day_of_week\"], \n            re_formula=\"~C(subject)\"\n          ).fit()\n\nand I am then using \n\n    lmm.random_effects\n\nto get the random effect parameters:\n\n    {\n      0: Group            0.543412\n      C(subject)[T.2]     1.096503\n      C(subject)[T.3]     0.015433\n      C(subject)[T.4]    -1.537438\n      C(subject)[T.5]    -1.027971\n      C(subject)[T.6]    -0.821780\n      C(subject)[T.7]    -0.416807\n      C(subject)[T.8]    -0.160226\n      C(subject)[T.9]    -0.160955\n      C(subject)[T.10]    0.152614\n      dtype: float64, \n    \n      1: Group            4.312876\n      C(subject)[T.2]     7.940261\n      C(subject)[T.3]     1.610896\n      C(subject)[T.4]     2.260298\n      C(subject)[T.5]     1.864585\n      C(subject)[T.6]     1.865523\n      C(subject)[T.7]     1.376299\n      C(subject)[T.8]     0.401185\n      C(subject)[T.9]     1.126144\n      C(subject)[T.10]    0.530364\n      dtype: float64\n    }\n\nBut I am confused. I though re\\_formula=\"\\~C(subject)\" would create random slopes for the output and I'm not sure how to interpret the output of random\\_effects in relation to the random slopes. I thought that there would be one random intercept and one random slope for each member of the group but I'm not sure what I'm seeing here. Could someone please explain?\n\nI know this setup will make some people thing that I meant to use day\\_of\\_week as the fixed factor and subject as the random factor but that is not my intent.",
        "created_utc": 1673480058,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Discussion] Statistics have fascinated me for as long as I can remember. When did you first become interested in the field?",
        "author": "RandWasIndeedCorrect",
        "url": "https://imgur.com/gallery/k4rxyyS",
        "text": "",
        "created_utc": 1673472888,
        "upvote_ratio": 1.0
    },
    {
        "title": "Accounting for number of matches when looking win/loss frequencies, other than Elo-Scores?",
        "author": "banjofreak625",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109dw3d/accounting_for_number_of_matches_when_looking/",
        "text": "So I’m comparing the win/loss frequency of players and comparing that to their elo ranking. I was expecting to find that the number of game won will diminish with the [rankings](https://imgur.com/Yoa1lve), and the opposite as will. The number of losses will increase as ranks [diminish](https://imgur.com/2qGtn9L). Ranks are determined by eloscore\n\nMy plots for games won show results that are [expected](https://imgur.com/4oHOpdf), but the losses show some [peaks and valleys](https://imgur.com/ZCIWwlj)! \n\nOne thing I need to account for here that may be skewing data is the number of games played. Some players show up everyday and play multiple games, others just one day and play a handful of matches.\n\n**Question: is there a way to set up these win/loss barplots that account for the number of days arrived and/or games played with out omiting any players?**\n\nOther factors:\n\n•Players are not obligated to show up everyday; arrival is voluntary\n\n•\tPlayers can play as many games they want in a day\n\n•\tPlayers can choose their opponents, but opponents can’t deny a match with out it counting as a loss\n\nThank you in advance!",
        "created_utc": 1673466275,
        "upvote_ratio": 1.0
    },
    {
        "title": "Minimum # of LPA Indicators",
        "author": "SquidleyPasta",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109cpoo/minimum_of_lpa_indicators/",
        "text": "Hello! I am wanting to run an LPA with only 2 indicators and can't seem to find any guidance on what the minimum number of indicators is to use. I have some justification for why I don't want to do an interaction between the two variables. Aside from some statistical power issues, I specifically want to find profiles of these two variables knowing there will likely be four groups (high, high; low, low; high, low; low; high), but these are exactly the groups I'm trying to uncover to see if they exist with these variables. Any help would be greatly appreciated!",
        "created_utc": 1673463504,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics as a job",
        "author": "Slow_District_197",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109apmz/statistics_as_a_job/",
        "text": "Hi guys, I already started studying statistics and I am sure this will be helpful in my business ventures one day, but if I need to find a job relating to the major what could that be? Sorry if the question is not suitable for the subreddit and I wish you all a good afternoon!",
        "created_utc": 1673458895,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any particular book suggestions on choice modelling?",
        "author": "HunkyRanger",
        "url": "https://www.reddit.com/r/AskStatistics/comments/109a7dj/any_particular_book_suggestions_on_choice/",
        "text": " I am reading Kenneth Train, but I need some books to explain mathematical models in the book, any book suggestion on maths involved would be helpful. If any other good reads on choice modelling available?",
        "created_utc": 1673457697,
        "upvote_ratio": 1.0
    },
    {
        "title": "Unsure why academic has used one one tail critical instead of the two tail.",
        "author": "Euphoric-Iron-956",
        "url": "https://i.redd.it/cjnpcxs7mhba1.jpg",
        "text": "",
        "created_utc": 1673456789,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using prince library to do MCA and inertia values do not look correct.",
        "author": "RipeProgram",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1098vhc/using_prince_library_to_do_mca_and_inertia_values/",
        "text": " \n\n    mca_cols = df.select_dtypes(['category']).columns # instantiate MCA class mca = prince.MCA(n_components = 3)  # get principal components mca = mca.fit(df[mca_cols]) mca.explained_inertia_ mca.total_inertia_ \n\nI am attempting to do  MCA with intention to feed the components into a clustering algorithm.  However, when I look at the total\\_inertia\\_ I get a value of appx 70 and  the explained inertia values for the 3 components are as follows:\n\n\\[0.004080415064925553, 0.0035643891792985346, 0.003260362150059927\\]\n\nThe  total inertia seems higher than what I've seen in examples for this  library and the explained inertia values seem much lower. Can someone  please explain what the reason might be and how I should interpret the  total inertia value?",
        "created_utc": 1673454523,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for a math tutor (with a focus on Statistics)",
        "author": "brokenTamberine",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1098j7v/looking_for_a_math_tutor_with_a_focus_on/",
        "text": "Hi, I'm currently looking for a tutor for studying mathematics (with a focus on statistics).\n\nMy background:\n\n\\- majored in compsci and physics in undergrad. Took math courses that were required for compsci and physics, but no real courses for the math major. Graduated 5 years ago, and currently work as a software engineer, so forgot a lot of stuff.\n\n\\- good mathematical maturity (have no problem reading advanced textbooks by myself)\n\n\\- trying to learn math for a possible career change / side projects\n\n\\- having tried to learn by myself, failed to be consistent in reading and solving problems (realized that I needed some external motivation / pressure to stay on track).\n\nI'll be focused on going through common grad school topics on statistics (plus some undergrad material for review). It would be wonderful if I can find someone that recently finished (or are working on) their phd in math (or statistics) that can take some time to skim ahead in a textbook, provide me solid deadlines every week in reading and pick out important problems to solve from the textbook. Maybe meet once a week to check in on my progress and possibly ask questions if I have any. The person wouldn't need to give me lectures, since I'll just be reading textbooks. I'm starting off by reading Statistical Inference by Casella &amp; Berger.\n\nIf this sounds interesting to you, please let me know! I'm happy to negotiate a price.",
        "created_utc": 1673453683,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sample Test interpretation.",
        "author": "DunsparceLover69",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10987vu/sample_test_interpretation/",
        "text": "Hello, for my psychology statistics I'm required to be able to comment on the findings of my study. I understand what each statistic represents and means for each group for the fist image, but the second image im not sure how to start. \n\n&amp;#x200B;\n\nCould anyone explain the meaning behind these stats and how they apply to each other\n\nhttps://preview.redd.it/7mmirazvsfba1.png?width=650&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f10bec5edf52cfc9290117b6657d294d725d1f2c\n\n&amp;#x200B;\n\nhttps://preview.redd.it/i4h2fgulsfba1.png?width=1376&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9ba6a576a6b6dd9881cc9048220231aa0bf744d8",
        "created_utc": 1673452899,
        "upvote_ratio": 1.0
    },
    {
        "title": "What am I doing wrong with this Chi square?",
        "author": "Greedy-Resident3596",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10979oa/what_am_i_doing_wrong_with_this_chi_square/",
        "text": "I’m practicing Chi squares and I’m trying to duplicate the results found in a research article:\n\n”Older adults (65+ years) were less likely to have confidence (χ2 (1) = 4.553, p = 0.033) (&lt;65 years, 56%, n = 177; 65+ years, 41%, n = 29). “\n\n  \nI did a chi square:\n\nO         E.         D.          (O - E)2.          (O - E)2/E  \n65+  29.       103.      74.        5476.               53.165\n\n&lt;65  177.     103.      -74.       5476.                53.165\n\ntotal: 206.                                                         106.33\n\n&amp;#x200B;\n\nWhy am I getting 106.33 here and how are they getting to 4.553? I feel like I must be making a very obvious mistake here but I cannot find it. Can someone point me in the right direction? I would really appreciate it!",
        "created_utc": 1673450518,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about regression",
        "author": "yesuknonoukno123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1095vkk/question_about_regression/",
        "text": "\nSome questions about regression:\n\nIn a simple linear regression:\n\n - can the dependent variable be a categorical variable (after converting to dummy)?\n- can the independent variable be a categorical variable (after converting to dummy)?\n\n\nIn a multiple linear regression:\n\n - can the dependent variable be a categorical variable (after converting to dummy)?\n- can the independent variables be a categorical variable (after converting to dummy)?\n\n\nAlso, why so we have to convert categorical variables into binary columns, and not simply convert a category with 6 unique values into 1-6?",
        "created_utc": 1673446868,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confidence of receiving all results from from a limited random sample",
        "author": "eglish",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1092x1v/confidence_of_receiving_all_results_from_from_a/",
        "text": "Hi everyone!, I'm new to this sub.  I am working on a little personal data work and I'm looking to  calculate the probability that I have all results from a randomized, limited search result size.  I imagine this sub would be able to guide me.\n\nI have a search query that will return up to 500 random results from a pool of unknown size.  Running the search multiple times will continue to be random and I have no reason to believe results would change as a result of a previous last search.  In the search, there is a unique ID, so I can very easily identify duplicate records.\n\nI would expect over enough search attempts, I can find new/duplicate records and I should have diminishing new results over time.  So I would also expect my confidence of having all records to eventually approach 100%, but never hit 100%.\n\nIs there a way I can calculate this confidence level about the likelihood that I've received/discovered all results?\n\nMy implementing application is Excel and Power Query (M...DAX), if that makes any difference or code can be written in such a manner.",
        "created_utc": 1673438096,
        "upvote_ratio": 1.0
    },
    {
        "title": "Linear mixed model warning",
        "author": "Alicia-Emily",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1092aq5/linear_mixed_model_warning/",
        "text": "  \n\nHi,\n\nI'm sorry for asking a question about linear mixed models for the second time this week (different from the other one though).\n\nThe design is as follows: I have data from both partners of heterosexual couples and some individual cases (not all partners participated). I assigned everyone a couple number. I'm predicting a continuous DV from interval and categorial predictors. I specified a linear mixed model in SPSS 27 with my predictors as fixed factors (categorial as factors, interval as covariates) and couple number as a random effect.\n\nHowever, I get a warning that says: \"The final Hessian matrix is not positive definite although all convergence criteria are satisfied. The MIXED procedure continues despite this warning. Validity of subsequent results cannot be ascertained.\" It seems to have something to do with one of my fixed factors (covariate), because when I ran the analysis without it, the warning disappeared. The variable is not an ideal predictor (low variability), but I really need to keep it in my model, because it is an important one for my hypotheses.\n\nAny suggestions on what to do?",
        "created_utc": 1673435833,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to interpret a negative result for gender?",
        "author": "BrainFood98",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108q879/how_to_interpret_a_negative_result_for_gender/",
        "text": "There's multiple variables but in just confused with gender here.\n\nLogistic regression looking at likelihood of survival in a natural disaster.\n\nMales = 0, Females = 1\n\nResult:\n\nB =   - .740\n\nExp(B) =   .520\n\nHow do I interpreted that?\n\nI understand how to do it for age. But with gender just being male or female, I'm a little confused.\n\nThanks!",
        "created_utc": 1673397024,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bootstrap",
        "author": "Sheeta_27",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108odph/bootstrap/",
        "text": "When I'm using a bootstrap methodology to estimate a parameter, in the context of a simulation study : \n\nShould I favour doing a lots of bootstrap but each one have a few resamples? Or should I favour doing few bootstrap but each one have a lots of resamples?",
        "created_utc": 1673392555,
        "upvote_ratio": 1.0
    }
]