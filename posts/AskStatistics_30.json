[
    {
        "title": "Beginner Student Prepping for Exam (Can't understand Reasoning behind Answer (p value / Z test)",
        "author": "klmer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8w28el/beginner_student_prepping_for_exam_cant/",
        "text": "The question is true or false....\n\nFrom a large population with µ = 10 and σ = 7 we take a random sample with N = 49. The sample mean is equal to 12. The probability of finding such a value for the sample mean (12 or more ) is equal to  .3859\n\nI tried using z= (sample mean - population mean) / standard error, then plugging that into a z table... but I got 0.228. \n\nApparently the answer is suppose to be True, and not 0.0228. Why?\n\nThe answer sheet said: \"If the null hypothesis is true, than the probability of finding sample means as extreme or more compared to the observed sample mean is equal to the p-value\" \n\nCan anyone explain this, like.... I'm resitting my exam in a week and trying to figure this out. ",
        "created_utc": 1530716191,
        "upvote_ratio": ""
    },
    {
        "title": "Finding Factor Correlations in SPSS",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8w194c/finding_factor_correlations_in_spss/",
        "text": "I'm developing a scale. After using an exploratory factor analysis, the 21 items loaded on 5 factors.  I used principal component extraction and varimax rotation.  I'm trying to figure out how to get the correlations of these 5 factors. I've tried ticking all the boxes in the Correlation matrix box, but can't seem to get it to look how it does online e.g. ([https://www.google.co.za/search?q=factor+correlations&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwiB0tjhtoXcAhWsAsAKHeyiCgUQ\\_AUICigB&amp;biw=1366&amp;bih=635#imgrc=iUTdcNrBG\\_SUAM:](https://www.google.co.za/search?q=factor+correlations&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwiB0tjhtoXcAhWsAsAKHeyiCgUQ_AUICigB&amp;biw=1366&amp;bih=635#imgrc=iUTdcNrBG_SUAM:)) \n\nIt also looks as if viewing factor correlations is only part of principal axis factoring? Am I mistaken?\n\nI apologize If I am not making any sense, stats is not my thing. ",
        "created_utc": 1530707139,
        "upvote_ratio": ""
    },
    {
        "title": "Suitable forecasting method",
        "author": "Snaitas1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vzz9p/suitable_forecasting_method/",
        "text": "Hi everybody,\n\nI have to make a forecast based on this particular set of data that ranges from 1995 to 2017 (23 data samples). The forecast if for a 3 year period (2018-2020)\n\n[https://imgur.com/a/AyqjHIX](https://imgur.com/a/AyqjHIX)\n\nAt first I have used the expert modeler tool in SPSS which lead to the use of a Brown model. This lead to a mean average of error of around 2&amp;#37; and a R squared of about 96&amp;#37;. The Ljung box test was favorable.\n\nIs there any better approach to this particular set of data?\n\nThanks in advance.",
        "created_utc": 1530691718,
        "upvote_ratio": ""
    },
    {
        "title": "Suitable forecasting methods",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vztm3/suitable_forecasting_methods/",
        "text": "[deleted]",
        "created_utc": 1530689850,
        "upvote_ratio": ""
    },
    {
        "title": "How to interpret a standard deviation greater than the mean?",
        "author": "hectorfhr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vz28m/how_to_interpret_a_standard_deviation_greater/",
        "text": "If the mean time spent by students in extracurricular activities is 0.1 hours per week and the standard deviation is 1.6, the range would be from -1.5 to 1.7 hours per week. So, how do you interpret spending -1.5 hours in extracurricular activities? How can a standard deviation be greater than the mean? Thanks",
        "created_utc": 1530681355,
        "upvote_ratio": ""
    },
    {
        "title": "The value of this z score does not makes sense to me. Please help me understand the problem.",
        "author": "ayayron0408",
        "url": "https://imgur.com/iObkuqZ",
        "text": "",
        "created_utc": 1530652169,
        "upvote_ratio": ""
    },
    {
        "title": "Simple Vocabulary Question",
        "author": "Juch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vuptp/simple_vocabulary_question/",
        "text": "Hello all,\n\nI'm trying to figure out how to qualify some data points. I have volumes recorded at different times (mL). This data can be displayed directly, or compared to a mass (mL/kg), or compared to mass over a certain time (mL/kg/hr). How would you refer to each of the ways to display this data? \"Normalized by weight\" and \"normalized by weight and time\" are the only suggestions I've heard so far, but those seem overly complicated. This feels simpler to me but I don't know what words to use.\n\nThanks.",
        "created_utc": 1530643457,
        "upvote_ratio": ""
    },
    {
        "title": "Binomial variance of a football season.",
        "author": "ahhhhhhhhyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vug5k/binomial_variance_of_a_football_season/",
        "text": "Recently encountered [this article](http://archive.advancedfootballanalytics.com/2010/11/randomness-of-win-loss-records.html), which contains the following tidbit about a football season:\n\n&gt;The binomial variance over 16 trials (games) is always 0.125^2. \n\nHowever, when I calculate binomial variance based on sig^2 = np(1-p), with p = .5 and n = 16, I get sig^2 = 4. Working backwards, I was able to solve for a p that yields a variance of .125^2 if p = 0.999. I guess my question is, **Why does the author use .999 instead of .5 when talking a sample due entirely to randomness?**",
        "created_utc": 1530641461,
        "upvote_ratio": ""
    },
    {
        "title": "cross classified model (lmer function)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vu36e/cross_classified_model_lmer_function/",
        "text": "[deleted]",
        "created_utc": 1530638940,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing means of data that are only &gt;0 (T-test?)",
        "author": "Biotechjones",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vtwer/comparing_means_of_data_that_are_only_0_ttest/",
        "text": "Hello biologist here.\n\nI am generating data for bacterial counts. By definition my data cannot be less than zero. In the past I have compared the averages of my bacterial counts by a T-test.  \n\nNow I am concerned that I am not using the appropriate stats test. How would you best compare data that is only &gt;0? ",
        "created_utc": 1530637561,
        "upvote_ratio": ""
    },
    {
        "title": "Multilevel Analysis -Using gender as a Covariate",
        "author": "LUnknowing",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vt3u7/multilevel_analysis_using_gender_as_a_covariate/",
        "text": "I\\`ve got a question about multilevel analysis for longitidinal data.\n\nI\\`m using mixed models in SPSS and i\\`m interested in the cross-level-interaction between condition (Level 2) and time (level1), to explore, if the treatment has worked . There are three measurments so i used dummy-coding to only compare T1 against T2 and T1 against T3. Now I recognized, that there are differences in my sample in terms of gender between experimental and control group, which influences my results. So my question is: How can i put in Gender as a simple covariate in the multilevel analysis? I just want to partial out the influence on the other results!\n\nI\\`m thankfull for every advice!",
        "created_utc": 1530631784,
        "upvote_ratio": ""
    },
    {
        "title": "Brushing up on simple probability calculations.",
        "author": "ahhhhhhhhyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vsckk/brushing_up_on_simple_probability_calculations/",
        "text": "It's been a while since undergrad, and unfortunately I've forgotten a great deal of basic statistics. One such problem that I'm struggling with is calculating the probability of a certain outcome of coin flips given N total flips.\n\nThe particular way to solve it I am interested in does not rely on the binomial theorem, but instead uses standard deviation and I think maybe Z-scores. \n\nProblem (as best as I can remember):\n\n**Probability of getting exactly 5 heads out of 10 flips.**\n\nFrom what I can recall, you calculate the fraction of successful events over the total (.5), subtract this from the mean (which is also .5) and divide by the standard deviation to get a Z-score. The things I'm having trouble with:\n\nWon't this value be zero, and if not, how do we go about getting the standard deviation of a coin toss for N tries?",
        "created_utc": 1530625830,
        "upvote_ratio": ""
    },
    {
        "title": "Using statistics to determine required number of quality control tests per sample size (in order to catch defects with a certain probability)?",
        "author": "CalculusIsEZ",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vqxg5/using_statistics_to_determine_required_number_of/",
        "text": "I am working on a project of which the requirement for a certain process is to ensure that a maximum amount of defective products are caught by a QC tasting procedure (in the production line).\nIt is not feasible (with respect to a limiting product cycle time) to test every product.\n\nI want to use a statistical analysis to determine the frequency with which I need to QC test products in the production line, if I want a (for example) 98% certainty that I have caught all defects with the test.\n\nFor instance: I perform a statistical analysis and conclude that, to reach a 98% certainty that I have caught all defects in a given product batch, I need to tests at least 1 in every 4 products.\n\nMy question:\nWhat type of analysis fits my descriptions above? What data would I need to perform this analysis?\n(I took only an introductory course in statistics during university).",
        "created_utc": 1530611246,
        "upvote_ratio": ""
    },
    {
        "title": "Logistic regression odds ratios comparison?",
        "author": "positiivikko",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vquyk/logistic_regression_odds_ratios_comparison/",
        "text": "If I have a logistic regression model with several independent variables that are all significant at the .05 level, what can I say about their contribution to the model? I have understood that odds ratios are effect size measurements but I'm uncertain of how to interpret them as such. Do larger odds ratios indicate a greater effect on the dependent variable? Can you compare the odds ratios between different independent variables? For example, if I have two categorical variables, education and gender, and the odds ratios for the education categories vary between 1.5 and 3 (for example education level 2 has an odds ratio of 1.5 with the reference group, level 3 has an odds ratio of 2, level 4 an odds ratio of 3), and the odds ratios for the gender groups vary between 5-7, are the larger odds ratios for gender any indication that gender would be a 'stronger' predictor in the model? Thanks in advance!",
        "created_utc": 1530610355,
        "upvote_ratio": ""
    },
    {
        "title": "What measure(s) can I use to show that one strategy has a percent likelihood of gaining fixation against another strategy that exceeds the initial frequency of the strategy? (Details are in text.)",
        "author": "idster",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vpm45/what_measures_can_i_use_to_show_that_one_strategy/",
        "text": "I'm trying to show that one simulated evolutionary strategy (strategy 1), when practiced against another simulated strategy (strategy 0), has a percent likelihood of gaining fixation (that is, of eventually becoming practiced by every individual in the population) that exceeds the initial frequency of the strategy.  In other words, if strategy 1 starts at 10% of the population, then it has a greater than 10% likelihood of gaining fixation.  What is a good way of determining this?  I can run the two strategies against each other tens of thousands of times, varying the initial frequency.  I can also run strategy 0 against a strategy which is the same as strategy 0 but named differently (in order to determine a difference due to randomness) and I can do this tens of thousands of times, varying the initial frequency, as well.",
        "created_utc": 1530595089,
        "upvote_ratio": ""
    },
    {
        "title": "Is this the correct null and alternative hypothesis (and explanation) for this question?",
        "author": "NoahPM",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vpig8/is_this_the_correct_null_and_alternative/",
        "text": "&gt;Diet Guide magazine claims that juice fasting for a week is an excellent way to lose weight.  An article states that you can lose an average of 12 pounds per week on a juice fast. The Atkins diet group is very skeptical of the claim and wants to prove it is actually less than 12 pounds, and selects an SRS of 20 individuals who juice fast for a week.  The sample of 20 individuals has a mean weight loss of 10.3 pounds, with a standard deviation of 4.8 pounds. It is known that weight loss follows a normal distribution  \n&gt;  \n&gt;State the appropriate null and alternative hypotheses.  \n&gt;  \n&gt;H0: u = 12  \n&gt;  \n&gt;H1: u &lt; 12  \n&gt;  \n&gt;The researches wish to prove the that the mean weight loss per week on a juice fast is less than 12 lbs.  To do this they must reject the claim that the mean is 12 lbs, in order to prove their claim that it is less than 12 lbs.\n\nI'm asked to explain how I came to my conclusion as well.  Null and alternatives aren't quite that intuitive to me yet to be confident in my explanation.  Could I mention that since it is a 'less than' claim the alternative must be the claim?  I don't know what exactly she's looking for, I'm just trying to be as thorough as possible.",
        "created_utc": 1530594028,
        "upvote_ratio": ""
    },
    {
        "title": "Thesis- Compositional Analysis in R?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vp370/thesis_compositional_analysis_in_r/",
        "text": "[deleted]",
        "created_utc": 1530589752,
        "upvote_ratio": ""
    },
    {
        "title": "Question about combining probabilies",
        "author": "PhitPhil",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vog3y/question_about_combining_probabilies/",
        "text": "Hey there statisticians!\n\nSo I'm working with some natural language processing tools (TextBlob for anyone out there who cares). One of the nice things about TextBlob is that when working with spelling corrections, if a word is misspelled, you can ask for the 3 most likely words and the probability of that the suggestion is the correct word you were looking for. I have a phrase, like `Vitamn k defcincy` where vitamin and deficiency are both misspelled. When I ask each of these for a spelling suggestion, `vitamn` is returned with `vitamin` with a probability of 1. k also gets a probability of 1, but defcincy get the probabilities of `deficiency' with a probability of .71\n\nThere are other suggestions, but for the task I'm trying to complete, I have a way set up to deal with the other suggestions, and thus aren't an issue at the moment. However, I'm still trying to give the probability that the spelling is what is suggested. \n\nSo in a nutshell, I'm trying to figure out a way to properly combine the probabilities of 1, 1, and 0.71 without having access to much more data. \n\nDoes anyone have a suggestion on how to properly combine these probabilities to an overall probability for the phrase? \n\nThank you!",
        "created_utc": 1530583630,
        "upvote_ratio": ""
    },
    {
        "title": "Useless and unimaginative student stuck coming up with a topic for my Master's \"thesis\"",
        "author": "nobr41n",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vnwbd/useless_and_unimaginative_student_stuck_coming_up/",
        "text": "So this is not the typical theoretical question of how ANOVA or something works, but I hope it's still acceptable for this sub (or should I try over at /r/statistics instead?)\n\nFirst of all, I write \"thesis\" because it's only a 15 ECTS credit project, i.e. half a semester full time, rather than a full semester like most other theses (at least where I'm from?) - which may be relevant in terms of what's realistic to achieve within the time frame.\n\nI'm genuinely feeling like giving up and just look for a job with my Bachelor since I have zero imagination or actual interest/curiosity to come up with a decent subject (Hell, I essentially just did this one year Master's degree because I was too apathetic to look for a job after finishing my Bachelor and just continuing with my University's Master's programme seemed like a convenient way to get that out of the way for another year). Are there any websites or resources out there with suggestions of theses/projects, or perhaps you personally have a good idea? Figured I have nothing to lose by throwing out the question hope to get lucky.\n\nThanks in advance",
        "created_utc": 1530578986,
        "upvote_ratio": ""
    },
    {
        "title": "Unsure of Whether to Take the GRE Math Subject Test",
        "author": "Thejurok",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vns6i/unsure_of_whether_to_take_the_gre_math_subject/",
        "text": "I'm currently looking into statistics masters programs. One program that I think looks great is University of Washington's, however there is a problem. Their website claims applicants \"are strongly encouraged but not required to take the GRE Mathematics Subject Test\". From this, it sounds like the subject test is more or less necessary, but I'm having doubts about taking it for a few reasons:\n\n1. I'm a computer science major, with a math minor. It sounds like a lot of the test is calc and linear algebra, which I got A's in, but it's been ~2 years since doing any calc. The test also covers certain topics I have not taken a class on (namely Diff Eq and Analysis).\n2. UW seems to be a pretty competitive program, and I feel something short of a pretty good score could actually *hurt* my chances of acceptance.\n3. It's practically the only masters program I've seen that *does* recommend the math subject test. It seems like studying for this would be a lot of work for one school, and there's always the possibility of not even getting in.\n\nI feel very confident in getting ~90% percentile on the quant section of the GRE, but the subject test is something else. I'm not looking to dodge hard work, and I'm willing to put in the time for studying for this, but I'm trying to be realistic about some of the problems listed above. Any advice would be appreciated. Thanks!",
        "created_utc": 1530578069,
        "upvote_ratio": ""
    },
    {
        "title": "Handling drop-outs in a repeated measures analysis",
        "author": "Liz_Lemon-PhD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vlr02/handling_dropouts_in_a_repeated_measures_analysis/",
        "text": "Need some help folks...I am running a within-subject study that requires 6 visits, and I have a total sample size of 17. Two of the 17 subjects completed 3 out of 6 visits. I believe that we should use the data from those two subjects and analyze with proc mixed. One of my advisors is of the opinion that we should not use those subjects' data because it is contrary to the whole repeated measures design and the resulting analyses would be difficult to explain in a manuscript. I want your opinions--is there a statistical (or other) reason that we should not use the data from those two non-completers? Thanks.",
        "created_utc": 1530561663,
        "upvote_ratio": ""
    },
    {
        "title": "Checking normality for paired data in R: A basic question.",
        "author": "Brittster0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vld0g/checking_normality_for_paired_data_in_r_a_basic/",
        "text": "Hi all,\n\nI am working with paired data.  Currently, I am checking the normality before running t-tests (or an alternative if needed). My understanding is that normality *of the differences* is of interest when making a decision about the appropriate test for  paired data. \n\nI have viewed the data using plots and histograms but normality is difficult to assess visually due to a small sample size, so my advisor has asked me to run the Shapiro Wilks tests. My data is in 'wide format', so when writing the code I just want to compare two columns in my data frame.  I am running the test on the values after subtracting one condition's values from another. Here is an example of the code:\n\n shapiro.test(variableLabvisit2$indata - variableLabvisit1$indata)\n\nI just want to check in with those of you are are more experienced: Is this the appropriate way to run a Shapiro Wilks test on the normality of the differences? Does this seems appropriate? Any suggestions on alternatives?\n\nJust a note:  After assessing normality, I will also be assessing homogeneity of variance using LeveneTest()\n\nThank you in advance!",
        "created_utc": 1530558702,
        "upvote_ratio": ""
    },
    {
        "title": "Two Bernoulli distribution - hypothesis-testing",
        "author": "oren_a",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vhg1t/two_bernoulli_distribution_hypothesistesting/",
        "text": " I'm simplifying a research question that I have at work. Assuming I have 2 coins each with a different probability of head, let's call heads a success (p)  \n. Those are biased coins each with a different p, and I do not know the probability of success of each coin, but I do got a sample:\n\n    Coin 1 - 4 head 3 tails Coin 2 - 3 head 1 tails \n\nNow I will like to reject an hypothesis that - coin 1 p= 0.3 and coin 2 p = 0.5  \n, how can I do that?\n\nOr/And I will also like to not reject a different hypothesis (state that it is not very very unlikely that this sample came from this hypotheses) - coin 1 p=0.7 and coin 2 p=0.7  \n (Is it the same calculation just depend on the p value?).\n\nAlso if I am in the same situation with more than 2 coins (5 coins with different size of sample for each coin)\n\nThank you.",
        "created_utc": 1530525431,
        "upvote_ratio": ""
    },
    {
        "title": "How could I improve my model that predicts white wine quality?",
        "author": "quant_king",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vfbdb/how_could_i_improve_my_model_that_predicts_white/",
        "text": "Hey all! I wanted to share with you all the work I have been doing to create a simple and understandable model that classifies white wine as low quality or not. I'd really appreciate your assistance in looking for areas where it could be improved without sacrificing on interpretabilty. Moreover, I don't want to depart from logistic regression. Check out [the whitepaper / explanation here](https://github.com/pmaji/stats-and-modeling/blob/master/classification/logit/logistic_regression.md). \n\n  \nIt's part of some self-teaching I've been doing of all things statistical modeling, and I would really appreciate your thoughts, feedback, and constructive criticism! If you have any questions or feedback, please leave them via the Issues tab. ",
        "created_utc": 1530500068,
        "upvote_ratio": ""
    },
    {
        "title": "Stock Market Money Question",
        "author": "-Spaids-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vfa9z/stock_market_money_question/",
        "text": "I'm creating a theoretical statistic where I make 100 trades with $2,000, risking $200 a trade. I have a 12&amp;#37; potential upside and an 8&amp;#37; potential downside with a 55&amp;#37; win rate. What is the statistical chance of me making money (Prob &gt; 2000) and probability of me losing money (Prob &lt;2000). It would be great if it could be graphed too, showing the chances of attaining each dollar amount.\n\nI made an excel sheet that randomly projects this too. [https://docs.google.com/spreadsheets/d/1rSooCSxqM4KV2hFxBdNy9xQ1TY7UidBT3scFJvbtW5A/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1rSooCSxqM4KV2hFxBdNy9xQ1TY7UidBT3scFJvbtW5A/edit?usp=sharing)",
        "created_utc": 1530499779,
        "upvote_ratio": ""
    },
    {
        "title": "[SPSS: InterRater Reliability] Computing Kappa, no measures of association are computed, best way to report this?",
        "author": "Chocobuny",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vf0cx/spss_interrater_reliability_computing_kappa_no/",
        "text": "Hey, so I'm looking at the IRR of a study I did where two raters rated 12 cases across 11 items. On two of the items both raters scored all cases 0, which causes an error when calculating Kappa in SPSS:  \n\"No measures of association are computed for the crosstabulation of x1 * x2. At least one variable in each 2-way table upon which measures of association are computed is a constant.\"  \nI understand the problem here, however the rating of \"0\" is still a decision by the rater, so the two raters giving the all cases a \"0\" on two of the items still, in my mind, shows great inter-rater reliability. I am wondering what is the best way to report this in my write-up? I considered just omitting the scores in the table and writing a small note underneath, but I'm wondering if there is a better way to do it.",
        "created_utc": 1530497083,
        "upvote_ratio": ""
    },
    {
        "title": "Hello im taking a basic statistics class and need some help with two questions",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/statistics/comments/8vdlry/hello_im_taking_a_basic_statistics_class_and_need/",
        "text": "[deleted]",
        "created_utc": 1530493066,
        "upvote_ratio": ""
    },
    {
        "title": "Bernoulli Numbers: What are they? How can I calculate them?",
        "author": "Darth_Marrr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ve7lo/bernoulli_numbers_what_are_they_how_can_i/",
        "text": "I found a succinct proof for finding the variance of a logistic distribution making use of the first and second moment's where the moments are simplified by utilizing the quantile function for the logistic distribution. Subsequently, after a little manipulation, the final n\\^{th} moment is an elegant identity that makes use of the absolute value of the Bernoulli numbers. I believe I can make use of this on my up coming comprehensive exam, but I may need to explain what those Bernoulli numbers are, however I am having a tough time distilling the definition(s) down to a graspable level. \n\nCould anyone explain them to me please?",
        "created_utc": 1530489432,
        "upvote_ratio": ""
    },
    {
        "title": "Why does SPSS exclude one independent variable after applying interaction?",
        "author": "RuffRyderss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vdzwa/why_does_spss_exclude_one_independent_variable/",
        "text": "SPSS  excludes one independent variable after applying interaction in linear  regression. Is this because of multicollinearity or is there something  wrong in my model/dataset?\n\nMy model is DV = B0 + B1Ratio + B2Dummy\\_NewLaw + B3Ratio\\*Dummy\\_NewLaw.\n\nBasically  I measure the impact of Ratio after the implementation of a new law,  before and after effect (this is done by the interaction B3). The dummy  (B2) is equal to 1 if the ratio is after the new law. My data looks like  the image in attachment. \n\nSPSS  keeps trowing out the variable Ratio (B1) from the model. Individually I  can do a regression on the DV, but not all together. Whats the reason  behind this and how can I solve this?\n\nhttps://i.redd.it/jrjxbcja7f711.png",
        "created_utc": 1530487400,
        "upvote_ratio": ""
    },
    {
        "title": "Need help calculating probability distribution of a time-series forecast",
        "author": "mamessner",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vdmdf/need_help_calculating_probability_distribution_of/",
        "text": "Hi everyone - \n\nI need to forecast revenue for the next six months based on monthly revenue data from the past 5 years. I did a time series analysis based on this video series: ([https://youtu.be/gHdYEZA50KE](https://youtu.be/gHdYEZA50KE)).\n\nReally happy with the results, and it matches the historical data and seasonality of this revenue (this is one product, really) pretty closely.\n\nSo I have the next 6 months forecast, but I'd like to build a little normal error distribution around each future month's forecast for 2 reasons: 1 - Would like to present a low/medium/high value for each month, and 2 - I want to give my team a revenue target that they are 75&amp;#37; likely to hit.  \n\n\nI imagine that by using the residual noise, I can find the standard deviation of the difference between the actual values and the predicted values and look for the 25th &amp;#37;ile revenue value for the first predicted month. But doesn't the prediction get less and less accurate as you go into the future? is there a way to generate a normal distribution of predicted revenue for each point in the forecast, rather than just one predicted point? Can I multiply the probabilities somehow or something?  \n\n\nHere is my spreadsheet with everything calculated: \n\n[https://drive.google.com/file/d/1EbJRxaGkwldh7N3W9kGI9os7oDreOgxZ/view?usp=sharing](https://drive.google.com/file/d/1EbJRxaGkwldh7N3W9kGI9os7oDreOgxZ/view?usp=sharing)",
        "created_utc": 1530483877,
        "upvote_ratio": ""
    },
    {
        "title": "Question about statistical inference",
        "author": "Islamiyyah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vclav/question_about_statistical_inference/",
        "text": "I've started to read a little bit about statistics in order to get a head start on the upcoming semester and it has left me a little bit confused. I hope to clear something up.\n\nOkay, here's the situation. Assume we're interested in the income distribution of a population and are trying to find the mean income. If we got data on the entire population we would get a distribution. But this is not a probability distribution, right, since there is nothing stochastic about it. There just exists a certain distribution at a certain time. \n\nHowever, if we were trying to infer properties about this distribution from a sample we would model the income distribution as if it was the probability distribution of a stochastic variable. We let X be the stochastic variable that decides the income of any given person. Drawing a sample of n persons can then be interpreted as finding n values of X and we can use this to find an estimate for the parameter E(X) which would then be our best guess for the mean income in the population.\n\nI'm unsure how to phrase the question. In general, is it correct to think about inference as making up a model involving stochastic variables and using a sample to estimate the parameters of their probability distributions? And then use this model to say something about the actual distribution in the population? So it's like a two-stage process...\n\nSorry it turned out to be so long and not involve a clear question. Appreciate all replies. ",
        "created_utc": 1530475053,
        "upvote_ratio": ""
    },
    {
        "title": "Unsure about conceptual understanding of multilevel experimental design... Advisor has no clue. I've done MLM before but not with an experimental design.",
        "author": "LeSpyFox",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vbbon/unsure_about_conceptual_understanding_of/",
        "text": "Long story short, my advisor and committee have no knowledge on how to analyze this data the right way (basically, not just running the ANOVAs even though it's group data and violates the assumption of independence). I've used R before and I know I can teach myself how to use lme etc. packages but the conceptual stuff and other multilevel questions are throwing me for a loop.\n\nData structure:\n\n93 teams of two people (186 people total)\n\n1 experimental condition A with 2 levels\n\n1 experimental condition B with 2 levels\n\nContinuous dependent variables, probably just going to run them separately, no need for a MLM SEM\n\nCollected the continuous dependent variables at 3 time points\n\nHave two control variables (continuous)\n\nI've been trained in multilevel modeling, but only with continuous predictors. How do I run a MLM with categorical predictors? My research seems to be telling me:\n\n\\-Fixed effects for the categorical predictors, no centering needed (so first do the null model, then the fixed effects to test if there's any effect on the dependent variables)\n\nBut that's where my knowledge ends. How will I know if there's group differences? How do I control for my two control variables? I feel like I'm losing it and I'm alone in this. My online searches aren't getting me anywhere and my committee is effectively ignoring me while they're just traveling the world. I'm sorry, I just don't know what to do anymore.",
        "created_utc": 1530464817,
        "upvote_ratio": ""
    },
    {
        "title": "I am in need of help, hit a wall with PhD proposal for Chapter Three.",
        "author": "tends2forgetstuff",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8vb7lb/i_am_in_need_of_help_hit_a_wall_with_phd_proposal/",
        "text": "Up until my chapter three through Quan I and II I was told I could use two validated surveys with no issues. Now a committee member is saying, since my audience has never been surveyed I have to go through validating the surveys. Both surveys have been used for decades and have tons of data. I simply want to compare two surveys - how do I go about doing this in a statistical manner? I am sitting here in tears as I thought I was about ready to go to defense [we defend twice at my school - 1st prior to data gathering and then final] and now I have hit a brick wall. One audience - to surveys to see if the two survey topics have any relationships. ",
        "created_utc": 1530463899,
        "upvote_ratio": ""
    },
    {
        "title": "lmer mixed effects - repeated measures nested? crossed?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8varc9/lmer_mixed_effects_repeated_measures_nested/",
        "text": "[deleted]",
        "created_utc": 1530460011,
        "upvote_ratio": ""
    },
    {
        "title": "Do I report a Multivariate Regression in the same way as a Multiple Regression?",
        "author": "Oppee123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8v9bwy/do_i_report_a_multivariate_regression_in_the_same/",
        "text": "Hi, so I've conducted a Multivariate Regression on my data and have gained the results, but I'm unsure as how to report the results. I have tried to research what to do but it only comes up with multiple regression.\n\nSo in my write up, do I just report the results in the same way as a multiple regression?\n\nAny help would be appreciated, thanks!",
        "created_utc": 1530444977,
        "upvote_ratio": ""
    },
    {
        "title": "Does it make sense to report confidence intervals with chao1 or Shannon's index?",
        "author": "Qandyl",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8v8fx2/does_it_make_sense_to_report_confidence_intervals/",
        "text": "Not sure if this sub is the right place, apologies. Using EstimateS for richness and diversity of bees. First time using it/chao1 so I'm in a bit over my head. \n\nUsing EstimateS for richness and diversity of bees. Would it make sense to display my CIs on my chao1/Shannon curves? I'm assuming no for the latter from the information I've been able to find (and that they're not given in the output anyway). What about ± 1 SD around the curve for Shannon's? It makes the figures look lovely but I'm not sure how valuable the extra information is in a statistical context.\n\nFurther, how do I report these in the text? Do I give the largest SD/CI for each index, or a range? Even a published example would be great.\n\nAny help appreciated!",
        "created_utc": 1530431148,
        "upvote_ratio": ""
    },
    {
        "title": "Significance of multinomial logistic model",
        "author": "_RIO_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8v6hwv/significance_of_multinomial_logistic_model/",
        "text": "I have data of 1000 obs. with 10 variables, some of them categorical(not ordered) and some continuous. I need to create a model to predict one of the categorical unordered variable.\n\nI've run a multinom. logistic regression in R with all 9 of the remaining variables, and then obtained the z values by dividing the coef with the std error. I've been told a z value of 2 or more means that the coef. is significant.\n\nI'm having trouble to determine which variables are significant because some of them have a value of 2 in regard to one category but a small number in regards to other.\nTo explain better let's say I want to predict y in terms of x1,x2 and x3, and y has categories 1 2 and 3. What do I do when let's say x1 has a zvalue of 3 for category 2 and a value of 0.01 for category 3?  \n\n",
        "created_utc": 1530407737,
        "upvote_ratio": ""
    },
    {
        "title": "Ordinary Least Products Regression Analysis",
        "author": "leon08alan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8v60kd/ordinary_least_products_regression_analysis/",
        "text": "I have measurements of 38 samples using two instruments. Due to a low budget, measurements were not replicated. \n\n**I want to determine the agreement among the two instruments.** \n\nFor this, I first performed a one sample t-test to compare the means of the differences of each measurement corresponding to each sample. The results imply that there is a significant deviation from 0. Meaning that there is no agreement among the instruments. \n\nNow I want to determine if this is due to a fixed bias or a proportional bias. After reading many articles citing Bland-Altman method, I came across a paper by Ludbrook ([link provided](https://pdfs.semanticscholar.org/79ab/ccae0df4e66143a2863800cc24984c2f2d46.pdf)) that mentions the use of an ordinary least products regression analysis (OLP). \n\n**Does anyone know how to conduct an OLP regression with R or STATA ?** (Before getting asked to do a Bland-Altman plot, I want to specify that will be comparing both results from Bland-Altman plot and OLP). ",
        "created_utc": 1530402868,
        "upvote_ratio": ""
    },
    {
        "title": "Are non-Hispanic whites more likely to commit interracial crime?",
        "author": "ryu238",
        "url": "https://thecrimereport.org/2018/06/22/interracial-crime-study-in-l-a-finds-whites-more-likely-to-assault-blacks-than-the-reverse/#",
        "text": "",
        "created_utc": 1530398392,
        "upvote_ratio": ""
    },
    {
        "title": "Sample Size question",
        "author": "hilsdenr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8v1ysr/sample_size_question/",
        "text": "I find sample size calculations confusing.  I am looking for a bit of advice here.\n\nI am trying to determine whether the introduction of factor X into the environment is temporally correlated with outcome Y\n\nThe event rate is about 35 events per year on average.\n\nThe standard deviation of the event rate is 3\n\nI am looking to get an 80&amp;#37; chance of identifying a 10&amp;#37; different at the 95&amp;#37; confidence level.\n\nWhat that information what would my sample size need to be.  And since I expressed the event rate in per year terms would my sample size then be number of years I am looking at.\n\nThanks",
        "created_utc": 1530366607,
        "upvote_ratio": ""
    },
    {
        "title": "Chance of two things happening",
        "author": "Rule0fnine",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8v1pk4/chance_of_two_things_happening/",
        "text": "Hello, I don't know if anyone has asked this question, they probably have so sorry if you have to repeat it. I have a question in regards specifically to a drop rate in a game. So if two seperate items have the same drop rate is the effective drop rate half the rate for either item individually? so for example item A has a 1/512 chance and item B has a 1/512 is the probability of getting either of those items 1/256? or is that not how it works. Thanks. Not sure if i wasnt clear enough. will clarify if needed.",
        "created_utc": 1530363869,
        "upvote_ratio": ""
    },
    {
        "title": "Extrapolation of my anecdotal findings seems to suggest a lot more people from my country Bangladesh are immigrating to Australia than the Australian government statistics. What is going on here?",
        "author": "powerofinformation",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8v02ob/extrapolation_of_my_anecdotal_findings_seems_to/",
        "text": "I don't know much about statistics and I am horrible at math. I only did four statistics courses during my Economics Bachelors. \n\nAnyways, here's the thing. I live in Bangladesh and A LOT of people I know have immigrated to Australia. By immigration I mean people who went there with an intent to live permanently, whether they are there illegally, legally, gaming the system (like extending student visas), permanent residency, citizenship etc. Most people I know are mostly permanent residents with some being citizens. Now, this is based on anecdotal evidence obviously. It seems like tons of people in my circle (that is friends of friends and so on) have immigrated there. So I made a tally and found at least 400 people who have immigrated and that's just people who I know. That is friends of friends, facebook friends, etc etc. In course of my work in a bank I have also seen lots of people immigrating there as well. \n\nAnd that is only from people I know. Now, if I extrapolate this and consider my anecdotal experience to be a sample then the numbers on the Australian government's website does not match up. For eg. Here [in this thread](https://www.reddit.com/r/AskAnAustralian/comments/8npf35/what_is_the_total_number_of_bangladeshis_living/) you can see that people are saying its 50,000. But to me it seems like over 500,000+ maybe? I don't know. \n\nIt can't be such that only the majority of the people in my tiny social circle is living in Australia while no one else from Bangladesh is. So there will be at least some extrapolation of my anecdotal findings. \n\nEnglish is not my native language and I am not good with Statistics. Can someone explain what is happening here? Why there is such a wide discrepancy between the number of Bangladeshis living in Australia as reported by the Australian government and my understanding/experience?",
        "created_utc": 1530340976,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to see if the number of strikes last year was significant from the previous 24 years. Am I doing things right?",
        "author": "maliy_yastreb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uwwv1/trying_to_see_if_the_number_of_strikes_last_year/",
        "text": "Hi, I'm very much an amateur in statistics, so I'm not sure if this is the right thing for what I'm trying to find out. Any opinions appreciated.\n\nThe UK Office of National Statistics published new data about the labour force recently including [information about strikes (stoppages)](https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/workplacedisputesandworkingconditions/datasets/labourdisputeslabourdisputesannualestimates). Since a new Trade Union Act was passed in 2016, I was wondering if the number of stoppages begun in 2017 (67) was a statistically significantly decrease from the number of stoppages begun across 1993 (the year after the previous Trade Union and Labour Relations Act) to 2016.\n\n[Here's](https://docs.google.com/spreadsheets/d/1NVfhAvr34lIaqKgySyKkcMzLiDa0S-0qYjNqee6aLXc/edit#gid=1685513926) what I came up with - I went the long way with variance and standard deviation to refresh my memory of the formulae but spreadsheet calculations come to the same. 67 stoppages in a one-tailed test with an alpha level of 0.05 comes to P = 0.0256, which seems significant. Is that right, or is there a different test I ought to be doing with data like this?\n\nThanks for your help.\n",
        "created_utc": 1530308241,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to demonstrate CLT in R",
        "author": "uclano97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uwskx/trying_to_demonstrate_clt_in_r/",
        "text": "Hi everyone, I had a question about the CLT for sample means.\n\nUsing this for loop, I am creating a sampling distribution for the sample mean consisting of 1000 observations. The problem is, when I take the mean of the sampling distribution, the output is always something around 15.22, compared to the actual mean of 19.89742. My question is, if the central limit theorem holds, shouldn't the mean of the sampling distribution be the population mean? Or at least closer to it? Thanks!\n\nHere is my code\n\npopulation=read.table(\"[http://pages.stern.nyu.edu/\\~jsimonof/Casebook/Data/Tab/census1.TAB](http://pages.stern.nyu.edu/~jsimonof/Casebook/Data/Tab/census1.TAB)\", header=T)\n\nmeans=matrix(rep(0,1000),ncol=1)\n\nfor(i in 1:1000){\n\nmeans\\[i\\]=mean(population$Income\\[sample(population$Income,300)\\])\n\nhist(means)\n\n}\n\nmean(means)\n\nmean(population$Income)",
        "created_utc": 1530307282,
        "upvote_ratio": ""
    },
    {
        "title": "What do you look for in a master's degree in statistics?",
        "author": "throwaway-applicant",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uvfd8/what_do_you_look_for_in_a_masters_degree_in/",
        "text": "I'm currently trying to decide whether I should head to this fairly well-ranked program (top 50 worldwide), and if I do what should my specializations be.\n\nI understand that different people would have different interests (going into industry/business vs pursuing further research), except at this point I don't even have a super specific research goal in mind to jump at. Still, there's probably a few common key markers that one should look/check for. What do you think I should keep in mind when assessing this?",
        "created_utc": 1530296704,
        "upvote_ratio": ""
    },
    {
        "title": "Wording the Hypothesis for a Multiple Regression",
        "author": "thoughtsandthefeels",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uv2bh/wording_the_hypothesis_for_a_multiple_regression/",
        "text": "I searched for this answer but haven't been able to find it yet- could someone please help?\n\nI did a study where I did a simple multiple regression on Likert scale items.  2 IV's and a 4 DVs (actually there more more variables, but the study is only reporting on that part of the data).\n\n I worded my hypotheses like this:\n\n*H1: A greater level of ( independent variable) will lead to a greater level of (dependent variable).*\n\nI received critique that said my hypothesis should be worded in terms of relationships instead of saying one thing leads to another because I am not testing causation in this study.  \n\nSo the question is, could I reword it as:\n\n*H1:  (Independent variable) will have a positive linear relationship with (dependent variable)?*\n\nTo me, this sounds good, but it also sounds like the wording for a correlation analysis, not a multiple regression.\n\nIf the wording doesn't work, can anyone suggest better wording? \n\nThanks for any help you can offer.",
        "created_utc": 1530294004,
        "upvote_ratio": ""
    },
    {
        "title": "One bag full of $10, $20, $50, $100 bills. How much would you pay to have 2 pulls?",
        "author": "grovebost1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uujo9/one_bag_full_of_10_20_50_100_bills_how_much_would/",
        "text": "The bag is full of equal proportions of $10, $20, $50, and $100 bills. The EV of one pull is $45. But what if you have the chance to take one pull and return the bill if it’s not to your liking. How much would you pay to play that game? I got $60\n\nEdit :  pasted the work in one of the thread replies",
        "created_utc": 1530290222,
        "upvote_ratio": ""
    },
    {
        "title": "FWHM Question!",
        "author": "What_The_Chuck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8usura/fwhm_question/",
        "text": "I’m using a curve fitting software for a Gaussian fit and it is giving me the option to select “Number of FWHM widths to fit.” I understand the concept of a FWHM, but what about the data analysis and specifically location of the peak would this change? Thanks in advance! ",
        "created_utc": 1530276457,
        "upvote_ratio": ""
    },
    {
        "title": "please take a look at the relationships and conclusions this youtuber is making and is propagating them unto the public ... for example at 10:20 - the s curve graphs male vs female comparison ... please point what he did right, wrong and conclusions that could be made ... thank you ...",
        "author": "InGearX",
        "url": "https://youtu.be/7vqRbScCIPU",
        "text": "",
        "created_utc": 1530248744,
        "upvote_ratio": ""
    },
    {
        "title": "If Ben Roethlisberger was accused of his crimes in 2018, would he still be in the NFL?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/nfl/comments/8ua3il/if_ben_roethlisberger_was_accused_of_his_crimes/",
        "text": "[deleted]",
        "created_utc": 1530236362,
        "upvote_ratio": ""
    },
    {
        "title": "Contrasting Scatter Plot and Correlation result",
        "author": "Sweetcandiee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uoyqx/contrasting_scatter_plot_and_correlation_result/",
        "text": "Hey guys!\n\nI'm currently working on a study and I'm  doing a secondary data analysis. I did a scatter plot graph and there was no correlation. Then I also ran an SPSS correlation analysis and the r is 0.591. What's wrong? Please help!",
        "created_utc": 1530233391,
        "upvote_ratio": ""
    },
    {
        "title": "Proper Description of a Hypothesis for Moderation",
        "author": "SparklyYay",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uodz3/proper_description_of_a_hypothesis_for_moderation/",
        "text": " I read a paper where a person was hypothesizing to see moderation.A was predicting B.\n\n1. They said that for participants at HIGH levels of C, the relationship between A and B would be stronger.\n2. But then they something like that LOW levels of C would not change the association between A and B.\n\nI don't understand what they mean by #2. Couldn't #1 be rephrased to say that for participants at LOW levels of C, the relationship between A and B will be weaker? I don't get the idea of saying it \"wouldn't change\" the relationship.Is it possible they think that very few people are at high levels of C so that analyses of people with low C will be comparable to the analyses that don't include a moderator...? ",
        "created_utc": 1530228077,
        "upvote_ratio": ""
    },
    {
        "title": "Combining multiple means with sd",
        "author": "Healur",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8unx6u/combining_multiple_means_with_sd/",
        "text": "Hi r/AskStatistics,\n\nI'm pretty much beginners level when it comes to statistics and analysis and I'm not sure if this is the place to ask so please bear with me.\nRight now I'm trying to combine two data: \n(1) mean 39.9 ± sd 5.7 [n=7] &amp;\n(2) mean 35.3 ± sd 2.3 [n=28]\nSadly I don't have the data from which they calculated these means so I'm not sure how to combine 1 and 2. Also any suggestions on which graph to use to visual the results? (I'm using PRISM 7)\nAny advice is appreciated, I'm really lost right now.",
        "created_utc": 1530224124,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence interval for a percentage, unknown population size",
        "author": "saccharind",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8unvk1/confidence_interval_for_a_percentage_unknown/",
        "text": "It's possible to calculate a confidence interval for a percentage, correct? Treating µ = % of X occurring, with an unknown σ. and unknown population. And, is a confidence interval what I'm looking for, or is it something else?\n\nIf I have a bag filled with items, and the number of items in this bag is unknown, how many single draw and replace attempts do I need to do to determine (within a 95 or 97% confidence interval?) the chance of pulling an item A that is in this bag? \n\nIt's been more than ten years since I took stats in high school/college, so my terminology is likely wrong and/or I don't know what I'm asking for here.",
        "created_utc": 1530223752,
        "upvote_ratio": ""
    },
    {
        "title": "Principle components analysis",
        "author": "elsathebunny",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8unkax/principle_components_analysis/",
        "text": "Hi guys,\n\nIve been running a PCA to develop a psychometric scale, trying to reduce the number of items we have in our prototype and to define a structure to the scale. Based on the scree plot, it indicated an inflection at component 5, however, i initially thought it stopped at factor 4 and so based the rest of my analysis on a scale with 4 components. I showed my PhD supervisor this, and she agreed 4 factors so I thought all was well - however, she now says there were 5 and what was my reason for going with 4 factors. Ive checked for communalities and nothing loads below 0.4, I have no issue with crossloading either. A colleague of mine said that a 4 factor model is correct as on component 5, i dont have any primary factor loadings of at least 0.6. I don't know where he found that reference, and he can't seem to provide one. Has anyone heard that a primary factor loading needs to be at least 0.6? i'd be very grateful for any advice or suggestions.\n\nhttps://i.redd.it/f75nzhtr6t611.png\n\nhttps://i.redd.it/mo54mv1t7t611.png\n\nhttps://i.redd.it/na1a3khzct611.png",
        "created_utc": 1530221264,
        "upvote_ratio": ""
    },
    {
        "title": "Analysis of a Pre-Test/Post-Test Design For an Educational Intervention",
        "author": "HenshinHero_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ullf0/analysis_of_a_pretestposttest_design_for_an/",
        "text": "Hi r/AskStatistics, can someone give me a hand? \n\nMe and my research group are trying to validate the effectiveness of a educational booklet we developed. For that, we applied a test on the subject for a group of students, then gave them the booklet, then re-applied the test, and are trying to check if the booklet improved their knowledge. You know, standard pre-test/post-test stuff. \n\nHowever, none of us are used to this kind of study, and I'm having a bit of trouble on how exactly to deal with the data.\n\nWhat I have done so far is: I've gathered the percentage of correct answers for each question of the pretest and posttest results (thus giving me ten percentages of people who answered each question correctly). Then I divided those in two groups (pre and posttest respectively) and did a t-Test between those two groups. I found signifficance, but not a very strong one.\n\nIs that the correct approach? I have the snarky feeling that it isn't. Any help is immensely appreciated. ",
        "created_utc": 1530207013,
        "upvote_ratio": ""
    },
    {
        "title": "Longitudinal Mixed Models with LMER - Can I test two biomarkers for associations without including \"day\"?",
        "author": "barackalacka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ujwf1/longitudinal_mixed_models_with_lmer_can_i_test/",
        "text": "Forgive my ignorance — learning R and some regression on my own so my current body of knowledge is swiss cheese. Right now I’m learning to run mixed models and had a question on one of my attempts. \n\nSince I don’t want to get too specific about my project, here is a hypothetical situation with made-up data:\n\nI have 10 subjects and I’m monitoring them longitudinally as I subject them to stressful conditions (shocking them randomly or something). I’m taking two daily measurements: cortisol levels and eye contact. \n\nI’m running a few tests afterwards to test for relationships between variables. Please see (3) for my main question. \n\n1. What’s the relationship of time post-stress initiation and eye contact? In R, I’d test eye contact as a function of experimental day: lmer(Eye.Contact ~ Day + (1|Person.ID))\n\n2. What’s the relationship of time post-stress initiation and cortisol? Same idea as above, but this time cortisol would take the place of eye contact. lmer(Cortisol ~ Day + (1|Person.ID))\n\n3. Is cortisol predictive of/associated with eye contact? Here is my main issue. Originally, I decided to run a test like: lmer(Eye.Contact ~ Cortisol + (1|Person.ID). So, eye contact as a function of cortisol. But should I also include a term for the day, or is that already “included” in the cortisol measurement?\n\n\nPlease let me know if I’m butchering or missing anything!\nThanks so much for any guidance you’ve got! ",
        "created_utc": 1530195977,
        "upvote_ratio": ""
    },
    {
        "title": "Book with blend of programming and statistics",
        "author": "ruchit007",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uitqf/book_with_blend_of_programming_and_statistics/",
        "text": "I have started with ML and want suggestions of some books that will provide depth knowledge\nStatistics to solidify the foundation for Ml",
        "created_utc": 1530188199,
        "upvote_ratio": ""
    },
    {
        "title": "Self-study advice: Which book should I read first?",
        "author": "ANewPope23",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uhja2/selfstudy_advice_which_book_should_i_read_first/",
        "text": "I am finishing 'Introduction to the Theory of Statistics' (Mood, Graybill, Boes). Now I want to learn how to do statistics but with many variables (matrices).\n\nI am considering the following books:\n\n1. Methods Multivariate Analysis (Rencher)\n\nhttps://www.amazon.com/Methods-Multivariate-Analysis-Alvin-Rencher/dp/0470178965/ref=sr_1_3?s=books&amp;ie=UTF8&amp;qid=1530175623&amp;sr=1-3&amp;keywords=alvin+rencher\n\n2. Linear Models in Statistics (Rencher)\n\nhttps://www.amazon.com/Linear-Models-Statistics-Alvin-Rencher/dp/0471754986/ref=sr_1_1?ie=UTF8&amp;qid=1530189102&amp;sr=8-1&amp;keywords=linear+models+in+statistics\n\nWhich is more logical to be read first? Or should I read something else?",
        "created_utc": 1530175704,
        "upvote_ratio": ""
    },
    {
        "title": "What is the best way to use a time series to predict another variable?",
        "author": "Pocshi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ugfqm/what_is_the_best_way_to_use_a_time_series_to/",
        "text": "Example: I have a time series measuring heart rate. Beats per minute for 24 hours. In this scenario I want to know if time of day and heart rate is related to heart diseases. Meaning if having high heart rate at the start of the day is associated with heart diseases compared to having high heart rate throughout the day or compared to those with high heart rate at the end of the day.\n\nIs there a way to analyse this data? I was thinking about clustering time series first and see if there is any difference in heart rate throughout the day then comparing those with heart disease/no heart disease. I realised this method may be incorrect cause the clustering may not take into account heart disease variable initially.\n\nThis is just an example though.\n\nWas wondering if it is possible to analyse this sort of data for categorical variables (heart disease/no heart disease) and continuous variables (e.g. amount of exercise a person does) as well.",
        "created_utc": 1530161421,
        "upvote_ratio": ""
    },
    {
        "title": "2 bags with 100 marbles, one is 99 black marbles, 1 white, the other is 99 white, 1 black.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uesp2/2_bags_with_100_marbles_one_is_99_black_marbles_1/",
        "text": "[deleted]",
        "created_utc": 1530145546,
        "upvote_ratio": ""
    },
    {
        "title": "How can I use this data (sorry I didn't realise I couldn't paste a table)",
        "author": "JoeSalmonGreen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ucbg4/how_can_i_use_this_data_sorry_i_didnt_realise_i/",
        "text": "Imagine I run a cake shop, and I have a table of data like this from calling customers up at random and asking what they thought of my cakes. Once a month I can choose one flavour to focus on and make better. I’m assuming that simply picking the flavour with the highest ratio of ‘made sick’ probably isn’t a statistically sound thing to do, how can I use this data best?\n\nImagine I run a cake shop, and I have a table of data like this from calling customers up at random and asking what they thought of my cakes. Once a month I can choose one flavour to focus on and make better. I’m assuming that simply picking the flavour with the highest ratio of ‘made sick’ probably isn’t a statistically sound thing to do, how can I use this data best?\n\n[Link to data](https://docs.google.com/spreadsheets/d/1vb9fET_3EIc4kgwkknzXmvmn8Xcp5RWOH5wFB5SZJNk/edit?usp=sharing)\n\n\nCake Flavour    Enjoyed    Didn’t enjoy   Made sick    Total\n\nApple                90                15              2              107\n\nPear                 63                14               1              78\n\nPickle                9                   3              9                21\n\nCucumber        13                   2               8              23\n\nTuna                2                    0               7              9\n\nCherry             44                  9               6               59\n\nPotato              24                 8               5                37\n\n\n",
        "created_utc": 1530125845,
        "upvote_ratio": ""
    },
    {
        "title": "Best way to perform a moderated multiple regression with several possible moderators?",
        "author": "BorisMalden",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8uc3to/best_way_to_perform_a_moderated_multiple/",
        "text": "I'm currently planning a study which will have five continuous predictor variables (X1, X2, X3, X4, X5) and one continuous outcome variable y. Additionally, I'd like to explore 4 potential continuous moderators (M1, M2, M3, M4). \n\nFrom what I understand (which isn't very much, so please correct any errors I make), if I was looking at just X1, M1, and y, I'd create the interaction term X1*M1 and enter that into the model, so the equation would be: y = β0 + β1X1 + β2M1 + β3X1*M1. If X1*M1 is significant, then M1 is a significant moderator (at which point I could convert M1 into a categorical variable and compare the regression slopes at the different levels to see the extent of the moderation).\n\nExpanding that to a multiple regression with X1 through X5, my regression line would be: y = β1 + β2X1 + β3M1 + β4X1*M1 + β5X2 + β6X2*M1 + β7X3 + β8X3*M1 + β9X4 + β10X4*M1 + β11X5 + β12X5*M1 (again, please correct me if that's not right). Wherever the interaction term is significant, there is evidence that the relationship between that predictor and the outcome is moderated by M1.\n\nWhen extending this further to assessing the effects of M1 through M5, is there a way to enter all moderators in the model simultaneously? Can I construct a stupidly long equation with all the possible Xs and Ms and their interaction terms, or is that not the correct way of doing it?  Is it better to just assess the effects of each potential moderator separately? And, if more than one moderator is significant, what's the best way to conduct follow-up analyses to explore this in more depth?\n\nAny and all advice gratefully appreciated, particularly if you dumb down the jargon enough that I can understand it",
        "created_utc": 1530124323,
        "upvote_ratio": ""
    },
    {
        "title": "ancova vs linear regression for non normal variable w significant covariate (SPSS)",
        "author": "jessicalafatale",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ubt5c/ancova_vs_linear_regression_for_non_normal/",
        "text": "hi there! if anyone can pls help me..\n\ni am doing a project to look at group differences between treatment vs no treatment.\n\nfor one of my outcome variables (mood), it is NON normally distributed. we also have a significant covariate-age.\n\nso when I ran the Mann Whitney U test (which should adjust for non normality since its nonparametric test), i got an insignificant result for group difference on outcome variable: mood\n\nthen I ran an ancova with the age covariate controlled for and now my result is significant.\n\nbut then my stats advisor told me to transform the mood variable with log (x+.01) and re run the linear regression. i now get an insignificant result. And even w the transformation, the pp plot shows skewness still.\n\nso stats advisor is being super MIA and I am working on a deadline here.\n\nDo you guys happen to know what I should do? Am I ok to report the ANCOVA or no, because ancova doesnt take into account for non normality? Do I need to report the linear regression on the transformed mood variable? Do I need to somehow take out the outliers (i believe I have 2 values that SPSS deemed outliers) ? but then how would I report that in my finding, is that like lying or fudging data?? I don't want to do anything wrong here.",
        "created_utc": 1530122294,
        "upvote_ratio": ""
    },
    {
        "title": "Is there bias using odd vs even iterations for various modeling purposes?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ub5kl/is_there_bias_using_odd_vs_even_iterations_for/",
        "text": "[deleted]",
        "created_utc": 1530117793,
        "upvote_ratio": ""
    },
    {
        "title": "If one wanted to rank things via round-robin tournaments of user-decided pairwise comparisons, what's the least amount of randomized matches and users-per-match that could reliably predict the final results?",
        "author": "dfghjkfghjkghjk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8u9rvw/if_one_wanted_to_rank_things_via_roundrobin/",
        "text": "While I like 5/5 ratings, some things, (like college courses, software, Askreddit replies, etc), are too specific or time-consuming to reasonably expect people to learn what the \"average\" is. In such cases pairwise comparisons seem like they would work better, but just 30 alternatives create 435 possible comparisons and, since preferences are subjective, each comparison could benefit from using it's own random sample: which would mean 435*300=130,500 comparisons. Intuitively, it kinda feels like it might be most efficient to add the win/less scores of beaten alternatives to the winner scores.\n",
        "created_utc": 1530107492,
        "upvote_ratio": ""
    },
    {
        "title": "Not sure what test to run on my experiment data.",
        "author": "raspberries_for_life",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8u9aj9/not_sure_what_test_to_run_on_my_experiment_data/",
        "text": "I have conducted an experiment with 19 participants, in which reaction times were measured. Each participant did the same task.\n\nThe task had 3 conditions: happy/fear/neutral. Each participant did multiple trials per condition, resulting in multiple reaction times per condition per participant. I took the median of these reaction times, resulting in a median reaction time for each condition per participant.\n\nI also have each participant classified as either high or low in anxiety.\n\nHow do I run a test to see if there is a difference in reaction time for each condition for people low vs people high in anxiety?\n\nAny help is appreciated!!",
        "created_utc": 1530103386,
        "upvote_ratio": ""
    },
    {
        "title": "What post hoc test do I run if my one-way ANOVA data is not normally distributed?",
        "author": "ohhfasho",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8u6lvm/what_post_hoc_test_do_i_run_if_my_oneway_anova/",
        "text": "I'm writing the statistical analysis section of my thesis prospectus and full disclosure, I'm not stats savvy.  I am proposing a one-way ANOVA to determine whether there is a statistically significant difference between my independent groups and propose a Tukey's HSD post hoc test to identify where the difference between the various groups occurred. \n\nTukey's is used if my data are normally distributed. What post hoc test can I use if my data are not normally distributed?\n\nCould it be Games Howell post hoc test?",
        "created_utc": 1530072913,
        "upvote_ratio": ""
    },
    {
        "title": "Probability when rolling a dice repeatedly",
        "author": "freedamanan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8u564g/probability_when_rolling_a_dice_repeatedly/",
        "text": "\n\nif you roll a (fair) dice then guess whether the next roll is (strictly) higher\nor lower, what's the probability of getting the right answer 5 times in row?\n\nI'm not sure how to handle this. \n\nFor example if I have r1 = 4, then I'm going to guess 'lower' with p=3/6\n\nThen if r2 = 1 I would guess higher with p=5/6 etc.\n\nI don't really know if I can factor everything in though. It seems that the\nworst case is if I have 3 or 4, and then I have p=3/6. But I need to get a 3 or\n4 for that to occur, the probability of which is 2/6.\n\nSo I find it quite confusing to keep track of it all, and I'm wondering what the\napproach is.\n\nThanks\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "created_utc": 1530059374,
        "upvote_ratio": ""
    },
    {
        "title": "Experimental Design and Measuring a Rate of Success",
        "author": "mkuehn10",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8u2xy0/experimental_design_and_measuring_a_rate_of/",
        "text": "Suppose you create an experiment for baseball players where you assign an extra batting coach (treatment) to a group of players and then use a control group of players who did not get the extra batting coach. If I want to know what the impact on the batting average was, what would be the best way to measure this? Note that baseball players sometimes get injured or get sent to the minors, so they may come and go throughout the season. I'm not sure if that changes this or not.\n\n1) Would I just look at the total hits divided by the total at-bats for every players in the treatment group and then compare that to the control group? \n2) Would I need to find the individual batting averages of each player and then somehow find the average of that and compare across groups?\n\nI have been researching how to measure something like this that makes the most sense and have been unable to find anything that addresses this. Forgive me if I am missing something obvious!",
        "created_utc": 1530041605,
        "upvote_ratio": ""
    },
    {
        "title": "table soccer matchmaking",
        "author": "GonziHere",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tzw2e/table_soccer_matchmaking/",
        "text": "Hi,\n\nwe play table soccer in group of like 10 people and we would like to rate each player. We have outcome of the set? (say 10:7), we know if one team won in 2 or 3 sets, we might change shooter/gatekeeper position per set, and we randomly choose teams at the beginning.\n\nWe now calculate score in points per match (3 for 2:0 win, 2 for 2:1 win, 1 for 1:2 loss, 0 for 0:2 loss) but we don't know how to incorporate player skill into it. Did you play against two bad players? one good, one bad? should good player lose its rating because he was paired with bad player against two good ones?\n\nThere are systems in chess that solve that, but they are \"team\" based (as in one player is whole team). But we have team that wins or looses, yet we want to track performance of each player of that team.\n\nCan anyone point me in the right direction?\n\nedit: I am interested in fair scoring but also in fair matchmaking (if I pick 4 people, how to choose who will play with who?)",
        "created_utc": 1530018741,
        "upvote_ratio": ""
    },
    {
        "title": "What test to use: distance from forest edge vs. plant abundance",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tzed5/what_test_to_use_distance_from_forest_edge_vs/",
        "text": "[deleted]",
        "created_utc": 1530014089,
        "upvote_ratio": ""
    },
    {
        "title": "What are novel regression methods?",
        "author": "mertblade",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tyak6/what_are_novel_regression_methods/",
        "text": "Hi all, I want to make a summer project. What I have in my mind is to use novel regression or machine learning methods for regression.\n\nCould you please suggest me novel, sophisticated regression methods or any new machine learning algorithms?",
        "created_utc": 1530000632,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate variance of the ratio of two independent variables?",
        "author": "ConstantFuture",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8twwrt/how_to_calculate_variance_of_the_ratio_of_two/",
        "text": "",
        "created_utc": 1529985198,
        "upvote_ratio": ""
    },
    {
        "title": "How do you use Gauss-Newton Method to fit a curve to a set of data?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tsp24/how_do_you_use_gaussnewton_method_to_fit_a_curve/",
        "text": "[deleted]",
        "created_utc": 1529949890,
        "upvote_ratio": ""
    },
    {
        "title": "How applicable is statistics to the commercial sector?",
        "author": "ANewPope23",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ts3ti/how_applicable_is_statistics_to_the_commercial/",
        "text": "How useful is statistics for getting a commercial (tech/finance/not academia/not government) job?\n\nAre there areas of statistics that are a lot more useful than other areas?\n\nI often hear that the kind of statistics taught in a traditional Master of Statistics degree is the kind of statistics used for analysing experiments and studies, so that kind of statistics is mostly useful only for government/academic/pharmaceutical jobs. I often hear that the kind of statistics that will be useful in the near future (for jobs in 'data science') is the machine learning kind of statistics which seems to be quite different from the stuff taught in a traditional MS in Statistics.\n\nFurthermore, I often hear that statistics is not that important for machine learning. I wonder if this is true. Since so many computer science grads and electrical engineering grads work in machine learning, statistics must not be that important for machine learning, right?\n\nWould love to hear your thoughts on this. (I'm a prospective MS Statistics student)",
        "created_utc": 1529945645,
        "upvote_ratio": ""
    },
    {
        "title": "probability of drawing a card",
        "author": "evox288",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8trg86/probability_of_drawing_a_card/",
        "text": "i was playing the board game pandemic and i was interested on the probability of drawing an epidemic card from the draw pile.\n\nthe game is set up by dividing the player deck into 10 equal stacks and putting 1 epidemic card into each stack. the full deck with epidemic cards are 93. (which i understand makes 9.3 cards per stack or 3 stacks of 10 and 7 stacks of 9) but for simplicity i have been just saying its 9 cards per stack.\n\nat the end of each players turn they draw 2 cards from the top and resolve any epidemic cards.\n\nso far i have been doing drawing without replacement of each individual stack of cards being \\~ 1/9, 1/8, 1/7... until the epidemic card is inevitably drawn. however without replacement. is the probability just 1/9, or 1/8, or 1/7... for whatever position we are in the stack, or does the fact that we draw two cards at a time mean it becomes (1/9)+(1/8), or 2/9, 2/7...?\n\ni also cannot figure out how these probabilities add to p=1 and that is bothering me. \n\nany help is greatly appreciated. cheers",
        "created_utc": 1529940861,
        "upvote_ratio": ""
    },
    {
        "title": "How can I calculate a percentage response to a multi-choice question after removing one of the responses?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tr8or/how_can_i_calculate_a_percentage_response_to_a/",
        "text": "[deleted]",
        "created_utc": 1529939225,
        "upvote_ratio": ""
    },
    {
        "title": "Simulate an answer to a 'choosing' question",
        "author": "freedamanan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tr2z2/simulate_an_answer_to_a_choosing_question/",
        "text": "Here's the question : \n\n*****\n\n**You have 100 noodles in your soup bowl. Being blindfolded, you are told to take\ntwo ends of some noodles (each end of any noodle has the same probability of\nbeing chosen) in your bowl and connect them. You continue until there are no\nfree ends. The number of loops formed by the noodles this way is stochastic.\nCalculate the expected number of circles**\n\n\n******\n\nThere are answers for this, but I wanted to try and write a script for it.\n\n\nI expected the values to converge to something specific, in a way that's more\nobvious than what I'm seeing.\n\nRun 1 : https://imgur.com/a/99ALBP6\n\nRun 2 : https://imgur.com/a/0SkfNxz\n\nSo from these I'd say that the answer is about 5.2...\n\nBut the answer is ~ 3.28 (from [here](https://math.stackexchange.com/a/1417332/184261))\n\nSo I'm wondering how one would create a simulation for this, as I seem to have\ndone something wrong.\n\nIf the answer could be in R or Python that would be appreciated.\n\nHere is my working : \n\n\n    import pprint\n    import random\n    import statistics\n\n    from matplotlib import pyplot as plt\n    # instead of seaborn I'm using ggplot style\n    plt.style.use('ggplot')\n\n    # question\n\n    # You have 100 noodles in your soup bowl. Being blindfolded, you are told to\n    # take two ends of some noodles (each end of any noodle has the same\n    # probability of being chosen) in your bowl and connect them. You continue\n    # until there are no free ends. The number of loops formed by the noodles this\n    # way is stochastic. Calculate the expected number of circles\n\n    def get_evens_and_odds(n):\n        \"\"\"if n = 5 then this will return\n\n        evens = [0,2,4,6,8]\n\n        odds = [1,3,5,7,9]\n\n        \"\"\"\n        evens = [2*x for x in range(int(n))]\n        odds = [2*x + 1 for x in range(int(n))]\n        return evens, odds\n\n    # number of noodles\n    n=100\n\n    # number of experiments to run\n    exp = 1500\n\n    # This will store the mean number of circles found so far at each experiment.\n    # So at experiment number 3 it will store mean(exp1, exp2, exp3). I'm expecting\n    # this to converge to a value as the experiments go on.\n    means = []\n\n    # array of the number of circles found at each experiment.\n    circles = []\n\n    for j in range(exp):\n        # two lists of n elements, representing each end of the noodles\n        # the number of circles found on this iteration\n        c = 0\n        k = 0\n        for i in range(int(n)):\n            a,b = get_evens_and_odds(n - k)\n            # choose random elements from a and b\n            ca = random.choice(a)\n            cb = random.choice(b)\n\n            # see if they're the same value (ie the same noodle)\n            if cb == ca + 1:\n                c += 1\n            k += 1\n\n        circles.append(c)\n        means.append(statistics.mean(circles))\n\n    plt.scatter([x for x in range(len(circles))],means)\n    plt.show()\n\n    print(statistics.mean(circles))\n\n    # actual result\n    noodles = list(range(1,101))\n    sum = 0\n    for n in noodles:\n        sum += 1/(2*n - 1)\n    print(sum)\n\n\n",
        "created_utc": 1529938018,
        "upvote_ratio": ""
    },
    {
        "title": "Bayes Theorem",
        "author": "turkeyandlamb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tphib/bayes_theorem/",
        "text": "I have looked at Bayes Theorem for the last four hours and even looked up explanations online but I do not understand it. Can someone explain to me what this theorem is in terms of its variables?",
        "created_utc": 1529923204,
        "upvote_ratio": ""
    },
    {
        "title": "If I'm a member of two lottery syndicates, and I win with one of the syndicates, does the other syndicate have a much lower chance of winning?",
        "author": "ayrscot94",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tp6yr/if_im_a_member_of_two_lottery_syndicates_and_i/",
        "text": "Not much to add to the question but I'm wondering if, statistically, the second syndicate has any lower chance of winning the lottery as a result of me being in it, since it seems highly improbable that I would win twice?\n\nAs a note, I have not won the lottery and I'm asking hypothetically.",
        "created_utc": 1529919511,
        "upvote_ratio": ""
    },
    {
        "title": "How To Compare Two Score Distributions?",
        "author": "cherno11",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tlkvv/how_to_compare_two_score_distributions/",
        "text": "I want to see if there is a statistical difference between how I score books to how the average Goodreads user scores the same books (out of 5 stars). Raw data (n=36) [here](https://imgur.com/eXsY4N0).\n\nIs this what a paired two-tail t test is for?\n\nThen, if I find there *is* a difference, how do I assess the size of that difference? I assume that any effect size will come with a confidence interval?\n\nTrying to play catch-up on some forgotten college stats – appreciate any and all help. Cheers!\n\nCredit: This was inspired by u/caseyjosephine's super cool [post](https://www.reddit.com/r/52book/comments/8svp5x/im_halfway_through_and_i_did_an_analysis_on_my/) on r/52book.",
        "created_utc": 1529880168,
        "upvote_ratio": ""
    },
    {
        "title": "How do I do a statistical test to find the confidence interval of a ratio of two paired variables?",
        "author": "ConstantFuture",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tljbp/how_do_i_do_a_statistical_test_to_find_the/",
        "text": "",
        "created_utc": 1529879780,
        "upvote_ratio": ""
    },
    {
        "title": "Do school shootings usually happen at the school the shooter attended?",
        "author": "Carocrazy132",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tkoe2/do_school_shootings_usually_happen_at_the_school/",
        "text": "I feel I've always subconsciously assumed this was typically the case, but I actually thought about it today and realized I have no idea!  Is it usually a generic revenge mindset, or specifically that that place is where they went through hardship?",
        "created_utc": 1529872477,
        "upvote_ratio": ""
    },
    {
        "title": "Can I do a statistical test with a nominal independent variable and several other interval independent variables?",
        "author": "throwway1-10",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ti83o/can_i_do_a_statistical_test_with_a_nominal/",
        "text": "Hi guys, \n\nSo I'm trying to finish my thesis project for my Applied Linguistics MA program, and I'm at the stage where I've collected all my data and I realized way too late that I've made this research far too complicated for myself by having multiple dependent and independent variables. \n\nThe topic of my thesis is about the effects of CLIL (a bilingual secondary school program) on native language proficiency. I've done several different language proficiency tests, so I have several interval DVs. Now I want to compare groups: the CLIL group and the non-CLIL group. So far so good, if I wanted to test just these variables I suppose I could do a MANOVA?\n\nBut as I said I've made it far too complicated for myself. I also included a questionnaire in my research, asking the participants about their language motivation/attitudes, self-assessed proficiency and extramural use of English. These are Likert-scale questions that I've converted into interval variables. I would also like to include these variables to see if they have any effects on the the DVs. \n\nSo now I have several interval DVs, one nominal IV and three interval IVs. Is it possible to test all of these variables in a single statistical test(/model?)?\n\nTo make it even more complicated, I have also tested another group: a class of older students consisting of former CLIL students and non-CLIL students. The aim of testing this group was to see if any effects, if they are found, persist in later years. So this would create another nominal IV: time (with two levels). But I suppose this could be treated as a sort of post-test? I can also treat this group as a separate experiment and create a separate research question, because this is probably way too complicated to include in a statistical test with the other variables. \n\nIt's also actually not a problem if I have to use several statistical tests and if there will be beta errors it's okay, because during our masters program we have only been taught very basic statistics (t-tests, one-way ANOVA, correlation, and I think linear regression, but I never really got the hang of regression). \n\nSorry for the wall of text, but I need your help! I'm freaking out just a little bit and feel extremely stupid for not thinking enough about the statistical problems sooner... Does anyone know what kind of statistical test(s) I can use with the data that I have?\n\nI hope one of you can help me. I'd be forever grateful! Thanks in advance!",
        "created_utc": 1529851343,
        "upvote_ratio": ""
    },
    {
        "title": "What are the odds of rolling 14% or lower (of a possible 100%) 69 times in a row?",
        "author": "IrashiHeart",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tfwuy/what_are_the_odds_of_rolling_14_or_lower_of_a/",
        "text": "Title.",
        "created_utc": 1529819917,
        "upvote_ratio": ""
    },
    {
        "title": "Am I overthinking this simple probability question? I feel like not enough information is given for this question??",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tes4h/am_i_overthinking_this_simple_probability/",
        "text": "[deleted]",
        "created_utc": 1529806833,
        "upvote_ratio": ""
    },
    {
        "title": "Bayes' Theorem and Conditional Probability: P(B|A^c)",
        "author": "Socrato",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tef4u/bayes_theorem_and_conditional_probability_pbac/",
        "text": "I'm following along with the coursera bayesian statistics course because I have to admit, I did not have a good bayesian education with my statistics.\n\nBut I'm having a problem with bayes' theorem, and the example the teacher uses. \n\nWe're looking at the HIV test example\n\n* Pr(+ | HIV) = .977\n* Pr(- | No HIV) = .926\n\nAnd in our example, the region has \n\n* Pr(+) = 0.0026\n\nWe want to know P(HIV | +) so we use bayes' theorem. \n\nIn the denominator we need Pr(B | A^c )\n\nIn our case this is Pr(+ | No HIV) which he calculates as Pr(- | No HIV)^c or 1 - Pr(- | No HIV) and 1-.926\n\nWhy is this so? Does that imply that P(B|A^c ) = P(B^c | A^c )^c ? What exactly am I missing here, I'm feeling a bit foolish. \n\nThanks!\n\n",
        "created_utc": 1529802846,
        "upvote_ratio": ""
    },
    {
        "title": "Why do residuals in simple linear regression have constant variance but not in multiple linear regression?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tcjzz/why_do_residuals_in_simple_linear_regression_have/",
        "text": "[deleted]",
        "created_utc": 1529784386,
        "upvote_ratio": ""
    },
    {
        "title": "applying linear regression on rainfall data",
        "author": "lfklucas",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8tchme/applying_linear_regression_on_rainfall_data/",
        "text": "For my last task of statistic class I have to apply **linear regression**.\n\nMy family notes the amount of **rainfaill, frost, and gale** in the last 20 years, as well as **grain production** data on some properties, during the same period.\n\nI was thinking of doing something like: **How anomalies (*****or adverse conditions?*****) in time imply in the production of grains**.\n\nMy question is: how to use this data? My teacher said to make the average of rainfall, but a good media does not imply a good production, because we can have dry and excessive rain in the same season.\n\np.s.: english isnt my native language.\n\nthanks in advance",
        "created_utc": 1529783800,
        "upvote_ratio": ""
    },
    {
        "title": "[request] Regression Tutorials - building up a model from scratch",
        "author": "freedamanan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8taziq/request_regression_tutorials_building_up_a_model/",
        "text": "I'm struggling to find a nice guided tutorial on Linear Regression and building a model with it. \n\n[There are some things like this](http://stattrek.com/regression/regression-example.aspx?Tutorial=reg), but I was hoping for something that was perhaps more applied? As in - someone gets some data, builds the model around it and all of the relevant tests whilst doing so. \n\nSorry if this is a bit vague, perhaps I'm searching for the wrong thing. I'm just using \" linear regression modelling tutorial \" in google. \n\nthanks",
        "created_utc": 1529770556,
        "upvote_ratio": ""
    },
    {
        "title": "question on ANOVA and contrast tables in R",
        "author": "everyparallel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t9w9d/question_on_anova_and_contrast_tables_in_r/",
        "text": "first of all this is not for homework - i'm doing this mostly to refresh the experimental design/analysis course i learned last year. i would like to do ANOVA on the following generalized problem:\n\nY: response, can be Red or Green\n\nX2: explanatory variable one, has 2 factors: Good and Bad\n\nX1: explanatory variable two, has 7 factors: Monday, Tuesday, ..., Sunday\n\n\nI want to analyze and quantify the effects of X1 and X2 and their interaction X1*X2 on Y. And I would like to get an ANOVA table with a row for each level of the each effect and interaction. So I'm running this in R: \n\n    fit&lt;-aov(Y~X1+X2+X1*X2, data=df)\n    summary(fit)\n\nAnd I'm getting this:\n\n                     Df Sum Sq Mean Sq F value  Pr(&gt;F)    \n    X1           6   1.73   0.288   1.171   0.319    \n    X2            1   4.93   4.927  20.034 8.3e-06 ***\n    X1:X2    6   0.53   0.088   0.358   0.906    \n    Residuals      1253 308.14   0.246 \n\nbut for example, for X1, I want to see a row for each day of the week (or 6 rows, one for each degree of freedom), and similarly for X2, and X1:X2. I have to set up a contrast table somehow right? I cannot for the life of me figure out how to do a contrast table here. Any help is appreciated!",
        "created_utc": 1529760174,
        "upvote_ratio": ""
    },
    {
        "title": "How can I compare the value of points between positions in fantasy football?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t7e1b/how_can_i_compare_the_value_of_points_between/",
        "text": "[deleted]",
        "created_utc": 1529726377,
        "upvote_ratio": ""
    },
    {
        "title": "Financial Econometrics - Conditional Volatility of Continuously Compounded Returns - Interested in your Thoughts.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t4vg6/financial_econometrics_conditional_volatility_of/",
        "text": "[deleted]",
        "created_utc": 1529701994,
        "upvote_ratio": ""
    },
    {
        "title": "How do I know if I really know anything?",
        "author": "kingshagcorpse",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t4r98/how_do_i_know_if_i_really_know_anything/",
        "text": "I know the title sounds stupid. I recognize that no one can know everything and we all experience self-doubt sometimes, but how do I know if I really understand something? My training is in anthropology (a discipline not known for having much quantitative analysis), but I actually did learn about and use statistics all the time. It just never felt like my skills were up to snuff. I've tutored college students in the subject (and got paid to do it), and I helped someone interpret their regression model on another sub the other day.\n\nI'm not sure what my question is. I wouldn't expect to know as much about statistics as someone with a graduate degree in statistics, but do I actually know anything? What are foundational concepts I should have a firm grasp of? What tips you off as to whether or not someone knows what they're talking about?",
        "created_utc": 1529701044,
        "upvote_ratio": ""
    },
    {
        "title": "Sampling distribution shape?",
        "author": "NoahPM",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t40qk/sampling_distribution_shape/",
        "text": "I've been asked to explain the change of shape in sampling distributions when the sample size is increased using statcrunch.  I'm not sure how to explain these shapes?  I'm not even sure what words to use to describe them.  If I was asked to describe the distribution of the sample means, I would say the distribution gets more narrow and centered as the sample size increases, but as far as the sampling distribution, there's just more units marked.  I mean I suppose they too become less spread apart, but it's not the distribution that becomes less spread out, just the individual units?\n\nhttps://i.redd.it/2bt4818hql511.png\n\nhttps://i.redd.it/wy329myiql511.png\n\nhttps://i.redd.it/pxtan10xpl511.png",
        "created_utc": 1529695123,
        "upvote_ratio": ""
    },
    {
        "title": "How to control for heteroskedasticity in a spatial regression analysis?",
        "author": "suchascenicworld",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t3o6o/how_to_control_for_heteroskedasticity_in_a/",
        "text": "Hello,\n\nI am running a spatial analysis regarding how the utilisation distribution of my focal animal species are influenced by other variables. \n\nAs the response is a utilisation distribution, the data is not naturally normally distributed. I did run a fourth order transformation though,  and as such, the residuals from the OLS are seemingly normally distributed. \n\nHere are the residuals for the non-transformed ols\n[Imgur](https://i.imgur.com/luNM2Ny.jpg)\n\n\nHere are the residuals for 4th order transformation OLS\n[Imgur](https://i.imgur.com/MTYZFFR.jpg)\n\nFollowing, I ran several more complex models (lag error, durbin lag, durbin error) and the error model reigned in terms of AIC, controlling for spatial autocorrelation, and Lagrange.\n\nAnyways, I ran a studentized Breusch-Pagan test with the error model (as it seemed to be the best fit) and I received the following results:\n\ndata:  \nBP = 25.452, df = 5, p-value = 0.0001139\n \nThis suggests that heteroskedasticity is present (from what I believe). However, heteroskedasticity is in all (4th order transformation) models (although, Auto-correlation is low in the error)\n\nI am essentially trying to get rid of this heteroskedasticity and\nhope to control for it but do not have the statistical knowledge to be able to have an understanding on what to do best. I believe that transforming the response variable would be best, but I am not yet sure. \n\nAny insight would be incredibly helpful.",
        "created_utc": 1529692425,
        "upvote_ratio": ""
    },
    {
        "title": "CFA: Interpretation of 'Estimates'",
        "author": "Toddlou",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t3jx0/cfa_interpretation_of_estimates/",
        "text": "Hi,\n\nI   am currently working on confirmatory factor analysis using AMOS. I   would need some help to understand the concept of the 'Estimates'. In my   output, I have the Regression Weights, all with p &lt;.001. The next   line is \"Standardized Regression Weights\". There I get an 'Estimate' for   every factor. What can I tell upon those numbers? I get that the   p-value needs to be significant - but what does the estimate value per   se mean? Are there any limits which I have to consider (should be   negative / positive &gt;/&lt; .xx)? I didn't find any literature yet,   which explains the values more in-depth.\n\nI would be very happy if someone could explain this concept to me.\n\nCheers!",
        "created_utc": 1529691511,
        "upvote_ratio": ""
    },
    {
        "title": "Excluding outliers",
        "author": "Ihavedreamyblueeyes",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t2gdq/excluding_outliers/",
        "text": "I have identified some outliers in my data. There are multiple variables in the dataset, but only few cases have outliers on few variables.\n\nMy question is whether I should completely remove cases from dataset or only outlier data points and use value on other variables for those cases.",
        "created_utc": 1529683165,
        "upvote_ratio": ""
    },
    {
        "title": "Regression questions",
        "author": "mvaa12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8t2bem/regression_questions/",
        "text": "Hey, I got a regression model that looks like this:\nV1= 1.5+0.0013V2-0.003V3+ 0.0003origin2 +0.004origin3\nWhere V1, V2 and V3 are continous variables and origin is categorical with 3 categories. \nMy questions are:\n\n1) How would you interpret those results? \n\n2) Also, I got that p value for origin3 is not significant and for origin2 is significant, I dont know what do with that? What that means?\n\n3) where is the origin1 and how that effects the model? \n\nThank you for answers, appreciate it :) ",
        "created_utc": 1529682070,
        "upvote_ratio": ""
    }
]