[
    {
        "title": "What is work life like in stats?",
        "author": "callme_kibbles",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vk33c/what_is_work_life_like_in_stats/",
        "text": "So I have a semester and a final project to go for my Master's and I have a certification for statistical analysis. My adviser has recommended getting a data analysis job for real world experience (not to mention money) before figuring out what to do for my Ph.D.\n\nWhat is the day-to-day grind like? Is the job challenging and thought provoking or am I going to be staring at spreadsheets in Excel for the rest of my working life? Any tips, tricks or advice to someone about to enter the workforce in this field?\n\nMy question is aimed more at anyone in the ambiguous sounding title \"Data Analyst\" but certainly is open to anyone in the field of stats or data in general. Thank you in advance!",
        "created_utc": 1517883178,
        "upvote_ratio": ""
    },
    {
        "title": "Help with rank ordered logit models",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vjrqx/help_with_rank_ordered_logit_models/",
        "text": "[deleted]",
        "created_utc": 1517880132,
        "upvote_ratio": ""
    },
    {
        "title": "Developing a complex model after several years out of stats",
        "author": "friendnamedboxcar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vjm5p/developing_a_complex_model_after_several_years/",
        "text": "(cross-posted to /r/econometrics) \n\nI'm measuring the impact of two independent variables (one continuous and one binary categorical) on one dependent variable (continuous). I would like to figure out the best way to do this, and to do it as a sort-of time series (there is another variable for year, and I have three years of data).\n\nI've been out of analytics for a few years and am getting back into it, so I'm pretty rusty. Used to do economics research and intermediate analytics as a full time job for a year and a half before getting into other work.\n\nLet me know what other characteristics you'd need to know to get a sense of how to analyze the data. The dataset is large, maybe 15-20k observations, with about 2k of those having the categorical value of \"yes\"/1.",
        "created_utc": 1517878695,
        "upvote_ratio": ""
    },
    {
        "title": "Duan’s Smearing Factor: Square Root Transformation",
        "author": "Richard_Bolitho",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vj1hy/duans_smearing_factor_square_root_transformation/",
        "text": "Why, when using Duan’s smearing factor on square root transformations, does the mean of the predicted values exactly equal the actual sample mean. As opposed to the log transformation where the mean is different?",
        "created_utc": 1517873461,
        "upvote_ratio": ""
    },
    {
        "title": "Flight airplane status variable (early, on-time, late) - nominal or ordinal?",
        "author": "ProfessorJinxy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7viugv/flight_airplane_status_variable_early_ontime_late/",
        "text": "I have a contingency table with information of several cities and rather flights made it early, on-time, or late. I have read the definition and looked at examples for nominal vs. ordinal levels of data, but I keep going back and forth between what level of data the variable flight status is? Any thoughts would be appreciated. Thanks!",
        "created_utc": 1517871787,
        "upvote_ratio": ""
    },
    {
        "title": "Is statista a reliable source of statistics?",
        "author": "SpaceGhost1992",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vgxre/is_statista_a_reliable_source_of_statistics/",
        "text": "",
        "created_utc": 1517856547,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for advice on non degree, online courses",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vgxjj/looking_for_advice_on_non_degree_online_courses/",
        "text": "[deleted]",
        "created_utc": 1517856495,
        "upvote_ratio": ""
    },
    {
        "title": "Rules to create a population pyramid.",
        "author": "aleph_heideger",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vf8ao/rules_to_create_a_population_pyramid/",
        "text": "Hi all.\nOne quick question: today I was asked by one of my co-workers about what rules to follow when building a population pyramid: things such as age interval and whether to place female on the right of left of the y axis. The fact is that although I always heard that a population pyramid was built placing from left to right male - female I could not find a single reference about it online. Later on I will check some textbooks I have at home but could someone point me on the right track. Or is it just one of those \"rules\" that simply do not exist?",
        "created_utc": 1517842407,
        "upvote_ratio": ""
    },
    {
        "title": "Margin of error",
        "author": "ecumenist1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vel5f/margin_of_error/",
        "text": "If you assess a sample of a larger population, is the margin of error based on the size of the population or the number that tested positive in the sample? For example, if my entire population is 1 million, and my sample tests positive 50 percent of the time with a three percent margin of error, do I say that 500k test positive plus/minus 30k, or plus/minus 15k?",
        "created_utc": 1517835610,
        "upvote_ratio": ""
    },
    {
        "title": "I have some data on birds, wondering what tests would be best applied to it to come up with some interesting results.",
        "author": "throwawaythequays",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vckbb/i_have_some_data_on_birds_wondering_what_tests/",
        "text": "I have 90 recordings for three species of birds, from two different locations, and two types of habitats; urban and forest.\n\nFrom each recording I can use maximum frequncy, minimum frequency, length of song, number of 'elements' in the song, and any other aspect of a birdsong really. Just looking for any input on what tests might come up with interesting results.\n\nI've already determined that minimum frequency is higher in urban habitats than rural habitats. Is there any test that can combine multiple 'factors' of a birdsong and test them together?",
        "created_utc": 1517807671,
        "upvote_ratio": ""
    },
    {
        "title": "Help with determining type of variable",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vb07w/help_with_determining_type_of_variable/",
        "text": "[deleted]",
        "created_utc": 1517791452,
        "upvote_ratio": ""
    },
    {
        "title": "Combinatorics/probability question",
        "author": "mirsss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vari5/combinatoricsprobability_question/",
        "text": "For example, there is a shipment of 15 valves, 3 of which are defective. The assembly plant randomly selects 4 valves from this shipment. What is the probability that all four valves are defect-free?\n\nI calculated the total number of valves that could be chosen, so 15P4, and then I divided the number of valves that are not defective by that number, so 12/32760, which is 0.0003663. Why is this wrong? ",
        "created_utc": 1517789091,
        "upvote_ratio": ""
    },
    {
        "title": "Help finding a religion dataset",
        "author": "nahleaveit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7v7ptg/help_finding_a_religion_dataset/",
        "text": "Hi, I am looking for a dataset that includes both respondents religion and respondents partners religion. It must also be conducted multiple times over time. So far, attempts have not been fruitful. Census data mostly seems to exclude these. Please you're my last hope.",
        "created_utc": 1517762133,
        "upvote_ratio": ""
    },
    {
        "title": "Youtube Channel for learning Statistics from the ground up",
        "author": "75seconds4",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7v6s62/youtube_channel_for_learning_statistics_from_the/",
        "text": "Undergrad student here hoping to learn stats in depth before the semester starts. Already done with Calc 1,2 and halfway through 3. Can anybody suggest a youtube channel or any other resource for studying statistics? Preferably something that starts from the basics and builds up to graduate level stats",
        "created_utc": 1517751853,
        "upvote_ratio": ""
    },
    {
        "title": "Struggling regarding relationship between Poisson and binomial",
        "author": "ayeandone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7v4uel/struggling_regarding_relationship_between_poisson/",
        "text": "In college, I remember my professor describing that Poisson approximation of binomial distribution is used when the standard deviation is small. I don’t remember the exact reasoning for how small standard deviation leads to non normality though. Does the mean affect the distribution being non normal? Like the mean being small, coupled with the small standard deviation, doesn’t give the graph room to normalize or something like that, because binomial distribution doesn’t go below zero on the x-axis?\n \nRegarding when Poisson approximation is used I was a little confused too. On one hand I thought it was used for binomial type problems where the mean is small and p is small. On the other hand I’ve read things online that say Poisson and binomial problems are totally different, for example: ‘Poisson counts the number of occurrences in an interval given a certain average occurrence rate per interval. It can have values 0, 1, 2, 3, ... binomial counts the number of occurrences out of a fixed number N of possibilities, where any one occurrence happens with probability p.’.\n \nSo is the Poisson Approximation to the Binomial Distribution just used for binomial type problems where the mean and p are small, and the distribution is therefore called Poisson Distribution? How does this Poisson Approximation to the Binomial Distribution relate to the Poisson problems that look at the occurrence rate per interval? I guess I’m just confused about the relationship between the two because some people are saying Poisson and binomial are different things, and others are saying you can use Poisson to approximate binomial.\n \nFinally, when the mean and p is small, does n being large make the distribution even less normal, and therefore Poisson approximation should be used rather than binomial normal approximation?\n \n",
        "created_utc": 1517720927,
        "upvote_ratio": ""
    },
    {
        "title": "I feel like this should be very easy, how do i calculate the estimated slope coefficient?",
        "author": "hilljr",
        "url": "https://i.redd.it/l6qf6v6p34e01.jpg",
        "text": "",
        "created_utc": 1517713419,
        "upvote_ratio": ""
    },
    {
        "title": "Is this correct?",
        "author": "sluggedb0y",
        "url": "https://i.redd.it/z2ue4bahk3e01.jpg",
        "text": "",
        "created_utc": 1517706960,
        "upvote_ratio": ""
    },
    {
        "title": "What is the probability one specific person will be selected to be in your group of 5 from a class of 30?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7v1lzn/what_is_the_probability_one_specific_person_will/",
        "text": "[deleted]",
        "created_utc": 1517687451,
        "upvote_ratio": ""
    },
    {
        "title": "need a recommendation for a stats book",
        "author": "raspberry_swirl116",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7v1i3x/need_a_recommendation_for_a_stats_book/",
        "text": "I'm looking for a good stats books that focuses not on how to obtain a statistic but on how to interpret statistics and why certain methods were used.  Any recommendations?",
        "created_utc": 1517686487,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for some good examples of statistical modelling",
        "author": "Yaff",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7v0afw/looking_for_some_good_examples_of_statistical/",
        "text": "I have a background in machine learning, and lately I've been trying to learn a more disciplined approach to 'data science'.\n\nI'm going through All of Statistics by Larry Wasserman, which does a great job of explaining the theory, however I would like to learn more about how to conduct and report analyses in practice. It's easy to find examples of bad statistics, but I am looking for the exact opposite.\n\nWhat are some papers or technical reports you've seen which are particularly good in that respect, and are also accessible to non-experts? For example, papers that are particularly enlightening at the level of a beginning graduate student in statistics.\n\nI'm interested in regression, forecasting, causal inference, and feature engineering, but I'm open to anything. I'm also flexible about the specific field - econometrics, social sciences, medicine... Anything that shows good statistical / scientific practice.",
        "created_utc": 1517675406,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing Odds Ratio",
        "author": "Actual_Homo_Sapien",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uztuc/comparing_odds_ratio/",
        "text": "Is it reasonable to compare odds ratios from different studies and draw a general conclusion? For example with these made up numbers... \n\nStudy A showed that drug X had an effect with OR 2.5, 95%, CI 1.5-3.5 \n\nStudy B showed that drug Y had an effect with OR 1.5 95% CI 1.2- 2.1\n\nStudy C showed that drug Z had an effect with OR 4  95% CI 3.8-4.2\n\n\nAll studies had a p-value &lt;0.05, and lets say that they were of good quality and sufficiently powered. \n\nCan I make any conclusions about comparing the drugs without having to run a statistical analysis comparing the studies to eachother? \n",
        "created_utc": 1517670847,
        "upvote_ratio": ""
    },
    {
        "title": "How does having a small standard deviation cause the binomial distribution to not follow the normal curve?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uyfls/how_does_having_a_small_standard_deviation_cause/",
        "text": "[deleted]",
        "created_utc": 1517650397,
        "upvote_ratio": ""
    },
    {
        "title": "kruskal wallis or Friedman test",
        "author": "Ihavedreamyblueeyes",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uwsp1/kruskal_wallis_or_friedman_test/",
        "text": "Hi guys I have a question.\nIn my data every subject answered a question related to attitude towards a certain product (same product type, but different manufacturer). Since every study participant gave his opinion on all three products, should I use Kruskal wallis or Friedman test to test differences between these assessments. All questions were answered at the same time.\n\nVariables were measured on ordinal scale and that is why I am only considering nonparametric alternatives.",
        "created_utc": 1517627090,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate volumes of the cells within a Voronoi diagram, ideally using R?",
        "author": "The_Sodomeister",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uv4km/how_to_calculate_volumes_of_the_cells_within_a/",
        "text": "Hello all, I'm looking to use the volume of a vertex's Voronoi cell to assign a local density score to each point. \"deldir\" seems to be the standard library for Voronoi/Delaunay calculations, but it seems to only give me neighbor information. From the deldir documentation:\n\n&gt; Returns a data frame with 10 columns. The first 4 entries of each row are the coordinates of the endpoints of one the edges of a Dirichlet tile, in the order (x1,y1,x2,y2). The fifth and sixth entries, in the columns named ind1 and ind2, are the indices of the two points, in the set being triangulated, which are separated by that edge. The seventh and eighth entries, in the columns named bp1 and bp2 are logical values. The entry in column bp1 indicates whether the first endpoint of the corresponding edge of a Dirichlet tile is a boundary point (a point on the boundary of the rectangular window). Likewise for the entry in column bp2 and the second endpoint of the edge. The nineth and tenth entries, in columns named thirdv1 and thirdv2 are the indices of the third vertex of the Delaunay triangle whose circumcentre constitutes the corresponding vertex of the edge under consideration. (The other two vertices are indexed by the entries of columns ind1 and ind2.)\n\nIt seems like maybe I could get clever by recording the vertices of each polygon according to \"shared vertex\" criteria and then calculating the area of each polygon, but that'd probably be a real pain.\n\nIs there any way to do this conveniently?\n",
        "created_utc": 1517610525,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating confidence intervals with binary data",
        "author": "chupalegra",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uuddr/calculating_confidence_intervals_with_binary_data/",
        "text": "I carried out a test to see if our computer software correctly identified data on a pass/fail basis under a number of different scenarios. The software itself is being qualified, as we are looking at the old vs. new method of identification under those different scenarios. What is the best statistical method to check this data?\n\nI've googled around and found a binomial proportion confidence interval, however I'm not certain if it fits? Any advice would be helpful.",
        "created_utc": 1517604258,
        "upvote_ratio": ""
    },
    {
        "title": "How to perform a hypothesis test without population information.",
        "author": "MadSkillsMadison",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uuagm/how_to_perform_a_hypothesis_test_without/",
        "text": "I recently collected a sample of bird weights at my work, and I want to test some hypothesis on their average weight. However, reading through examples and info, I always get stuck because my books assume I already know population standard deviation and sometimes the population mean. \n\nWhat do I do if I don’t have this kind of information? Assume based on a large sample?",
        "created_utc": 1517603561,
        "upvote_ratio": ""
    },
    {
        "title": "Probability: how to calculate effectiveness/failure rate of using multiple birth control methods?",
        "author": "swill128",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7utrq7/probability_how_to_calculate_effectivenessfailure/",
        "text": "Let's say a guy is using a condom which is 85% effective. And the girl has an iud which is 98% effective. How would I calculate the probability of pregnancy?",
        "created_utc": 1517599370,
        "upvote_ratio": ""
    },
    {
        "title": "Survey Question",
        "author": "andrewhoohaa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7utikl/survey_question/",
        "text": "Hello everyone!\n\nI have a survey where respondents are broken up by region and  each region has a different number of respondents. I've calculated responses deemed to be bad and totaled them by region. How do I determine which region is worse? \n\nFor example:\neast (n=20) = 70 bad responses,\ngreat lakes (n=85) = 315 bad responses,\nMid Atlantic (n=95) = 361 bad responses,\nand Midwest (n=83) = 250 bad responses.\n\nHow do I tell which region is actually the problem area?  \n\nThanks for any and all help! ",
        "created_utc": 1517597304,
        "upvote_ratio": ""
    },
    {
        "title": "kruskal wallis or Friedman test",
        "author": "Ihavedreamyblueeyes",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7utbdv/kruskal_wallis_or_friedman_test/",
        "text": "Hi guys I have a question. \nIn my data every subject answered a question related to attitude towards a certain product (same product type, but different manufacturer). Since every study participant gave his opinion on all three products, should I use Kruskal wallis or Friedman test to test differences between these assessments. All questions were answered at the same time.\n\nVariables were measured on ordinal scale and that is why I am only considering nonparametric alternatives. ",
        "created_utc": 1517595703,
        "upvote_ratio": ""
    },
    {
        "title": "What test is this for out of model stability?",
        "author": "[deleted]",
        "url": "https://i.redd.it/1j71esgjftd01.jpg",
        "text": "[deleted]",
        "created_utc": 1517584227,
        "upvote_ratio": ""
    },
    {
        "title": "How do I account for a covariate in a regression analysis",
        "author": "YourFriendlySpidy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7urudi/how_do_i_account_for_a_covariate_in_a_regression/",
        "text": "I have two scale variables (independant and dependant) and a catagories scale that I think will have had a huge effects on my results. How do I analyse the relationship between my In and D while taking into account this catagories variable?\n\nEdit: I'm on SPSS if anyone has more specific advice to this program",
        "created_utc": 1517583654,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for Help Understanding Random Forest Output",
        "author": "ElizaEllipsis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uouet/looking_for_help_understanding_random_forest/",
        "text": "Hi,\n\nWhile I have a decent background with data, I am brand new to the statistics and data science part of it. I am hoping you can help shed some light on what I am trying to figure out now.  \n\nI am using a Random Forest model to try and predict who is going to exit our company. When I run it, the Confusion Matrix says that I have about 90% on the No (who will stay) and 82% on the Yes (who will exit). My understanding is that those are pretty good numbers.\n\nWhen I look at the scores given to each person and compare it to real data, what I am getting is less than I expect. For example, out of 20,000 people, only about 500 are given a Yes score of 50% and greater. Out of those 500 people, less than half (45%) actually left. In reality, closer to 2,000 people left the company.\n\nAre those numbers that the model is giving what I should really expect? If they are, and statistically it makes sense, that's fine. Then my next question is, how do I explain the (seemingly) poorly predicted numbers to my boss? That is, before I go back and earn a degree in statistics.\n\nThank you.",
        "created_utc": 1517545655,
        "upvote_ratio": ""
    },
    {
        "title": "Having some trouble answering a question using R. Any help would be appreciated",
        "author": "BeautyByBoyfriend",
        "url": "https://imgur.com/a/7gc1y",
        "text": "",
        "created_utc": 1517530001,
        "upvote_ratio": ""
    },
    {
        "title": "How to find correlation of two variables over time?",
        "author": "surferfeet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7un9vq/how_to_find_correlation_of_two_variables_over_time/",
        "text": "Hi,\nI have two datasets over time: Population Abundance and Ocean Temperature. I want to see if there is a correlation between ocean temperature and abundance over time, more specifically if temperature affects population abundance and if they are inversely related (e.g. as temp goes up at a given time, population goes down at that given time). How would I go about doing this statistical analysis? I'm not sure which statistical test I would use for this. I'm using RStudio.\n\nThank you for any help or suggestions",
        "created_utc": 1517529874,
        "upvote_ratio": ""
    },
    {
        "title": "Specifying the 'group' variable in exploded logit (rank ordered logit) with Stata",
        "author": "ar_604",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7umk62/specifying_the_group_variable_in_exploded_logit/",
        "text": "I also posted this on [stackoverflow] (https://stackoverflow.com/questions/48570121/specifying-the-group-variable-in-exploded-logit-rank-ordered-logit-with-stat). I'm new to exploded logit (rank-ordered logit) models. It seems straightforward enough to run the regression, but this is very much an instance of getting an output, when I don't know that I've written the code correctly. (I'd almost prefer to get an error!)\n\nFor a bit of context, these are the results of a questionnaire wherein individuals ranked characteristics of a service. agecat and sex are probably self-explanatory.\n\nUsing the rologit command, I specify the dependent variable, and the independent variables, then the group... but what should this group be?\n\n    rologit rank agecat sex, group (quest_num)\n\n\nMy question is, what should the 'group'. In this case, quest_num is the question number (I.e. the item that was ranked, 1-6). The idea is that results are grouped by everyone's ranking for each specific question - but I am not sure that this is correct?\n\nHere is the material I was using as a guide: https://www.stata.com/manuals13/rrologit.pdf",
        "created_utc": 1517523521,
        "upvote_ratio": ""
    },
    {
        "title": "Calculate probability of one population being higher than another?",
        "author": "rcianfar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ulxzr/calculate_probability_of_one_population_being/",
        "text": "I have two populations with 30 data points each:\n\n1. Mean = 11, Stdev = 5\n2. Mean = 10, Stdev = 3\n\nHow do I calculate the probability that one data point from population 1 will be greater/less than one data point from population 2?\n\nThanks",
        "created_utc": 1517518597,
        "upvote_ratio": ""
    },
    {
        "title": "Are there any guidelines to decide between using actual numbers vs. percent?",
        "author": "OriginalName317",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ulj75/are_there_any_guidelines_to_decide_between_using/",
        "text": "Sorry in advance if this isn't the right sub for this question.\n  \nI'm doing work in building financial reports, and this question keeps coming up. Real numbers or percent? I know the answer's contextual, but I don't have any rule set in my head to make it more than a case by case basis. Here's a pile of invoices, some are closed, some aren't. Should the report say 56% are closed, or of $170,000 worth of invoices, $70,000 are closed, etc.? How does one know which is the best way to describe the data?",
        "created_utc": 1517515353,
        "upvote_ratio": ""
    },
    {
        "title": "Need help replication figures and tables using eviews",
        "author": "Reagorn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ulb9b/need_help_replication_figures_and_tables_using/",
        "text": "Never used eviews and we're supposed to figure it out on our own, but I can't find any helpful resources. https://econpapers.repec.org/article/eeejimfin/v_3a16_3ay_3a1997_3ai_3a2_3ap_3a233-254.htm\n\n\nThats the paper. I need to replication Table 1-3,5, but no idea how to figure out how to make an autocorrelation table. I have the data that my teacher gave to me to use",
        "created_utc": 1517513604,
        "upvote_ratio": ""
    },
    {
        "title": "Excel calculates dates WRONG, so please be careful.",
        "author": "richard_sympson",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uk40z/excel_calculates_dates_wrong_so_please_be_careful/",
        "text": "I just discovered this today and I am flummoxed as to why it's the case.  According to Microsoft Office's [online support documentation](https://support.office.com/en-us/article/date-systems-in-excel-e7fe7167-48a9-4b96-bb53-5612a800b487), Excel has 2 ways of storing date objects: relative to a 1900 start date, and relative to a 1904 start date.\n\nThe online documentation says that the default for 2011 and 2016 Office for Mac (and also 2016 Office for Windows as my work computer uses) is the 1900 system, which according to Microsoft:\n\n&gt; In the 1900 date system, dates are calculated by using January 1, 1900, as a starting point. When you enter a date, it is converted into a serial number that represents the number of days elapsed since January 1, 1900.\n\nHowever, this is *wrong*.  The example they give, of July 05, 2011, does get stored as the number 40729 (like all programs, Excel stores dates and times as floating points; specifically, an integer increase in such a number in Excel corresponds to one day).  However, there are **NOT** 40729 days between July 5, 2011, and January 1, 1900.  There are 40727 days.  You can check that [here](https://calendarhome.com/calculate/days-between-2-dates), and with various other programs like R.\n\nThe Excel 1900 system is completely bonkers.  If you have the settings at the 1900 system, then Excel converts a plain \"0\" (zero) to the date \"1/0/1900\", which is of course *not a date*.  You cannot even convert a negative number to a date, the output is a string of #'s (this is normally an indicator that the cell is not wide enough to display the contents, but this is not the problem).  Excel's method of storing dates, as floating point days, effectively stores them as integer days from December 30, 1899.  When Excel converts a floating point 1 to a date, with this alleged 1900-01-01 origin, it displays 1900-01-01 itself.  When R would convert that number, it displays 1900-01-02.\n\nBUT WAIT THERE'S MORE.  Did you notice before that I said Excel is off by 2 days?  But, I just showed that R and Excel are off by only 1 day with the number \"1\" when using 1900-01-01 as an origin.  **That's because Excel thinks that 1900-02-29 is a day, when there was actually no leap year that year.**  1900 was not a leap year, there is no February 29 in 1900.\n\nI am not entirely sure if the 2004 system is correct for all of the problems I brought up here.  It does actually convert 0 to \"1904-01-01\" correctly.  But if you use Excel, and especially if you convert Excel files through to other programs, make sure you change your settings to the old 1904 system.",
        "created_utc": 1517504187,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to find Death by firearm statistics for the US",
        "author": "Fluffeh_Panda",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uk1nq/trying_to_find_death_by_firearm_statistics_for/",
        "text": "I'm trying to find statistics that break down gun deaths via suicide, self defense, illegal use, etc. I can't seem to find any good ones, all I see is articles with no citing.",
        "created_utc": 1517503646,
        "upvote_ratio": ""
    },
    {
        "title": "One multinomial logistic model versus two binary logistic models",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ujt6y/one_multinomial_logistic_model_versus_two_binary/",
        "text": "[deleted]",
        "created_utc": 1517501754,
        "upvote_ratio": ""
    },
    {
        "title": "Help on interpreting regression coefficients",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ujsfr/help_on_interpreting_regression_coefficients/",
        "text": "[deleted]",
        "created_utc": 1517501588,
        "upvote_ratio": ""
    },
    {
        "title": "Correct test for non-normal, related, unequal sized samples?",
        "author": "marsh_man_dan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ujmtr/correct_test_for_nonnormal_related_unequal_sized/",
        "text": "Hey All,\n\nI have two sets of data that I want to compare the mean (or median). The data is not normally distributed and not independent of one other (readings taken at different time periods from the same location). I wanted to use Wilcoxon signed rank test but that requires equal sample size, which I don't have. Is there an alternative test I could use? I also was thinking of taking a sub-sample from the larger sample to compare to the smaller one. Is this a good idea? How do I do this for non-normally distributed data? Thanks! ",
        "created_utc": 1517500310,
        "upvote_ratio": ""
    },
    {
        "title": "How to find out which part of normal distribution relates to each of the four types - True positives, False positive, False negative and False positive.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uii52/how_to_find_out_which_part_of_normal_distribution/",
        "text": "[deleted]",
        "created_utc": 1517489296,
        "upvote_ratio": ""
    },
    {
        "title": "Testing a localization algorithm",
        "author": "Gannebamm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ui04t/testing_a_localization_algorithm/",
        "text": "first some context:\nI currently do my master thesis (see researchgate link below for more info) in geoinformatics and try to use a new type of sound localization algorithm based on time difference of arrival (TDOA).\nMy hypothesis is that for the localization you could minimize the remaining STD to optimize the localization). I have created a small simulation for my algorithm which will place 5000 points between simulated sensors and the algorithm will try to localize the signals. The difference between signal position and localized position is stored as error_m (error in meters). The remaining STD is stored as localized_std.\n\nNow to my question with my educated guesses as a start:\nThe Shapiro tests show (?) that the data is not normally distributed and therefore a Spearman test should be performed (?). The scatterplot shows a heteroscedasticity for both variables, therefore, the following rule applies: the higher the remaining STD the lower the chance to correctly predict the remaining error (?).\n\nWhat is your opinion about the values? Are they good enough to prove the algorithm is working? I think they do (but I am biased :D).\n\nHere is the link to my data (csv) and some R code I have written to test it. The R code shows some output as comments and has the ggplot function for the referenced scatterplot:\nhttps://gist.github.com/gannebamm/82b61cb003125527245e35ef73d9767c\n\nMy master thesis project on researchgate:\nhttps://www.researchgate.net/project/ASSOS-Animal-Sound-Sensor-Observation-Service",
        "created_utc": 1517482963,
        "upvote_ratio": ""
    },
    {
        "title": "Is Wilcoxon Ranked Sum test the correct approach for this problem?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ug1o7/is_wilcoxon_ranked_sum_test_the_correct_approach/",
        "text": "[deleted]",
        "created_utc": 1517457677,
        "upvote_ratio": ""
    },
    {
        "title": "Boxplot help: Why might there be no maximum on the boxplot on the left?",
        "author": "throwawaythequays",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ufjld/boxplot_help_why_might_there_be_no_maximum_on_the/",
        "text": "Just made this boxplot using R studio and haven't encountered a boxplot without a min/max yet. Just wondering what this might mean?",
        "created_utc": 1517452783,
        "upvote_ratio": ""
    },
    {
        "title": "Need quick help on Applied Multivariate Analysis (interpret a question to put me on track)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ufepe/need_quick_help_on_applied_multivariate_analysis/",
        "text": "[deleted]",
        "created_utc": 1517451437,
        "upvote_ratio": ""
    },
    {
        "title": "Methods to deal with missing not at random values?",
        "author": "MaryToddLinkedIn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uezgd/methods_to_deal_with_missing_not_at_random_values/",
        "text": "I'm trying to estimate a couple models and would like to be able to control for a few variables (3 variables out of ~30-60, depending on the model) that have missing values for close to 90% of observations (but I have over 600,000 observations).\n\nThe variables are dummies but are only recorded if a 1 is reported, so I think this qualifies as MNAR (a 1 means it's a 1, a missing value means it could be a 0 or a 1, and no 0's are observed).\n\n1. Is there a term for missing data of the pattern I've described (only positive/affirmative values are non-missing) so I can more easily try to find and read up on possible fixes?\n\n2. If I'm not planning on using the variables for inference (just as controls, and even then just for robustness checks), is the % missing and # of observations fine for me to try to find a method to deal with, or should I just leave these variables out?\n\n3. I can potentially make (fairly reasonable) assumptions to fill in 0's and 1's for a lot of observations based on other information about each observation. Is this something to consider doing before multiply imputing the remaining missing values (or whatever method I could use, if it's still proper to do some sort of imputation)?\n\nAny help at all would be greatly appreciated",
        "created_utc": 1517447371,
        "upvote_ratio": ""
    },
    {
        "title": "Is there any way to derive the equation for a discontinuous function such as this one? I want to model wait time at a stoplight as a function of time",
        "author": "Lou_Dude929",
        "url": "https://i.imgur.com/Y2snfkY.jpg",
        "text": "",
        "created_utc": 1517446622,
        "upvote_ratio": ""
    },
    {
        "title": "Simple Linear Regression in R: Interpreting F-Statistic",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uej99/simple_linear_regression_in_r_interpreting/",
        "text": "[deleted]",
        "created_utc": 1517443309,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical Averaging - How would you summarize data that stems from a calculation?",
        "author": "Stryken101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ue451/statistical_averaging_how_would_you_summarize/",
        "text": "Hello everyone,\n\nI am seeking to understand how to represent calculations from sampling. I am not quite sure of the exact terminology. However, I will try my best to describe my question through a scenario.\n\nSuppose we are trying to find the mean BMI (Body Mass Index) for some population.\n\nWe sample for height and weight of random individuals and now its time to do our calculations. Would it be a more faithful representation of our data if we were to...\n\na) take the mean of the height and weight that we found and calculate the BMI from those numbers.\n\nb) take the BMI for every individual measurement and the take the mean of that result\n\nc) some other option I'm not thinking of\n\nThank you for your input\n\nEDIT: \n\nThe formula for BMI is...\nBMI = weight (kg) ÷ (height^2 (m2)) ",
        "created_utc": 1517439644,
        "upvote_ratio": ""
    },
    {
        "title": "ELI5 inhibition synergy",
        "author": "suchsmith",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7udbg9/eli5_inhibition_synergy/",
        "text": "It has to do with a dietary study involving two compounds, which each have some beneficial effect when used separately, but she wants to know if using them together has a synergistic effect going beyond the simple sum of the two effects. For example, one compound showed a 2% benefit in one test and the other showed a 3% benefit by itself. However, when used together the total benefit turned out to be 9%, more than the 5% benefit that might be expected if the two compounds made only independent contributions.\n\nSo she thinks that maybe she wants a statistical test that indicates synergy by rejecting the hypothesis that the contributions independently add together. She is wondering if this this just a \"simple\" Chi-squared test.\n\nTable 5 shows the results of just one such test. The two factors are called, EC and PB2. The first three lines show the results of using EC alone at three different concentrations. The next three lines show the results of using PB2 alone at three concentrations. The last three lines show the combined effects of EC and PB2 used together.\nhttps://imgur.com/5Tar9Xr\n\nUsing both EC and PB2 at low concentrations (63%). Increasing EC but not PB2 yields 66% for a 3% improvement. Increasing PB2 but not EC yields 65% for a 2% increase. But when both are increased together, the result is 72% for a 9% increase instead of just the 2% + 3% = 5% increase if the contributions were independent.\nhttps://imgur.com/H74uEjE\n",
        "created_utc": 1517433252,
        "upvote_ratio": ""
    },
    {
        "title": "What do you consider to be the most fundamental statistics inequality?",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ud4mb/what_do_you_consider_to_be_the_most_fundamental/",
        "text": "In analysis I'd probably say that the triangle inequality is the most important fundamental inequality. I guess the arithmetic, geometric and harmonic mean are also basic and useful. \n\nI'm wondering what there is of this nature related to statistics. \n\nPerhaps this is a daft question, just something that I was wondering about",
        "created_utc": 1517431735,
        "upvote_ratio": ""
    },
    {
        "title": "How should I calculate this probability? It looks like a multinomial distribution, but without reinsertion.",
        "author": "NeokratosRed",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ucngz/how_should_i_calculate_this_probability_it_looks/",
        "text": "So, I was trying to calculate the following probability:  \n\n- There are P people playing a game. \n- Each one chooses a number from 1 to K randomly.  \n(Each number has the same probability 1/K of being chosen)\n\nNow, what I need to find is this:  \n\n**\"Given a number N, what is the probability that nobody chooses N and at least two people choose every number below N?\"**  \n\nFor example:  \n\n- For **N=1** I just need that nobody chooses 1. \n- For **N=2** I need that nobody chooses 2 and at least two people choose 1.  \n- For **N=3** I need that nobody chooses 3, at least two people choose 1 and at least two people choose 2, and so on.  \n\nNote that the probability should be zero if the players are not enough, *i.e.* if P&lt;2(N-1).   \nI found the probabilities for N=1, N=2, N=3 and N=4 manually, but it became too hard after that.  \n\n**Where should I look to find a general formula for any N?**      \nI tried the Hypergeometric but I don't think it's correct.     \n\n",
        "created_utc": 1517428091,
        "upvote_ratio": ""
    },
    {
        "title": "looking for statistics on telecom customer service",
        "author": "hawkman22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uc7e7/looking_for_statistics_on_telecom_customer_service/",
        "text": "hello,\n\nI am looking for stats on customer service in Telecom (mobile services), preferably globally but can use any western nation if possible.\n\nhere is the question I'm trying to answer:\n-why do clients call customer service at their phone provider?\nif there was a breakdown, for example:\n-20% billing issues\n-30% tech support\n-20% changing plans\n-20% misc\n\nthanks!",
        "created_utc": 1517424620,
        "upvote_ratio": ""
    },
    {
        "title": "I need help selecting an anova type for a project (research question: How does contextual setting affect perceived annoyance)",
        "author": "sockwarrior",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uawdg/i_need_help_selecting_an_anova_type_for_a_project/",
        "text": "I am new to statistics but need to do some for a project. I will set out what my experiment is below and hopefully someone could point me in the right direction. \n\nI am testing to see how contextual setting changes annoyance levels in human subjects. \n\nMy survey will have three contextual settings and within each of those contextual settings will be three different types of noise. The question will be: 'With the context above in mind, please rate on the scale provided below how annoying or not you find the sound'\n\nThere are only three sounds each all appearing once in each of the three contexts. \n\nSo i have two independent variables (Context and Noise type) and i have the dependent variable of 'annoyance'.\n\nWhat would be the ideal way to analyse this data set? \n",
        "created_utc": 1517414473,
        "upvote_ratio": ""
    },
    {
        "title": "Read the community info page, still need help",
        "author": "jaketb193",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7uafjn/read_the_community_info_page_still_need_help/",
        "text": "Hi, looking for some basic advice for a retrospective medical study. \n\nPopulation X has the disease “X”. There are two subpopulations of X, “X1” and “X2”. X1 is roughly 85% of X, and X2 is 15%. Sometimes, testing of population X shows the binary finding “Y” (every member of population X has had this test). Also sometimes, people in the population X have the symptom “Z” (every member of X either does or does not have Z).\n\n1. I would like to show that members of population X with finding Y are more likely to be subpopulation X2 than X1.\n2. I would like to show that as the number of years a person has been in population X increases, the probability of having finding Y increases.\n3. I would also like to show that members of population X with finding Y are more likely to have symptom Z than members of population X without finding Y.\n\nWhat are the appropriate statistical tests for these three inquiries? How would I power a sample of X appropriately (power 0.9, alpha 0.05)?\n\nThank you very much in advance.",
        "created_utc": 1517410624,
        "upvote_ratio": ""
    },
    {
        "title": "Kaplan-Meier and confidence intervals",
        "author": "idontwanttodothis199",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u9yx3/kaplanmeier_and_confidence_intervals/",
        "text": "Is there a way to build Kaplan-Meier curves that also show the confidence interval curves on SPSS?\n\nI tried this but I can't get it to work:http://www-01.ibm.com/support/docview.wss?uid=swg21477074\n\nThanks!",
        "created_utc": 1517406315,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for statistics on party affiliation or political ideology by race AND income",
        "author": "778779",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u8nvo/looking_for_statistics_on_party_affiliation_or/",
        "text": "I want to find out whether the voting habits of rich [insert ethnic group here] differ from poor ones, and the extent to which they do.  If this is not the right subreddit to ask for help finding data, please tell me where to go because Google is not helping tonight.",
        "created_utc": 1517389688,
        "upvote_ratio": ""
    },
    {
        "title": "Finding the CDF of the Weibull Distribution",
        "author": "purple-2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u8cbj/finding_the_cdf_of_the_weibull_distribution/",
        "text": "$f(t|\\lambda, \\beta, \\theta)=\\lambda \\beta (t-\\theta)^(\\beta -1) e^(-\\lambda(t-\\theta)^\\beta)$\n\nI know I need to integrate it, I'm thinking use substitution where $u=t-\\theta$. I'm lost after that though.",
        "created_utc": 1517384945,
        "upvote_ratio": ""
    },
    {
        "title": "Interaction in an RCT",
        "author": "bruceli1113",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u7zqx/interaction_in_an_rct/",
        "text": "Hey everyone, new here and I'm not sure if this is the right place to post this but my PI asked me this and I wasn't sure about the answer. If we have a clinical trial testing for a drug (let's say A) vs. placebo effects on lowering glucose levels, diabetes can also be considered an important predictor. However, since we have randomization, the diabetes is uncorrelated with the treatment group of the patient. \n\nHow does the inclusion of diabetes in the model increase efficiency of finding the effect estimate? Also, is it important to look for interaction of diabetes and treatment in this case?\n\nDoes the answer for the first question have to do with the variance inflation factor being equal to 1 and minimizing the variance of beta?\n\nThanks in advance :) ",
        "created_utc": 1517380371,
        "upvote_ratio": ""
    },
    {
        "title": "Would this work as a rule of thumb when learning DV and IV?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/statistics/comments/7u7fze/would_this_work_as_a_rule_of_thumb_when_learning/",
        "text": "[deleted]",
        "created_utc": 1517378897,
        "upvote_ratio": ""
    },
    {
        "title": "Variance of 3 different outcomes(only 1 can happen at a time)",
        "author": "stacktoplease",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u7rpf/variance_of_3_different_outcomesonly_1_can_happen/",
        "text": "i have 3 possible outcomes\n1. stdv of 0 with probability 0.8\n2. stdv of 10 with probability 0.1\n3. stdv of 20 with probabilty 0.1\n\nOnly one can be true at a time, anyone can tell me how i can calculate the stdv of the entire experiment. ",
        "created_utc": 1517377715,
        "upvote_ratio": ""
    },
    {
        "title": "boxplot",
        "author": "mikeemice",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u6vyb/boxplot/",
        "text": "What does it mean when box plots have the same starting point? Meaning to say that if its vertical, the lower end of the rectangle is all at the same point but not the top? ",
        "created_utc": 1517369049,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating average time to wait before X occurs?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u5ixb/calculating_average_time_to_wait_before_x_occurs/",
        "text": "[deleted]",
        "created_utc": 1517356514,
        "upvote_ratio": ""
    },
    {
        "title": "Question regarding use of difference rule",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u50ze/question_regarding_use_of_difference_rule/",
        "text": "[deleted]",
        "created_utc": 1517352179,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating Raw Moments",
        "author": "astralbeast28",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u4t6o/calculating_raw_moments/",
        "text": "So I'm trying to understand the difference in the E[x^n] for the raw moment and how that relates to the mean. I know the first raw moment is the mean so E[x]=m1. The second raw moment should then just be E[x^2] but that should not be the same as the mean squared right? But I don't fully understand why. If I could get any help understanding that, it would be great! Thanks",
        "created_utc": 1517350377,
        "upvote_ratio": ""
    },
    {
        "title": "Question about Spearman's rank coefficient formulas",
        "author": "wntz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u4rn3/question_about_spearmans_rank_coefficient_formulas/",
        "text": "So I have a column of x's 680, 630, 640, 720, 670, 690, 660, 770, 700, 720, 720, 690, 850) and a column of y's (72, 77, 80, 85, 77, 90, 92, 98, 95, 90, 100, 106, 120). As there are some ties between those numbers, should I use the simplified formula or the full one? Tried to solve this with both and the results were different. Simplified came up to 0.6621 and the full one came up to 0.5682 (or maybe I just made some mistakes during my calculations). The full version is very time consuming too so should I use this one on a test? I'd appreciate any help because I'm not too good at statistics and I have a very important test tomorrow. \nPS. not sure if I expressed my problem correctly, English is not my first language.",
        "created_utc": 1517350017,
        "upvote_ratio": ""
    },
    {
        "title": "Help me understand the true odds of winning something here",
        "author": "Dame2Grow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u4iec/help_me_understand_the_true_odds_of_winning/",
        "text": "Hi, I was wondering if someone could help me understand something that I just don’t know how to work out as I don’t have any background whatsoever in statistics. \n\nWhat I’m trying to understand is what the true odds of say winning a scratchcard are when there are a lot of them in circulation, a defined % of winning cards overall and also a defined % of winning cards for each prize category. \n\nI’ll try and explain using an example from real life. There’s a scratchcard in the UK called the £1 million purple and on the Camelot website it states that there are 21,000,000 scratchcards of this type in circulation and that there is a 3.70 overall chance of winning a prize on each scratchcard. It says that the total value of prizes for the game represents 70.99% of the total face value of the scratchcards.  Above this information there is a graph showing each prize that you can win, starting from £5 and going up to £1 million and next to each prize it shows how many scratchcards of the 21,000,000 total have that prize amount in circulation and what the odds of winning that prize are. \n\nI understand that it says 1 in 10 for a card prize that has say 2.1 million cards in circulation out of the 21 million total cards as that is 10% of the amount of cards out there and so clearly you have a 1 in 10 chance of getting it. What I don’t understand however is how you would calculate the TRUE odds of getting each prize as even though it’s a 1 in 10 card, there are more prizes in circulation hence why there is a 1 in 3.70 overall chance of winning a prize. \n\nIf winning £5 is 1 in 10 because there’s 2.1 million cards with this prize in circulation \nWinning £10 is 1 in 20 because there’s 1.05 million cards with this prize in circulation \nWinning £20 is 1 in 40 because there’s 525,000 cards with this prize in circulation \n\n... as an example, what is the formula for understand what the true odds of getting each card are if in reality whenever you buy a card your odds of getting any win are 1 in 3.70? \n\nI’ve tried my best to explain that, MANY thanks to anyone who can help me out or even just point me in the right direction\n\n",
        "created_utc": 1517347950,
        "upvote_ratio": ""
    },
    {
        "title": "Univariate analysis of two groups",
        "author": "BuckTheBarbarian",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u3rwl/univariate_analysis_of_two_groups/",
        "text": "This is more of a general question I had when thinking about the gender pay gap. Usually people will refer to a multivariate analysis of such a concept that can better explain why there is a difference in wage. However, isn't the point of such a study - and others - to be able to determine the difference just from one variable? (In this case gender) I imagine that when taking a sufficiently big sample the other variables should more or less be equally distributed between both groups, which makes me think that ultimately it is not necessary to consider other factors since all of them even out in a large sample size. Am I looking at this the wrong way? ",
        "created_utc": 1517342071,
        "upvote_ratio": ""
    },
    {
        "title": "Vouch for stats arxiv access?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u3qxn/vouch_for_stats_arxiv_access/",
        "text": "[deleted]",
        "created_utc": 1517341841,
        "upvote_ratio": ""
    },
    {
        "title": "Scatter plot scoring",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u33kv/scatter_plot_scoring/",
        "text": "[deleted]",
        "created_utc": 1517336814,
        "upvote_ratio": ""
    },
    {
        "title": "What test can I use to determine causality in a neuron computer model where I can run arbitrary trials?",
        "author": "BayesMind",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u2wky/what_test_can_i_use_to_determine_causality_in_a/",
        "text": "I have neuron models that are connected and can excite or inhibit neurons they're connected to.\n\nSo, causality could either be excitation or inhibition.\n\nAlso since this is experimental (instead of observational) I can run any trials I need, and intuitively, if a statistical model could choose what trials to run, it seems like it would be better able to tease out the underlying causal structure.\n\nIs there anything like that, that can take advantage of choosing what trials would gain the biggest increase in confidence for the underlying causal model?\n\nI'm just diving into causality for the first time and need some guidance.",
        "created_utc": 1517335281,
        "upvote_ratio": ""
    },
    {
        "title": "Question regarding statistical analysis",
        "author": "StatsAI",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u2sl1/question_regarding_statistical_analysis/",
        "text": "Hello all.\n\nI have a question regarding statistical analysis and maybe you can help. \n\nSo I have a study design where participants are presented with two options in random order (let's say they have to choose between (1) and (2)). My research question is to determine if participants more often decide to (A) swap from 1 to 2; (B) swap from 2 to 1; (C) remain on 1 or (C) remain on 2. How can I compare this four conditions? This in the context of a game we asked participants to play, and not all participants played the same amount of obstacles (two obstacles they had to choose one: 1 or 2). Should i turn these numbers in percentages or ratios?\n\nPlease note, that I have a small sample (n=30).\n\n\nI would pretty much appreciate your help with this issue.\n\n",
        "created_utc": 1517334418,
        "upvote_ratio": ""
    },
    {
        "title": "Intraclass Correlation in MLM: How to interpret increase?",
        "author": "Arizona94",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u2phb/intraclass_correlation_in_mlm_how_to_interpret/",
        "text": "Hi all,\n\nLet me start by saying that I do not have a solid understanding of traditional MLM. I am working on an academic article in which MLM was required to analyze customer and employee data, so my knowledge of MLM has been based on its application with dyadic data rather than traditional use.\n\nI am using the two-intercept approach to estimate two equations simultaneously and then partitioning the variances for both dyad members. So far everything has made sense, but I am stumped with this final part and am hoping someone more knowledgeable can help me.\n\nCompared to the empty model, my final model has predictor variables and interactions. The sample size is sufficiently large and the chi-square test indicates significantly better fit. Overall, the total variances for both dyad members decreased by over 60%, but the ICC for customers increased. I was expecting the opposite to happen (introducing \"employee interpersonal adaptive behavior\" would explain a portion of the variance in customer and employee satisfaction ratings). So now I am not sure how to interpret this. [Results of empty model vs full model](https://imgur.com/a/lKD1c)\n\nAny suggestions would be most appreciated! Thanks!",
        "created_utc": 1517333736,
        "upvote_ratio": ""
    },
    {
        "title": "A/B test convoluted variable",
        "author": "lentebriesje",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u2kk9/ab_test_convoluted_variable/",
        "text": "I'm having a discussion with a colleague about a test case, but the last time i had any formal statistics trainig is years ago and I'm failing to find the words to convince my colleague. So I hope you can either work as a second opinion and tell me i'm silly, or help me to put it into words that can help get the message across. \n\nPlease give me some leeway in how i formulate this topic. I think it's highly likely i would butcher terminology (without intending to do so) as English is not my first language and within this area of expertise i'm afraid i might trip up easily. Nevertheless i hope my message will come across through sentences preceding or following any confusing areas.\n\nJust in case our formal education background can explain anything. Mine is in marketing/social sciences, hers is in biology. \n\nThe case. A digital newsletter with 6 topics, a plethora of potential interactions that could all influence eachother. \n\nA colleague who does translations tells me one of the topics is completely insensible to the group of people we want to reach. So she translated the original story and wrote something entirely different aswell. Something she thinks is more suited to our target demographic. I implemented it as an equal split A/B-test case. Part of translation is a sensitivity to various cultural backgrounds, so this is the case a translators opinion would overrule mine. I proposed to put it to the test, because it doesn't happen often we diverge from preconceived texts and I thought i'd be interesting to see if a significant result comes of it. \n\nSo this is the outline. The other colleague i disagree with has the following argument. The text has complete changed and therefore the test can't be good. The message changed, the phrasing, word choice etc. So if the result is different we won't know the exact variable that caused the result. \n\nMy view is this. Yes, we changed a lot of potential variables by completely uprooting the text. And sure, we won't know whether the phrasing caused it or not. But that's not what i want to measure.\n\nAnd it could also very well be that the both variations are a good option due to how different variables interact with eachother. I get that that's the weakness of changing more variables. I'm not sure if it's at all possible to tweak more than a single word, which wouldn't have much statistical power(?), to change just one variable. But that's a side-track.\n\nWhat i'm interested to see if how the original text compares to the gut-feeling of our translator and whether i should put more stock in her opinion for future reference or caution against it. At a higher level I hope to find out whether the feeling of our translator is on the money. I'm not as concerned about phrasing or word choice. \n\nSure, this A/B test in itself speaks merely to this single case because we changed too many variables. And this exact situation will never present itself again. But I don't think we're working in a vacuum where we can change only one variable. Contamination of factors is bound to happen and opportunities to test outside the newsletter might not extrapolate well to the newsletter. \n\nI think of this specific instance as a part in a larger series of such tests (should opportunities arise) to speak to the overal value of the cultural sensitivity layer our translators add in the process. \n\nNothing serious is at stake for anyone. Other than hurt feelings, maybe. It's clear to everyone involved that it's rather subjective what learnings we might take from it.\n\nThat's the scope/motivation/test framework behind my 'research'. And this summarizes the discussion i had with the colleague i disagree with. I'd like your second opinion. I feel i'm misunderstood and like to clarify my position, but don't know how. Or maybe i'm the one being mistaken. Please tell me! Does this make sense or is this a bad test?\n",
        "created_utc": 1517332674,
        "upvote_ratio": ""
    },
    {
        "title": "Anyway to fit it into an anova?",
        "author": "papalagirauscher",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u27l9/anyway_to_fit_it_into_an_anova/",
        "text": "Hey! \nFollowing problem, i testet some participants and am not able to fit it properly into an anova.\nGroups: \nWith help and additional help, with only help\nData: performance in a test\nProblem: The partipants are twins and therefore belong together and need to be analysed together. this means, that there are two “metagroups” (twin one and twin two). Those are either in the help or the help+additional group (one twin can be help, the other one help+additional group), but the performance data is from both twins together (they took the test together).\nIs there a way to fit this into an anova if I want to find out, if the factor help+ additional has a signifiacant impact on performance? Or do I have to Analyse every twin separately?\n\nThanks in advance!",
        "created_utc": 1517329848,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a test available in Excel to test for different Monthly averages before/ after a change?",
        "author": "Icecoldmotherfucker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u0rab/is_there_a_test_available_in_excel_to_test_for/",
        "text": "Greetings! i have average monthly crime rates, before and after the police gained access to a new penalty/ enforcement ability. a while after new penalty came in, another factor was introduced that could really effect things too, by dramatically increasing enforcement of any of those penalties enacted.\n\nTutor recommended to first test whether there was a significant difference between averages post penalty/pre new factor and post penalty/ post new factor to see if it effects average. Then a difference between means repeated measures test for averages/before after penalty introduction. What tests do you think should i be using?Really poor currently, Any i could do in excel?  i don't have a specialized statistics software.\n\nthanks",
        "created_utc": 1517316388,
        "upvote_ratio": ""
    },
    {
        "title": "Does it make any sense to multiply a random variable with a constant or a parameter?",
        "author": "statrowaway",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u0j39/does_it_make_any_sense_to_multiply_a_random/",
        "text": "Does it make sense to multiply a random variable with a parameter p or the constant 2 for instance?\n\nLets say X~geom(p)\n\nIs it possible to write pX ?  Now what exactly would this mean? The density function of pX would be the density of X multiplied by p?",
        "created_utc": 1517313629,
        "upvote_ratio": ""
    },
    {
        "title": "Which test or tests should I use?",
        "author": "jiancko41",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u05kp/which_test_or_tests_should_i_use/",
        "text": "\n\nSorry for my bad English. I have a questionnaire that has 10 questions that have answers like \"none\", \"few\", and \"a lot\". I don't want to test each question individually and see how many answered \"few\" to question number 6. What I want to do is to see that how many answered \"few\" to 10 questions. How should I do it? I'm using SPSS. \n",
        "created_utc": 1517308520,
        "upvote_ratio": ""
    },
    {
        "title": "Higher Order Moments",
        "author": "ThePengwyn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7u02ph/higher_order_moments/",
        "text": "Are there any higher order (standardised/centralised)moments or cumulants that account for multi-modal/multi-peaked distribution properties? The first 4 are all properties of a central mode/local maxima.\n\nThanks",
        "created_utc": 1517307368,
        "upvote_ratio": ""
    },
    {
        "title": "Has anyone used power bi?",
        "author": "swill128",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ty6o2/has_anyone_used_power_bi/",
        "text": "Wondering what you think of it.. how’s it work compared to similar programs? What can it do that excel can’t? Or access? Or other stats packages.  ",
        "created_utc": 1517283506,
        "upvote_ratio": ""
    },
    {
        "title": "Using an ordinal measure of consensus as a nominal measure of consensus",
        "author": "captainthomas",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ty4b8/using_an_ordinal_measure_of_consensus_as_a/",
        "text": "While trying to answer [this post](https://www.reddit.com/r/AskStatistics/comments/7tu5ul/spread_of_data/), I went down a bit of a rabbit hole of different ways of measuring consensus among judges/raters, and I came across [this paper](https://www.sciencedirect.com/science/article/pii/S0020025511000776).  One of the consensus measures proposed in the paper is one designed for ordinal ratings data in which consensus is defined as 1 minus the fraction of the maximum possible variance in ratings which the observed variance in ratings represents.  The idea behind it is that the variance is minimized when every judge's rating is the same, and when half the judges choose one extreme of the rating scale and half choose the other extreme, the variance is maximized.\n\n\nPut another way, if J judges use a Likert-type scale with ordered categorical options from 1 to k to rate something, the minimum variance of the ratings is 0, the maximum variance of the ratings is given by (k – 1)^(2)/4, and the consensus of the ratings is given by 1 – ((4\\*\\(observed variance of ratings))/(k – 1)^(2)).  This measure has the advantages of 1) being bounded by 0 and 1; 2) clearly corresponding to a property of the original data; and 3) approximately following a chi-squared distribution, making statistical inference from it easy (unlike, say, kappa).\n\n\nThat would seem to work just fine for ordinal data.  My question is whether it could also be made to work for inherently unordered nominal data like that in the post that led me to it.  Let's say you have three nominal categories, A, B, and C, and ask J judges to pick one of them.  You could then just say A = 1, B = 2, and C = 3, and use that ordering to calculate the consensus measure defined above.  But unless all J picks are evenly distributed across the three categories, that measure's value will change depending on which nominal categories are mapped onto the extremes of the scale.  What, then, if you calculate the value of the consensus measure for every possible permutation of mapping the categories A-C onto the numbers 1-3 and take the mean value of this measure across all of them?  Is there a sense in which this mean value would represent an overall consensus regardless of the order of the categories?  Would the properties that make the consensus measure above attractive for ordinal data be preserved?  Am I re-treading ground that someone else has already covered far more elegantly?",
        "created_utc": 1517282885,
        "upvote_ratio": ""
    },
    {
        "title": "Regression Model for Count Data?",
        "author": "mysidopsis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7txsxr/regression_model_for_count_data/",
        "text": "Hello everyone,\n\nApologies if this is a basic question, because it seems to me it should be yet I haven't been able to find a clear answer. \n\nI'm trying to test my data to determine if changing different variables changes the dependent variable. In this case, I'm trying to determine if the dependent variable, 'number of successes' (count data), is actually dependent on a number of different factors, or if these factors don't impact the 'number of successes' at all. The various factors that I'm testing are all count data as well. \n\nObviously, if these things are correlated the data points should look more or less like a straight line on a graph. If they're not, it'll just look like a random cloud of data points.\n\nSince they're real world data, the points kind of look like they make a straight line, but it's hard to tell just by looking so... Stats. \n\nI feel like like I'm going a bit crazy - this really can't be this hard! Should I just be using an ordinary least squares regression although the data is all count data?\n\nThank you so much.",
        "created_utc": 1517279761,
        "upvote_ratio": ""
    },
    {
        "title": "Organizing data over a long period of time for ~100 countries",
        "author": "sozialwissenschaft97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7twx9d/organizing_data_over_a_long_period_of_time_for/",
        "text": "I'm currently conducting research on the relationship between democratization and globalization since 1970. My sample is ~100 countries. My ultimate goal is to perform some type of regression analysis. How do I organize the data for this for so many years, though? I'm not sure if this is too vague a question, so please let me know if you need more details. Thanks.",
        "created_utc": 1517271441,
        "upvote_ratio": ""
    },
    {
        "title": "Reporting confidence intervals in APA style",
        "author": "muffin80r",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7twrs3/reporting_confidence_intervals_in_apa_style/",
        "text": " \n\n \n\nHi all! I am trying to figure out the best way to report some statistics following the APA style guide and wondered if I could ask 2 questions.\n\n \n\n \n\nSay I have confidence intervals for a proportion in 2 groups of n=500. Group 1 has an outcome of 10%, CI +/- 2.6% and group 2 has an outcome of 20% +/- 3.5%. The APA style blog here http://blog.apastyle.org/apastyle/2010/06/formatting-statistics-using-brackets.html says you report a CI in the format 95% CI [lower limit, upper limit].\n\n \n\nQuestion 1: Would I just use percentages as my units in the above format, so for group 1 I would say 95% CI [7.4%, 12.6%]?\n\n \n\nQuestion 2: To me it makes a lot more sense to just report the difference in outcomes between groups and report the interval for the difference eg “There was a difference of 10% in the outcome of interest, 95% CI [5.6%, 14.4%]”. Is this approach supported anywhere in the APA recommendations or any other sources beyond common sense I could use to justify this approach? I can’t find anything in my style guide but I only have the 4th ed :/\n\n",
        "created_utc": 1517270149,
        "upvote_ratio": ""
    },
    {
        "title": "Sphericity Help..",
        "author": "HorseWizard",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tvtqq/sphericity_help/",
        "text": "BACKGROUND ON MY EXPERIMENT :\nA one-way repeated measures ANOVA within-participants experimental design was used to determine whether the impact of change blindness varies with the type of change presented.\nThere were three experimental conditions (congruent, incongruent and within category) and one control condition (no change). The control condition was built into the experiment to help rule out alternate explanations of the experimental results. The dependent variable was the reaction time of the participants when they identified a change in the scene and was recorded electronically. All stimuli were presented in a random order so as to prevent bias.\n\nQUESTION\nMy SIG came back as .000 so I presume this result means that sphericity is not signiciant so I have to use greenhouse figures ...\n\nMy Greenhouse Geisser came back as .907 (what the hell does that mean?)",
        "created_utc": 1517262133,
        "upvote_ratio": ""
    },
    {
        "title": "Formatting a 3d model in R",
        "author": "bowman9",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tu8k7/formatting_a_3d_model_in_r/",
        "text": "I am attempting to construct a 3d model in r with a quadratic plane running through my points. Here is my script so far: \n    library(car)\n    library(rgl)\n\n    nn &lt;- my_stuff_yo$`poop1`\n    amb &lt;- my_stuff_yo$`poop2`\n    inc &lt;- my_stuff_yo$`poop3`\n    scatter3d(x=nn,y=inc,z=amb,fit=\"quadratic\",xlab=\"poop1\",ylab=\"poop2\",zlab=\"poop3\",point.col=\"black\",axis.col=c(\"black\",\"black\",\"black\"),axis.scales=TRUE,\n    axis.ticks=TRUE,text.col=\"black\",surface.col=\"gray48\",grid.col=\"gray0\")\n\nSo I have a few problems with the design of the graph. First, I do not want the points to show up, I just want to see the plane. Second, I want to reverse the direction of the axes of my two independent variables. Finally, the axes are showing some pretty silly numbers that make no sense, and I want them to be whole numbers (they are showing 3.25, 5.5, 7.75, etc., but I want them to be 3, 4, 5, 6, 7...). Any help on achieving this would be greatly appreciated!\n    ",
        "created_utc": 1517249526,
        "upvote_ratio": ""
    },
    {
        "title": "Spread of data",
        "author": "nahtazu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tu5ul/spread_of_data/",
        "text": "I run a Grammy Pool at my company [results here, names removed](https://docs.google.com/spreadsheets/d/1ziKV9J-GwzqMfIH9e4-6fJK5wef-R4SL95vUPI9YzlE/edit?usp=sharing) and I'm trying to figure out which category had the widest \"spread\" - i.e. the least consensus among participants on who was going to win. Two questions:\n\n\na) How would I determine that statistically? I assume it's standard deviation but I'm not sure how to approach it.\n\n\nb) [I have a spreadsheet with everyone's guesses for each category](https://docs.google.com/spreadsheets/d/1ziKV9J-GwzqMfIH9e4-6fJK5wef-R4SL95vUPI9YzlE/edit?usp=sharing). How would I process the data to determine the \"spread\"?\n\n\nThank you!",
        "created_utc": 1517248947,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating Effect Size or Sample Size in Levene's Test",
        "author": "mr-datascientist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tt5wj/calculating_effect_size_or_sample_size_in_levenes/",
        "text": "**What I usually do**\n\nI use the \"Power &amp; Sample Size estimator\" in Minitab to help me through my problem. I've only had 2 samples so far, so there exists the tool, documentation and formulae for 2-sample t-test or test of equal variances for 2 samples.\n\nhttps://imgur.com/a/g19Vu\n\n**My challenge now**\n\nI'm running a test of equal variances for k-samples. I'm using Levene's test, which works fine for me. But, how do I estimate the power-samplesize-effectsize for this test? ",
        "created_utc": 1517240939,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics Homework",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tsu4g/statistics_homework/",
        "text": "[deleted]",
        "created_utc": 1517238034,
        "upvote_ratio": ""
    },
    {
        "title": "Adding variables together for an indexvalue",
        "author": "Hakizu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tssid/adding_variables_together_for_an_indexvalue/",
        "text": "Hey guys,\n\ni have a quick question\nI want to create something like an variable which shows me the value for a sphere.\nFor example i want to measure the sphere of welfare outcomes goals.\nOutcomes goals are going to be measured by the Items\n\n “Do you agree or disagree that social benefits and services in [country] prevent widespread poverty?” \n\nand\n\n\"Do you agree or disagree that social benefits and services in [country] lead to a more equal society?”\n\nThey use the same scale (1 to 5), is it doable to add both items together and use the new values as an indicator for the \"sphere\" they are referring to? \n\nThanks in advance",
        "created_utc": 1517237628,
        "upvote_ratio": ""
    },
    {
        "title": "Quick Math Stats Example Explanation",
        "author": "shortstufdan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tqimi/quick_math_stats_example_explanation/",
        "text": "So, I am taking mathematical statistics 2 at UCONN and one of the slides had a question as follows: \"Suppose you have a sample of nine independent observations Y1, Y2, ... , Y9 from a normal population with mean 1 and variance 2. What is the distribution of (sigma i=1 to 9 ((Yi-1)/2))/3?\" The answer is N (0,1). Another problem, though, is the same question except the summation is divided by 9 and the answer is N (0,1/9). I think i understand why this one makes sense because Z has a standard normal distribution but that is divided by 9. With this logic, however, the first problem should be N (0,1/3) as Z still has a standard normal distribution which is divided by 3. I am very new to statistics and feel uncomfortable with it, so any clarification of this would be extremely helpful and appreciated! Thanks!",
        "created_utc": 1517208222,
        "upvote_ratio": ""
    },
    {
        "title": "calculate probability of multiple variables",
        "author": "loudlyroundly",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tpt3k/calculate_probability_of_multiple_variables/",
        "text": "I have the following problem which i would think would be a pretty common type of challenge: \n\nIf I have 100 purchasers with a 66% chance of buying a $50 item (otherwise the item is $20) and 65% chance of buying 2 items.\n\nWhat is the revenue?\n\nSo the revenue depends on what are the odds of the prices ($20 or $50) for the 65% chance of buying 2 items. \n\nCan anyone help? Thank you, Lawrence",
        "created_utc": 1517199932,
        "upvote_ratio": ""
    },
    {
        "title": "Please help - AP Statistics Z scores question due in 3 hours",
        "author": "katnissjul",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tp0z7/please_help_ap_statistics_z_scores_question_due/",
        "text": "I don’t know if this is the right place to ask this question but r/homeworkhelp isn’t helping and this is urgent, so I’m gonna ask here.\n\nI keep getting questions that require me to “use the standard distribution table or similar means to find the z-score such that the area to the right of z is 0.2(or some other number)”\n\nI can’t figure out how to do this on my TI-84 calculator. I thought that I could go into normalcdf and put 0.3 as my lower value, but apparently that’s not right because the answer is 0.52 and I got 0.38. \n\nWhat am I doing wrong???? \n",
        "created_utc": 1517191609,
        "upvote_ratio": ""
    },
    {
        "title": "GLM help",
        "author": "D5rektbw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tnit1/glm_help/",
        "text": "Hi, \n\nI’m fairly new to stats and just wanted to ask if what I’ve done makes any sense or if I’ve done something completely random.\n\nLong story short we looked at jellyfish pulse rates in various light conditions (natural light, natural dark, artificial light and artificial dark).\n\nWe predicted/wanted to find out if light of any kind triggered faster pulse rates in comparison to dark states.\n\nI’ve split my data into 3 headings - 1. Natural/Artificial 2. Light/Dark and 3. Number of pulses.\n\nFrom here I used an two way anova test visually represented on a box plot. \n\nWas that at all correct?\n\nThanks,\n\nA hopeless undergraduate",
        "created_utc": 1517177293,
        "upvote_ratio": ""
    },
    {
        "title": "Correct regression test in medical research",
        "author": "Confused_Medic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tnd41/correct_regression_test_in_medical_research/",
        "text": "Hi there,\n\nI'm a current ob/gyn resident and have been tasked with reviewing some of our center's data for research.\n\nWe're looking at the incidence of a specific birth trauma in mothers delivering in our unit, and trying to work out any risk factors that would perhaps help us predict this better in the future.\n\nI've looked at some of the research in the literature and most big studies seem to refer to \"multivariate regression\" in their methodology section. I've had a look at the UCLA page and it tells me that for my data I should be using a Factorial Logistic Regression. Pardon my ignorance, but is this the same thing?\n\nMy data has a single DV, either the woman has the trauma or doesn't. The IVs are all categorical, though are ordered mostly. For example, we have grouped ages into 20-24, 25-29, etc. Similarly we have grouped BMI. I have data on approx 12000 women, with about 2% of those having the outcome of interest (birth trauma). For IVs I have somewhere between 10-12, though I must check again.\n\nI've been playing around in R and have been using the following formula, which has spat out some significant results. Though that may mean exactly nothing!\n\n    glm(Birth_Trauma ~ Baby_Weight + Mother_BMI, data = mumData, family = binomial())\n\nThe question I'm trying to answer is whether having a certain characteristic(s) is a risk factor for a certain outcome. \n\nI do have access to a statistician, though they are really only on a very brief consulting basis, and I'd like to have an idea of where to go before talking to her.\n\nAny help or guidance would be great. Let me know if I can clear anything up?",
        "created_utc": 1517175875,
        "upvote_ratio": ""
    },
    {
        "title": "Transform then parametric test OR non-parametric test?",
        "author": "ParlyWhites",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tmybz/transform_then_parametric_test_or_nonparametric/",
        "text": "I am in the second week of a non-parametric regression course, and I am still trying to wrap my head around what factors lead someone to choose to perform a non-parametric test or transform their data and then perform a standard regression. For example, it is completely kosher to perform a log transformation on skewed data and then enter the transformed data into a standard regression, but it is also reasonable to perform a non parametric test instead on the original data. At what point does one decide to go with one over the other? Thanks, friends! ",
        "created_utc": 1517172202,
        "upvote_ratio": ""
    },
    {
        "title": "Determining the negative predictive value of a new test",
        "author": "Talia_Centaurette",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tmaj8/determining_the_negative_predictive_value_of_a/",
        "text": "I had a question regarding a new algorithm that processes MRI scans for the presence of infection and which sites of the body are infected.\n\nI am assessing the negative predictive value of the algorithm in detecting infections (i.e. if the algorithm comes back as negative in 100 cases, how many of these are truly negative?) The output of the particular algorithm in question has to be interpreted by a radiologist, hence it's accuracy also depends on correct interpretation.\n\nI've got a set of patients with proven infections and another group without proven infections, these will be age and sex matched. These cases have been retrospectively collected, hence the diagnosis has already been confirmed using gold-standard testing and the patients have already been treated (or not, depending on if they had infections).\n\nI'll process their images using the new algorithm, collect output, anonymize and randomize said output and give them to 2 senior radiologists (who are familiar with the new algorithm) to correctly interpret. These radiologists will determine if the patient had an infection, and which sites of the body were infected. The interpreted results are then collected and processed to determine the negative predictive value of the new test.\n\nI have some questions about this process: 1. What would you call this method? 2. Is it single-blinded or double-blinded? 3. How big should my sample size be to make this a statistically significant test? 4. How would I correlate between the two radiologists? 5. How do I calculate the negative predictive value?\n\nPlease note that my question is not about diagnostic tests or MRIs (I wouldn't put it up on this forum if it were). My question is about how to assess the negative predictive value of a test - it could be anything with a negative/positive result. It's a purely statistics-based question. Thank you for your help.",
        "created_utc": 1517166579,
        "upvote_ratio": ""
    }
]