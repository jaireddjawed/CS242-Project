[
    {
        "title": "Which test is suitable to compare between these 2 process?",
        "author": "nfya06",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11e1rc6/which_test_is_suitable_to_compare_between_these_2/",
        "text": "Hi,\n\nI'm currently doing some data analysis to compare 2 different process which is manual strip process and auto-layering machine process. I took percentage of scrap to compare these 2 processes. My spec is 0.6. Does Anova or F-test or T-test is suitable to show whether it is significant different on these 2 processes?",
        "created_utc": 1677576224,
        "upvote_ratio": 1.0
    },
    {
        "title": "Initializing k-means with Ward's method/HCA",
        "author": "_throw_hawaii",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11e0kri/initializing_kmeans_with_wards_methodhca/",
        "text": "Hello! Do you know any paper or theory book dealing with the technique I mentioned in the title? I'm just finding quite superficial stuff on this topic, but I am curious to read something more detailed.. Thank you in advance!",
        "created_utc": 1677571583,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help analyzing survey responses",
        "author": "thingsishouldkn0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dyuhj/help_analyzing_survey_responses/",
        "text": "Hi everyone, I am wondering how to analyze answers to a multiple-choice question.\n\nFor example, the total n= 130. 25 people chose answer choice A, 15 people chose answer choice B, and the rest chose others. \nDid answer choice A outperform answer choice B at a statistically significant level? Am I even using the term correctly? \n\nAnswer choices were randomized",
        "created_utc": 1677565193,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Bias Variance tradeoff for Machine Learning (Need help ASAP)",
        "author": "pmorri",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dx6gx/q_bias_variance_tradeoff_for_machine_learning/",
        "text": "I have an interview for a data science internship in two days, I need insight on thoroughly explaining the bias variance tradeoff for machine learning,  because I'm sure they will want a rigorous explanation, also any insight into probable questions they might ask would be great!",
        "created_utc": 1677559692,
        "upvote_ratio": 1.0
    },
    {
        "title": "is hierarchical ordinary least squares regression the same as hierarchical regression?",
        "author": "greenyouth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dvek5/is_hierarchical_ordinary_least_squares_regression/",
        "text": "Trying to understand what terms I can use in my paper. I conducted hierarchical regression where I added the IVs in blocks",
        "created_utc": 1677554228,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need data to take to my boss about PTO.",
        "author": "AndThenThereWasLily",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dv1jb/i_need_data_to_take_to_my_boss_about_pto/",
        "text": "Hello! I work in a healthcare setting where there are 19 employees who do my job. About 15 years ago, there were only 10. At that time, my department was limited to having only 2 employees off at a time, due to coverage issues and patient care. My management still wants to hold us to the “2 off at a time” rule but I’d like to show them how unreasonable that is due to our employee numbers increasing. I just became director of my department and I’ve struggling with PTO requests due to the number of people asking off the same days.\n\nFor example, 15 years ago, they allowed 20% of the employee group off. Now, they’re only allowing 10%. I understand how to say that, lol. \n\nWhat I’m hoping to accomplish, is something along the lines of how there are 52 Mondays and 52 Fridays in a year, so there are 104 days the majority of people want off. Given that there are 19 of us…how can I show the percentage of overlap, or maybe the odds of people wanting the same day off? I can’t quite put into words what I’m looking to show (otherwise maybe I could figure it out on my own?). If anyone has any data I could use in my favor, as well as any argument backed up by statistics, I would be so appreciative. Thanks!",
        "created_utc": 1677553208,
        "upvote_ratio": 1.0
    },
    {
        "title": "Controlling for effort in a binomial GLM/GAM",
        "author": "AwwwFiddlesticks",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11duzf3/controlling_for_effort_in_a_binomial_glmgam/",
        "text": "I am modelling species presence against a number of environmental variables to investigate whether these variables have an influence on species' presence. Species were recorded on an acoustic recorder and presence and absence in a clip was given as a 1 and 0, respectively. Each species is being modelled separately with the same environmental variables.\n\nSince species presence was recorded as a 1 or 0, I have chosen to use a binomial error distribution. The problem I am currently working on is that not all recordings were the same duration. The recorder was duty cycled, resulting alternating 5 minute and 1 minute clips. If I was using a Poisson or other count error distribution, I could use an offset term to control for unequal effort. However, I am unsure how to do this for a binomial error distribution as I have read that offsets can only be used this way for count error distributions. \n\nDoes anyone have recommendations on how to account for the variable duration of clips in my model? I have struggled to find concrete answers for this online and in the literature and would appreciate any tips. Thanks!",
        "created_utc": 1677553037,
        "upvote_ratio": 1.0
    },
    {
        "title": "Please help I have a question. If a school says : a score that is greater than one standard deviation below the class mean. If class mean is 77 and the Sd is 8, what you will be the passing grade? Thanks in advance",
        "author": "Outside_Plenty_5136",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dsfp0/please_help_i_have_a_question_if_a_school_says_a/",
        "text": "",
        "created_utc": 1677545966,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting Chi-squared goodness of fit test statistic (calculated using python scipy)",
        "author": "Sid_2001",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dr18q/interpreting_chisquared_goodness_of_fit_test/",
        "text": "Hi, \n\nI have plotted the daily returns of the S&amp;P 500 since 1975 as a histogram with 250 bins. I have then attempted to find the best distribution to fit to the data. This is a fairly standard exercise. I don't want to show my actual plot but it looks very similar to this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/975jbsmkgtka1.png?width=600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b7f85bdd77770929ad6ff827dc5f5815172564f0\n\nThe normal distribution actually fits better to my data than it does on the graph above, but the point of this exercise is to demonstrate that a lévy-stable distribution (alpha = 1.2) fits better than a normal distribution and a Cauchy/Lorentzian distribution. In my case, this is true. Visually speaking, the lévy-stable distribution is better at matching the fat-tails of the histogram than the Cauchy/Lorentzian is, and far better than the normal distribution is. In order to justify this, I have also performed chi-squared goodness of fit tests, with the observed values being those of the histogram, and the expected values being that of the distribution (normal or Cauchy/Lorentzian or Lévy-stable).   \n\n\nIn order to do this, I used  `scipy.stats.chisquare()` .  Below is how I implemented it\n\n    def Gaussian(x,mu,sigma,A):\n        return A*np.exp(-0.5 * ((x-mu)/sigma)**2)\n    \n    \n    # define a Lorentzian function\n    def Lorentzian(x, x0, gamma, A):\n        return A * gamma**2/(gamma**2+( x - x0 )**2)\n    \n    # define a lévy-stable function\n    def levy(x, alpha, beta, gamma, delta):\n        return levy_stable.pdf(x, alpha, beta, gamma, delta)\n    # Gaussian fit\n    gopt, gcov = curve_fit(Gaussian, bincentres, counts, p0 = initial_guess)\n    \n    # Lorentzian fit\n    lopt, lcov = curve_fit(Lorentzian, bincentres, counts, p0 = initial_guess)\n    #chi-square goodness of fit\n    g_exp = Gaussian(bincentres, gopt[0], gopt[1], gopt[2])\n    g_chi2 = chisquare(counts, np.sum(counts)/np.sum(g_exp) * g_exp, ddof = len(bincentres)-len(initial_guess))\n    \n    l_exp = Lorentzian(bincentres, lopt[0], lopt[1], lopt[2])\n    l_chi2 = chisquare(counts, np.sum(counts)/np.sum(l_exp) * l_exp, ddof = len(bincentres)-len(initial_guess))\n    \n    s_exp = levy(bincentres, 1.2, 0, 0, 1/200)\n    s_chi2 = chisquare(counts, np.sum(counts)/np.sum(s_exp) * s_exp, ddof = len(bincentres)-len(initial_guess))\n\n`levy()` and `curve_fit()` also come from scipy. Now when I print the results:\n\n    The chi-square test statistic for the Gaussian fit is 1.6553454357828934e+221 \n    The chi-square p-value for the Gaussian fit is 0.0 \n    The chi-square test statistic for the Lorentzian fit is 79.84675426206937 \n    The chi-square p-value for the Lorentzian fit is 4.58667124884552e-18 \n    The chi-square test statistic for the Lévy-Stable fit is 40.74788374241689 \n    The chi-square p-value for the Lévy-Stable fit is 1.418108534509982e-09\n\nI'm a bit confused on how to interpret these results. First of all, I am pretty sure the Gaussian chi-square test statistic is unfeasibly high, though visually speaking the fit looks like its trying to match the histogram. I think the values for Lorentzian and Lévy-stable are more reasonable, and as I understand it, the lower chi-square statistic does imply it fits better. How then do I interpret the p-values? Also, I suppose you can't see my input data, so maybe it's not possible to know, but any ideas why my Gaussian test statistic is so high (probably incorrectly) would be much appreciated too.\n\nMany thanks",
        "created_utc": 1677542220,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q][D] Probability of choosing 4 tiles all at once vs. one at a time",
        "author": "jabberwock91",
        "url": "/r/statistics/comments/11dprua/qd_probability_of_choosing_4_tiles_all_at_once_vs/",
        "text": "",
        "created_utc": 1677539136,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] How to interpret regression coefficient of 0 with a significant p-value? And small marginal R2 with much larger conditional R2?",
        "author": "permanenthouseguest",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dksqd/q_how_to_interpret_regression_coefficient_of_0/",
        "text": "Background: I ran a mixed effects model to see if goal domain (categorical variable with 5 levels) predicts goal pursuit (numerical value). The fixed effects included not just goal domain but also time, because it was a longitudinal study and I have to control for time (though to be honest I'm not 100% sure what that means). The random effects included participant since there were 67 of them. \n\nThe model output shows that the p-value for time is significant, but the regression coefficient is 0, so I'm having trouble interpreting this. \n\nAlso, the marginal R2 is 1% while the conditional R2 is 53%. My understanding is, this means the model explains 53% of the variation in the data when accounting for the random effects (of participant) and fixed effects (of goal domain and time), but is unable to explain much of the variation when considering only the fixed effects. This could indicate that the random effects of participant are playing a significant role in explaining the variation in the data. Would this interpretation be correct?\n\nWould greatly appreciate any help, thanks so much!",
        "created_utc": 1677526981,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can P-Values be used in the Hedge's g formula for a meta-analysis?",
        "author": "arkansas0428",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dhtt8/can_pvalues_be_used_in_the_hedges_g_formula_for_a/",
        "text": "I have the P values from 12 studies that measure the statistical significance of the effect of food labels on consumer food choices. I came across the Hedges g to measure the effect size, so I converted the p-values (e.g. p&gt;0.05 -&gt; 20 ot p&gt;0.01 -&gt; 100) so that it reflects that larger numbers = larger significance, to input into the Hedge's g formula. I got hedges g = 0.79.\n\nCan someone please advise what number I should use instead of the p value or if there is a different way to use the p-values for a meta analysis? Or does my method make sense? Thank you",
        "created_utc": 1677519867,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to Compare Two Means of Beta Distributions",
        "author": "mp48",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dhm3z/how_to_compare_two_means_of_beta_distributions/",
        "text": "Is there any test or systematic way to do this? I have two Beta Distributions and want to test if the means are different by a statistically significant amount.",
        "created_utc": 1677519329,
        "upvote_ratio": 1.0
    },
    {
        "title": "Research help",
        "author": "Genteseth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dhg3z/research_help/",
        "text": "Hey,\n\nI am not the most well informed in statistics. I'm doing research that I need help with for statistical analysis. I have 2 groups of 2 juvenile macaques and will be comparing response to mirrors. Group 1 receives an occluded (filmed) mirror so they cannot clearly see their reflection. Group 2 receives a normal mirror. We will code behaviors associated with social behaviors and self-directed behavior (using the mirror to investigate themselves). They will then receive the other mirror and coded the same way. We will do 5 trials. They hypothesis is that with more exposure, self directed behavior will increase and social behaviors will decrease. What is the best way to test and show statistical significance?",
        "created_utc": 1677518933,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trying to evaluate correlation between a binary variable and a continuous variable",
        "author": "Knox4075",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dhbs0/trying_to_evaluate_correlation_between_a_binary/",
        "text": "Hi everyone. I'm working on an independent research project, and I'm trying to figure out how to determine which of my proposed models best explains the variation in my data. I took one stats class a few years ago, so I'm a bit out of my depth here.\n\nSo I have this discrete variable (from survey data) of how competitive people think their state will be in an upcoming election. There are around 3600 responses, and each is either 0 (competitive) or 1 (uncompetitive). I have developed a few \"indices of competition\" from past election results. For example, some average the vote margins of all statewide elections that have been held in the past n years. This gives each state a unique index value between 0 (most competitive) and 1 (least competitive). So now we have around 3600 (x,y) points for each index, with the x being the respondent's state's index value and the y being their 0 or 1 competitiveness response.\n\nI'm trying to figure out which index is most correlated with people's perceptions of how competitive their state is. If the index is good, the more competitive the state is (the closer x is to 0), the more the y values for respondents in that state would skew towards 0. If the index is bad, the distribution of y values within the state would have little dependence on x.",
        "created_utc": 1677518646,
        "upvote_ratio": 1.0
    },
    {
        "title": "trying to figure out how much harder it is for one group to reach success vs another",
        "author": "Odd_Safe_5585",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dfo2q/trying_to_figure_out_how_much_harder_it_is_for/",
        "text": "I swear I'm not a student looking for homework help lmao- I'm a business owner doing market research and I need a way to describe a disparity I'm seeing. I would be really grateful if someone could help me out--\n\nTo simplify the situation, we'll say we have a group of 1000 people looking for jobs. 500 are red and 500 are green. \n\n100 are hired. 88 are red, and 12 are green. How much more difficult is it to get a job as a green? Or, how much of an advantage do the red have? Thank you!",
        "created_utc": 1677514531,
        "upvote_ratio": 1.0
    },
    {
        "title": "What type of statistics are most relevant to present with preliminary low-n data?",
        "author": "thequantumlady",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11det9o/what_type_of_statistics_are_most_relevant_to/",
        "text": "I'm a UX researcher and I've been tasked with sharing some results from my latest project that is not yet complete (not even close, really). My study is collecting data to compare a \"score\" on a particular task for two distinct groups (beginners vs. experts) where I expect a discrepancy, but I've only been able to schedule n=11 for only one of the groups so far.\n\nNot sure that it matters, but in this case, a low score is better. The way the score is calculated is based on some literature for my niche topic, but the population in that study was extremely skewed low for various reasons, meaning the score ranges they suggested to describe proficiency on this task are essentially irrelevant. As a result, I'm trying to use the data I collected to estimate my own proficiency ranges that seem appropriate for the population I'm testing. I have plenty of qualitative data I've collected (full AV recordings during data collection, my personal notes as facilitator).\n\nI'll admit, I'm a bit rusty on stats. So what I have so far is basic summary statistics (mean, median, min, max, SD) and I've calculated T-scores, all of which are within 2 SD of this small sample. Not sure if there's any purpose in plotting T-score vs. actual score (figured it would simultaneously show all scores are withing 2 SD), but I did.\n\nPart of my analysis is trying to cluster these scores to describe different groups within the data. I did this kind of haphazardly by inspection, in part because it's technically 1D data and I can already visually see some groups forming that match up with the qualitative observations I made of my subjects during the tests.\n\nWhat else should I keep in mind for an analysis like this?",
        "created_utc": 1677512400,
        "upvote_ratio": 1.0
    },
    {
        "title": "Linear mixed effects random intercept question",
        "author": "DysphoriaGML",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ddohs/linear_mixed_effects_random_intercept_question/",
        "text": "Dear all, I am trying to understand the effect of adding a random intercept to my specific model, my toy data has 2 nested factors, plus one regressor.\n\nTo give you an actual example:\n\nI have 5 researchers each of them doing 10 measures of the same protein but with 10 different tests, and I want to determine the effect of a second protein on the first but this has been measured once by each researcher.\n\nSo for each researcher (R), I have 10 measures from different tests (T) of the same continuous variable (YP). Furthermore, I have as regressor a continuous variable (XP) for each person (F1).\n\nso the lmer model will be:\n\nY \\~ X + (1|R) + (1|T) + (1|T:R)\n\nso that *(1|R)* is the random intercept of each researcher,  *(1|T)* is the random intercept for each measure coming from the same test independently from the researcher. While (1|T:R) is the random intercept of the measures of each researcher.\n\nBut my question is: does it make sense to include *(1|R)*? adding it, or not, changes the results massively. It is correct to assume each researcher will have a bias in the measurements,but isn't *(1|T:R)* already accounting for it eventually? Which random structure makes the most of sense for you? \n\n&amp;#x200B;\n\nThanks",
        "created_utc": 1677509506,
        "upvote_ratio": 1.0
    },
    {
        "title": "Fisher's exact test versus RR's 95% CI",
        "author": "SiMoon_MD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dcucs/fishers_exact_test_versus_rrs_95_ci/",
        "text": "Hi everyone! I have a question.\nThere's compared 2 groups for 2 level non-quantative variable (2*2 table). I used the FET (min. cell quantity &gt; 5) for comprising cose groups are quite small (18 vs 30). I got p = 0,032, but Risk Ratio's 95% CI is between 0.99 and 2.9. How can i interpret this? I can't just get significant Risk ratio (effect), or there is not significant difference at all?",
        "created_utc": 1677507269,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] Confusion between multivariable and multivariate analysis",
        "author": "krjpharmD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11dcjvh/question_confusion_between_multivariable_and/",
        "text": "I am a non-science faculty member, and I never had training in statistics. I am doing a project looking at underrepresented minority status as a predictor of pharmacy student residency match rates. I did logistic regression in SPSS, but I honestly can't tell if I should call that multivariable or multivariant logistic regression. I know this might be an elementary question, but thanks for your time.",
        "created_utc": 1677506492,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Help] Understanding shape of the curve SPSS Mann Whitney U test",
        "author": "ExtremeAlps1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11d214w/help_understanding_shape_of_the_curve_spss_mann/",
        "text": " Hi, I need some help, one of the assumptions of the Man Whitney u test is determining whether the independent variables have a similar shape, they mostly determine this by eyeballing it. Is there a better way of dealing with this without visually eyeballing it? Can I determine it based on the raw values of skewness of kurtosis (I am using SPSS).",
        "created_utc": 1677470163,
        "upvote_ratio": 1.0
    },
    {
        "title": "need basic help - due in 2 days",
        "author": "lingling40000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11czkda/need_basic_help_due_in_2_days/",
        "text": "hi, a statistics noob here (explain it like i’m five). I have to use revman to do a meta analysis forest plots for RCTs and non-randomised controlled trials.\n\ni’ve also been trying to see what is the best way to present results of non-controlled pretest-posttest studies via forest plot. I found out online that the inverse variance method is good for that. please correct me if i’m wrong.\n\nalso, is cohen’s d = SMD?  some psychology studies report d-value and t-values while revman only has fields for SMD and Z and p value. can I check which should I fit into what? i’ve opened the revman calculator and it wants raw data like mean and n and sd of control and expt groups but I don’t have control groups in pretest-posttest designed  studies. what can I do then?\n\nthanks in advance :) really tearing all my hair out due to statistics here",
        "created_utc": 1677462840,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help Interpreting inverted hazard ratio",
        "author": "goodhood1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cyyt9/help_interpreting_inverted_hazard_ratio/",
        "text": "To my understanding I can say that if I have a hazard ratio of 0.71 that I have a 29% risk reduction in treatment A versus treatment B meaning that I have a 29% lower chance of developing the bad outcome in treatment A versus treatment B. Mow would I be correct in this assumption if I were to invert the hazard ratio giving me a value of 1.41 and saying that now treatment B give me a 41% higher chance, or risk of developing the bad outcome compared to treatment? \n\nThese values are examples to help me understand inverted hazard ratios, because when they start using the word compare I get confuse. This is the rationale I came up with. Please give me any insight that would help me clear this up. Thank you.",
        "created_utc": 1677461125,
        "upvote_ratio": 1.0
    },
    {
        "title": "how to compare the order of keys in multiple dictionaries?",
        "author": "B4Zuk4",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cwcfl/how_to_compare_the_order_of_keys_in_multiple/",
        "text": "I am new in statistics. I work mostly with spatial data. I have a large number of dictionaries (10-30 observations each) that i need to compare. My hypothesis is that some share similar sequence of keys (total number of keys/categories 24). I need to learn how to extract the ones with most similar sequencs of keys. Then compare their values. I am using R. Any packages you recommend or methods for this ?\nThanks in advance.",
        "created_utc": 1677454212,
        "upvote_ratio": 1.0
    },
    {
        "title": "Am I really learning statistics if I’m having to look up answers and formulas often for help?",
        "author": "Adopted_Jaguar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cv6m7/am_i_really_learning_statistics_if_im_having_to/",
        "text": "\nI’m in an online statistics class in a data science master’s. I don’t have a lot of background in statistics. I feel like I’m tracking along with all of the reading and coursework. But when it comes to the questions on quizzes and tests, I feel like I always need help to get started on answer.\n\nOh, is this normal? I feel like I am either cheating (not real cheating, but cheating myself) or not really learning what I need to learn.  \n\nAny advice on learning statistics specifically for data science?\n\nAm I alone in this?\n\nThanks!",
        "created_utc": 1677451397,
        "upvote_ratio": 1.0
    },
    {
        "title": "Biology. Bar graphs. When we have p value associated with like three of the bars in a bar graph versus two bars in a bar graph, what does that mean?",
        "author": "wallpapersdance",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ctvvx/biology_bar_graphs_when_we_have_p_value/",
        "text": "Ex: some kind of p value generated from three experimental numbers? versus a p value generated from two experimental numbers?\n\n&amp;#x200B;\n\nI see this in biology alot where there is a bar graph with multiple brackets on top of the graph, with the brackets sometimes encompassing two or maybe three of the bars of the bar graph (and p values associated with each bracket) and am not sure what I'm looking at.",
        "created_utc": 1677448355,
        "upvote_ratio": 1.0
    },
    {
        "title": "A game offers a 40% chance to get a new character if you watch an ad. I’ve watched 25 ads to try to get a new character. I was unsuccessful ALL 25 TIMES. What are the odds of not getting it once out of every single try?",
        "author": "Conscious_Ad_8678",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11csfuc/a_game_offers_a_40_chance_to_get_a_new_character/",
        "text": "",
        "created_utc": 1677444990,
        "upvote_ratio": 1.0
    },
    {
        "title": "Demographic data analysis",
        "author": "ramzee155",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cs6tt/demographic_data_analysis/",
        "text": "Hi everybody,\nI have some data on age, income, gender and race with respect to a study on energy bar consumption. Which statistical analysis test/method should I employ to understand the data better? I’m not a statistic expert or anything of that sort… I just need a basic idea of any simple tests that will help me in analyzing the demographic data I’ve collected. \nThanks!!",
        "created_utc": 1677444414,
        "upvote_ratio": 1.0
    },
    {
        "title": "Greater than 100% Margin of Error - What Does that Mean?",
        "author": "J0E_Blow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cr4w6/greater_than_100_margin_of_error_what_does_that/",
        "text": "[Margin of Error  \\&gt; 100&amp;#37;?](https://preview.redd.it/s74gm2ks8lka1.png?width=820&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f09772e3c327378f3df12a966e9b6ce3dd5137a7)\n\nHi all,\n\nI was looking at [immigration patterns across the United States](https://flowsmapper.geo.census.gov/map.html#) because I'm considering moving to Colorado and am not sure if that's a wise idea or not. I noticed for the some data the margin of error is greater than 100%. As I understand/remember it margin of error relates to confidence or accuracy. So for example 10% margin of error indicates that the collector of data is 90% confident of the data's accuracy. ^((a lone neuron is firing and trying to remind me that there's something called confidence intervals)) So 100% would be they have no confidence in the data - I.E it's wrong. What the hell is a ***1758%*** margin of error?\n\n&gt;\"We're ***really*** ***REALLY*** sure this data is wrong*!!!*\"",
        "created_utc": 1677441953,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can something with a low chance be created into a high chance if given enough influential variables?",
        "author": "humbledaur",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cq8qe/can_something_with_a_low_chance_be_created_into_a/",
        "text": "Meaning, let’s say most student athletes don’t get into professional league because of there being a 30% chance of succeeding.\n\nHowever, if you practice day and night, you would be then closer to being a part of that 30% chance, correct? Or no? So, the 30% would then be more of an 70-80% chance for us individually, if we practiced very hard?\n\nOr, you want to start a business, but know that 70% of businesses fail. If you commit yourself 100% to your business, you can increase your chance by a lot to be a part of that 30% that succeed?\n\nAt the same time, I understand that nothing in life is guaranteed, even if we commit ourselves 100%. We can do everything right and still fail. However, the difference could be if we are persistent, we can keep creating new opportunities by constantly trying. Again, even after trying over and over, even if nothing is guaranteed, could we then conclude that there is a strong chance that success can happen if, not only we commit ourselves 100%, but also keep trying over and over (creating more times to commit ourselves 100%)?",
        "created_utc": 1677439812,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bayesian Statistics and the Gambler's Fallacy",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cosos/bayesian_statistics_and_the_gamblers_fallacy/",
        "text": "I am reading up on Bayesian statistical methodology. I am curious if through the Bayesian framework a gambler at a casino can finally attribute \"due-ness\" to the next spin of the roulette wheel? I.e. can a gambler say that black is \"due\" after a long run of reds. Obviously this is seen as silly in a frequentist framework because all spins are independent of on another, however a Bayesian approach seems to allow subjective thoughts of the next spin into a mathematical formula through use of a prior. So my question is can we avoid the gambler's fallacy and say the black is \"due\" if we are being Bayesian?? Any thoughts or opinions on this are appreciated.",
        "created_utc": 1677436346,
        "upvote_ratio": 1.0
    },
    {
        "title": "LSE vs UCL - MSc Statistics",
        "author": "No_Canary_5299",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cohhj/lse_vs_ucl_msc_statistics/",
        "text": "Hi all, want to know which of these schools is preferable for MSc in Statistics.\n\nUCL or LSE?",
        "created_utc": 1677435601,
        "upvote_ratio": 1.0
    },
    {
        "title": "Assist a noob",
        "author": "Stunning-Mention-641",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cnq78/assist_a_noob/",
        "text": "I am working on a project which might lead to nowhere but wanna see it through. It is a stock market analysis project. Ultimately it is for selling call options and determining the ideal out of the money strike price to select to minimize the likelihood of assignment. The project's statistical component is that I want to analyze the monthly returns of the S&amp;P 500 index.\n\nMy rough analysis of the last 23 years of data shows an average performance of positive 0.62% There is great variance in performance (bear: dotcom bubble burst, '08 financial crisis, covid collapse / bull: covid recovery, FAANG dominance) with the best month returning 12.8% and the worst month returning negative 16.8%\n\nI'm not a mathematics mind however, and Id like some help. I think that the monthly return would be labeled a 'continuous random variable' in stats terms. I'd like to create a probability density function to give insight into expected levels of monthly returns. \n\nAnyone interested in assisting? Also soliciting insight into making this a robust study...I can further discuss the project with someone interested.",
        "created_utc": 1677433860,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help - Data set for ESG relevent articles",
        "author": "CertainAd4053",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cmzt4/help_data_set_for_esg_relevent_articles/",
        "text": "Hi all, I want to work on a statisitcal analysis for my Bachelors Thesis. Since I have never worked emperically, I have no experience finding suitable datasets. I found some costly databases (7k or more).  We do have a Refinitiv Access at our university, but they dont seem to have the necessary data (as far as I know, I would love to be proven wrong on this). To be exact, I would need some kind of data set that tracks either news about ESG performance or articles/news related to it.\n\n&amp;#x200B;\n\nWould be very thankful for all kinds of hints where to look :)",
        "created_utc": 1677432309,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to interpret the probability of getting a tail in 6th round, which first 5 is also tail?",
        "author": "SuggestionStraight86",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11clx3s/how_to_interpret_the_probability_of_getting_a/",
        "text": "For cases like there is already 5 tails in a row, wt's the probably of getting a tail again in the 6th round?\n\nIs it 0.5 or 0.56?\n\nI know it is an independent event so it is 0.5 chance and using conditional probability I can also get 0.5 but as a human I am more into the probability of having tail again is really small (0.56)\n\nSo is it like thinking from 2 perspective? one is considering the current toss only and one is thinking about the whole series(6 tosses) together. Which one should be better applied to real life usage? Such as the dice games in casino / lottery, I always think ok if its having 3 dices with values less than 10 for 5 times already, the next round should hv a result more than 10\n\nI know the similar questions are very likely been asked before, please free feel to redirect me to the answered thread.",
        "created_utc": 1677429967,
        "upvote_ratio": 1.0
    },
    {
        "title": "Regression estimates validation",
        "author": "vvustR",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11civat/regression_estimates_validation/",
        "text": " How one can validate and, more specifically, cross-validate regression coefficients and their standard errors (for obtaining p-values)? As I understand, such technics are usually used for showing robustness of predictive power of a model. I want to exploit it for showing robustness of sign and significant of regression coefficients.\n\nLet’s say, I have logistic regression on panel data (country-year). Can I just fit regressions without each unique country ( \"leave one group out\" cross validation) and take mean from the coefficients and their SE for p-value calculation? I don’t think so because of unknown distribution of b/se…",
        "created_utc": 1677422559,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which test(s) for comparing 2 DVs against 5 IVs",
        "author": "rugby_14",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cgfop/which_tests_for_comparing_2_dvs_against_5_ivs/",
        "text": "Hi everyone,\n\nI'm a little bit rusty on this sort of thing since it's been a long time since I've had to pick a statistical tests so I am after some advice, if I may. I've had a read of the [recommended link](https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/) in the sidebar and a biomeasurement book but had some follow up questions. \n\nI'd be most grateful if you can offer some advice and guidance. \n\n  \n\n  \n# Background\nAnatomical study, 25 subjects. Analyses being done in SPSS.\n\n\n# Variables\n\n|**Dependant**|**Independent**|\n:--|:--|\n|Anatomical Status (Nominal: Yes/No)|Width (Continuous; assume *normal distribution as per Kolmogorov-Smirnov Test*)|\n|Visual Status (Nominal: Yes/No)|Shape (Ordinal: 1/2/3)|\n||Age (Continuous; assume *normal distribution as per Kolmogorov-Smirnov Test*)|\n||Gender (Nominal: M/F)|\n||Storage (Nominal: A/B)|\n||Type (Nominal: A/B/C/D)|\n||Side (Nominal: Right/Left)|\n\n# Questions\n* Is there a significant difference between the two dependant variables i.e. degree of agreement between Anatomical vs Visual status?\n* For each dependant variable, is there a significant difference between it and the independent variables in turn?\n\nThank you very much in advance :)",
        "created_utc": 1677416152,
        "upvote_ratio": 1.0
    },
    {
        "title": "I'm trying to calculate an estimated value of a mean of a population based on a sample and can't seem to do it, please help",
        "author": "iwasborninmybirthday",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11cficm/im_trying_to_calculate_an_estimated_value_of_a/",
        "text": "I don't understand statsitic and probably never will :(  \nI have a home work that goes like this:\n\nthe follwoing values of a certain active substance were taking from a sample of a medicament:  \n8, 79, 78, 79, 77, 82, 74, 78, 83, 76, 86, 79, 86, 69, 80 \\[mg\\]  \n\n\nhow can we measure the mean of the whole population of the produced medicament?\n\nI did the t.test in R and got a 95% confidence interval and of where the mean could be, is that the mean of the whole population? and how to caculate the standard deviation of the whole population also based on this sample?\n\nThanks in advance.",
        "created_utc": 1677413396,
        "upvote_ratio": 1.0
    },
    {
        "title": "Path smoothing algorithms",
        "author": "Ocumura",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11c3eo2/path_smoothing_algorithms/",
        "text": "Hey everyone, I'm currently sorting through one of the UI/Ux studies for the university. Now I am faced with the problem of what anti-aliasing methods are used to smooth the movement of the cursor and where to find the code for them. Since I just decided to deal with technologies to increase the accessibility of the PC, mathematics and programming are difficult.\n\nThe next step is to capture the new cursor path in the experiment.",
        "created_utc": 1677379733,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking to grasp geometric understanding of formulas",
        "author": "Critical_Loss2234",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11c297i/looking_to_grasp_geometric_understanding_of/",
        "text": "Hi! I started self-studying stats and probability a few months ago. So far I understand the concepts and can apply them on exercises but I noticed I don't really know what the formulas *mean*.\n\nBoth the material I'm reading and videos I've found all present the formulas as if their meaning and where they come from is all immediately obvious. No explanation of the formulas whatsoever. \n\nFor example, atm I'm looking at the correlation coefficient formula and asking where do all the sqrt and squares come from. But this applies to every single formula I've encountered.\n\nWithout a graphical/geometrical understanding I find it difficult to remember them and feel my understanding to quite shallow. Could you recommend some resource that focuses on this aspect of stats formulas?\n\nThanks!",
        "created_utc": 1677376319,
        "upvote_ratio": 1.0
    },
    {
        "title": "how to find number of classes?",
        "author": "c8ten_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11byf3c/how_to_find_number_of_classes/",
        "text": "my homework is asking the boundaries, midpoint and width of 46-50. i’m not sure how to figure out the boundaries or width or even the number of classes. any help would be greatly appreciated!",
        "created_utc": 1677365878,
        "upvote_ratio": 1.0
    },
    {
        "title": "Checking normality with several variables",
        "author": "Front_Union_7556",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11by4of/checking_normality_with_several_variables/",
        "text": "Relatively new to medical statistics.\n\nResearch summary:\n2 Non-randomised groups, one gets a medical device &amp; standard care and the other group is only standard care.\n\nDevice arm only have 61 patients and the control arm has 208.\n\nI'm checking if there is any baseline characteristics between the groups but checking for normality first in each group (checking weight, BMI, renal function, blood pressure). Some of the variables have p-value &gt;0.05 in wilks and others have &lt;0.05.\n\nWas wondering could i just use non-parametric analysis to compare the two groups' characteristics and also use non-parametric analysis when I get to the outcome analysis?",
        "created_utc": 1677365152,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] I'm struggling to understand how, in an categorical multiple regression, adding in a continuous variable can change the beta p-values.",
        "author": "bennettsaucyman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bv8dd/q_im_struggling_to_understand_how_in_an/",
        "text": "So I ran a regression with an outcome variable (CSI) and a categorical predictor variable (RT) than has two levels (A and B). I made A my baseline variable, and found that the difference between A and B was significant p = .001.   \nCSI \\~ RT\n\nThen I added in a continuous predictor variable (ACI), without an interaction.   \nCSI \\~ RT + ACI  \nI made A my baseline variable again, and found that the difference between A and B was significant p = .003.   \n\n\nMy understanding is that if you have a categorical variable as a predictor in a regression, it's an ANOVA, and so basically the 'slopes' of A and B are flat (they're just the means). But when I add in a continuous predictor (ACI), all of the slopes go from being flat to being whatever ACI is. What I'm confused about is that, since there is no interaction, the slopes are all the same. If the slopes are all the same, then the distance between each group (A and B) is the same at every level of the continuous predictor variable (ACI), including the means of the groups. Which means that the means did not change, and so the difference between the means in the first model (CSI \\~ RT) are the same distance apart as in the second model (CSI \\~ RT + ACI). So what could be changing the p-value of the difference between the betas of A and B if the distance between the means didn't change? Did adding in ACI alter the way that the model partitioned the variance or something? Does adding in ACI change the value of the standard error of the beta coefficients??",
        "created_utc": 1677357881,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sub group analysis for a meta-analysis in R - Error",
        "author": "Civil_Rhubarb7065",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bu7g8/sub_group_analysis_for_a_metaanalysis_in_r_error/",
        "text": "Dear community,  \nI'm trying to run a sub group analysis for a meta-analysis by using the metapsyTools package in R. In this package the code for running subgroup analysis is the following:\n\nsubgroupAnalysis(.model,\n\n...,\n\n.which.run = .model$[which.run](https://which.run)\\[1\\],\n\n.round.digits = 2,\n\n.nnt.cer = NULL,\n\n.tau.common = FALSE,\n\n.html = TRUE)\n\nWhen I enter information correctly for the analysis I run, I get this error:\n\nError in \\`purrr::map()\\`:\n\nℹ In index: 1.\n\nCaused by error:\n\n! Argument 'common' must be a logical.\n\nDoes anyone know how I can overcome this problem?",
        "created_utc": 1677355375,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best Quantitative Software?",
        "author": "schultzy828",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bt0wd/best_quantitative_software/",
        "text": "Preparing to run some data for a quantitative study in behavioral health. What software do you recommend to run the data? User-friendliness would be preferred. MAXQDA, SPSS, Excel? Any feedback would be helpful - thank you.",
        "created_utc": 1677352458,
        "upvote_ratio": 1.0
    },
    {
        "title": "Youth Risk Behavior Survey missing data",
        "author": "GamerDoc82",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bt0v8/youth_risk_behavior_survey_missing_data/",
        "text": "Has anyone worked with this particular dataset? I have managed to open the file in SPSS by using the syntax that CDC provides with the file. However, there are some variables that have no responses (not just missing data, but literally no response). The cell is blank; it doesn't even have the decimal for sys-missing.\n\nDid I do something wrong or misunderstanding? It seems odd that out of such a huge sample, that there would be some variables that have zero responses.",
        "created_utc": 1677352456,
        "upvote_ratio": 1.0
    },
    {
        "title": "Stats test for 1 descrete DV and 4 IV?",
        "author": "islaisla",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bsyzr/stats_test_for_1_descrete_dv_and_4_iv/",
        "text": "Hello! I have been reading the same things for hours so I'm stuck! I recently found out my deadline for a lab report in biology is on monday, not in a month!   \nThe experiment has groups of 5 people. They put bacteria on the 1st hands then they shake hands with the next person. I can't work out a test because the dependent variable is cfu count (discrete nominal) and the IV is no. of handshakes 1-4, or person's hand 1-5. (ordinal or categorical but the categorical would lose the meaning of the exp. because each handshake is connected to the first handshake. It seems one way anova doesn't suit ordinal data. I have been off sick for two years so i've forgotten a lot! I want to show that yes, the bacteria is spread, and as expected it decreases with each handshake. I am currently working on whether my data are ND or not.  \nthankyou for any help x",
        "created_utc": 1677352326,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bayes Rule",
        "author": "No-Media9325",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11boxul/bayes_rule/",
        "text": "On average fraternal twins ( could be of different sex) occur once in 150 births and identical twins ( both have the same sex) once in 400 births . A male person had  a twin brother who died in birth. Assume that an equal number of boys and girls are born on average. What is the probability that the male was an identical twin?   \nI am having problem with this question. I have done similar problems before but still cannot grasp enough information to convert into Bayes Rule formula.",
        "created_utc": 1677342356,
        "upvote_ratio": 1.0
    },
    {
        "title": "CDF of uniform distribution, explantion on equality",
        "author": "Sen_7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11boidz/cdf_of_uniform_distribution_explantion_on_equality/",
        "text": "&amp;#x200B;\n\nhttps://preview.redd.it/e0hfzchoycka1.jpg?width=687&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=9f00fd37f5000b948b13c7a22679b77e243e6fec\n\n&amp;#x200B;\n\nhi, it was given that X1,X2\\~Uniform(0,1) and we are looking at 0&lt; t &lt;1\n\nand they are both i.d.d( independent and identically distributed )  \ncan someone explain to me please why P(X1&lt;= t X2) = t/2 ?",
        "created_utc": 1677341232,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is an example of a linear regression model with both a low r-squared and low p-value?",
        "author": "Unlucky_Outside",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11boeyk/what_is_an_example_of_a_linear_regression_model/",
        "text": "I read somewhere that having both a low r-squared and a low p-value is possible, but this just confused me since most models I see usually have a low r-squared and high p-value (or vice versa).",
        "created_utc": 1677340985,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Determining ETH's own movements while excluding BTC's influence on it.",
        "author": "xxepsteinxx",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bochq/q_determining_eths_own_movements_while_excluding/",
        "text": "ETH and BTC are cryptocoins which change in value, much like stocks.\n\nThe problem is that ETH that is heavily influenced by BTC's movement in price. (like all altcoins) Is it possible to determine ETH's own movements by excluding BTC's influence? Is regression analysis the way to do it?\n\nThis is something that's been bugging me for a week now and I haven't managed to find an answer through googling. I think it has to do with regression analysis because ChatGPT told me so, but it may be lying.\n\nSorry if it's not actually relevant to statistics. (if possible, please direct me to the right sub)",
        "created_utc": 1677340824,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are You Aware of Data Literacy?! Your Input Is Needed!",
        "author": "capstonesurvey2023",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bo000/are_you_aware_of_data_literacy_your_input_is/",
        "text": "Survey Link: [https://georgetown.az1.qualtrics.com/jfe/form/SV\\_6gwKQ7s83VvCzSS](https://georgetown.az1.qualtrics.com/jfe/form/SV_6gwKQ7s83VvCzSS)\n\nhttps://preview.redd.it/54vklb88vcka1.png?width=1080&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=59430cdfbd823b60dfcf24bcebeb900da3136a80",
        "created_utc": 1677339923,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to generalise the results of cross correlation?",
        "author": "every_other_freackle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bn0a3/how_to_generalise_the_results_of_cross_correlation/",
        "text": "**Data:** I have 1000 independent time series. Each of these 1000 contains two variables A &amp; B.   \n**Goal:** My goal is to find out wether there is a statistically significant correlation between A &amp; B for ALL 1000 time series.  \n**Problem:** I have performed cross correlation for each of these 1000 time series separately and found out that in some cases A&amp;B are indeed correlated but in some cases they are not correlated at all.  \n**Question:** How should I generalise this result and make a statement about all the 1000 entry dataset as a whole? How do I test that the few  entries that are correlated are actually significant?  \nThank you!",
        "created_utc": 1677337319,
        "upvote_ratio": 1.0
    },
    {
        "title": "Fasting blood glucose level of 7 patients are given and their blood sugar level after 75g oral load and glucose levels are given below : *Two sets of data one at fasting and one after 75g oral load* Use appropriate statistical test whether oral glucose has significantly increased the blood sugar le",
        "author": "waiitwhattt",
        "url": "https://i.redd.it/yl02o1l6wdka1.jpg",
        "text": "",
        "created_utc": 1677334346,
        "upvote_ratio": 1.0
    },
    {
        "title": "what statistical technique will be applied and explanation too",
        "author": "gojo_saturo_sensei",
        "url": "https://i.redd.it/syj3292xudka1.jpg",
        "text": "",
        "created_utc": 1677333921,
        "upvote_ratio": 1.0
    },
    {
        "title": "anyone care to help..",
        "author": "gojo_saturo_sensei",
        "url": "https://i.redd.it/v358vw3nudka1.jpg",
        "text": "",
        "created_utc": 1677333828,
        "upvote_ratio": 1.0
    },
    {
        "title": "please I need help",
        "author": "waiitwhattt",
        "url": "https://i.redd.it/gbldyma6udka1.jpg",
        "text": "",
        "created_utc": 1677333673,
        "upvote_ratio": 1.0
    },
    {
        "title": "Modelling rare events - house sales",
        "author": "jqewr22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11blk6o/modelling_rare_events_house_sales/",
        "text": "[removed]",
        "created_utc": 1677333296,
        "upvote_ratio": 1.0
    },
    {
        "title": "Urgent: Can't install medmod on Jamovi",
        "author": "IntelligentWrap5422",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bkj3s/urgent_cant_install_medmod_on_jamovi/",
        "text": "I have homework due tomorrow with moderiert regression and I can't install medmod. It says \"bad gateway\". I installed the latest version of Jamovi and deinstalled other modules that were incompatible with the latest version. Still, it doesn't work. Please help.",
        "created_utc": 1677330184,
        "upvote_ratio": 1.0
    },
    {
        "title": "Calculation Number of Adverse Reactions in Larger Sample",
        "author": "merdanosman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bi6eu/calculation_number_of_adverse_reactions_in_larger/",
        "text": "Hi everyone, \n\nThere is a question that I need help with. 0 myocarditis reactions occurred in the COVID vaccine trial with 48066 people. What would be the 95% CI of the number of people who experience myocarditis among 1000000 people? Actually, I have seen a  post about this online, and it claims CI=0-60. However, I did not understand how they calculated this. I have used online Poisson CI calculators and found a different result.",
        "created_utc": 1677322020,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Noncoverage Error Rate = Type 1 Error Rate?",
        "author": "BarracudaLegitimate2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11bfo1z/q_noncoverage_error_rate_type_1_error_rate/",
        "text": "Hi guys, I'm coming from a background of psychology. I've read that it is possible for the non-coverage error rate to be the same as type 1 error rate when inappropriately using t-based statistical procedures for multiple comparisons. I was wondering how this was the case, if it even is the case. Does this have something to do with the overall nominated value of alpha?\n\nThank you!",
        "created_utc": 1677312357,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would you give me an amazing idea? Multivariable analysis in finance.",
        "author": "Emotional-Panic-9081",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11b7hwy/would_you_give_me_an_amazing_idea_multivariable/",
        "text": "[removed]",
        "created_utc": 1677286473,
        "upvote_ratio": 1.0
    },
    {
        "title": "I’m looking for a statistic that I can apply to describe diversity in eight landcover classes.",
        "author": "corey4005",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11b6q2b/im_looking_for_a_statistic_that_i_can_apply_to/",
        "text": "I have a set of landcover percentages for a 5km area across separate soil moisture stations. I have pulled landcover percentages from the national landcover database that describe various landcover classes. For example, each of these 5km areas have 8 classes with varying percentages of agland, developed (urban), wetland, barren, etc. \n\nI thought I could use something like the Sharon diversity index, but I believe it requires some count of the number of each species over the total, whereas I have percentages only. What statistic could I use to classify these areas which have varying landcover percentages?",
        "created_utc": 1677284446,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would studies based on public datasets technically need corrections for multiple comparisons?",
        "author": "DonQui_Kong",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11b6277/would_studies_based_on_public_datasets/",
        "text": "Some large biomedical datasets are public after they have been used for the original study. Often new analysis from other research groups use the data set again and those results can get published too, since other questions are asked and therefore the results are novel.\n\nHowever, technically we are testing multiple times on the same data set, so wouldn't corrections for multiple comparisons (Bonferroni, multiple contrast tests, etc.) be necessary?",
        "created_utc": 1677282759,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would studies based on public datasets technically need corrections for multiple comparisons?",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1677282701,
        "upvote_ratio": 1.0
    },
    {
        "title": "What test statistic to use for dichotomous data?",
        "author": "ri0t333",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11b1kxy/what_test_statistic_to_use_for_dichotomous_data/",
        "text": "Doing a study for my consultant, first time dealing with dichotomous data so not sure what test to use. Basically the question is if Robotic surgery has better post-op continence rates compared to laparoscopic. \n\nSo I have basically have the number of people: \n\n *(at 3 month)*| Continent | Incontinent\n---|---|----\nLap | X | A\nRobot | Y | B\n\nWhich test would tell me if there's a significant difference between the two types of operations.",
        "created_utc": 1677271521,
        "upvote_ratio": 1.0
    },
    {
        "title": "Good day folks. I just wa nt to ask, what statistical test to use if I have categorical DV and 2 or more Interval/ordinal IV. thanks x",
        "author": "Gloomy-Cricket-1791",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11azgt8/good_day_folks_i_just_wa_nt_to_ask_what/",
        "text": "",
        "created_utc": 1677266204,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confused as to what test to use?",
        "author": "drbizcat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ayrnu/confused_as_to_what_test_to_use/",
        "text": "",
        "created_utc": 1677264467,
        "upvote_ratio": 1.0
    },
    {
        "title": "intergal of likelihood function",
        "author": "Cultural-Doubt-9935",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11aypaw/intergal_of_likelihood_function/",
        "text": "So in the proof of the np lemma the integral of the likelihood function is used, what exactly does the integral of the likelihood function give us , does it give us the area and if it does,how?",
        "created_utc": 1677264306,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help - Why we condition X when we want to find an expected value of OLS estimator?",
        "author": "TrovadorAngolano",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11aymvt/help_why_we_condition_x_when_we_want_to_find_an/",
        "text": "So I' m learning econometrics and I have that doubt as written in the tittle, I don't understand why when we want to find if the estimator (I'll call beta1 hat) is centered , we condition on X the estimator like E(Beta1hat | X ). Why can't we just make E(Beta1Hat) ?",
        "created_utc": 1677264137,
        "upvote_ratio": 1.0
    },
    {
        "title": "I want to get a degree in statistics but it seems hard, need some advice",
        "author": "Raskrj3773",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11awzrf/i_want_to_get_a_degree_in_statistics_but_it_seems/",
        "text": "So im currently a junior in high school in Texas that was trying to determine what kind of career I want. I know I want to do something with math involved, I've always had a liking for mathematics and its something I like learning in school. I was trying to look at the degrees that involved mathematics, and found that the main 3 were Mathematics (of course!), Statistics, and Economics. I want to be a quantitative analyst, and it requires knowing coding, advanced mathematics, and knowledge of data, and it seems that statistics is the beat route for me to take to it. It also just sounds interesting! \n\nBut im genuinely contemplating if I could ever get in. I currently have a 2.9/4.0 GPA which is horrible. I also have not taken an SAT or ACT test, I know that could possibly heighten my chances of getting into TAMU college. \n\nWhile on the website checking the FAQ from the Dept. Of Statistics, I found some prerequisites, and they are listed as being: \n\n\"Applicants are required to have finished taking Calculus I and II, Matrix Algebra, Calculated-based Probability, and have some experience with computer programming. Most of our admitted students have taken a real analysis course before entering the program. Applicants with a strong background in mathematics, statistics, and computer science are preferred.\"\n\nOk, what? I'm currently in algebra 2 and next year i am taking a class labeled as \"Advanced Quantitative Research\", not precalculus or trigonometry. I don't have enough time to learn these all by myself in like 1 year. \n\nI also looked up real analysis, and it seems like its just really complicated Calculus. I definitely won't be prepared for that.\n\nSo maybe, I was thinking, I could go to community college, Learn calculus 1,2 there, get a good gpa, get a great SAT score, learn in my free time Matrix Algebra and calculated based probability, possibly take a real analysis course or research it when I have knowledge in calculus, and then apply to get into the online masters for statistics at TAMU. \n\nI could also learn some programming languages while I'm in my high school years to be prepared. (Python, R, SAS, ect.) I'm not going to understand the math so I might as well learn the coding right now.\n\nI don't know, I just really want to be a quant but the road to it seems so complicated. Can someone help me and give me advice on how to reach my goal? Thank you so much fr reading.",
        "created_utc": 1677259991,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing GPS precision...",
        "author": "nemom",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11auvao/comparing_gps_precision/",
        "text": "Normal, consumer-grade, handheld GPSes have a standard (95%) accuracy of 10 meters.  I know a guy who says he has a way to get better results.  He gave me 51 measurements to compare to survey-grade (2cm) coordinates.\n\nThe average of his measurements is 5.1 +/- 3.3 meters.  I'm looking to create a million randoms in the 10m to compare it to...  Can I just choose random values from 0 to 10 to simulate the distance from the actual pin?  If so, the average would be ~5 and his 5.1 doesn't seem as amazingly good as he makes it out to be.\n\nOr, do I have to simulate coordinates in the 10m circle and calculate distance since the area of the 9-10m ring is almost twenty times that of the 0-1m ring?\n\nThanks.\n\nPS: Of course, his system could be easily evaluated if he would take three measurements each time; his way, a single point, and an averaged point.  But, he won't.",
        "created_utc": 1677254666,
        "upvote_ratio": 1.0
    },
    {
        "title": "Calculating a %",
        "author": "NNSFW1234",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11au8nu/calculating_a/",
        "text": "Cross-posted in r/math and r/dividends .\n\nI've got a set of stocks I want to invest in, that I want to contribute a regular amount to monthly, and I've got a mathematical formula in mind for distributing that spend, but I can't figure out how to get the math to work.  Basically, I'd be looking at an idealized %Yield (that I would define for myself), and how far an individual stock deviates from that would determine the percentage of my spend for that stock, given a fixed amount.\n\nTo set this with arbitrary numbers, I'm looking at a set of 100 stocks, with $10,000 to spend.  So, just a normal distribution would be $100 per stock.  But, if I'm looking at the %Yield, I set my idealized %Yield at 5%- so, the closer a stock is to that 5%, whether over or under, changes how much I spend on that stock.  So if it's at 5.1% or 4.9% Yield, I would spend a higher percentage of the $10,000 on that stock than one that had a 1% or 9% Yield.  \n\nI've got an Excel sheet that calculates the deviations from the average, but I can't figure out a way to invert that deviation so it subtracts from the spend, rather than adding to it (currently, the further a deviation, the higher the spend).",
        "created_utc": 1677253047,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does anyone know the correct answer to this?",
        "author": "dreihodenjoe",
        "url": "https://i.redd.it/tmxeqgg5n5ka1.png",
        "text": "",
        "created_utc": 1677252470,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does anyone know the correct answer (with justification) for this?",
        "author": "dreihodenjoe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11atzxa/does_anyone_know_the_correct_answer_with/",
        "text": "",
        "created_utc": 1677252402,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing two different methods of measurement",
        "author": "miraculix28",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11aslsr/comparing_two_different_methods_of_measurement/",
        "text": "Dear statistical smart people,\n\n\n\nBackground:\n\nI had an idea for a \"new method\" to measure a temperature of interest, which I would like to test. However after talking to a colleague I am uncertain on my plannings of the set-up an the final statical evaluation.\n\n\nHypothesis:\n\nMy hypothesis is, that there is a \"good agreement\" of the old and the new method of measurement and thereby that the new method could be an alternative for the old method. In this case a \"good agreement\" means there should be no greater difference of ± 0.1 Kelvin / degree Celsisus in-between old and new method.\n\n\nMy Plan (was):\n\nMy plan was to build two or three \"prototypes\" and then measure the temperature with each \"prototype\" against the old \"standard\" in a model to do a \"proof of concept\" study. If viable there would be a possibility to generate more prototypes to do a \"real-life\" test. In order to achieve a sufficient amount of datasets (with these three \"prototypes\" I planned on taking the temperature every 5 minutes over a time period of 2 hours. Thereby generating 72 datasets (1 set every five minutes over 2 hours (24), with every \"prototype\" (24x3)). Following forward I planed an evaluation using the Bland-Altman plot / technique.\n\n\nQuestions:\n\nAs mentioned earlier I talked about my plans with a colleague of mine. He expressed his concerns regarding the usage of Bland-Altman with repeated measurements. However I was unable to find a statical method which is more suitable for this situation than Bland-Altman.\n\n- what is r/AskStatistics point of view on using the Bland-Altman plot on data which was acquired using repeated measurements?\n\n- can you recommend a more suitable statical method to tackle this problem? (Sadly these prototypes are not that cheap, so just \"building more\" is not an option)\n\n\nThank everyone for reading up to this point. I am looking forward to your insights! Kind regards!\n\nYours miraculix28",
        "created_utc": 1677248705,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using JMP or R to find the correlation between two different time series?",
        "author": "ManagerHonest7546",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11acfrp/using_jmp_or_r_to_find_the_correlation_between/",
        "text": " Suppose I'm measuring concentrations of gas over time as well as relative humidity. How can I find the correlation between these two time series?\n\nDo you think all four methods in this article are okay to use? The comments suggest they do, but I want to check opinions here.\n\n[https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9](https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9)",
        "created_utc": 1677195517,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best numeric method to find alpha that maximizes the likelihood function (known n, xi’s)",
        "author": "Sromero6153",
        "url": "https://i.redd.it/gqtvyvpoz1ka1.jpg",
        "text": "",
        "created_utc": 1677190242,
        "upvote_ratio": 1.0
    },
    {
        "title": "Standard Deviation",
        "author": "jrdubbleu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a9g3n/standard_deviation/",
        "text": "I'm having a moment (no pun intended) with standard deviation. I have a list of values (below), for which the SD = 4.52 -- I am trying to determine what values are more than 2 SDs from the mean. Can I just multiply 4.52 \\* 2 and anything greater than 9.04 is more than 2 SDs from the mean? I've been neck deep in R all day and I've lost the plot here. \n\n|7 5 6 8 5 6 11 22 20 8 10 11 13 6 5 6 6 9 7 6 11 9 5 7 8 11 8 8 4 8 11 6 20 8 20 13 4 17 5 6 5 9 6 10 19 3 6 11 9 6 13 8 5 5 |\n|:-|",
        "created_utc": 1677188087,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpretation of Prediction Intervals in Regression",
        "author": "Zarick_Knight",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a9d3q/interpretation_of_prediction_intervals_in/",
        "text": "A simple linear regression model is fit using height (in cm) to predict weight (in kg) of people.\n\nFor someone that is 180 cm tall, a 95 confidence interval for the mean weight is (92.57, 93.97) meaning for all people with a height of 180 cm, we can be 95% confident the mean weight is between 92.57 and 93.97.\n\nAlso, for a height of 180 cm, the 95% prediction interval is (55.9, 130.54). Which of these interpretations would be most appropriate:\n\n1: For a person with a height of 180 cm, we can be 95% confident their weight is between 55.9 and 130.54 kg. \n\n2: For everyone with a height of 180 cm, about 95% of weights are between 55.9 and 130.54 kg.\n\nOr is there a better interpretation? I'm really not \"confident\" we can use the phrase confidence in the interpretation of the prediction interval.",
        "created_utc": 1677187881,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] How to calculate effect size for binomial data?",
        "author": "fireguyV2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a8qwz/q_how_to_calculate_effect_size_for_binomial_data/",
        "text": "Running a set of individuals through a learning test 7 times. The only data that is collected is whether they succeeded or failed at the task. I ran binomial tests but now I'm stuck trying to figure out my effect size. I'm more used to working with continuous data. Would I simply do a risk ratio test? Something alokg the lines of 1/(estimated probability - expected probability)?Would that be the equivalent to the effect size? I've seen some threads also use Cohen's h as well. How should I decide between which one?",
        "created_utc": 1677186356,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Help understanding and evaluating gradf(W; zeta)?",
        "author": "Amun-Aion",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a7epi/q_help_understanding_and_evaluating_gradfw_zeta/",
        "text": "What does it mean when the gradient in an algorithm is listed as something like gradf(W; zeta) or more complicated like gradf(W - alpha\\*gradf(W; zeta\\_1); zeta\\_2)?\n\nI know generally that gradf(x; zeta) indicates that this a stochastic gradient on batch zeta, but how do you evaluate this numerically?  For instance, I have lots of data (not symbolic or analytical equations) so I'm not sure how to actually find this gradient if I just have all the numbers.  Because if I evaluate it then I get to the point of just taking the gradient of a matrix of numerical values, and I don't know what that means.\n\nI analytically found the derivative of my loss function (so I'm assuming gradf means the gradient of my loss function), but I'm not sure how I would actually implement all the other input parameters.  I have my loss function and my analytically-found loss gradients as functions in Python and I'm currently doing scipy.optimize.minimze to minimize my weights (W), so if I am given W to use I'm not sure what I would be minimizing in this case.  Or would I just enter W and get the returned value? I've been using scipy.optimize.minimize with or without a max\\_iter of 1 step in order to do gradient descent, for reference.",
        "created_utc": 1677183094,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about correlations coefficients",
        "author": "toiletbombb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a6wy2/question_about_correlations_coefficients/",
        "text": "Can my x-values be a range? For example can x=3.9-3.5 and i can still calculate my correlation coefficient?",
        "created_utc": 1677181909,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics: What is the expected value and variance of X, X being the number of people you have to check to have birthdays in all 365 days of the year.",
        "author": "AlexByLogic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a6i8s/statistics_what_is_the_expected_value_and/",
        "text": "",
        "created_utc": 1677180901,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Is William Briggs' \"Uncertainty - The Soul of Modelling, Probability and Statistics\" (2016) worth reading?",
        "author": "slavabiencanadien",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a650p/q_is_william_briggs_uncertainty_the_soul_of/",
        "text": "I'm studying statistical analysis at the undergraduate level. I recognize that philosophy of the discipline is usually reserved for postgraduate studies, and given the \"spiciness\" of some of Briggs' assertions about the state of the discipline viz. replication crisis... I'm wondering if reading this book will do me more harm than good at this time in my studies?\n\nLooking for any relevant thoughts or advice. Thanks!",
        "created_utc": 1677179998,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] What does gradient with a subscript mean and how to evaluate it for (S)GD?",
        "author": "Amun-Aion",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a34md/q_what_does_gradient_with_a_subscript_mean_and/",
        "text": " I have three equations:\n\n    a(t) = a(t-1) - n*grad( a(t-1); zeta(t) )\n    b(t) = b(t-1) - n*grad_b( b_bar(t-1); zeta(t) )\n    b_bar(t) = a(t) + (1/N)*(b(t)^2)\n\nWhat's the difference between grad(x) and grad\\_b(x)? I recall from Multivariable calc it usually means \"gradient in the direction of b\" but then what does just grad(x) mean? I thought the gradient always gives the direction of maximum steepness/ascent, so what does direction mean? My main issue is that I don't understand how the two gradients are different and thus don't know how to evaluate them.\n\nThis is for an ML model where I have a loss function L(t): do these gradients not just refer to the gradients of my loss function? Note that a, b, and b\\_bar are all matrices actually, so I don't know what computing the gradient over these matrices would even mean (which is why I'm assuming that this is talking about taking the gradient of my loss function). Specifically, does grad(a(t-1)) mean the gradient over my existing history over matrix a or does it mean over my loss function? I'm not sure why they list just one \"point\" instead of the full history or a full function, e.g. like using grad(x) instead of what they appear to be doing which is grad(x(t-1)).\n\nEDIT: Zeta indicates that this is a stochastic gradient but I think I can just ignore that for the purpose of this question, I don't think it being SGD changes what the gradients are other than the fact they are taken over some batch.  Maybe I'm wrong though?  I'm trying to implement it just as GD right now.",
        "created_utc": 1677172756,
        "upvote_ratio": 1.0
    },
    {
        "title": "Zero inflated multilevel model in SPSS possible?",
        "author": "kbc518",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a1z3p/zero_inflated_multilevel_model_in_spss_possible/",
        "text": "I use SPSS and have no familiarity with other software programs. I successfully downloaded the STATS ZEROINFL R extension in SPSS, however I'm not sure if it has the capabilities that I need. I need to run zero inflated generalized multilevel/mixed effects models but I don't see any new/added options for zero inflated models with this extension (Mixed Models --&gt; Generalized Linear --&gt; Target Distribution). I do see the new options under Generalized Linear Models --&gt; Zero Inflated Count Models, but that option does not allow for multilevel data. Am I missing something, or does this plug in not do what I need to do? Does anyone know how I can run a zero inflated generalized multilevel model in SPSS?",
        "created_utc": 1677170010,
        "upvote_ratio": 1.0
    },
    {
        "title": "For testing association, is there any particular rule of thumb for using a regression over a correlation or vice-a-versa?",
        "author": "Palmer-Eldritch-2021",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a1x8t/for_testing_association_is_there_any_particular/",
        "text": "So, I was using the overall null hypothesis test from a regression (two numeric variables) and a logistic regression (categorical outcome, numeric predictor) to show an association among different variables.  The question came up as to why not use a correlation for the same purpose and I don't actually know why one would be favored over the other if the goal is only to just to show p-values on the possibility that the two variables are associated.  I understand why either test would be chosen if you wanted strength of association, prediction, etc., but is there any difference in power between a logistic regression versus some non-parametric correlation or between a regression and a normal correlation?  The results are practically the same, right?",
        "created_utc": 1677169888,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probit Model",
        "author": "darklightningfan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a0pvj/probit_model/",
        "text": "I have some data and would like to build a model to indicate the likelihood a scenario occuring. The ideal outcome is to predict whether the scenario is -1, 0 or 1 using probit. \n\nAny ideas on how to build this? Either in excel or some stats software?",
        "created_utc": 1677166946,
        "upvote_ratio": 1.0
    },
    {
        "title": "Resources for a statistically incompetent medic",
        "author": "JamesTJackson",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a0j3u/resources_for_a_statistically_incompetent_medic/",
        "text": "I'm a medic involved in research (both medical and biomedical), and I've realised my knowledge of statistics is pretty poor. This goes right down to the very basics! What are some good resources (including textbooks or online resources) to take me from fundmentals to pretty solid stats? Even better if it does it using R or Python!",
        "created_utc": 1677166462,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Which statistical analysis to use?",
        "author": "Free_Construction462",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11a0h06/q_which_statistical_analysis_to_use/",
        "text": "I have conducted the same experiment over 30 times with the exactly same parameters- one is dosage and other is efficiency. Throughout the experiment the dosage was kept the same, and efficiency for each test was noted. I wanted to ask which statistical analysis can I use to claim that the results are statistically significant and are indeed repeteable?",
        "created_utc": 1677166314,
        "upvote_ratio": 1.0
    },
    {
        "title": "What on earth is degree-corrected stochastic block model and how you can fit it using a non-parametric Bayesian method?",
        "author": "johndatavizwiz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119zzc2/what_on_earth_is_degreecorrected_stochastic_block/",
        "text": "Just as the title says, I'm trying to wrap my head around this topic I've found in a paper and I don't know where can I learn more about those. If any of you know something more on this topic, please help me understand.",
        "created_utc": 1677165055,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to correctly analyze the difference in performance between two social media groups?",
        "author": "every_other_freackle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119zx7h/how_to_correctly_analyze_the_difference_in/",
        "text": "I recently got my hands on some large Instagram posts/user data and I want to find an answer to one common social media theory: Posting more frequently will result in more views/subscribers. Everybody seems to know this but I haven't seen studies that actually prove it.\n\n  \nMy current idea is to split the users into two groups. Where group A is posting often and group B is posting rarely and compare the average number of views between these groups. However, I am not sure how can I definitely know that whatever result I find is due to posting frequency and not some other hidden variable that I am not taking into account (i.e. subscriber count, image type, number of shares. etc...)  \nCan somebody suggest what the right approach would be for a study like this? If there are any good reference studies that try to achieve a similar result please share your thoughts!  \nThank you!",
        "created_utc": 1677164909,
        "upvote_ratio": 1.0
    },
    {
        "title": "I'd like to determine what effect a policy has had on student enrollment in multiple programs (electives) within all secondary schools of several states. What statistical knowledge will I need to learn?",
        "author": "teacherofderp",
        "url": "/r/statistics/comments/119d8am/reid_like_to_determine_what_effect_a_policy_has/",
        "text": "",
        "created_utc": 1677163986,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to use the argument stype in boot-package in r?",
        "author": "Massive-Row8428",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119xdla/how_to_use_the_argument_stype_in_bootpackage_in_r/",
        "text": " The stype argument in boot of r can take 3 values: \"i\" which is the default, \"w\" or \"f\"\n\nWhat is the concept of w and f? I really tried to find articles about these stuff, but they are scarce. Can anybody explain them to me and show examples of codes on how to use them for linear regression model (implementing bootstrapping)?",
        "created_utc": 1677157954,
        "upvote_ratio": 1.0
    },
    {
        "title": "What to do when regression assumptions are not met?",
        "author": "Hour_Woodpecker_906",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119wnre/what_to_do_when_regression_assumptions_are_not_met/",
        "text": "My professor once said that never go for non-parametric and report what you get. But is that right?",
        "created_utc": 1677155740,
        "upvote_ratio": 1.0
    },
    {
        "title": "Power Analysis for mediation analysis, if the relation between M and Y is non-linear.",
        "author": "Tristan251",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119vx6q/power_analysis_for_mediation_analysis_if_the/",
        "text": "We are planning on conducting a research on how being facial recorded while conducting an instructional video affects learning outcomes. So, we have two conditions; one condition in which people make an instructional video, and one condition in which students make an instructional podcast. We hypothesize that the only significance differences between the two conditions are arousal related, which would affect learning outcomes. \n\nSo the analysis we are planning on conducting is a mediation analysis in which condition (x) influences arousal (m) which has a non-linear relation with learning outcomes (y). Since according to literature, the relation between arousal and learning outcomes is inversely u-shaped, a quadratic term should be sufficient to account for the non-linearity. \n\nMy question now is, how could we conduct a power analysis a-priori? We want to know what sample size we need to find the hypothesized effect sizes. All of the tools we could find to do power analysis with mediation do not have an option to add a polynomial. \n\nAny advise would be greatly appreciated.",
        "created_utc": 1677153344,
        "upvote_ratio": 1.0
    },
    {
        "title": "Ordinal data: how to graph it and do stats",
        "author": "Honest-Steak-4278",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119rsun/ordinal_data_how_to_graph_it_and_do_stats/",
        "text": "I think I've gotten myself a bit confused about an incredibly basic aspect of statistics!\n\nI have an experiment, let's say with a control and a treatment group. For each of these groups I do 5 replicates, and the experiment involves counting foci under a microscope, so I get x foci per 1000 cells. \n\nSince this is data I have counted rather than measured, this is ordinal data, correct? Is it best to basically treat it like continuous data and plot it on a dot plot/box plots and use a t test or Mann-Whitney U test? Or should I be treating it differential to another experiment which is continuous (for example measuring the florescence of the cells in each group)?\n\nThanks for your help!",
        "created_utc": 1677137739,
        "upvote_ratio": 1.0
    },
    {
        "title": "According to chatgpt, choosing 1,2,3,4,5,6 for lottery have less chance of winning?",
        "author": "ShortPutAndPMCC",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119otux/according_to_chatgpt_choosing_123456_for_lottery/",
        "text": "I asked about a 6 out of 49 numbers lottery. I asked if choosing above titled numbers will lower chances of winning, and the answer is Yes. \n\n“If you choose numbers with a pattern or any non-random selection, the number of possible combinations reduces significantly, reducing your chances of winning. For example, if you choose numbers like 1, 2, 3, 4, 5, 6, you only have one possible combination, which means your odds of winning are 1 in 13,983,816.\n\nIn contrast, if you choose random numbers for each draw, you have the same chance of winning as any other player, because each set of six numbers has the same probability of being drawn.”\n\nBut why? Isn’t 1 in 13,983,816 the same probability for all possible 6 digit combinations?",
        "created_utc": 1677127822,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are the Odds of Sharing a Class With Any Students Twice?",
        "author": "notmonkeymaster09",
        "url": "https://www.reddit.com/r/AskStatistics/comments/119nqb1/what_are_the_odds_of_sharing_a_class_with_any/",
        "text": "I have been trying to solve this for a while and I cannot for the life of me get an answer that makes sense:\n\nIn a college with a group of 10,000 other people at the same grade level, assuming there are 30 people per class on average and there are 4 class periods on average, what are the odds that you share any 2 classes with any random student.\n\nThis is assuming the 10,000 possible matched students are perfectly evenly spread out among the classes. \n\nI've tried talking to several resources and cannot get any answer whatsoever and looking over different textbooks, I can't find any situation that would match my scenario.",
        "created_utc": 1677124550,
        "upvote_ratio": 1.0
    }
]