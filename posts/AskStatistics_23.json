[
    {
        "title": "College Suggestions",
        "author": "dmalmer3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9id9w4/college_suggestions/",
        "text": "I’m a junior in high school and I’m very interested in a career in statistics and I was just looking for any suggestions of colleges I can start looking into that have good statistics programs, thanks for the help!",
        "created_utc": 1537747540,
        "upvote_ratio": ""
    },
    {
        "title": "How to combine factors without having too much overlap?",
        "author": "smallchimp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9icrmy/how_to_combine_factors_without_having_too_much/",
        "text": "[Example](https://www.basketball-reference.com/players/j/jamesle01/splits/) \nOkay, so in the provided example, we have multiple \"splits\" (differences from the total given different criteria) such as month, opponent, length of rest etc. Each split has its own specific boundaries, but there's overlap. There could be a Friday night home game in October before the All-Star break against Boston, an Eastern Conference team in the Atlantic division. That includes 6 different splits, but how do you come up with a \"multiplier\" that compares all of these stats together compared to his average.\n\nAt home, LeBron scores 2% less at home compared to his career average per game.\nBefore the All-Star break, he scores ~1.5% less compared to his career average per game. \nIn October, he scores around 16% less than his career average per game\n...and so on. When creating a combined number that factors in all of those criteria, how do you avoid counting the same data points over and over again?\n\nSay that the values came out to  (as decimals):\n1.01, .93, 1.00, .88, 1.09, .97. You don't just multiply/average them, right?",
        "created_utc": 1537743296,
        "upvote_ratio": ""
    },
    {
        "title": "Help with some probability theory.",
        "author": "LittleKidsAreAwful",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ib6ao/help_with_some_probability_theory/",
        "text": "I've been playing around with this.... it's pretty straightforward with a venn diagram but I can't seem to justify everything without one. Anyone know where I can go to get pointed in the right direction?\n\nhttps://i.redd.it/ii3yfy42j1o11.png",
        "created_utc": 1537731245,
        "upvote_ratio": ""
    },
    {
        "title": "Is this histogram positively skewed?",
        "author": "major_waters",
        "url": "https://i.redd.it/f3fzpuydwxn11.png",
        "text": "",
        "created_utc": 1537687345,
        "upvote_ratio": ""
    },
    {
        "title": "I am not sure how to get an accurate sample size based on skewed survey, help?",
        "author": "xxred_baronxx",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9i5p8n/i_am_not_sure_how_to_get_an_accurate_sample_size/",
        "text": "I have a general population of voters that breaks up to be 40% Republicans , 30% Democrats, and 23% Independents. I have a population of that general population that is 60% dem, 11% repub, and 29% indep. Of that smaller population only 3% have responded to the survey. How do I figure out how to get an accurate sample size to determine how the general population would respond to the survey question? Obviously the smaller population is weighted and isn’t representative of the general population. How do I know with a high degree of confidence that the answers to the survey represent the general population?",
        "created_utc": 1537675193,
        "upvote_ratio": ""
    },
    {
        "title": "[basic] Confidence interval",
        "author": "tukeysbinges",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9i0itf/basic_confidence_interval/",
        "text": "I'm not too sure where I've gone wrong with this. \n\nquestion https://i.imgur.com/MA4tScV.png\n\nanswer https://i.imgur.com/l7SXqIU.png\n\n\nI think that what I've done is instead construct a confidence interval for the savings, rather than the average savings? \n\nOr is that wrong too? \n\nBasically I'm not too sure how to describe the error that I've made here. \n\nYou can see the question and it's working here http://www.statisticshowto.com/finite-population-correction-factor/",
        "created_utc": 1537630224,
        "upvote_ratio": ""
    },
    {
        "title": "General confusion about distribution types",
        "author": "FreakingOutTheNGHBRD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hx6vb/general_confusion_about_distribution_types/",
        "text": "Hey, I would normally take this to a tutor, but it is the weekend and I am away from school.\n\nI am taking a statistics class and I am really having trouble figuring out the bigger picture.\n\nWe went from probability and counting techniques, to random variables, pmf, cdf, expected value, and std.dev, for discrete random variables. This is where I get lost. We then went into binomial distributions, which have different random variables, pmf, cdf, expected value, variance, and std. dev than what was previously introduced. So now I am confused... Why were these things introduced in a general sense in the first place if we were only going to modify them to be applicable to different types of distributions? Then we got into hypergeometric distributions,  negative binomial distributions, and poisson distributions. If every distribution is a specific type of distribution, then what are the original formulas and definitions for rv, pmf, cdf, expected value, and variance used for?\n\nIs the distribution type defined by the random variable type?\n\nThe textbook they have us using is organized like garbage. It feels like there is no logical order in how things are being introduced, and I am not seeing how all of these concepts are related and their hierarchy.",
        "created_utc": 1537592327,
        "upvote_ratio": ""
    },
    {
        "title": "What is the probability my roomates and I meet each other in the corridor to piss during the night?",
        "author": "carkey",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hwt4j/what_is_the_probability_my_roomates_and_i_meet/",
        "text": "So, I live with two roomates, we all have shitty bladders that means we need to go for a piss at least once each during the night but we never cross paths even though there is only one bathroom in the house. We've lived together for a couple of years but never our piss paths have crossed so we thought we'd try and work out why.\n\nWhile talking about it we agreed that, at most, we probably would go 3 times but on average it's much closer to 1. We agreed we probably take less than 2 minutes per 'piss period' (pp) and so the maximum is 2pp per person (or, if you're that way inclinded, 2pppp...sorry).\n\nSo, going by an average of 6 hours during the night we overlap on sleeping, we have 3 people exercising 2pppp per night (or, if you're that way inclined, 2pppppp...I'm not sorry).\n\nWhat is the probablity any 2 of the 3 pissers would meet during a 6 hour period given they might each indiviually piss 3 times, taking 2 minutes per traversal to the bathroom, piss period ('pp' if you had forgotten) and traversal back to their room?\n\nStats was my worst subject in maths to be honest and the other two pissers are arty-farty kinds so we're at a loss as to how to work it out. My reasoning was that if the period is 6 hours and I piss a maximum of 6 minutes per 6 hours, then an encounter by a weird 6-hour awake freak happens 1/60th during the 6 hours.\n\nIf there are 3 of us, does that mean it becomes 3/60ths and therefore 1/20th chance at least 1 person meets another 1 person on the way towards, during or back from night pissing?\n\nThank you for reading this shit, I mean piss, I mean 2pppppp.",
        "created_utc": 1537588362,
        "upvote_ratio": ""
    },
    {
        "title": "Question on setting up a research question",
        "author": "Dulout",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hwdnk/question_on_setting_up_a_research_question/",
        "text": "I'm goofing around with some data that was scanned from a website that reports on game sales, I forget what it was called. \n\nThe data is sorted by Genre, Sales by region (NA, EU, JP, Other, Total), Year published, and a few other things with so many missing values it's pointless to list them. \n\nI went ahead and totaled the regional sales by genre and created a chi square and did the Chisquare value and Cramer's V value:\nhttp://prntscr.com/kxa1v0\n\nWhat I want to answer is this: Is there a significant difference between genre in the NA region and the JP region by genre. According to the Square, There is some significant difference in the roleplaying category, as well as the shooter category. \n\nHow do I go about exploring this Chi square even further and describing it? Is there a good way to create a model using this chi square?\n\nHow does one go about exploring this result further? I've never had to develop a paper around an idea or concept like this before, it's really new and I'm not sure how to begin discussing it.\n",
        "created_utc": 1537584049,
        "upvote_ratio": ""
    },
    {
        "title": "Multilevel modeling in a repeated measures design",
        "author": "HimmelLove",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hujyp/multilevel_modeling_in_a_repeated_measures_design/",
        "text": "I have a few related questions. I conducted an experiment with three distinct groups (A, B, C). Each participant completed the same task three times. I analyzed the data using a repeated measures ANOVA. My three research questions were (1) does group A differ from groups B and C at time 1, (2) does group A differ from groups B and C at time 3, and (3) do the groups show a different rate of improvement across the trials. I should also mention that there is much larger variance for every trial in group A than for groups B or C.\n\n&amp;#x200B;\n\nI want to create a multilevel model for this data. I am currently reading through Gelman and Hill's book, but very few examples involve this type of design. Google is helping but I am still unsure. Any help is great appreciated!\n\n&amp;#x200B;\n\nThis is my current attempt using lme4 in R, but I have no idea if this is correct:\n\nlmer(dv \\~ group\\*trial + (1|participant)\n\n&amp;#x200B;\n\nMy questions are:\n\n1. Can I answer all three questions from the same model?\n2. Which effects should be random and which should be fixed?\n3. Are there recommended books or articles? I don't mind doing a lot of work, I just want to understand it. My mathematical ability is so-so, but I am pretty comfortable with multiple regression and repeated measures ANOVA/MANOVA.\n4. Conceptually, what is the difference between analyzing the model using MLM v. repeated measures ANOVA?\n\nI know these are some big questions. Thank you for taking the time to read this!",
        "created_utc": 1537567918,
        "upvote_ratio": ""
    },
    {
        "title": "Please help me understand interaction terms for a binary logistics regression",
        "author": "MoonlightSeeker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hqh2o/please_help_me_understand_interaction_terms_for_a/",
        "text": "I’m way over my head trying to figure out how to interpret interaction terms. I will really appreciate your responses. I did a google search and read a lot of the articles that came out but I still don’t get it. \n\nMy DV: student passed math class\nIVs: Professor, ethnicity, primary language spoken at home, GPA, gender, socioeconomic status \n\nI ran interactions professor by all other factors. Most of them were not significant. I found the interaction of Professor A dummy variable and identifying as Other race dummy variable was significant but I’m not sure what does it mean.\n\nThank you in advance for your help. \n",
        "created_utc": 1537540273,
        "upvote_ratio": ""
    },
    {
        "title": "Inference",
        "author": "jah-lahfui",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hpj64/inference/",
        "text": "Hey,\nI was refreshing my memory and i still have doubts with inference i dont get a lot things.\n\n1) Inference is getting a statistics about a population from a sample size N. But how do you know that the statistics that u got is near the value for the population? So how can you measure the error between the value you have got and the real value for the population?\n\nIs there any good free ebook for this stuff? With advanced topics in inference?\nCentred inferenced and all related stuff?\n\nThanks a lot",
        "created_utc": 1537534393,
        "upvote_ratio": ""
    },
    {
        "title": "Stats help appreciated - how do I proceed with this question?",
        "author": "mremri",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hk5xx/stats_help_appreciated_how_do_i_proceed_with_this/",
        "text": "Hello all,\n\nI have 9 different variations of the same measurement, though each different variation is enough to change the result from each variable so I am not expecting the same answer, and there is no reference. For each variable I compute a quality metric that must be above 3 to ensure good data. My question is, after I initially discard all data with a value less than 3, how do I select the best variation from the ones that remain. Intuitively you would simply choose the variation with the highest quality metric but I would like to statistically test this. \n\nCan I just do Mann-Whitney on the remaining variables to see which ones are significantly better?\n\nThank you for reading and any advice is appreciated. \n\n",
        "created_utc": 1537486503,
        "upvote_ratio": ""
    },
    {
        "title": "How to model this experiment? Repeated measures mixed model",
        "author": "dirtyuzbek",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hjyqa/how_to_model_this_experiment_repeated_measures/",
        "text": "Hello, I need advice on how to model this experiment and how to determine significant effects.\n\nSo in my experiment I have 2 conditions (a and b).\n\nFor each condition I have 3 plates of cells (a1-3...)\nEvery day for 4 days I take a picture of random cells from each plate and measure my dependant variable (dv).\nIn this case my independent variables are day and condition.\nAt first I thought it would be a repeated measures anova, but that's not the case because I'm not measuring the same cells each day. I am measuring random cells from the same plate each day.\nI want to see the effect of the condition and the effect of day, and their interaction as well.\n\nI'm also slightly confused as to which effects would be described as fixed or random, but I believe my model should have both.\n\nInternet searches only show repeated measurements analysis on individual subjects, but here I am repeatedly measuring the from the same plate, not the same cell.\n\nAnalogous example is if you had 2 school districts (imagine each district had 3 schools) each testing a different curriculum. Every 3 months you would get the grades from random students from each school.\nThen you would want to compare how effective the curriculums are against each other.\nThe students who were sampled are random, so their grades are not repeatedly measured, but the schools from which they are sampled stay the same.\n\nI have experience in Python, but to simplify things for me I am using SPSS.\n\nAny and all advice is very helpful. Thank you.",
        "created_utc": 1537484930,
        "upvote_ratio": ""
    },
    {
        "title": "Am I comparing the correct variables or is there just not a correlation?",
        "author": "smallchimp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hiyfl/am_i_comparing_the_correct_variables_or_is_there/",
        "text": "(Reference included below)\n\nI'm trying to find a correlation (or lack there of I suppose) between the rate that points per minute (AG column) occur given different minutes played (AF column). So in basketball talk, I want to know whether a player's efficiency increases at the same rate whether he plays more minutes or less minutes. So if he scores 1 point per minute when playing 20 minutes, will he score 1 point per minute at 40 minutes or will there be diminishing returns to where it's a lower PPM rate. \n\nIn the \"s. curry\" sheet, I gave two chart examples where I did it AF/AG (minutes played/points per minute) in the top chart and AF/AE (Minutes played/fantasy points) in the bottom chart. Did I pick the right equation in either and the correlation is minimum or did I do it wrong/pick a totally wrong formula? \n\n[Reference](https://docs.google.com/spreadsheets/d/1wWkBpW8mleOEUfs9OTes9CT06hnlhEzUYpZdIJYJgVo/edit?usp=sharing)",
        "created_utc": 1537477258,
        "upvote_ratio": ""
    },
    {
        "title": "Suppose you are given the same test set and two binary classifiers. Is it possible that Classifier 1 has higher accuracy than Classifier 2, but Classifier 2 has both higher precision and higher recall than Classifier 1?",
        "author": "randomasiandude22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hi7u4/suppose_you_are_given_the_same_test_set_and_two/",
        "text": "The context is evaluation of classifiers in Machine learning.\n\n\nI wanted to ask if it is possible for a variation of Simpson's Paradox that games both precision and recall to occur even when we are using the exact same test sets?\n",
        "created_utc": 1537471976,
        "upvote_ratio": ""
    },
    {
        "title": "Confused as to how to get the mean and standard deviation",
        "author": "[deleted]",
        "url": "https://i.redd.it/s1rsfrttgfn11.png",
        "text": "[deleted]",
        "created_utc": 1537464255,
        "upvote_ratio": ""
    },
    {
        "title": "Spine fitting - graphics and statistics",
        "author": "tukeysbinges",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hgw3v/spine_fitting_graphics_and_statistics/",
        "text": "Do concepts from splines in graphics transfer over to splines in statistics? \n\n",
        "created_utc": 1537462250,
        "upvote_ratio": ""
    },
    {
        "title": "A simple inquiry: Would a statistician say there's a correlation here?",
        "author": "Layixide",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hg0pz/a_simple_inquiry_would_a_statistician_say_theres/",
        "text": "https://imgur.com/svmOgDw\n\nIs there any correlation between blue and purple?  \n\n  ",
        "created_utc": 1537455739,
        "upvote_ratio": ""
    },
    {
        "title": "A simple inquiry: Would a statistician say there's a correlation between the blue columns and the purple columns?",
        "author": "[deleted]",
        "url": "https://imgur.com/svmOgDw",
        "text": "[deleted]",
        "created_utc": 1537415993,
        "upvote_ratio": ""
    },
    {
        "title": "Hoping for some help, how accurate are the statistics this article is using?",
        "author": "JustPutDownTheFork",
        "url": "http://tynan.com/motolife",
        "text": "",
        "created_utc": 1537415652,
        "upvote_ratio": ""
    },
    {
        "title": "Manufacturing error rate question",
        "author": "omni222",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hbg9i/manufacturing_error_rate_question/",
        "text": "I'm trying to put together a spreadsheet, but I have no background in statistics, so I apologize in advance if I don't even know how to adequately explain myself.\n\nImagine Company A makes 100 widgets, and 5 are defects.\nCompany B makes 70 widgets, and 3 are defects.\nCompany C makes 150 widgets, and 7 are defects.\nAnd so on. Imagine there are 45 companies.\n\nWhat I want is to compare relative manufacturing error rates. Defects / widgets doesn't seem to capture the whole story, in a case where Company D only had 1 defect, but also only made 40 widgets.\n\nMy solution was to take each company's errors and divide by the total errors for all companies, so you get a percentage of all errors. Then divide the company's widgets by the total widgets. Then divide that error percentage by that total widget percentage. I don't know what this number actually represents, or if it's accomplishing what I want to get at.\n\nThanks for your help.",
        "created_utc": 1537410270,
        "upvote_ratio": ""
    },
    {
        "title": "Testing variables between sites",
        "author": "Deathowler",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hb5r4/testing_variables_between_sites/",
        "text": "So I am struggling to choose the right tests and ways to implement it for my grad study.\n\nEssentially I am measuring the frequency of species caught at remote cameras in sites of various treatment. I.e one of them has trees, another has just grass, one with just soil etc. Nested within those sites is the time time mammals were caught on camera(Day and night) and where the camera was located(near a water source, food source etc).\n\nSo my data essentially looks like this\n\nSite 1: Day, number of a species: X\nSite 1: Night, number of species: X\nSite 2: Day, number of species:X etc.\n\n\nI am struggling mostly because the dataset has a lot of zeros and it's definitely not normal.\n\nI was thinking of a non-parametric T test but other options are welcome. ",
        "created_utc": 1537407787,
        "upvote_ratio": ""
    },
    {
        "title": "Can statistics be used to \"crack\" an algorithm?",
        "author": "smallchimp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hapgn/can_statistics_be_used_to_crack_an_algorithm/",
        "text": "If you have multiple sets of an end-number and each data point used to come up with it, is there a way to determine the formula used to come up with the final number? There would be multiple data sets so change could be seen from set to set, indicating where weight may be placed in the formula. Would this be some sort of linear regression where the r-squared value would show the weight used for each data category?",
        "created_utc": 1537403989,
        "upvote_ratio": ""
    },
    {
        "title": "It makes sense to make inferences of an entire population based on data from a portion of the population. But does it make sense to use data from the whole population to make inferences about a subset of the population?",
        "author": "AbCzar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9hakjm/it_makes_sense_to_make_inferences_of_an_entire/",
        "text": "",
        "created_utc": 1537402835,
        "upvote_ratio": ""
    },
    {
        "title": "Help choosing a stats test- I'm dumb",
        "author": "Fridaypenis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h82qx/help_choosing_a_stats_test_im_dumb/",
        "text": "Hello! I'm a biology student who has no brain for statistics. I've tried, I swear. Sometimes I think I have a handle on it, but I lose it before my next project.\n\nI'm looking to compare a diet of a species in the wild and compare it to their diet in captive settings.\n\nSo, the data I would have would look something like \n\n*Wild           %browse     %fruit     %protein\n\n*Facility1     %browse     %fruit     %protein\n\n*Facility2     %browse     %fruit     %protein\n\n*Facility3     %browse     %fruit     %protein\n\n*Facility4     %browse     %fruit     %protein\n\netc etc\n\nI would also probably ask if each facility's animals have a certain health problem common to this species that may be diet related.\n\nThank you for any help you're willing to give me!",
        "created_utc": 1537384285,
        "upvote_ratio": ""
    },
    {
        "title": "How many samples should I apply to calculate alpha crombach, and should I consider another statical test to validate a survey questionnaire concerning an Investigation of the labor situation of civil engineers in Honduras?",
        "author": "ract17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h79yc/how_many_samples_should_i_apply_to_calculate/",
        "text": "Hi, I am conducting an investigation concerning the labor situation of civil engineers in Honduras.\n\n&amp;#x200B;\n\nI already calculated the alpha Cronbach with 10 samples, but I'm not sure the ideal amount to perform the test. Also, I would like to know if I should make another statistical test to validate the questionnaire.\n\nHere is an explanation of the hypothesis and the objectives of this investigation:\n\nThe hypothesis to prove is : \"Being a member of the Honduran Association of Civil Engineers DOES NOT generates value in getting better job conditions\" (understood by \"conditions\" as better wages, less overtime work, less gender gap, extra benefits like health care, labour rights, etc.)\n\nThe main objective is\n\nUnderstand the labour reality in Honduras for the professionals of Civil Engineering\n\nSpecific objectives :\n\nDefine the total number of civil engineers that are members the Honduran Association of Civil Engineers vs the quantity of professionals of civil engineering that are not part of the organization\n\nDefine the percentages of men and women that work as civil engineers\n\nDetermine the age of the population of civil engineers in Honduras\n\nCalculate the unemployment percentage of civil engineers\n\nCalculate the percentage of civil engineers that are employees, employers, independent, etc\n\nDefine in which area of the country are more civil engineers working\n\nDetermine from which university is graduating the majority of civil engineers\n\nDefine the percentage of civil engineers that can speak more than one language, have master degrees and PHDs\n\nDefine the real minimum wage that it is being payed to the civil engineers\n\nAnalyze which aspect is more determinant to obtain a better salary (labor experience, academic level, gender, knowledge of more than one language, age, working in far-away projects )\n\nAnalyze the labour situation of the civil engineers by each sector of the industry (road construction, project management, education, research, structural, supervision, hidraulic, earthworks, residential, materials production, comercial, etc.)\n\nAnalyze the situation of the companies where the civil engineers work (quantity of employees, local or overseas firm, stability)\n\nDetermine the average working time for a civil engineer\n\nDetermine the existing gender gap in the industry\n\n&amp;#x200B;",
        "created_utc": 1537378761,
        "upvote_ratio": ""
    },
    {
        "title": "Non-Parametric Bootstrap: Ideas for a project",
        "author": "Darth_Marrr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h5kmn/nonparametric_bootstrap_ideas_for_a_project/",
        "text": "I am taking a grad topics class on Non-Parametric models and I have chosen to do a final topic presentation on the non-parametric bootstrap. I would like to present on a unique slant to this topic, it is open ended, but I would prefer to avoid re-simulating the data from the textbook.\n\nWhat are interesting questions/simulations/ideas on the non-parametric bootstrap that would make for an interesting study and presentation?",
        "created_utc": 1537366946,
        "upvote_ratio": ""
    },
    {
        "title": "How to measure treatment effect on an ordinal variable?",
        "author": "One_Piezoelectricity",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h57b4/how_to_measure_treatment_effect_on_an_ordinal/",
        "text": "Hi guys,\n\nI have a large number of individuals who are observed at two points in time, t0 and t1.\n\nAt each point in time a person belongs to a societal group with an ordinally ranked status (from lowest status 1 to highest status 10).\n\nI want to measure if the exposure to a binary treatment between t0 and t1 significantly changes a person's status.\n\nWhat would be the most suitable statistical test to do this?\n\nI am mostly unsure about how to deal with the initial status.\n\nIf I use e.g. ordinal logistic regression, do I simply include initial status as a control variable? Or does it make more sense to make the *change in status* from t0 to t1 the dependent variable and let this be a function of the dummy?",
        "created_utc": 1537364149,
        "upvote_ratio": ""
    },
    {
        "title": "Thesis Adviser bailed on me.. I have survey data and not sure what to run",
        "author": "MrJeef",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h4svt/thesis_adviser_bailed_on_me_i_have_survey_data/",
        "text": "I have a small sample (\\~30 respondents) of survey data (25 questions) which is all in likert scale (1-5). I have 5 'business performance' questions and several variables I'm testing to see if those who utilized certain techniques had better performance.\n\nThis instrument was designed well, and covers the topic of my thesis, surveying industry professionals. I have my Literature Review done, and my Methodological review almost done, and so now is the time to run the numbers. \n\nThe only thing is that my thesis adviser left the university over the summer, and I don't know how she planned to run these, \n\nI've played with regression before, but I recall her saying we could do T testing and Anova on this. I tried to download SOFA and run some numbers there, but it's not like things I've done before and so I'm not sure which test to choose, when I use Anova it says I don't have data in certain groups depending on the variable.. which is also likely very true, it is a small sample. \n\nBasically, is t here any software that you can point me towards where I can run some low level correlation as I described in the first line? I'm a total newbie and I don't know where to start.. but can make sense of results once I know what number to look at.",
        "created_utc": 1537360902,
        "upvote_ratio": ""
    },
    {
        "title": "Question for Hotelling's T2 One Sample test. Am I making the right assumptions?",
        "author": "Casual-Swimmer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h17o7/question_for_hotellings_t2_one_sample_test_am_i/",
        "text": "TLDR:  Hotelling's T2 One Sample Test is applicable if the number of samples is greater than the number of parameters.  \n\nSo I'm thinking for an experiment running a Hotelling's T2 test, however, I'm not sure if my assumptions are correct.  I want to compare a single sample vector (n = 1) with 10 parameters (k = 10) to their expected values to determine if the sample is significantly different from the given population.  Some of the parameters are correlated to each other.\n\nI was thinking of using the Hotelling's T2 One Sample test.  The test assumes n = number of samples, and k = number of parameters for each sample.  Calculating the p value can be accomplished by transforming the T2 statistic to an F statistic and finding the value in the appropriate F-distribution with degrees of freedom (k, n-k).  However, for my example, since I'm comparing only a single sample to a standard mean, the degrees of freedom would be (10, -9) and there are no tables for such values.  In order to work, n&gt;k.  Site describing the test can be found [here](http://www.real-statistics.com/multivariate-statistics/hotellings-t-square-statistic/one-sample-hotellings-t-square/). \n\nSo based on my understanding, the Hotelling's T2 test only applies if there are more samples than parameters being compared.",
        "created_utc": 1537323531,
        "upvote_ratio": ""
    },
    {
        "title": "Multiple linear regression - correlating independent variables but low VIF",
        "author": "capacop",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h13an/multiple_linear_regression_correlating/",
        "text": "I am looking at the effects of both depth of turbines and distance offshore wind projects are built to shore on the overall capital expenditure of the projects. I am looking to test this via regression analysis.\n\nBoth distance to shore and turbine depth are pretty strongly correlated with an r of .784, suggesting multicollinearity, meaning perhaps a multiple regression analysis cannot be applied here. However when performing a collinearity diagnostics in SPSS, the VIF for both independent variables is 2.510, which suggests the collinearity is low.\n\nhowever I am not sure whether testing them via multiple regression is the right approach, due to the high correlation. Would I be better off performing two separate single regressions? The high R number and low VIF has me stumped. Ideally I want to be testing both of these variables. Can anyone please advise? My statistics knowledge is fairly basic and I am struggling to wrap my head around all of this.\n\nThanks",
        "created_utc": 1537322551,
        "upvote_ratio": ""
    },
    {
        "title": "My Real Life Game Theory: Performance Appraisal via Paired Comparison",
        "author": "ticktockaudemars",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h0l76/my_real_life_game_theory_performance_appraisal/",
        "text": "My large company (&gt;2000 employees) does performance evaluations via paired comparisons. Is there a strategy I should use to maximize my overall status in the company? Should I rank those most comparable to me higher then those more distant than me? If everyone on my team used the same strategy, would we rank higher overall?\n\n&amp;#x200B;\n\nNB: this a thought experiment and an intellectual curiosity, I plan on actually ranking people fairly based on who I think is better.",
        "created_utc": 1537318453,
        "upvote_ratio": ""
    },
    {
        "title": "Analyzing Database with Complex Nesting Structures",
        "author": "statgirl25",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9h0188/analyzing_database_with_complex_nesting_structures/",
        "text": "Hi all,\n\n  \nI'm trying to understand how to best estimate a series of regression models on top of a fairly complex database structure (complex to me, at least). It's complex because it has a \"dyadic\" type of structure that introduces various forms of nesting. I don't think fixed effects are appropriate. I think some sort of random effects model might be appropriate, but the nesting structure is not the sequential structure I learned about in graduate school (e.g., employees neatly nested within schools, which are in turn neatly nested within school districts). I'm hoping someone can point me in the right direction. \n\n  \n I try to outline everything as clearly and concisely as possible below. \n\nUnfortunately, I can't describe the actual methodology and variables due to client confidentiality restrictions (it seems silly -- but I'm not supposed to. I'm also new, and this is my first job out of graduate school). In order to seek help from strangers online, I had to imagine a make-believe scenario and variables that introduced the same sort of statistical issues I'm facing. Please pardon any awkwardness that results. I also created a dummy database (pasted below this wall of text) so that it's easier to envision the database structure (but, whereas this dummy database has 50 observations, the database I'm responsible for has approximately 20K).  \n\nr/https://docs.google.com/spreadsheets/d/1WXyPIc5jGH_9Ts90wqCrnJT10jyNvHbeLxhm3KrJNC0/edit?usp=sharing\n\nI'm so extremely grateful for any and all advice. Also, if there is a more suitable forum where I might post this question, please let me know!\n\n* Employees (evaluators) are shown 10 other employees (alters) at random from a fixed pool of fifty and asked to evaluate the \"approachability\" of each alter. Thus, each evaluator occupies 10 rows in the database, one for each evaluation. These records are not independent: As with longitudinal data, each evaluator is nested in herself/himself. In the dummy database, I assigned different IDs to evaluators (\"evalator\\_id\"). These IDs map on to alter IDs. The focal dependent variable is shown in the final column (\"dv\\_evaluator\\_rating\\_of\\_alter\\_approachability\"). This final variable represents the score given by the evaluator (\"evaluator\\_id) about the specific alter who is listed in the same row. \n* We first need to estimate the effects of certain evaluator characteristics on the dependent variable. These associations are more intuitive in the context of my actual data and I'm struggling to think of suitable analogues: Perhaps there are reasons to believe that, on average, evaluators who are older (\"evaluator\\_age\") tend to evaluate alters as more approachable. And we want to estimate this effect. I believe that our need to estimate the effects of evaluator characteristics precludes us from introducing an evaluator fixed effect, since such a fixed effect would be perfectly collinear with e.g., evaluator age. \n* Second, we are interested in simultaneously modeling whether certain *alter* characteristics predict *evaluator* ratings.  As an example, perhaps alters who are higher or lower in intelligence (\"alter\\_iq\") are evaluated more favorably, on average. Thus, we wish to include alter intelligence as another independent variable predicting approachability, as perceived by evaluators. Notably, since evaluators were asked to rate the same pool of alters, the same alters appear in the database multiple times. This seems to introduce another layer of nesting that is difficult for me to get my head around.  The only type of random effects models that I'm familiar with from grad school followed a sequential nesting structure, where lower order grouping is nested within a higher order grouping (e.g., schools nested within districts). That doesn't seem to apply here. And adding a fixed effect for alter\\_id would wipe away important variance.\n* My major points of confusion stem from how to model evaluator characteristics and alter characteristics simultaneously, while accounting for the various forms of non-independence that are inherent to the data. I'm not sure if it matters, but I'll mention it just in case: We also control for a series of \"dyadic\" variables that reflect evaluators in relation to alters (dyadic is my terminology, as far as I know, since I don't know how best to describe these variables). A contrived example might be that, on account of homophily preferences or other such things, females evaluate other females as more approachable (I'm making stuff up here, but that probably happens). This could be reflected as a binary variable in the database, such that records are assigned the value 1 if evaluator\\_id and alter\\_id are the same gender, else 0 (\"same\\_gender\").\n\n  \n",
        "created_utc": 1537313977,
        "upvote_ratio": ""
    },
    {
        "title": "Clustering Factory Sites",
        "author": "PotatoGone_Wild",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gznof/clustering_factory_sites/",
        "text": "Hello everyone,\n\nI'm currently struggling with some manufacturing data and I am trying to create clusters without interfering myself. That is I don't want to decide what the clusters should be made of but let the k-nn, DBSCAN or some other clustering method give me the results.\n\nCurrently, I have what has been produced which day mapped to the factory location. I have 2 years of data. I can map this data with another database to get information about the produced product e.g. Process 1 is used, product 1 belongs to Category 7 etc.\n\n&amp;nbsp;\n\nBut I don't know how I can run the clustering algorithm on this data. Does it work if I aggregate the data monthly first, then create my own features based on the monthly aggregated data such that it is in the following order;\n\n&amp;nbsp;\n\nC1P= Category 1 Percentage\n\nPDS=Production Size\n\n&amp;nbsp;\n\n\nPlant| PDS Jan 17| PDS Jan 16 |C1P Jan 16| C1P Feb 16| etc...\n---|---|----|----|----|----|\nPlant 1 | 10000| 9000| 0.373| 0.279| c| \n\n&amp;nbsp;\n\nOr is there some other way I can approach this data?\n\nThanks a lot for the help!",
        "created_utc": 1537311096,
        "upvote_ratio": ""
    },
    {
        "title": "HW Chat?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gzb0h/hw_chat/",
        "text": "[deleted]",
        "created_utc": 1537308455,
        "upvote_ratio": ""
    },
    {
        "title": "Team Tournament Scoring Question",
        "author": "aia124",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gy7h3/team_tournament_scoring_question/",
        "text": "I'm coordinating a tournament where we're seeking to normalize (standardize?) our data. Can I get opinions on whether the method we're using makes sense?\n\n**BACKGROUND**:\n\nBasically, the tournament is 2 rounds. Each round features 3 judges evaluating 2 teams, and each team is comprised of 2 competitors. Scores are awarded individually, so at the conclusion of each round, the following is true: each competitor will have received 3 scores (one from each judge), and each judge will have awarded 4 scores (one for each competitor).\n\nHistorically, the scores for each competitor were added together to create the team score for the round. The round scores were then added together, and the teams were ranked. So this meant that each team's rank was based on the sum of: 3 scores for Competitor A for round 1, 3 scores for Competitor B for round 1, 3 scores for Competitor A for round 2, 3 scores for Competitor B for round 2.\n\n**CURRENTLY**:\n\nOur goal is to normalize the data.  We are planning to produce z-scores for each score that a competitor receives (i.e. each competitor will have 6 z-scores).  \n\n***My question is:*** once we have each competitor's 6 z-scores, what is the best way to reflect that data as a *team* score?  The current plan is to average each competitor's 6 z-scores (let's call the resulting average \"final score\"), and then to average the \"final scores\" for each team's two competitors.  *Does this make sense?*\n\nThanks so much. Not a statistician, but I can wield a spreadsheet pretty decently.\n\n",
        "created_utc": 1537300785,
        "upvote_ratio": ""
    },
    {
        "title": "Is 0,88 GFI acceptable?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gxeay/is_088_gfi_acceptable/",
        "text": "[deleted]",
        "created_utc": 1537295194,
        "upvote_ratio": ""
    },
    {
        "title": "How to fit residuals in time series?",
        "author": "Jenbo54",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gx9ko/how_to_fit_residuals_in_time_series/",
        "text": "If I have some time series data, then I can fit an AR(p) model on the data.\n\n&amp;#x200B;\n\nHowever, if I actually want to simulate from this data, I also need to provide a residual distribution. If I run arima.sim in R, then it just uses standard gaussian iid residuals, but this might not fit my data very well.\n\nHow do I choose residuals so that it fits my data? (By fit my data, I mean if I plot my simulation and my data in the same plot, they should be on the same scale).\n\n&amp;#x200B;",
        "created_utc": 1537294287,
        "upvote_ratio": ""
    },
    {
        "title": "How many questions can I delete to increase Model fit in Amos?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gv1i0/how_many_questions_can_i_delete_to_increase_model/",
        "text": "[deleted]",
        "created_utc": 1537278996,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a way to statistically control for a variable (attractiveness) in a within-subjects design?",
        "author": "faqir58",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gv0qz/is_there_a_way_to_statistically_control_for_a/",
        "text": "Let's say I have presented 3 men to participants (a given participant sees all 3 conditions): Andrew is 5'9\"; Bob is 6'0\"; and Craig is 6'3\". I ask participants about the **desirability** of each man, and also to rate his **physical attractiveness**. I want to find out if height influences a man's **desirability**. I want to somehow 'control for' the man's **physical attractiveness**. How do I go about this - some kind of ANCOVA perhaps?",
        "created_utc": 1537278831,
        "upvote_ratio": ""
    },
    {
        "title": "Paired Sample T-Test vs Independent T-Test",
        "author": "Oneiricer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gsvab/paired_sample_ttest_vs_independent_ttest/",
        "text": "Hi Guys,\n\nI would like your opinion on a group assignment I am working on. We are tasked with measuring the price of the exact same product at two separate super market duopolies to work out whether one store is cheaper than another in a statistically significant way.\n\n&amp;#x200B;\n\nI am arguing to use the paired sample t-test because it is the exact same product that we are measuring at two stores. (i.e. same brand, same manufacturer, same weight, etc). The assignment specifically asks us to measure the exact product that exists in both stores.\n\n&amp;#x200B;\n\nMy friends think it is independent because the two super markets control their own pricing etc and are independent of each other.\n\n&amp;#x200B;\n\nI am very confident i am right, just want to make sure that my confidence is based on statistically sound principles.\n\n&amp;#x200B;\n\nthanks\n\nOneiricer\n\n&amp;#x200B;",
        "created_utc": 1537257795,
        "upvote_ratio": ""
    },
    {
        "title": "Strength of relative risk vs odds ratio",
        "author": "GoProThrowO",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gq1mp/strength_of_relative_risk_vs_odds_ratio/",
        "text": "Hi everyone,\n\nNot a homework question.\n\nCan someone explain to me why relative risk is stronger than odds ratio?\n\nFor instance, if I did a retrospective study at a gym looking at how many people in the past two years were injured playing hockey and how many people were injured playing baseball, why is it better to say that you are (for instance) 5x more likely to be injured playing hockey than baseball, compared to saying if you were injured, it is 5x more likely that you were playing hockey?\n\nBoth of these would be retrospective. Would the relative risk one be called a cohort study and the odds ratio be case control?\n\nThanks!",
        "created_utc": 1537231475,
        "upvote_ratio": ""
    },
    {
        "title": "Average Treatment Effect",
        "author": "UtterGrandeur",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gpn9j/average_treatment_effect/",
        "text": "Hi,\n\nNot exactly understanding how ATE works here. \n\nLet's say I got a group of 1000 people. 600 are given some treatment. \n\nNow let's say of those who got treatment, they displayed X characteristic. \n\nThe 400 who instead got some placebo treatment, none of them displayed X characteristic. \n\n&amp;#x200B;\n\nHow would I calculate the average treatment effect here? Hopefully my above example makes sense ",
        "created_utc": 1537228302,
        "upvote_ratio": ""
    },
    {
        "title": "How to find error/SD of a set of values which already have an SD?",
        "author": "ice_shadow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gpjgc/how_to_find_errorsd_of_a_set_of_values_which/",
        "text": "So I am measuring size of nanoparticles, and the instrument outputs a certain SD already.\n\nNow for my data what I do is just average out the size and get a new SD. But that is different than the SD given by the machine. How do I account for the SD given by the machine as well as this for a \"full\" SD? \n\nI guess this is the same as how do I combine the different normal distributions. \n\nOr if I have a set of data where each point itself has an error, and then I want to average all of that how do I get the full error? \n\nI am aware of the sqrt(sum(s^2)) stuff but does that apply here? That isn't the same as a standars deviation of all the points though and doesn't account for that. ",
        "created_utc": 1537227496,
        "upvote_ratio": ""
    },
    {
        "title": "What is the difference between a confidence interval around SAMPLE mean and POPULATION mean?",
        "author": "Jenbo54",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9goaf9/what_is_the_difference_between_a_confidence/",
        "text": "Consider the following bootstrap procedure:\n\n&amp;#x200B;\n\nSimulate a sequence of standard normal variables. \n\nTake the sample mean. \n\nRepeat a million times.\n\n&amp;#x200B;\n\nThis procedure will give you a million sample means. Using these, you can generate a confidence interval by taking the 2.5% and 97.5 % quantiles. \n\n&amp;#x200B;\n\nNow, here's my question. I have seen people argue that this is a \"bootstrap confidence interval around the true mean\".... but, clearly, it isn't? Rather, it's a confidence interval around the sample mean, right? I mean, I've taken a million sample means, and then I've constructed a confidence interval based on that. \n\n&amp;#x200B;\n\nSo what is the difference between a confidence interval around a sample mean versus a confidence interval around the true population mean?",
        "created_utc": 1537218216,
        "upvote_ratio": ""
    },
    {
        "title": "A lot of classification methods (k-nearest neighbours, nearest centroid, k-means, etc) don't require a parameterized model. What can we say about the conditions under which they are valid (e.g. the range of parameterized models which they will work for)?",
        "author": "willbell",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gna9v/a_lot_of_classification_methods_knearest/",
        "text": "With a lot of traditional statistics, like ordinary least squares there's this assumption that the data say is normally distributed around a line with constant slope say.\n\nFor something like k-nearest neighbours, there are no modelling assumptions like that.  However there are certainly circumstances where it applies and circumstances where it does not (e.g. if the categories have identical distributions with respect to your measured variables, then obviously you aren't going to be able to use k-nearest neighbours or anything else to classify the points), so I am wondering if we know what those conditions are explicitly for some of these non-parametric methods.\n\nIf the answer is 'just go read XYZ [e.g. *Elements of Statistical Learning*]  textbook', I count that as answer, although esp. exact chapters is useful.",
        "created_utc": 1537211348,
        "upvote_ratio": ""
    },
    {
        "title": "Question about sample size",
        "author": "forgotmypassword314",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gn6yw/question_about_sample_size/",
        "text": "I am strangely confused about sample size.  Let me provide an example question.\n\nI have 3 schools, with 5 classes each.  For each month in a particular time period, I count the total times that students have left the classroom (assume each of the school/classes have the same number of students so no mean is needed, etc.)\n\nNow, I decide to try to predict the number of times that students will leave the classroom for the next 3 months.  I use two different methods to try predicting.  I get the predictions, and decide to do a t-test to determine the differences in the methods.  I tried 2 different ways of doing the t-test, and I'm not sure which way is \"better\"/\"right\" and what is the difference in interpretation of them both, so please help!\n\n(1) For each school/class, I compute the squared error (SE) for each of the 3 predicted months.  Thus, I have (3 SEs \\*3 schools \\* 5 classes) = 45 SEs for each method.  I do the t-test (n=45...?) and get t = 2.17, p=0.03, which is statistically significant.\n\n(2) For each school/class, I compute the \\*MEAN\\* squared error (MSE) over 3 predicted months.  Thus, I have 15 MSEs (because now I have 1 value for each month instead of 3 as above) for each method.  I do the t-test (n=15....?) and get t=2.04, p=0.044, BARELY statistically significant at a=0.05.\n\nI was lucky in that both of these showed significance, but could have easily been 0.055 and 0.045.  What would I have done then?\n\nSo my question is, which of these two ways was more \"correct\", and what is the difference in interpretation between them?",
        "created_utc": 1537210753,
        "upvote_ratio": ""
    },
    {
        "title": "SPSS Syntax question - advice on using multiple IF statements",
        "author": "jmpsychresearcher",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9glg4t/spss_syntax_question_advice_on_using_multiple_if/",
        "text": " \n\nI want to create groups based on responses to several variables (BMI, sex and eating behaviour). Something like the below:\n\nCOMPUTE GROUPS\\_EDEQ14 = 0.\n\nIF (BMI =1 AND Sex = 1 AND EDEQ14 =0) GROUPS\\_EDEQ14 = 1.\n\nIF (BMI =1 AND Sex = 2 AND EDEQ14 =0) GROUPS\\_EDEQ14 = 2.\n\nIF (BMI &gt;1 AND Sex = 1 AND EDEQ14 =0) GROUPS\\_EDEQ14 = 3.\n\nIF (BMI &gt;1 AND Sex = 2 AND EDEQ14 =0) GROUPS\\_EDEQ14 = 4.\n\netc...\n\nhowever, for one of the arguments I want to include a range (1-4) from the EDEQ14 variable and I don't know how to express this. I've tried the below but they didn't work. Help would be appreciated!\n\nIF (BMI =1 AND Sex = 1 AND EDEQ14 = 1 thru 5) GROUPS\\_EDEQ14 = 5.\n\nIF (BMI =1 AND Sex = 1 and EDEQ14 &gt;0 &lt;5) GROUPS\\_EDEQ14 = 6.\n\nIF (BMI = 1 AND Sex = 1 AND EDEQ14 1-5) GROUPS\\_EDEQ14 = 7.",
        "created_utc": 1537199293,
        "upvote_ratio": ""
    },
    {
        "title": "What can I do to increase low AMOS values?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gkw6s/what_can_i_do_to_increase_low_amos_values/",
        "text": "[deleted]",
        "created_utc": 1537195336,
        "upvote_ratio": ""
    },
    {
        "title": "Basic - Binomial distribution",
        "author": "tukeysbinges",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gk969/basic_binomial_distribution/",
        "text": "I've forgotten how to consider this type of question. \n\nI have draws with replacement from \n\n1,3,5,7\n\nand want to know the chance that in 400 draws there will be &lt;= 90 threes selected. \n\nSo this is like drawing from \n\n0,0,0,1\n\nand we want to know the chance of getting 90 or less from 400 draws. \n\nI think that I can use the normal approximation fine here, but I would like to use the binomial distribution itself ( rather than the normal approximation ) and I'm completely drawing a blank as to how I should consider this. \n\nThanks :)  ",
        "created_utc": 1537190454,
        "upvote_ratio": ""
    },
    {
        "title": "Estimate interval (help me solve this stat problem!)",
        "author": "milktea46",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ghrsz/estimate_interval_help_me_solve_this_stat_problem/",
        "text": "An internet cafe owner found out that 100 of his customers spend an average of 15hours a week on researching with a standard deviation of 4 hours. Estimate the interval for the average time spent by customers for research.\n\n",
        "created_utc": 1537163807,
        "upvote_ratio": ""
    },
    {
        "title": "Help with intro stats - probability of game",
        "author": "reddibeth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ggh6z/help_with_intro_stats_probability_of_game/",
        "text": "Hi! \n\nThis is a little question that my prof went through like the second day of class but I'm really struggling with it.\n\nThe question is - A and B play a fair game 3 times, what is the probability B wins exactly 2 games? \n\n&amp;#x200B;\n\nMy prof said it was 3/8 but I have no idea how he got there. So far I've tried to write out all the possibilities: which would be: AAA, AAB, ABB, BBB correct? Shouldn't that result in a 1/6 chance that B wins 2 games?\n\n&amp;#x200B;\n\nAny help would be appreciated. Thanks!!! :) ",
        "created_utc": 1537150970,
        "upvote_ratio": ""
    },
    {
        "title": "Rolling a 6 sided die and independent probabality.",
        "author": "helpmewithmath19",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ger3f/rolling_a_6_sided_die_and_independent_probabality/",
        "text": "So, the example of rolling a die once is clearly independent from rolling a die again. However I dont understand how set A will have the same probability of occourng presuming B occurs. \n\n&amp;#x200B;\n\nThe book I am reveiwing gives the example as follows: \n\nA={2,4,6} and B={5,6}\n\nP(A)= 3/6 = 1/2, while, presuming B occurs the probablility that A occurs is shown as |A∩B|/B= {6}/{5,6}=1/2 the same probablilty as A occuring independently. \n\n&amp;#x200B;\n\nSo all this makes sense to me but this bereaks down when I changed the sets to confirm my understanding. In the example I used for myself let A={1,2,4,5} and B={2,3,6}\n\nSo then P(A) would = 4/6 = 2/3 but while B occurs|A∩B|/B= {2}/{2,3,6}=1/3 \n\nThis is where I am confused. Shouldnt the probability of A be equal to|A∩B|/B?\n\n&amp;#x200B;\n\nThanks!",
        "created_utc": 1537136328,
        "upvote_ratio": ""
    },
    {
        "title": "Regression Test, test if alpha, beta = 0",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gei45/regression_test_test_if_alpha_beta_0/",
        "text": "[deleted]",
        "created_utc": 1537134452,
        "upvote_ratio": ""
    },
    {
        "title": "Response Surface Methodology in AP Statistics",
        "author": "ChiBetaMu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gecel/response_surface_methodology_in_ap_statistics/",
        "text": "Can I present response surface methodology to my student to whom I teach AP Stats, who has taken AP Physics, AP Calculus BC?\n\nI introduce some mathematical stats to said student (I do not assess him on it). Would it be appropriate to assign a watered down experiment that employs response surface methodology. He wants to be an engineer and I would love to show him this process with \\[this experiment\\]([https://statacumen.com/pub/Erhardt\\_STATS\\_48feature\\_Designing\\_a\\_Better\\_Paper\\_Helicopter\\_webversion.pdf](https://statacumen.com/pub/Erhardt_STATS_48feature_Designing_a_Better_Paper_Helicopter_webversion.pdf)). \n\n&amp;#x200B;\n\nIs this feasible? We don't cover RSM in AP Stats, but we do cover regression and factorial designs (albeit superficially).",
        "created_utc": 1537133212,
        "upvote_ratio": ""
    },
    {
        "title": "If I delete an item to increase Cronbach Alfa, should I use it on AMOS test or not?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gcr8r/if_i_delete_an_item_to_increase_cronbach_alfa/",
        "text": "[deleted]",
        "created_utc": 1537121673,
        "upvote_ratio": ""
    },
    {
        "title": "Basic probability, biased coin",
        "author": "tukeysbinges",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gcgko/basic_probability_biased_coin/",
        "text": "\nQuestion : \n\nA biased coin has a 0.1 chance of landing on heads. If tossed 400 times,\nestimate the change of getting exactly 40 heads.\n\n***********\n\nI'm not sure how to go about this. \n\nI get htat I can model the choice as drawing with replacement from \n\n        [0,0,0,0,0,0,0,0,1]\n    \nwhere 0=tails and 1=heads\n\nHow do I approximate the chance of exactly 40 though? \n\nBecause we have a large number of tosses ( 400 ) we can use the normal\ndistribution, so 40 is going to be 39.5 -&gt; 40.5 \n\nThen we want to find the area between these points? \n\nSo standardising each of them we have \n\n        (39.5 - 40)/6 = -0.0833\n        (40.5 - 40)/6 =  0.0833\n\nAnd the area between these two points is found using \n\n        In [19]: a = st.norm.cdf(-0.0833)\n\n        In [20]: b = st.norm.cdf(0.0833)\n\n        In [21]: a\n        Out[21]: 0.4668065001479894\n\n        In [22]: b\n        Out[22]: 0.5331934998520106\n\n        In [23]: b - a\n        Out[23]: 0.06638699970402118\n\n\nSo that I would answer 0.06, or 6 percent. \n\n\n**Does the above make sense?**\n\nThanks",
        "created_utc": 1537119575,
        "upvote_ratio": ""
    },
    {
        "title": "Problem in accuracy when calculating p value from a 95% confidence interval",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gb78e/problem_in_accuracy_when_calculating_p_value_from/",
        "text": "[deleted]",
        "created_utc": 1537110206,
        "upvote_ratio": ""
    },
    {
        "title": "How do I compare differences grand means with large variance at the subject-level",
        "author": "FireBoop",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9gax1d/how_do_i_compare_differences_grand_means_with/",
        "text": "I will try my best to word this. My question has something to do with a fixed-effects analysis I think. \n\nI have data on 25 subjects. In conditions A and B, subjects completed both and did 10-30 trials of each. I want to compare whether the grand mean of my dependent variable is different between A and B. \n\nI can do this by calculating the averages for each subject. And the performing a simple t-test for comparing means of the 25 data points for each A and B. However, this would value all points equally. What if subject 1 did the A trial 10 times and had greater within-subject variance than subject 2 who did the A trial 30 times. Shouldn’t subject 2 contribute more to the grand mean? (however not more than 3 times more?). This was my main question. How would within subject variance affect how the subject’s mean is used as a data point?\n\nI don’t think it is correct to use every individual trial results as a datapoint and get N=500. \n\n",
        "created_utc": 1537107851,
        "upvote_ratio": ""
    },
    {
        "title": "Normal Distribution explanation needed for coin toss question",
        "author": "extwa1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ga4x8/normal_distribution_explanation_needed_for_coin/",
        "text": "I'm having difficulty understanding the basics of the normal distribution.\n\nIn a large class of students, the lecturer asks each student to toss two coins 30 times and calculate the proportion of times the coins turned up 2 heads. The students then report their results and the lecturer provides a histogram.\n\nQuestion: What shape would you expect this histogram to be?\n\nSo the answer is a normal distribution but I don't understand how this can be, if the probability of getting 2 heads is 0.25. Could someone please help explain this to me?\n\nThank you.",
        "created_utc": 1537100336,
        "upvote_ratio": ""
    },
    {
        "title": "Negative t-value in one-way ANOVA planned comparisons test - What does this mean?",
        "author": "aiioe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g7uws/negative_tvalue_in_oneway_anova_planned/",
        "text": "**What does a negative t-value in the contrast test imply? And a negative value of contrast?** \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThe details of my analysis are as follows: \n\n&amp;#x200B;\n\nThree groups are being analysed, based on 2 hypotheses. I've run these as two contrasts in the same one-way analysis. \n\n1. A &gt; B&amp;C\n2. B &gt; C\n\nThe second contrast has returned the following results in SPSS: \n\nValue of contrast: \\-4.48\n\nStd. Error: 1.36\n\nt: -3.29\n\ndf: 60\n\nSig (2 tailed): .002\n\n&amp;#x200B;\n\nThe means plot indicates group B scores lower than group C. ",
        "created_utc": 1537071421,
        "upvote_ratio": ""
    },
    {
        "title": "Wilcoxon signed test power graph R.",
        "author": "AkemiSuzuki",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g7lfd/wilcoxon_signed_test_power_graph_r/",
        "text": "I’ve been trying to calculate and graph the correspondent power for the wilcoxon signed test in R  and haven't had any luck yet. \n\nI tried simulating two normal distributed samples, apply the wilcoxon test and finally calculating the p-value, but i'm not sure on how to obtain the graph for it, what I'm looking for is something like this posted by the user Glen_b in CrossValidated. \n\nhttps://i.stack.imgur.com/1tKtR.png\n\nIf any of you could help me out, I would totally appreciate it! ",
        "created_utc": 1537068745,
        "upvote_ratio": ""
    },
    {
        "title": "Help understanding Outlier Tests",
        "author": "ice_shadow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g5slf/help_understanding_outlier_tests/",
        "text": "So I am looking into Dixons Q Test or the Grubbs G Test for some physical chem data I got where 1 point is an outlier. I am trying to discard that point and it seems like I need to apply one of these tests to do so. \n\nI notice that most places use the 2 sided test, and then mention the 1 sided test in passing.\n\nhttps://www.itl.nist.gov/div898/handbook/eda/section3/eda35h1.htm\n\nI don't understand why you wouldn't use a 1 sided test and why do most articles just use the 2 sided? Isn't an outlier always going to be the biggest or smallest point in a data set? So why wouldnt you use the 1 sided?\n\nThis is important for me since my data point could be regarded an outlier under the 1 sided test but not the 2 sided test unless i change the significance level of 0.05 for 2 sided\n\nAlso, there are some other tests out there like what matlab uses here https://www.mathworks.com/help/matlab/ref/isoutlier.html which looks at the MAD median avg deviation. Under that test, I am also able to discard my point. \n\nIts important for me to justify removing that point as it makes my fit a lot better. \n\nI don't know for sure, but its possible that bad pipetting messed up that point I am trying to remove and my PI says outlier tests are the way to justify removing points. ",
        "created_utc": 1537052332,
        "upvote_ratio": ""
    },
    {
        "title": "Logistic regression in SPSS help",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g4rr5/logistic_regression_in_spss_help/",
        "text": "[deleted]",
        "created_utc": 1537043929,
        "upvote_ratio": ""
    },
    {
        "title": "Logistic regression in SPSS HELP!",
        "author": "somebiochemmajor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g4nuf/logistic_regression_in_spss_help/",
        "text": "[removed]",
        "created_utc": 1537043108,
        "upvote_ratio": ""
    },
    {
        "title": "How to get good marks in statistics?",
        "author": "eneajansen17",
        "url": "https://www.merchantcircle.com/blogs/statistics-assignment-help/2018/9/How-to-get-good-marks-in-statistics-/1560237",
        "text": "",
        "created_utc": 1537042684,
        "upvote_ratio": ""
    },
    {
        "title": "Stronger correlation with same regression coefficient?",
        "author": "alabrasa301",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g3k5n/stronger_correlation_with_same_regression/",
        "text": "So imagine there are two scatterplots of X and Y and each graph includes a line of best fit. These have the same y-intercept (bo), and the same slope (b1). BUT, the observations in graph 1 are much closer to the line of best fit than the observations in graph 2. Is it fair to say that X and Y have a \"stronger correlation\" than the relationship in the second graph? Even when the slope of the line of best fit is the same?\n\nThanks!",
        "created_utc": 1537034673,
        "upvote_ratio": ""
    },
    {
        "title": "doubts about propensity score matching.",
        "author": "rickz123456",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g3j7l/doubts_about_propensity_score_matching/",
        "text": "Hello everyone.\n\nI'm developing a study where I'm using propensity score matching for the first time.\n\nIn the questionnaire to the companies that I am analyzing, I have wights. my question is: What is the advantage / disadvantage of using the command the pw option for weights? The results obtained are different, although not very much. Why? (comand pscore).\n\nSecond question: between the pscore command and teffects the value of std errors are different, why?\n\nThank you all for helping.",
        "created_utc": 1537034503,
        "upvote_ratio": ""
    },
    {
        "title": "Is it possible to derive probability from a regression?",
        "author": "IBelieveInLogic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g2gye/is_it_possible_to_derive_probability_from_a/",
        "text": "Let's say I have and independent variable x and dependent random variable y, described by the regression y = a\\*x+b.  I would like to find the probability distribution for y.  Is there a way to get this from information about the regression, like the coefficient of determination?  Or would I need the the variance of the original data set?  Or is this just something that isn't a viable application?\n\n&amp;#x200B;\n\nThanks.",
        "created_utc": 1537026591,
        "upvote_ratio": ""
    },
    {
        "title": "Help me with probability distribution????",
        "author": "DaeguDude",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g256b/help_me_with_probability_distribution/",
        "text": "Please help me with probability distribution. Here's the example that is on my book.\n\n&gt;Ex3.7) There are 20 computers in a factory. 5 are bad computers, and 15 are good computers. And you pick 3 computers randomly, and let's define the number of bad product as X. Find a probability distribution of X.\n\n&amp;#x200B;\n\nhttps://i.redd.it/4v0g7ulw1fm11.png\n\nAnd the picture above was the calculation process of the example. But I don't get how they came up this calculation process.\n\n&amp;#x200B;\n\nSo, what's my calculation process is is,\n\n&amp;#x200B;\n\nhttps://i.redd.it/hgvefja63fm11.png\n\nSo basically, because when you first pick, there's 20 computers and you are picking a good computer, and then second pick there's 19 and also you are picking a good computer, and then third pick there's 18 and also you are picking a good computer again. But when there's bad computer, I calculated with the probability that I was going to pick a bad computer. That's how I calculated it. **And something kind of confuses me here is, when it's Probability of bad computer is 1 and 2, I had to multiply 3 at the end. I don't know why, but to get a result 1, I had to.**\n\n&amp;#x200B;\n\n**So, TL;DR, my question is,**\n\n1. **How did they(book or author) came up with that calculation process?**\n2. **Why did I have to multiply 3 at the end when Probability of bad computer is 1 and 2?**",
        "created_utc": 1537023963,
        "upvote_ratio": ""
    },
    {
        "title": "Event A has a 1.125% chance of occurrence per attempt and event B has a 4% chance of occurrence per attempt. Given 50 attempts, what is the probability of Event A and Event B happening at least once each?",
        "author": "TheReformedBadger",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g1kuh/event_a_has_a_1125_chance_of_occurrence_per/",
        "text": "",
        "created_utc": 1537019357,
        "upvote_ratio": ""
    },
    {
        "title": "How on Earth am I supposed to find SY*^2? I think I'm suppossed to use R software",
        "author": "thewickedalf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9g0snc/how_on_earth_am_i_supposed_to_find_sy2_i_think_im/",
        "text": "So, I'm stuck in a exercise, where they give me that y=0.2x+9 and that the mean of x is 5, the mean of y is 10. Sx=2 Sy=3 and Sxy0.8\nAnd they ask me to find the variance of the expected values which is nothing more than SY*^2 \n\n\nI think I should use R if it's possible because in the next section of the exercise they ask me for the determination cofficient of that adjustment. \n\nI know the theory, I know what's is the covariance, and standard deviation, I know how to find the linear model, using R, from a bunch of data, and I know what the determination coefficient (R^2) mean. But I've been literally half hour searching for this, I know it's easy, but I just don't have enough examples, my professor limits himself to put really easy stuff on class, and now he destroys us with this",
        "created_utc": 1537011116,
        "upvote_ratio": ""
    },
    {
        "title": "I don't understand why the principle of restricted choice is valid. Can anyone explain?",
        "author": "hyphenomicon",
        "url": "https://en.wikipedia.org/wiki/Principle_of_restricted_choice",
        "text": "",
        "created_utc": 1536995786,
        "upvote_ratio": ""
    },
    {
        "title": "STT 200 Question: Seems easy but confusing",
        "author": "mannigo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fysjn/stt_200_question_seems_easy_but_confusing/",
        "text": "So this stats class looks easy, but the terminologies seems to be confusion.\n\nThe homeworks have only 2 attempts. I already used one attempt. I got the first two and last two questions wrong. I need help (and explanation). Thanks.\n\n[https://imgur.com/a/U4oeCjz](https://imgur.com/a/U4oeCjz)\n\n&amp;#x200B;",
        "created_utc": 1536985816,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone explain Bayes' Theorem using this example. I already missed the question. I just want to understand. Correct answer is included.",
        "author": "Yesrek",
        "url": "https://imgur.com/IjMbjLk",
        "text": "",
        "created_utc": 1536964499,
        "upvote_ratio": ""
    },
    {
        "title": "RM ANOVA vs Mixed Modeling: questions re: interpretation and reporting",
        "author": "expecto_patronum_1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fw7z9/rm_anova_vs_mixed_modeling_questions_re/",
        "text": "Hi\nI posted a related question, and although I was not able to receive help with that here, I did work it out. However, I now have a question regarding what to report. \nI have some scores across time (15\", 30\", 45\" and 60\") and two groups (have Broad Autism Phenotype and do not have Broad Autism Phenotype). \nIn the RM ANOVA, the linear and quadratic trend were both significant. The cubic was not. This is what I wrote up: A Repeated Measures ANOVA with Broad Autism Phenotype Status as the between-group variable and 1st-4th 15” intervals as within-subjects variables indicated a significant main effect of time (F=8.60, p&lt;.001) but no interaction regarding Broad Autism Phenotype group membership (F=0.069, p=0.55). Examination of within subjects contrasts indicated a significant linear (F=10.53, p=.001) and quadratic effect (F=11.84, p=.001), but no cubic effect (F=2.32, p=0.13).\nIn the Mixed Model, the linear, quadratic, and cubic trends were all significant and each additional term improved the model fit as indicated by change in -2LL. With the cubic trend, the change was much smaller even though it was significant (see below). All the covariance matrices parameters stuff was significant regarding the randomness of intercepts, etc. \nI can email output to anyone who needs it for more info. \nHere are my questions:\n1. Should I report the MLL instead of the RM ANOVA (it seems like I would) ?\n2. As no covariance parameters were significant, does this suggest that there is just one distribution underlying the data, and/(or, if I have the first part incorrect) that the findings hold across subjects--there is no indication that there is a systematic difference in performance across BAP/Non-BAP groups? \n3. I ran the model with and without BAP/Non-BAP as a variable (BAP is not significant). Was this redundant, due to the covariance matrices? \n4. Is it reasonable to say that a quadratic trend best fit the data based on what I said above? The -2LLs were for linear=3450.04, quadratic=3430.82, cubic=3425.20. So very small change for cubic, even though it was just significant at p=.05. \nI will be unavailable this evening (MNSSHP) but will check and reply for questions after.\nThanks (very much) in advance. \n\n",
        "created_utc": 1536962146,
        "upvote_ratio": ""
    },
    {
        "title": "Signal Probability Question",
        "author": "TheEighthRedCoin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9funmz/signal_probability_question/",
        "text": "I'm in a situation where I'm fairly certain there's a black and white way to solve this problem, but I just don't know what the method is.\n\n&amp;#x200B;\n\nHere's the background. I have a Boolean signal from a controls system that is being recorded at an instant once every minute. From a day's worth of data, the signal reads TRUE once to every five or so FALSE's. In other words, when my program reads this signal at an instant in time within the first minute, the signal happens to show TRUE. Each of the next 5 instants that a read occurs, the signal shows FALSE.\n\n&amp;#x200B;\n\nI'm trying to figure out how many TRUE's are being generated per unit time. The incorrect assumption would be to say it is 1 TRUE every 6 minutes, because this doesn't account for the times that a TRUE occurs apart from those 6 instants of time that the signal is checked (maybe a TRUE happens for a few seconds halfway between the two measurement instants). It's safe to say that if the TRUE signal persists for a full minute, 1 TRUE per 6 minutes would be the case, but the quicker the TRUE signal comes and goes, the more instances there will be that go undetected.\n\n&amp;#x200B;\n\nThe thing is, I DO know how long the TRUE signal persists (about 1.5 seconds).\n\n&amp;#x200B;\n\nHow do I translate this amount of time AND my current data set of TRUE's and FALSE's on the minute for a day to know how many total TRUE's there probably are during the whole day (or just per unit time)?",
        "created_utc": 1536950785,
        "upvote_ratio": ""
    },
    {
        "title": "Regression problem? Or something much simpler?",
        "author": "akurilin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fuk74/regression_problem_or_something_much_simpler/",
        "text": "Hey folks, here's a sample problem I'm trying to solve and I would love your advice:\n\n&gt;You run an ice cream store. You sell n different flavors of ice cream You want to drive people towards buying a subscription to your store. You have data for when people purchase at your store, what flavors they purchase, how often, and when they ultimately subscribe.  \n&gt;  \n&gt;You want to figure out if there are several common purchasing patterns over time that your customers fall into before they subscribe. E.g. pattern 1 is a lot of people buying chocolate ice cream 6 times the first month, and buying chocolate and pistachio 8 times the second month, then suddenly 80% of them decide to subscribe on month 3. It's possible there are a few such patterns, and you want to isolate the more prevalent ones so you can focus your product strategy on those. Or perhaps there are no such patterns, and then you should spend your product effort elsewhere.\n\nI’m guessing it’s some sort of a regression problem (gradient descent?) where the ice cream flavors are different dimensions.\n\nI'm also open to simplify the problem by removing the month by month component and just say \"if you purchase chocolate 14 times and pistachio 8 times then on month 3 you subscribe\".\n\nWhat are your thoughts? Is there a particular technique I should look into that would lead to the answers? Thank you.",
        "created_utc": 1536950127,
        "upvote_ratio": ""
    },
    {
        "title": "Survey Statistics",
        "author": "Darth_Marrr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fui1k/survey_statistics/",
        "text": "I am working on a research project for a committee at my University and they administered training to a subset of the colleges within the school. To determine the effectiveness of the training they presented a survey before the training and a survey approximately a year after the training. The vast majority of the questions (around 180) are discrete i.e. select how you feel on a scale of 0 - 5.\n\nI have pruned the 180 variables down to exactly 54, but my main hurdle is that there are only 400 participants, which is going to lead to an egregiously overfit model. I could further prune through variable selection, but that is another question entirely. There are, however, groups of questions that fall under the same theme that to are (as mentioned above) all discrete on a 0 - 5 scale.\n\nMy Question: Can I take these grouped variables and calculate a \"mean score\" by summing up the values and dividing by the number of questions? (assuming all the questions have equal weighting i.e. they have equal importance in determining training effectiveness) e.g. for a 5 question subset my answers could be: 1,4,5,3,2 and my mean score would be (1+4+5+3+2)/5 = 3. Now, I not only have a continuous variable, but I have reduced the 54 variables to less than 10. This is far more manageable and not as vertiginous as the original 180. Though, what would the downside be?\n\n",
        "created_utc": 1536949709,
        "upvote_ratio": ""
    },
    {
        "title": "Multivariate Questions",
        "author": "I_Shall_Be_Known",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fsjka/multivariate_questions/",
        "text": "Now, before I start I do want to stay that I’m not posting here really for an exact answer, but I just don’t even know quite where to start learning so any help would be appreciated. The reality of the data is much more complex, but for simplicity sake:\n\nSay I own 100 ice cream trucks that all have 8-15 brands of ice cream. Using past sales history I want to determine what the optimal assortment of ice cream would be for each van? Obviously I want to make sure that I take incremental value into effect to make sure I’m maximizing, but not missing sales.\n\nRealistically, there’s probably 20 other variables that would have a statistically significant impact on sales. Weather, location, inventory depth, etc. I’d like to eventually learn a way to incorporate these in, but for now learning the basics is probably my best place to start.\n\nDoes anyone have advice on where to start/where to learn more for this specific problem?",
        "created_utc": 1536935940,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics: Probability of Auto Accident",
        "author": "GrayFox_13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fq9ry/statistics_probability_of_auto_accident/",
        "text": "Let P(A|rain)=0.05, P(A|snow)=0.15, and P(A|sleet)=0.2\n\t\t\t\t\t\t\t\nIf the probabilities of rain, snow, and sleet on any given day are 0.3, 0.4, and 0.5. respectively, what is the probability of A (Auto accident)?\t\t\t\t\t\t\n\nI know the answer is 0.175 but I need to know how to get to that value. Can someone please guide me on how you solve this type of problem?",
        "created_utc": 1536913716,
        "upvote_ratio": ""
    },
    {
        "title": "Coefficient of Variation",
        "author": "mribeirodantas",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fnxtf/coefficient_of_variation/",
        "text": "Hi,\n\n&amp;#x200B;\n\nI have seen several papers using Coefficient of Variation (relative standard deviation) to compare gene expression data. There are some methodologies for working with microarray data (where there are many probes for the same gene) , for example, where they suggest one should choose the probe with the highest coefficient of variation, which means the probe that has the largest amount of variation/information.  \n\n\nThe probes have different means, but it's the same scale. I have read the CV is a good measure for cases where your sets of data have different means and scales but why is it so used for gene expression data? Is there other measures that could also be used or even better? I have seen people talking about sd/var instead of sd/mean, for example.",
        "created_utc": 1536889290,
        "upvote_ratio": ""
    },
    {
        "title": "How do I do the math using this EPnp(S,t) Function?",
        "author": "Acutely__Obtuse",
        "url": "https://i.redd.it/cix7fhdoq1m11.png",
        "text": "",
        "created_utc": 1536862983,
        "upvote_ratio": ""
    },
    {
        "title": "How do I do this math?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fkcl8/how_do_i_do_this_math/",
        "text": "[deleted]",
        "created_utc": 1536862071,
        "upvote_ratio": ""
    },
    {
        "title": "Nonmonotonic logistic regression",
        "author": "Chu_BOT",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fjo4j/nonmonotonic_logistic_regression/",
        "text": "I have a categorical dependent variable and an ordinal independent variable. Is there a method similar to logistic regression that I could use to determine whether my ordinal variable is \"too little\", \"just right\" or \"too much\"? It seems logistic regression has to be increasing with my independent variable. ",
        "created_utc": 1536857427,
        "upvote_ratio": ""
    },
    {
        "title": "Help Picking Groups that Vary",
        "author": "lemonliner",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fjned/help_picking_groups_that_vary/",
        "text": "Hey everyone, I have a quick question:\n\nI am picking 50 genetic lines amongst 150+ lines of fruit flies. I want to pick 50 that span across a natural variation scale of lifespan and lifetime fecundity. I have the averages for all of lines, but I don't know how to pick 50 that span the range of low to high when I'm dealing with two different variables. Sorry if I'm phrasing this oddly, haha. \n\nObviously if it was just one variable like just lifespan, then it would be easy to pick 50 lines that range from low to medium to high average lifespan, but I'm not sure how to do this when taking into account both lifespan and fecundity. \n\nAny advice would be appreciated! Thanks.",
        "created_utc": 1536857283,
        "upvote_ratio": ""
    },
    {
        "title": "In a bind with my textbook, hoping to get photos of four pages. (Statistical Concepts for the Behavioral Sciences, 4th Edition)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fjmio/in_a_bind_with_my_textbook_hoping_to_get_photos/",
        "text": "[deleted]",
        "created_utc": 1536857123,
        "upvote_ratio": ""
    },
    {
        "title": "Is it OK to use a t test to test for a difference between \"before and after\" samples after confirming there is no autocorrelation (or doing so on the residuals after removing the autocorrelation?)",
        "author": "statsnerd99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fikex/is_it_ok_to_use_a_t_test_to_test_for_a_difference/",
        "text": "Say you wanted to test univariate time series data for a difference in the average amount of events that happened each month between 2017 and 2016 (n = 12 for both years obviously), and you do an ARIMA and confirm there is no autocorrelation. Doing a t test is fine here?",
        "created_utc": 1536849700,
        "upvote_ratio": ""
    },
    {
        "title": "Genetic Link vs Probability of Genetic Link Question",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ficgs/genetic_link_vs_probability_of_genetic_link/",
        "text": "[deleted]",
        "created_utc": 1536848105,
        "upvote_ratio": ""
    },
    {
        "title": "question to Biostatisticians regarding \"Spearman Correlation Coefficient\"",
        "author": "AvadaaKedavraa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fh3q5/question_to_biostatisticians_regarding_spearman/",
        "text": "Good day,\n\nim not a biostatsitciain (nor statistician) , however as a beginner researcher i am expected to know a bit about statistical methods used in articles.  I'm doing a research and i came across one study that used \" Spearman correlation coefficient\". the study that used that was comparing two prediction tools (two coma scales and their ability to predict in-hospital mortality as an outcome). \n\n&gt;Spearman’s correlation coefficient between the PFSS and the GCS was calculated to assess the criterion validity of the PFSS. Spearman’s correlation coefficients between the GCS and PFSS were high (p = .87 for the first rating and .89 for the second rating).\n\ni researched that but i couldn't find the answer i was looking for.\n\n**Can the Spearman correlation coefficient be used to adjust confounding factors in an observational study? what is it actually used for in that context?**\n\n&amp;#x200B;\n\nthank you in advance,",
        "created_utc": 1536837520,
        "upvote_ratio": ""
    },
    {
        "title": "Interest in PhD in Statistics",
        "author": "chenxu9711",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fdo8r/interest_in_phd_in_statistics/",
        "text": "Hello all,\n\nI have some questions after describing my profile below. Some keywords are in bold faces. \n\nStarting in October, I will be an undergraduate junior at the University of Chicago with an interest in going to PhD programs in Statistics in the US after graduation. By the time I graduate, I will have completed **two majors** (Computational and Applied Math and Economics with Specialization in Data Science, a new thing offered at UChicago) and **a joint B.S./M.S. degree in Statistics**. So far, **regarding courses** that are proof-based and math-heavy, I have completed: \n\n1. A year-long sequence in Accelerated Analysis in R\\^n (MATH 20310-20510) (Grade: 2 A and 1 A-)\n2. Abstract Linear Algebra (MATH 20250) (Grade: A)\n3. Two courses in Basic Algebra (MATH 25400-25500) (aka. Abstract Algebra) (Grade: 2 A)\n4. Ordinary Differential Equations (MATH 27300) (Grade: A)\n5. Complex Analysis (MATH 27000) (Grade: A)\n6. Markov Chain, Martingales, and Brownian Motions (MATH 23500) (Grade: A-)\n7. Statistical Theory/Method (STAT 24400) (Grade: A-)\n8. Honors Econometrics (ECON 21030) (Grade: A)\n9. Decision and Strategy (ECON 20770) (Grade: A) (aka. Honors Game Theory that requires first quarter of analysis as a prerequisite)\n\nIn total, there are **12 courses above, with 9 A and 3 A - and a GPA of 3.9175**. I started my college with Analysis because I have learnt Calculus before. I omitted others humanity and social science courses, as well as some major courses required by the ECON degree, since I heard that they are less concerned by committees. I will start taking two graduate courses in Statistics that meet my MS degree requirement.\n\nIn my first year summer, I **completed a REU paper** in Algebraic Combinatorics and Representation Theory (URL: [http://math.uchicago.edu/\\~may/REU2017/REUPapers/Xu,Chen.pdf](http://math.uchicago.edu/~may/REU2017/REUPapers/Xu,Chen.pdf)) and in this past summer, I participated in an intensive two-week summer school in Machine Learning at TTIC (description: [http://ttic.uchicago.edu/\\~suriya/website-intromlss2018/index.html](http://ttic.uchicago.edu/~suriya/website-intromlss2018/index.html)) and started doing finance research for Professors at Booth School of Business that involve programming in R and reading literature on matrix completion. \n\nI have been contacting professors for **letters of recommendations** (I have two letters from professors in Math and ECON and one from my mentor for Math REU, but he is a graduate student). Nevertheless, I do have questions on who would carry the most credentials and/or write convincing letters. Please PM me if you have some advice.\n\nI **do not** yet have others publications but will have my Master Thesis completed by the time I graduate (which is June 2020)\n\nI **have not taken** the GRE but will have my score for both the general tests and MATH subject test ready by the time I apply in the 4th year.\n\nAfter presenting my relatively lengthy background information, I have **several questions in mind**:\n\n1. What are some **best PhD programs in Statistics**? I consulted US News ([https://www.usnews.com/best-graduate-schools/top-science-schools/statistics-rankings](https://www.usnews.com/best-graduate-schools/top-science-schools/statistics-rankings)) as well as TopUniversities ([https://www.topuniversities.com/university-rankings/university-subject-rankings/2018/statistics-operational-research](https://www.topuniversities.com/university-rankings/university-subject-rankings/2018/statistics-operational-research)) but those two gave drastically different rankings (For instance, UChicago is ranked #6 by US News but #19 by TopUniversities)\n2. Based on my background, **what schools should I look into** (Note, I am an international Chinese student, which may cause a huge difference in application)? I realize that **my main weaknesses** lie in my lack of publication and unfamiliarity with finding professors who will write me letters. What are other factors I might have overlooked and need to strengthen?\n\nAny help is highly welcomed and appreciated. Please PM me for more details if you need any.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;",
        "created_utc": 1536801739,
        "upvote_ratio": ""
    },
    {
        "title": "Creating new class variables for multi-level modeling",
        "author": "Sociological_Duck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fbwjz/creating_new_class_variables_for_multilevel/",
        "text": "I have a procedure for creating class variables out of the individual level variables. I.e. creating national rates of education by mean of respondents who are college educated. This procedure involves code where I specify country as the class variable. \n\nBut what if I am building one from scratch? For example, I take 30 Freedom House democracy scores from their website. Is it sufficient to write out \n\nIf country = ‘Albania’ then freedomscore = 5;\nIf country = ‘canada’ then freedomscore = 7;\nEtc?\n\nOr do I need additional lines of code to tie this to countries rather than individuals?  ",
        "created_utc": 1536787610,
        "upvote_ratio": ""
    },
    {
        "title": "How do I analyze/Model data with floor and ceiling effects?",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9fa2h3/how_do_i_analyzemodel_data_with_floor_and_ceiling/",
        "text": "&amp;#x200B;\n\n[Big, broad shoulders](https://i.redd.it/vjyhbl08jul11.png)\n\nSo I was just modelling some data that has been aggregated from an unknown questionaire. I am currently trying my hands on Path analysis with the lavaan library. One of the assumptions is that the data is multinormally distributed. As you can clearly see, it is not. Now, I know how to transform skewed data, but how do I deal with data that has shoulder pads for tails?\n\nMy intuition would be to transform the variable into a binary variable. However, lavaan offers a ton of alternative estimators. Should I simply use a robust estimator?",
        "created_utc": 1536775134,
        "upvote_ratio": ""
    },
    {
        "title": "What is my mistake in calculating genetics probability??",
        "author": "stairbender",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f9925/what_is_my_mistake_in_calculating_genetics/",
        "text": "This is a question that relies on some genetics, but ultimately, I think I'm making a statistics mistake.\n\n&amp;#x200B;\n\nWe know for certain that dad IS heterozygous for a trait (Aa), and mom has a 1/30 chance of being heterozygous (Aa) and a 29/30 chance of being homozygous dominant (AA). Their offspring will get one allele (either \"A\" or \"a\") from mom, and one from dad (either \"A\" or \"a\"). IF the parent is heterozygous (Aa), the probability of a parent passing on \"a\" is 1/2, and the probability of passing on \"A\" is 1/2, and the two events are mutually exclusive. If the parent is homozygous dominant (AA - like the mom may be), the probability of passing on \"A\" is 100%. What is the probability that the offspring is heterozygous (Aa)?\n\n&amp;#x200B;\n\nYou can draw out the Punnet Squares and see there's clearly a 1/2 chance of kid being Aa (the bolded squares represent possible outcomes for the offspring):\n\n&amp;#x200B;\n\nIF mom is AA (29/30 chance):\n\n||A (from mom)|A (from mom)|\n|:-|:-|:-|\n|A (from dad)|**AA**|**AA**|\n|a (from dad)|**Aa**|**Aa**|\n\nOf these four possible outcomes for their offspring, two of them are Aa, so the probability of the offspring being Aa IF the mom is AA is 1/2.\n\n&amp;#x200B;\n\nIF mom is Aa (1/30 chance):\n\n||A (from mom)|a (from mom)|\n|:-|:-|:-|\n|A (from dad)|**AA**|**Aa**|\n|a (from dad)|**Aa**|**aa**|\n\nTwo of these four possible outcomes are also Aa, so the probability of the offspring being Aa IF the mom is Aa is also 1/2.\n\n&amp;#x200B;\n\nThus, 1/2\\*1/30 + 1/2\\*29/30 = 1/2\n\n&amp;#x200B;\n\nI understand this, but when I tried to calculate this myself based on my understanding of probabilities, I got a completely different answer.\n\n&amp;#x200B;\n\nThe way I figure it, P(child being \"Aa\") = P(receiving \"a\" allele from either parent, but NOT from both)\n\n\\^Since both parents can only pass on either \"a\" or \"A\", and you can only receive one allele from each parent, and if a parent does NOT pass on the \"a\" allele, then they must be passing on the \"A\".\n\nP(child receiving \"a\" allele from dad) = 1/2 (since the dad is Aa, and the probability of either of the alleles being passed is the same).\n\nP(child receiving \"a\" allele from mom) = 1/30 \\* 1/2 + 29/30 \\* 0/2 = 1/60 (since the mom has 1/30 chance of being Aa, and a 29/30 chance of being AA and unable to pass on the \"a\" allele).\n\nP(child receiving \"a\" allele from BOTH dad and mom) = (1/30 \\* 1/2 + 29/30 \\* 0/2) \\* 1/2\n\nso P(child being Aa) = P(child receiving \"a\" allele from dad) + P(child receiving \"a\" allele from mom) - P(child receiving \"a\" allele from BOTH dad and mom) = 1/2 + 1/60 - 1/120 = 0.50833333333\n\n&amp;#x200B;\n\nTLDR: 1/2 does not equal 0.50833333333.\n\nWhere is this discrepancy coming from?",
        "created_utc": 1536769481,
        "upvote_ratio": ""
    },
    {
        "title": "Something's wrong with my trying to calculate genetic probabilities?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f94rq/somethings_wrong_with_my_trying_to_calculate/",
        "text": "[deleted]",
        "created_utc": 1536768663,
        "upvote_ratio": ""
    },
    {
        "title": "How am I supposed to construct a linear regression model with the equations below?",
        "author": "Thargarov",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f7iuf/how_am_i_supposed_to_construct_a_linear/",
        "text": "I asked the professor several times but the only she said is to take clue from the Gauss-Markov assumptions to solve this.\n\n&amp;#x200B;\n\nI'm really stumped on how to do this. If someone could just show me one example (ideally question e please!) I can do the rest.\n\n&amp;#x200B;\n\nHere are the assumptions and the question:\n\n&amp;#x200B;\n\n[https://imgur.com/a/vPSSwni](https://imgur.com/a/vPSSwni)\n\n&amp;#x200B;\n\nI would appreciate any help in this.",
        "created_utc": 1536757164,
        "upvote_ratio": ""
    },
    {
        "title": "Real live case with Pharmacy Panel - What test should I use?",
        "author": "PharmaQuestionsGuy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f78a1/real_live_case_with_pharmacy_panel_what_test/",
        "text": "Hi Guys, this is a real live case:\n\nWe have an universe of 6.000 Pharmacies, we have a panel with 3.200 – There are 20.000 physicians/doctors that prescribe to those pharmacies. In 1 year there are a total of 204.840.504 prescription being sold at pharmacies level every year, our panel collects 60% of this prescriptions - 122.904.302 prescriptions/year.\n\nI already tested with some sample data and I know that prescription per physician follows a poissom distribution with an average of 10.242 prescriptions per physician.\n\nAssuming that our panel is equally distributed by Pharmacy Potential/Type and Geographical.\n\nWhat test should I do to test the probability of capturing 100% of all Physicians prescribing at least once during 1 year?\n",
        "created_utc": 1536754744,
        "upvote_ratio": ""
    },
    {
        "title": "t result in a paper claiming to use a two way anova",
        "author": "VapeV",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f6c00/t_result_in_a_paper_claiming_to_use_a_two_way/",
        "text": "Hi guys, I'm pretty bad at statistics but usually I can rely on google for most of the information I need, however I am really stuck on this one and I hope I can get some feedback.\n\n&amp;#x200B;\n\nBasically, it's my understanding that when using an ANOVA, there should be an f statistic, unlike a t-test. The paper I am reviewing claims they  \"conduct a two-way between participants analysis of variance (ANOVA; Camera presence \\[yes/ no\\]  Facial activity \\[lips/teeth\\])\"  but in their results they all seem to have t stats not f stats. For example:\n\n Are the two simple effects different from each other? Although the test of the 2 x 2 interaction was greatly underpowered, the preregistered analysis concerning the interaction between the facial expression and camera presence was marginally significant in the expected direction, **t**(162)  1.64, p  .051, one-tailed, \u0006p 2  .016, BF  2.163. As predicted, the analysis did not reveal any main effect of camera presence or of facial feedback.  (note the square is suppose to be partial eta squared).\n\n&amp;#x200B;\n\nAlso just as a follow up question, using GPower, if i wanted to predict the required statistical power of a two way ANOVA would I use the *f* test \"ANOVA: Fixed effects, special, main effects and interactions\"?\n\n&amp;#x200B;\n\nThanks r/AskStatistics!\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;",
        "created_utc": 1536746115,
        "upvote_ratio": ""
    },
    {
        "title": "Can Chi Square Test of Independence determine dependency?",
        "author": "Darth_Marrr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f4e2b/can_chi_square_test_of_independence_determine/",
        "text": "According to the hypotheses, if we reject the null we are concluding that there exists a relationship between two or more categorical variables, however if a relationship is determined to exist can we say the variables are dependent based on this test?",
        "created_utc": 1536724279,
        "upvote_ratio": ""
    }
]