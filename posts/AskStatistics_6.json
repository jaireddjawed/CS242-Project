[
    {
        "title": "Raters rated differently",
        "author": "Unpaved_path",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ytrdf/raters_rated_differently/",
        "text": "I need help. I asked raters to identify the level of students activity. Most of their rating of levels are not in agreement with eah other. \n\nEx. The student asked a question \nRater 1: Level 1\nRater 2: Level 3 \nRater 3: Level 5 \n\nFinal level: ?????\nWhat do I do? Please assist me on this. Your help will be highly appreciated.",
        "created_utc": 1676040377,
        "upvote_ratio": 1.0
    },
    {
        "title": "Phone number question style",
        "author": "No_Positive3065",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10yt0p3/phone_number_question_style/",
        "text": "Hello everybody, \n\nSo I have a biostatistics class in my university and this professor gives us exam question \"write your phone number\" and then you put letters to them, so for example X=2, Y=3. Now in the next question if you are asked to find Y, or given X you have to simply replace it with the number it corresponds to from your phone number. With this he then asks several questions after so for example find the mean, the median, the frequencies, IQR, standard deviation etc. \n\nI just wanna know if anyone else knows about this. I need practice questions and I can't seem to find anything on the internet yet.",
        "created_utc": 1676038412,
        "upvote_ratio": 1.0
    },
    {
        "title": "Skewed residuals in robust regression",
        "author": "frauensauna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ys6ra/skewed_residuals_in_robust_regression/",
        "text": "Hi all. Previously I have used Beta regression to analyse my data. These data are vocabulary sizes of infants according to a parental checklist (very skewed and bounded data). Many researchers in the field started using robust regression to analyse these data. When running my models again (from Beta to robust), I find more or less the same results. Only one factor was significant in Beta and not significant in robust regression, and I assume it was because the effect was driven by some influential outliers that get less weight in robust regression. All fine, but now I am worried about (very) skewed residuals in the robust regression models. Is this a problem?",
        "created_utc": 1676036211,
        "upvote_ratio": 1.0
    },
    {
        "title": "Unintuitive bivariate plots",
        "author": "akshayawahi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ys37d/unintuitive_bivariate_plots/",
        "text": "I have a dataset with one of the variables being a categorical variable with the categories being (high / medium / low) income risk. Bivariates show that I have a higher default rate in the medium risk bucket which is counter intuitive as I expect it to be higher in the high risk bucket. \n\nI note that the categories are skewed and I have very low counts of high risk but I would still expect the event proportion to be higher in high risk. Apart from the possibility that my categories are not well defined, is there any other possible explanation for this trend?",
        "created_utc": 1676035941,
        "upvote_ratio": 1.0
    },
    {
        "title": "need help to rank climate models based on nmb and nmse",
        "author": "dyinglittlestar",
        "url": "https://i.redd.it/pkubtewxnaha1.jpg",
        "text": "",
        "created_utc": 1676005737,
        "upvote_ratio": 1.0
    },
    {
        "title": "Odds Ratio Interpretation Help - \"As likely\" vs \"More Likely\"",
        "author": "UnderstandingBusy758",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10yf7c3/odds_ratio_interpretation_help_as_likely_vs_more/",
        "text": "&amp;#x200B;\n\nSay we have data like this: \n\n||Case |Control|\n|:-|:-|:-|\n|Exposed (Disease)|10|2|\n|Not Exposed (Disease)|5|20|\n\nWe end up with odds ratio of: (10)\\*(20)/(5\\*2) = 20\n\nHow would you interpret the odds ratio? Which one of these are right vs wrong? Are there better interpretations/\n\n\\- \"The odds of Case Group being exposed to the Disease is 20 times *more likely* compared to the Control Group?\"\n\n\\- \"The odds of Case Group being exposed to the Disease is 19 (=20-1) times *more likely* compared to the Control Group?\"\n\n\\- \"The odds of Case Group being exposed to the Disease is 20 times *as likely* compared to the Control Group?\"\n\n\\- \"The likelihood of exposure is 20 times\"\n\n\\- \"Odds are increased by a factor of 20\"\n\n\\- \"\"the odds of achieving the disease in case group is 20 times higher than that in control group\"\n\n&amp;#x200B;\n\nI am really confused as to when to use: \"as likely\" vs \"more likely\". It makes more sense to use more likely when you subtract one as a base.",
        "created_utc": 1676000233,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I interpret F ratios and T-test notation in an APA paper?",
        "author": "ittybittyleaf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10yaw85/how_do_i_interpret_f_ratios_and_ttest_notation_in/",
        "text": "I'm currently working on a project for my psych research methods class and I'm having trouble making sense of the notations. I know what the abbreviations mean, I'm just struggling to find what I need from the notation I'm given, if that makes sense. I need a mean or a percentage, but the research paper I was assigned doesn't give those values outright. Here are a few examples of what I'm working with:\n\n\"There was a significant interaction between information admissibilty and defendant's race, *F*(1, 260)=3.4, *p*&lt;.05\"\n\n\"There was no significant difference between the verdicts of those in the Black-defendant-admissible condition and those in the White-defendant-admissible condition, *t*(261)=.55, *p* \\&gt;.05\"\n\n\"There was a significant effect of information admissibility, *F*(2, 260)=40.5, *p* &lt;.001\"\n\nThe paper is \"Justice Is Still Not Colorblind: Differential Racial Effects of Exposure to Inadmissible Evidence\" by Johnson et al. (1995)\n\nNOTE: I already feel pretty stupid for struggling so much with this because I know that this is basic stats information and I'm a psych major who's supposed to be able to understand this stuff. I've always just had a lot of trouble with math and comprehending things like this so patience and kindness is greatly appreciated. I'm trying my best :(",
        "created_utc": 1675988043,
        "upvote_ratio": 1.0
    },
    {
        "title": "Use of dummy variables in binary logistic regression",
        "author": "kalindyx",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10yaqbg/use_of_dummy_variables_in_binary_logistic/",
        "text": " Hi everyone.\n\nI am currently working on a database with one numeric variable, a few binary (no/yes) variables and 2 categorical variables (all of these are predictor variables). My dependent variable is a yes/no variable.\n\nThe goal was to create a univariate logistic regression model for each predictor (dependent\\~predictor), see if they were significant, and use the significant predictors to build a logistic multivariate regression model for the same dependent variable (y\\~variable1+variable2+variable4, where 1, 2 and 4 are the significant variables).\n\nMy numeric variable will stay as numeric, and all the binary (no/yes) predictors were encoded as (0/1) variables.\n\nThe problem I found is in the non-binary categorical variables.\n\n**NON-BINARY CATEGORICAL PREDICTOR 1:**\n\nThere are 4 categories, but after NA removal, one of the categories only has 1 observation (doesn't cut it for binary logistic regression) and another category has 2 observations but all from the same category of the dependent variable (also not good for binary logistic regression). This leaves me with only 2 categories on this predictor. I don't know if I should code it as 0s and 1s and remove those 3 observations from the analysis or keep the observations and... i don't know... Note that the 2 categories left are 122 obs. and 12 obs. long, so it's very skewed; the 122 observations variable is the reference variable.\n\n**NON-BINARY CATEGORICAL PREDICTOR 2:**\n\n4 categories on this predictor variable (dummy1, dummy2, dummy3). As I said before, my goal is to build a univariate model for each predictor variable. But I'm having a lot of difficulty wrapping my head around how to interpret this model's results and coefficients.\n\nI don't know if I should create a univariate model for each dummy variable (y\\~dummy1; y\\~dummy2; y\\~dummy3) and evaluate separately or a model that includes all the dummys (y\\~dummy1+dummy2+dummy3). I'm having a lot of difficulty wrapping my head around how to interpret this, because I think of if as univariate as in my original variable and I start thinking that the 2nd option is the best one, but technically a dummy variable is a variable in itself and it would still be considered a univariate model. I just can't figure out which is the best option.\n\nMy first attempt didn't involve dummy variables, I just set each category as a level of the factor variable and did y\\~variable, but the results differ a lot and I have read in several places that using dummy variables gives stronger results.\n\nThoughts?\n\nThank you!!",
        "created_utc": 1675987592,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about notation for conditional probability distributions",
        "author": "crown_sickness",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10y8ayy/question_about_notation_for_conditional/",
        "text": "I've been trying to brush up on my stats recently, and I'm stumped at a piece of notation.\n\np(x,s|m)\n\nDoes this mean the probability of (x) and (s given m), or of (x AND s) (given m)? I'm not sure of the order of operations implied, or if it even makes a difference! \n\nThanks :)",
        "created_utc": 1675981730,
        "upvote_ratio": 1.0
    },
    {
        "title": "I am a beginner in statistics and looking for a statistics book to start with. Can anyone please suggest me something which has descriptive explanation of each terms and why we are doing something and how.",
        "author": "Technical-Dare1115",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10y71ka/i_am_a_beginner_in_statistics_and_looking_for_a/",
        "text": "",
        "created_utc": 1675978752,
        "upvote_ratio": 1.0
    },
    {
        "title": "I have a skewed dependent variable which is mostly (but not entirely) in integers. Is a negative binomial model suitable?",
        "author": "Peekochu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10y700g/i_have_a_skewed_dependent_variable_which_is/",
        "text": "I'm reposting thuis question from [crossvalidated](https://stats.stackexchange.com/questions/604813/i-have-a-skewed-dependent-variable-which-is-mostly-but-not-entirely-in-integer). Thank you!\n\nI have read that for highly skewed dependent variables (e.g., with excessive minimum values), a negative binomial model with robust standard errors may be best, even if the measure is not all integers. I am working with such a variable. It has a range of -1.2 to 4.2 with a mean of 0, and I will need to set the minimum to zero for use in the negative binomial model. The below blog post introduces this general approach, but I have seen it very few times in practice and don't want to upset my reviewers, who I expect will want OLS or an all-integer count dependent variable with a negative binomial model.\n\nhttps://blog.stata.com/2011/08/22/use-poisson-rather-than-regress-tell-a-friend/ See also the citations within.\n\nI have two questions. First, am I right about this (in other words, is this an appropriate modeling strategy)? Second, are their any recent or notable citations taking this approach to modeling skewed continuous dependent variables? Thank you.",
        "created_utc": 1675978645,
        "upvote_ratio": 1.0
    },
    {
        "title": "Do I have to make separate models for each \"class\" of y variable in Lasso Regression?",
        "author": "Vicious_Squid",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10y6zu6/do_i_have_to_make_separate_models_for_each_class/",
        "text": "Hi all, you may remember me from a month back when I explained my ineptitude in statistics and asked for help using stepwise regression. Thanks to this sub, I've switched to using Lasso regression!\n\nMy new problem now is due to logical complications with my data. It's growth data, and I am trying to determine what predictors impact growth. Depending on the age of the samples, the growth observed in a given year varies. For example, samples that are 2-3 years old grew the most, and samples that are over 5 years old grew very little. I have more samples under 5 years than samples over 5 years. This means if I kept all the data together, it would seem like a above average 5 year old is actually below-average for growth due to the data being skewed towards the 2 and 3 year old growth amounts. To account for this I've been making different models for each age.\n\nNow, my lasso results are all over the place, with each age's growth shown to be influenced by completely different factors. The results are also very different from simple linear and stepwise regressions I did before. Do y'all think it would be wise/possible to pool all the ages together to come up with one model that fits the best predictors for the whole set? And how to account for the differences in expected growth for each age? Hypothetically the growth of all years should be explained by at least similar variables.\n\nI've thought about using \"deviation from mean growth at age\" instead of net growth to standardize the data, thoughts? Again though, 3 units above average for a 5 year old is way more significant than 3 units above average in a 2 year old, so they're still not directly comparable. I'd need to reduce it to a percentage or equivalent units anyway.\n\nAny advice is appreciated!",
        "created_utc": 1675978634,
        "upvote_ratio": 1.0
    },
    {
        "title": "Chi squared vs Kolmogorov–Smirnov vs Anderson-Darling?",
        "author": "AHM8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10y4beb/chi_squared_vs_kolmogorovsmirnov_vs/",
        "text": "I’m a hydrology student and i’m using HEC-SSP to fit rainfall time series data into the best probability distribution, the program uses the 3 goodness of fit tests to compare the possible distributions\n\ncan anyone explain briefly what’s the difference between the 3 tests, or under what conditions should each test be used? i’ve been googling it and i cannot find any “layman” explanation, it’s all very intensive math/statistics theory.",
        "created_utc": 1675972345,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can the null hypothesis be \"true\" for continuous data?",
        "author": "AllAmericanBreakfast",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10y3lzq/how_can_the_null_hypothesis_be_true_for/",
        "text": "Usually, I've seen Type I errors defined as the chance of \"rejecting the null hypothesis when the null hypothesis is true.\" However, the null hypothesis is often an assertion that μ = 0 or some other specific value. But in continuous data, the chance of a variable having any particular specific value is zero. So in that sense, how can the null hypothesis ever be true? Is there some more specific way of stating what a Type I error is?",
        "created_utc": 1675970716,
        "upvote_ratio": 1.0
    },
    {
        "title": "Optimization Problem",
        "author": "amescani",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10y2n5d/optimization_problem/",
        "text": "I have a dataset that contains email data and purchases from a website. Essentially when someone comes to the website but does not make a purchase a follow up email is sent to them. For various reasons the email is sent 1 to many (usually within 2 weeks) days after the website visit. I am trying to determine if there is an optimal number of days to send the email after the person visits the website that would lead to the most purchases. What sort of statistical test would I use for this?",
        "created_utc": 1675968574,
        "upvote_ratio": 1.0
    },
    {
        "title": "What approach would you use for estimating the effects of hundreds of thousands of features?",
        "author": "aftersox",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xycib/what_approach_would_you_use_for_estimating_the/",
        "text": "I have a problem where we need to compare the impact on outcomes of hundreds of thousands of features. Fortunately we have a great deal of data, in one week we will have well over 1.5M cases to work with. The goal is estimate and compare the relative impact of hundreds of thousands of features on a set of different outcomes.\n\nI have considered dimension reduction or embeddings (topic models??), but the client in this case wants individual estimates for each feature. If I were to reduce the feature set, I would have to include another step that partials out each features individual contribution. But even then, I think that I would find it difficult to even estimate the dimension reduction step with this size of data.\n\nBuilding the full model matrix is infeasible - it would end up being well over 200GB. We don't have the infrastructure or budget for handling that.\n\nI'm considering some form of Monte Carlo approach where I systematically check random sets of features at a time. But I want to see if there has ever been research in that area. Does anyone know of any methods for accomplishing this sort of task?",
        "created_utc": 1675958554,
        "upvote_ratio": 1.0
    },
    {
        "title": "Survival Analysis",
        "author": "whois_this-guy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xwhoc/survival_analysis/",
        "text": "\nHey Reddit,\nDo you know anywhere I could get datasets for Survival Analysis? ( with time and event data)\n\nThis could be of varying sizes from few to large no. of variables, observations, censoring etc...\n\nLooking for a research, would be of great help.\n\nTIA.",
        "created_utc": 1675953949,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this the box plot and if so, where is the box part? (I need to interpret this, please refer me on which topics to study)",
        "author": "amberrpricee",
        "url": "https://i.redd.it/ynpehv1dc6ha1.jpg",
        "text": "",
        "created_utc": 1675953444,
        "upvote_ratio": 1.0
    },
    {
        "title": "Risk of bias when selecting for 3 SD above mean",
        "author": "twiceday6",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xvouy/risk_of_bias_when_selecting_for_3_sd_above_mean/",
        "text": "I have a dataset of \\~70,000 individuals who answered q's regarding minutes of physical activity per week, and I'm trying to weed out ones that seem implausible (e.g. 12 hrs/day). If I were to exclude individuals who answered 3 SD above the mean, would I be introducing significant bias from \"selecting\" a specific population? Even though I'm only excluding 1.5% of 70,000 datapoints.\n\nIf it helps, here are some approximate values.\n\nmean 221\n\nmedian 120\n\nvariance 111409\n\nstd dev 334\n\nmin 0\n\nmax 5040\n\nskewness 6.7\n\nkurtosis 68",
        "created_utc": 1675951871,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using Mann Whitney test",
        "author": "rpranaviitk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xv3ah/using_mann_whitney_test/",
        "text": "Can I use Mann whitney test, if I have just one sample of values and a population mean to compare against? I don't have the knowledge of the underlying distribution so using a non parametric test is preferred. But in the python implementation of Mann whitney I only see implementations to test for two samples rather than one sample. So my question is whether Mann Whitney is applicable if I have just one sample and want to test whether it's mean is similar to a population mean?",
        "created_utc": 1675950271,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any simple, real-life examples of the difference between correlation and covariance?",
        "author": "Ridyot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xq5k7/any_simple_reallife_examples_of_the_difference/",
        "text": "I don't need the math, I have all that. What I need is a simple real-life example of the difference between correlation/covariance to serve as a memory crutch! The more vivid the example the better so that it sticks in memory.",
        "created_utc": 1675933391,
        "upvote_ratio": 1.0
    },
    {
        "title": "How VIF works with negative r2?",
        "author": "MysteriousWealth2423",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xpg0q/how_vif_works_with_negative_r2/",
        "text": "All of you know about the story of the cutoff at VIF = 10... But calculating VIF as 1/(1-R2) negatives R2 can not be greater than 1! The solution could be consider the absolute value of the R2. Is It right?",
        "created_utc": 1675930677,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to Run Bayesian Hierarchical Multiple Regression",
        "author": "DBug1995",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xl3ug/how_to_run_bayesian_hierarchical_multiple/",
        "text": "I can run hierarchical multiple regression on SPSS and Bayesian Linear Regression - but no option for Bayesian hierarchical Multiple Regression. Does anyone know of any extensions or have an example of how to do this? Thanks!",
        "created_utc": 1675916243,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need help with a combitorial probability",
        "author": "chuchlepus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xjcez/i_need_help_with_a_combitorial_probability/",
        "text": " I'm making a program that calculates the odds of drawing a specific magic card from your deck. The problem that's been coming up is I'm trying to find out how to code the odds of hitting a desired card when drawing or seeing multiple cards at once. Say for example if you you got to choose the a card from the top 3 cards of your deck if you had 32 cards left in your deck and 3 copies of the desired card left in your deck the equation would like this (3/32+3/31+3/30)/3 I believe this leaves you with a 9.68% chance to draw your card. I believe I have a fundamental misunderstanding of how to calculate this formula so please explain to me how to solve this problem. As well as I paid very little attention in algebra so if youd please dumb it down for me I'd be incredibly grateful.",
        "created_utc": 1675911287,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS",
        "author": "Emotional-Ad-9285",
        "url": "https://i.redd.it/u8yxowgjr3ha1.jpg",
        "text": "Hi! I need help with a SPSS task. I am stuck for several hours. First column is years, second third and fourth are money. I need to calculate which sample means are different using tukey hsd. I tried using one way anova but i dont have a factor. What should i do? \nAlso, what to do if task states: Are all variables the same?",
        "created_utc": 1675904176,
        "upvote_ratio": 1.0
    },
    {
        "title": "Calculating p-value in Google Sheets",
        "author": "nosaystupidthings",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xdh9j/calculating_pvalue_in_google_sheets/",
        "text": "Hello, first off thanks for taking a look at my questions. This is a two parter.\n\n1. How does finding p-value differ between these two situations:\n\nSituation A: A/B testing two different experiences with roughly the same number of people, where I want to see if there's a statistically significant difference between the percentage of people who came back to the same experience the next day for the two experiences. I used abtestguide.com/calc for this situation, is that correct?\n\nSituation B: The same A/B test, but I want to compare the average number of actions per person for all the people who went through one experience on a particular day to the same metric for the other experience.\n\n2. Can I use the Google sheets ttest, ztest, or other function to calculate the p-values for both these situations? I'd much prefer to use an automated tool than go through the formulas myself.",
        "created_utc": 1675896541,
        "upvote_ratio": 1.0
    },
    {
        "title": "In a Game of Dobble, 9 random cards are drawn from a deck of 31 cards (6 symbols / card). Out of those 9 cards, what are the chances of there being 6 cards with the same symbol?",
        "author": "International_Pen672",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10xbcgi/in_a_game_of_dobble_9_random_cards_are_drawn_from/",
        "text": "",
        "created_utc": 1675891677,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it possible that we don't find a significant mediation model between X, j(mediator), Y, but it becomes significant through a conditional mediation dynamic (by adding a previous Moderator)?",
        "author": "niii27",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10x9ibk/is_it_possible_that_we_dont_find_a_significant/",
        "text": "Particularities; Variable X has an effect on Y only through Mod. So we have X\\*Mod -&gt; Y.   \nThen variable j is added in the game. We check for a potential mediation where X -&gt; j -&gt; Y, which comes out not significant.  \nWould it make sense to try and compute whether the mediation will become significant as conditional?  \nThat is, try and see whether (X\\*Mod) -&gt; j -&gt; Y? Or since the initial mediation is not significant, it doesn't make sense to look for second order effects (that is a moderation within it)?",
        "created_utc": 1675887430,
        "upvote_ratio": 1.0
    },
    {
        "title": "What's the analysis called where you test when in some time series data you can identify some target?",
        "author": "PrivateFrank",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10x6uko/whats_the_analysis_called_where_you_test_when_in/",
        "text": "I have a lot of trajectories which all start from the same spatial position but clearly end in either state A or state B.\n\nWhat I would like to do is to work out when, for each trajectory, when A or B can be predicted above chance (or with 85 or 95% accuracy or something).\n\nI swear I have read about it before but my brain is completely blanking on it.\n\nAny ideas?",
        "created_utc": 1675881056,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can H0 be equal to H1?",
        "author": "Pupkin333",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10x5dla/can_h0_be_equal_to_h1/",
        "text": "When I do Statistical hypothesis testing, can I define H0=H1? Or I must define them differently and as two different options?",
        "created_utc": 1675877518,
        "upvote_ratio": 1.0
    },
    {
        "title": "Histograms",
        "author": "Late_Werewolf_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10x3341/histograms/",
        "text": "Why are calculations from histograms only an estimate?",
        "created_utc": 1675872843,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trying to calculate battle odds for Levy&amp;Campaing series of games",
        "author": "jason-jo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10x0zr2/trying_to_calculate_battle_odds_for_levycampaing/",
        "text": "I am trying to calculate odds for the Levy &amp; Campaign series of board games. There are 3 currently published versions but the battles work the same. For this question we will only consider 1v1 battles.\n\nI posted the initial version of this on /r/askmath ([here](https://www.reddit.com/r/askmath/comments/10tlfh5/trying_to_calculate_battle_odds_for_levycampaing/)) but no one answered.\n\n**Relevant Game Rules**\n\nSo the way such a battle works is, there are 3 phases: archery, horse and melee (foot). Defenders hit first in each phase, then attackers. The way the hits are done is you count up every unit which can contribute to that phase (e.g. for most of the series there are no archery without special capability cards that give other units archery as well). These hits may have to pass through something, depending on where the battle takes place (e.g. if you storm a castle then you have to do a roll to see what hits get through the \"walls\"). Finally, once the number of hits are known, the player receiving the hits determines which of his unit will absorb the hit, one at a time and rolls against that units defence. If the roll is defence or lower the hit is \"absorbed\" and has no effect. If it's greater than defence then the unit routes and is out of the battle.\n\nWhen all 3 phases are complete they start again from the top until one side gives up or is eliminated. There are scenarios, though, where the number of \"rounds\" (i.e. the 3 phases) are limited to 1-4 max (therefor in some cases we would only need to calculate odds for 1 single round).\n\n**Example Scenario Background**\n\nSo, for the test scenario the defenders have 2 Knights and 1 Seargant and 4 Men-at-arms. Normally a Knight would have 4 defence and the others 3, but the defender has a card which gives the other ones 4 as well. The attacker has have 2 light horse and 6 militia and a special ability that turns them all into archers, contributing half a hit rounded up. So this means the defender has no archery so the attacker contributes 4 hits. The defender must absorb these and then we will go to the horse phase where they start: the knights contribute 2 hits each and the Sergeant 1, so 5, then the defenders contribute 1/2 per light horse so 1 hit, then the defender 4 Men-at-arms contribute 1 each, so 4. Finally the 6 militia contribute 1/2 each so 3.\n\nThe key is, though, that if you lose your units before they can attack they won't actually be contributing here. Hence the math question which I don't know how to calculate.\n\n**The math question**\n\nSo the initial attack is 4 dice to roll which eliminate a unit on 5 or 6. So that means each roll is 2/6 or 1/3 to succeed. Of the 8 possible hits from the defender, only a max of 4 of them could have been eliminated. So the chance that one of them is eliminated is 1/3, the second one would be 1/3\\^2 third 1/3\\^3, fourth 1/3\\^4 and the rest 0. So trying to work through this I get:\n\nAttacker (archery) - 4 hits: (each) 1/3 to hit\n\nDefender (horse) - 6 hits: (base) 5/6 to hit (see note 1 below)\n\n1. 5/6 \\* 2/3 = 10/18 = 5/9 (\\~55%) to hit\n2. 5/6 \\* (1-1/3\\^2 = 8/9) = 40/54 = 20/27 (\\~74%) to hit (I switched to this because everything else I tried made this number smaller when it should be growing, i.e. becoming more likely)\n3. 5/6 \\* (1-1/3\\^3 = 26/27) = 130/162 = 65/81 (\\~80%) to hit\n4. 5/6 \\* (1-1/3\\^4 = 80/81) = 400/486 = 200/243 (\\~82%) to hit\n\nrest: 5/6 (\\~83%) to hit\n\nAttacker (horse) - 2 hits: (base) 1/3 to hit\n\nNow I'm really stuck at this point. There are 6 hits incoming with the percentages shown above, how likely is it that these even survived? I guess I could pick the unit most likely to absorb the hit (i.e. the one with best defence) and keep applying to hit percentage there which will reduce the likelihood of it being eliminated before firing, etc.\n\nNote 1: to make things more complicated, the Attacker horse actually have 1 defence on one but 3 on the other, meaning those odds are not 5/6 above but somehow 1/2 and 5/6 with the 1/2 getting less likely with every roll.\n\nNote  2: I realised that in the calculations above where I say \"to hit\" I actually mean \"to eliminate\"",
        "created_utc": 1675870745,
        "upvote_ratio": 1.0
    },
    {
        "title": "Carrying out a survey on Public Transport and Active travel for my dissertation - Am i doing this right?",
        "author": "Massive-Bite-8020",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wxb15/carrying_out_a_survey_on_public_transport_and/",
        "text": "Hi All,\n\nI hope you're having a great day! Just looking for some advice on some data analysis methods for my dissertation. Essentially i have created a survey which is to examine the design of public transport (bus stops, train stations etc) and active travel infrastructure (cycle lanes, footpaths etc) and essentially the survey is split as:\n\n&amp;#x200B;\n\n1. Demographic factors - Age, gender, location, employment status, etc.\n2. Travel Patterns - whether they use the bus, train, cycle, walk, etc\n3. concerns with design - a matrix using 'highly concerned' to 'highly unconcerned' is used, with design factors such as sidewalk pavement surface, public lighting, proximity to traffic, splashing from cars etc\n4. Concerns with safety - whether they participant 'feels' safe walking in areas at day/night time, cycling concerns and waiting at stations etc.\n\nEssentially I am trying to find the relationship between how different participants answer, predominately based on gender and how say males typically are more comfortable walking alone at night time, and im trying to correlate the design influences that would allow women to feel more comfortable say with more street lighting, visibility etc.\n\nI currently was going to analyse the survey using:\n\n\\- Cross Tabulations\n\n\\- Correlation Analysis\n\n\\- Chi Square Testing\n\n\\- Classification Analysis\n\n\\- ANOVA (Analysis of variance)\n\nDoes this seem like a good idea? i have had statistics classes before unfortunately they were taught very abstract/by a complete useless professor who barely turned up, so any help would be appreciated or any further ideas or thoughts you may have\n\nThanks!\n\n\\- Stressed University Student",
        "created_utc": 1675864575,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I prove causality between two variables?",
        "author": "Rajsuomi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wx2ot/how_can_i_prove_causality_between_two_variables/",
        "text": "If correlation does not prove causation, what statistical test/analysis do you use to assess causality? I need to make sure that I take into account any confounding variable that can play a role into it",
        "created_utc": 1675863950,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using a Logistic/probit regression model with an unbalanced panel data",
        "author": "nesta1970",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wtejh/using_a_logisticprobit_regression_model_with_an/",
        "text": "I want to run a probit model where my outcome variable (i.e. drop\\_status) equals 1 if an individual stops using a given sub-reddit after a government policy is introduced, and 0 otherwise.\n\nIf my objective is learning about each user's dropout probability from Reddit based on their online behavior, I am not if my unit of analysis should be at the post-level or username-level? Specifically, each post is evaluated using a continuous sentiment variable (i.e. 1 for positive, -1 for negative, or 0 for neutral sentiment), so if I am want to learn about Kenny's probability of dropout based on the online sentiment of his many posts, shall I compute an average sentiment for each username and then run the model with a continuous variable based on average sentiment?\n\nHere is a data example:  \n\n\nusername        date         mood                       drop\\_status  Kenny       2020-09-02. -1 1  Kenny      2020-09-03. -1 1  Kenny      2020-09-07. 1 1  Cartman   2020-09-03. -1 0  Cartman  2020-09-06. -1 0  Cartman  2020-09-08. -1 0  Mackey   2020-09-03. 0 0  Mackey   2020-09-04. 0 0 Mackey  2020-09-08. 1 0 Kyle  2020-09-13. -1 1 Kyle  2020-09-14. -1 1",
        "created_utc": 1675852500,
        "upvote_ratio": 1.0
    },
    {
        "title": "clarification on the use of MCmC sampler for parameters estimation",
        "author": "ilrazziatore",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wt5a5/clarification_on_the_use_of_mcmc_sampler_for/",
        "text": "hi guys, i have used for the first time an mcmc sampler (emcee) to estimate some parameters (h,q,j,m)   by fitting a function f of these parameters to some data.\n\n as a log likelihood i have used the  [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) computed with the help of the covariance matrix between my data samples (x,y)\n\nNow i have a corner plot with the 1d and 2d marginal distributions and the credible intervals on the parameters\n\n&amp;#x200B;\n\nthe question is: now i want to propagate the errors with mcmc to obtain the credible intervals on the output of the function y=f , how the hell do i do it?\n\nshouldn't emcee give me a covariance matrix between the parameters from which i can sample? should i compute it from thechains?  thank you for your help",
        "created_utc": 1675851507,
        "upvote_ratio": 1.0
    },
    {
        "title": "What happened to statistics blog talkstats.com? I can no longer access!",
        "author": "Ridyot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wpmhr/what_happened_to_statistics_blog_talkstatscom_i/",
        "text": " I was using [talkstats.com](https://talkstats.com/) for assistance but the last few weeks I've been getting the error message \"Hmmm… can't reach this page\" when trying to connect. Its functionality was terrific, and Q&amp;A was very high quality.",
        "created_utc": 1675838168,
        "upvote_ratio": 1.0
    },
    {
        "title": "Betting Calculation",
        "author": "jebme123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wpd5q/betting_calculation/",
        "text": "This is a betting question based off of Prizepicks Taco Tuesdays where 3 player's lines are temporarily lowered. Let's assume each player has a 2/3 chance of going over the lower line and 2/3 chance of staying under the upper line.\n\nWhat would be my average winnings if I were to bet equal amounts on 3 parlays:\n\nP1 over low line x P2 under high line\n\nP2 over low line x P3 under high line\n\nP3 over low line x P1 under high line\n\nYou get paid out x3 if both of them hit.\n\nThe part that makes it difficult to calculate is if a player doesn't go over their lower line then that guarantees they will be under their upper line. And vice versa.\n\nWithout this, it's a simple calculation: (2/3) \\* (2/3) \\* 3 = 4/3. So about x1.33 average. But I'm assuming it would be closer to x1.5 average winnings.\n\nI know the problem is solvable but with the amount of combinations it got too tedious to solve by hand. Was wondering if there's an easier way to calculate the exact answer.",
        "created_utc": 1675837275,
        "upvote_ratio": 1.0
    },
    {
        "title": "can i use ANOVA if i am not comparing means?",
        "author": "Portal_to_the_Alps",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wo5vq/can_i_use_anova_if_i_am_not_comparing_means/",
        "text": "i want to compare the total number of clicks for 6 different types of messages. There is no mean to compare in this instance, just the total number of clicks. Can i still use this for the ANOVA test instead of the means? Thanks so much",
        "created_utc": 1675833328,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does anyone know a package in python that will print correlations of multiple variables and p-values in matrix format?",
        "author": "Curious-Fig-9882",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wlvjt/does_anyone_know_a_package_in_python_that_will/",
        "text": "I am trying to find something that will print the correlation matrix with p-value and I cannot seem to find one. Does anyone know of code or package in python that does that?\n\nThanks in advance.\n\n(I  submitted this to the data science subreddit and go voted down lol, is this a stupid question?)",
        "created_utc": 1675826492,
        "upvote_ratio": 1.0
    },
    {
        "title": "Could this quotation be plausible in this TV series?",
        "author": "Rafaelchavez",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wlesf/could_this_quotation_be_plausible_in_this_tv/",
        "text": "I've just watched The knick tv series which depicted events in the year of 1900 in the beginning of the modern medicine and in the first episode the main character said the life expectancy has grown from 39 years old to 46, could he say this at this time? Survival Analysis wasn't available at that time.",
        "created_utc": 1675825197,
        "upvote_ratio": 1.0
    },
    {
        "title": "Elementary Statistics Problem",
        "author": "Imaginary-Swim-1836",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wkrlr/elementary_statistics_problem/",
        "text": "Hi everyone! This is probably the dumbest question on this thread, but I have these elementary stats  assignments where the questions are either redundant or I just don't understand. So can anyone answer the following?\n\n**Question:**  \"What is the z-score of a person whose age is 40? How many standard deviations above the mean of the population is a person aged 40 years?\"\n\n**Mean** = 39.1\n\n**Z-score** **for someone age 40** = .05828675437015 (I'll round it after)\n\nIsn't the answer to:\n\n*&gt;How many standard deviations above the mean of the population is a person aged 40 years?* The same as the z-score?\n\nI thought z-score is the number of standard deviations a given data point lies above or below the mean, so a person age 40 is .05828675437015 standard deviations above the mean??? Or does he just mean within 1 standard deviation? Thanks in advance; super dumb, I know...",
        "created_utc": 1675823380,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the proper terminology/vocabulary to use for findings that haven't undergone statistical analysis?",
        "author": "Ambiva1ence",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10whq24/what_is_the_proper_terminologyvocabulary_to_use/",
        "text": "",
        "created_utc": 1675815124,
        "upvote_ratio": 1.0
    },
    {
        "title": "How bad did I mess up?",
        "author": "Ambiva1ence",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10wdlgk/how_bad_did_i_mess_up/",
        "text": "Not sure if it fits here, but I have to try. Ok, so basically, I forewent statistical analysis for my thesis. Initially, I was told that it was up to me and I didn't look too much into it, I don't know what I was thinking. My supervisor didn't tell me anything along the lines of \"You don't have a case without it\" after checking my findings, but I now worry that my observations (not results, without statistical analysis) and my work will be dismissible. How can I put it (e.g. under Methodology), so that they don't throw the whole thing out the window? Could it cost me my Master's?",
        "created_utc": 1675805030,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this too small a sample size to be indicative (link to study in comments)?",
        "author": "Shining_Silver_Star",
        "url": "https://www.reddit.com/gallery/10wdfri",
        "text": "",
        "created_utc": 1675804650,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this too small a sample size to be indicative?",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1675804605,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this too small a sample size to be demonstrative (link in comments to the study)?",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1675804549,
        "upvote_ratio": 1.0
    },
    {
        "title": "Professional Academic Service",
        "author": "Tutor_TifanyBrookPhD",
        "url": "https://www.reddit.com/r/AcademicPsychology/comments/10wah92/premium_and_professional_academic_contents/?utm_source=share&amp;utm_medium=web2x&amp;context=3",
        "text": "",
        "created_utc": 1675797901,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sampling strategy for multiple listing services in real estate",
        "author": "Proof-Combination334",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w8wky/sampling_strategy_for_multiple_listing_services/",
        "text": "Hey!\n\nI understand that homework questions aren't allowed here, but I'm currently working on an assignment that requires me to collect and summarize data. I've chosen an outline of the Toronto Regional Real Estate Board's Multiple Listing Service average price sold since 1977 that's freely available online. I need help figuring out what type of sampling strategy this is. I'm leaning towards saying it's a longitudinal sample, which would mean that the sample was taken from the same population of home sales over an extended period of time. It could also be convenience sampling because the data would already have been computerized and available online.\n\nThanks in advance",
        "created_utc": 1675793953,
        "upvote_ratio": 1.0
    },
    {
        "title": "Free Bayesian course",
        "author": "sonicking12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w8ihx/free_bayesian_course/",
        "text": "https://avehtari.github.io/BDA_course_Aalto/gsu2023.html",
        "created_utc": 1675793035,
        "upvote_ratio": 1.0
    },
    {
        "title": "What electives should I choose for my B.S. in Statistics?",
        "author": "Flaky-Substance-4090",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w8id8/what_electives_should_i_choose_for_my_bs_in/",
        "text": "I am doing a B.S. in Statistics. I like it a lot. I think I want to get a masters due to career decisions, however, I could be persuaded otherwise. I don't have a specific focus, besides not being an actuary. The requirements state that you must complete at least 2 Statistics electives, and at least 2 other Math/Statistics Electives. Based on what I have read here, more math is better if you want to get a Masters. Any general or specific advice is appreciated.\n\n# Math Electives\n\nFoundations of Modern Math: Introduces  elements of mathematical proof, focusing on the various forms and  methods, including direct proof; indirect proof; existence and  uniqueness proofs; mathematical induction; strong induction. Proof  methods are applied to mathematical statements taken from number theory,  geometry, and calculus. Develops the tools, terminology, and symbols of  advanced mathematics including deductive logic, sets, functions,  equivalence relations, number systems, and cardinality of sets.\n\n**Advanced Linear Algebra:** Linear equations and matrices, vector spaces, dual spaces and inner product  spaces, linear transformations, determinants, eigenvalues and  eigenvectors, systems of linear differential equations, and applications\n\n**Mathematical Modeling:** The construction of mathematical models to solve real world problems. Model types include continuous, discrete, deterministic and stochastic. The entire modeling process from construction of the model, fitting data to the model, analysis of the model including model selection, and  verification of the model covered. Examples from a variety of disciplines including biology, physics, economics and finance.\n\n**Numerical Analysis for Mathematics and Statistics:** Methods of numerical approximation of the value of functions, polynomials, systems of equations and integrals using a programming language such as  R, C++ or Mathematica. Topics may include accuracy of approximate calculations, root finding methods, interpolation, numerical differentiation and integration, numerical solutions to ordinary differential equations, regression, optimization and Monte Carlo  methods.\n\n**Mathematical Analysis I:** A rigorous introduction to the n-dimensional real number system. Topics include construction of the real numbers, topology of real numbers, continuity, derivatives and integration in multiple dimensions,  sequences and series, and sequences and series of functions.\n\n**Mathematical Analysis II:** A continuation of **Mathematical Analysis I**. Topics include implicit function theory, conformal mappings, Lagrange multipliers, special functions, transforms, uniform convergence of  integrals, calculus of variations, Fourier series, and Lebesgue integration.\n\n# Statistics Electives\n\n**Data Visualization:** Studies the value of visualization, historical and modern, and evaluation and critique of visualizations in literature and media. Develops methods, procedures, and application tools and software used to summarize and visualize data for both explanatory and exploratory purposes. Requires the creation of visualizations, including information visualizations and scientific  visualizations, and the assessment of when and how to best leverage data  visualization methods in a variety of contexts.\n\n**Nonparametric Statistics:** Studies nonparametric and distribution-free statistical procedures and techniques. Alternative procedures for one- and two-sample procedures, multiple comparisons, ANOVA, regression models, correlation, and analysis of categorical variables, including ordinal variables, will be examined via rank-based (e.g., Wilcoxon/Mann-Whitney, Kruskal-Wallis) and randomization-based methods (e.g., permutations, bootstrapping). Emphasis will be on analysis using statistical computing and programming software and the intuitive nature of nonparametric statistics.\n\n**Experimental Design and Analysis:** Studies the design, analysis, and follow-up procedures of experiments across disciplines. Includes one-way and two-way analysis of variance, completely randomized designs, factorial designs, Latin Squares, nested and split-plot design, repeated measures, block designs, analysis of  covariance, multiple comparison procedures, and incomplete designs.\n\n**Sampling Design and Analysis:** Studies the design, analysis, and follow-up procedures of sampling finite populations. Includes survey design, random, stratified, cluster, systematic sampling designs, analysis of quantitative and qualitative data collected through surveys and sampling. Emphasis on statistical considerations of sampling and non-sampling error.\n\n**Data Simulation and Analysis:** Uses Monte Carlo simulations to understand operating characteristics of a statistical method. Approximates bias and variance for point estimation, coverage probability for interval estimation, and Type I error rate and statistical power for hypothesis testing. Studies the impact of model misspecification and the importance of adjusting confounding variables in observational studies. Introduces how to build (parameterize) a  statistical model to address a research question of interest.\n\n**Bayesian Inference:** Studies the Bayesian approach to data analysis. Includes Bayes theorem, basic concept of Bayesian statistics, prior and posterior distributions, conjugacy, credible intervals, generalized linear models, statistical inference (with comparison to frequentist approach), prior elicitation, computational methods and applications to real world problems.\n\n&amp;#x200B;\n\nIn addition, here are the main upper division requirements:\n\n**Applied Probability and Statistics:** (Already completed) Provide foundation for applied probability and statistics methods including basic probability theory, sampling and experimental design, descriptive statistics, estimation of parameters, hypothesis testing, regression analysis, correlation, and an introduction to non-parametric statistics and statistical computing.\n\n**Service Learning for Mathematics and Statistics Consultants:** Service-learning placements in local non-profit organizations, school districts, and  community organizations help students deepen their understanding of  mathematical and statistical principles, techniques, and methodologies for effective consulting. Students will also study how the need for mathematical and statistical analysis can influence issues of equity within the local and global community. \n\n**Capstone Seminar:** Under the guidance of a faculty member, students complete a formal write up and present research in an area of their interest.\n\n**Generalized Linear Models:** Studies  the use of explanatory, confirmatory, and predictive linear models in data-driven decision making. Includes simple linear regression, multiple linear regression, variable selection methods, model comparison methods, generalized linear model, logistic regression, Poisson  regression, principal component analysis, times series models, and residual analysis using statistical computing packages.\n\n**Statistical Theory I:** Theory focused probability tools for statistics: basic probability rules,  description of discrete and continuous distributions, expected values, moments, moment generating functions, transformation of random variables, bivariate distributions, marginal and conditional  distributions, independence, multivariate distributions, concept of random sample, and Central Limit Theorem.\n\n**Statistical Theory II:** Theory focused framework for statistical inference: evaluation of point estimators (bias, variance, mean square error, relative efficiency,  consistency), methods of point estimation (method of moments, maximum likelihood estimation), sufficiency (Fisher-Neyman Factorization  Theorem), uniform minimum variance unbiased estimators, interval  estimation (confidence intervals from normal samples and large samples),  and theory underlying hypothesis testing.\n\nThere are also some ML classes at my school, which are in the compsci department. Do you think those courses would be helpful?",
        "created_utc": 1675793028,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to properly do principle of indifference",
        "author": "Synaap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w876c/how_to_properly_do_principle_of_indifference/",
        "text": "Suppose you meet an alien that you know nothing about. \n\nWhat's the probability that it is a liar? Under the principle of indifference, should it be 50%? Because it is either a liar or not a liar.\n\nHowever, suppose that the alien says 10 statements. What is the probability that it is lying about any of these? Shouldn't each statement have a 50% chance of being a lie? In that case, that probability that the alien ISN'T a liar isn't 50%, but rather (.5)\\^10 as each statement he says can be a lie, right? \n\nI'm just getting into this principle of indifference and a lot of it confuses me - it just feels like wording or rephrasing can just alter probabilities arbitrarily. Thanks!",
        "created_utc": 1675792311,
        "upvote_ratio": 1.0
    },
    {
        "title": "Association measure for small sample size(n = 5)?",
        "author": "Deepak_Singh_Gaira",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w863h/association_measure_for_small_sample_sizen_5/",
        "text": " \n\nHi everyone,\n\nI have a small project in a course. In it, my sample size has 5 observation each for two variables namely, stiffness (continuous ratio data type) and torque ratio ( a ratio of two values , which are continuous ratio data type). I want to measure association between stiffness and torque. Which correlation test or any association measure can I use?",
        "created_utc": 1675792238,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the best model for dealing with monthly data?",
        "author": "Lintaar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w7xhs/what_is_the_best_model_for_dealing_with_monthly/",
        "text": "My response variable is Revenue (by month) and I have 9 features also by month covering a 2 year span. Would regression work here or should I use something else to account for the fact it is broken out by month?",
        "created_utc": 1675791655,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dissertation Question",
        "author": "Patient_Jackfruit247",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w7m2m/dissertation_question/",
        "text": "I am conducting correlational research on the relationship between parenting styles (IV) and grit (DV). My measure for parenting styles provides three different scores (1-50) for three different types of parenting styles (authoritarian, permissive, authoritative), for both mother and father. The dependent variable grit, provides one score  (1-30).\n\nWhat statistical test could I use to determine if there is a relationship between parenting styles and grit? Can anyone direct me to online tutoring? My university has left me out to dry in a major way and I am so ill equipped to handle this on my own.",
        "created_utc": 1675790933,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for help with Mplus testing 1-1-1 Multilevel Mediation analysis",
        "author": "Psycheoniya",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w5xyk/looking_for_help_with_mplus_testing_111/",
        "text": "Hi! I am new to mplus. In my study I would like to test a 1-1-1 multilevel mediation model. Variables have been repeatedly measured, so occasions are nested within individuals. Any advice on how to get started on Mplus? Thanks.",
        "created_utc": 1675786992,
        "upvote_ratio": 1.0
    },
    {
        "title": "any advice for a PhD student with zero descriptive statistics knowledge",
        "author": "InvestigatorActual66",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10w27ax/any_advice_for_a_phd_student_with_zero/",
        "text": "My lab doesn't allow qualitative research, so I had no option but to conduct a quantitative research regarding the influence of big five on business performance. I'm wondering if you could give me any reassurance/reference, that can help me and is learning SPSS will be enough to conduct such study, with correlation/regression?\n\nAlso I don't understand these test values and significances that I see in some thesises 0.xxx",
        "created_utc": 1675777552,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting regression analysis",
        "author": "No_Tree_2274",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vz6sy/interpreting_regression_analysis/",
        "text": "If i have a beta value of 0.113 p&lt;0.001 and R\\*2adjusted = 0.003, what does that mean? \n\nI standardized the values before running the analysis\n\nIn order to analyze whether motivation moderates nudge effectiveness, I ran 3 \n\nseparate hierarchical multiple regressions. In step 1, I created the base model by including \n\nthe main effect of the default nudge (with the control condition coded as 0 and the default \n\ncondition coded as 1). In step 2, I added the main effect of motivation. In step 3, I \n\nadded the interaction term between the default nudge and motivation .",
        "created_utc": 1675767906,
        "upvote_ratio": 1.0
    },
    {
        "title": "Exploratory Factor analysis not making any sense",
        "author": "AliResearcher",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vygi7/exploratory_factor_analysis_not_making_any_sense/",
        "text": "[removed]",
        "created_utc": 1675765143,
        "upvote_ratio": 1.0
    },
    {
        "title": "What if exploratory factor analysis didn't make any sense?",
        "author": "AliResearcher",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vyfll/what_if_exploratory_factor_analysis_didnt_make/",
        "text": "[removed]",
        "created_utc": 1675765043,
        "upvote_ratio": 1.0
    },
    {
        "title": "Moderation with PROCESS",
        "author": "Acrobatic-Ice1411",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vxzel/moderation_with_process/",
        "text": "Hello there :)\n\nI am having some trouble with interpreting results from my moderation analyses completed with the PROCESS macro, and can't seem to find a lot of info online. \n\nThese are simple moderation analyses involving 1 independent variable X and 1 dependent variable Y, and 1 moderator variable M. \n\nI just hope to check that I am interpreting the results correctly\\~ If the interaction is statistically significant p &lt; 0.05, am I correct to interpret this as: M significantly moderates the association between X and Y. Or should the interpretation be: when M is included as a moderator, X significantly affects Y?\n\nIn regards to the interaction beta coefficient, if this is negative, am I correct to interpret this as moderator M weakens the association between X and Y? And if the beta coefficient is positive, moderator variable strengthens the association between X and Y? I feel like this doesn't seem right. \n\nFor the Johnson Newman values A and B, I interpret these as: M is a significant moderator for the association between X and Y for this range A-B? Or, X significantly affects Y when M is included as a moderator, for the value range A-B?\n\nI'm trying to figure out under which circumstances M, will X significantly affect Y, so I'm hoping to find a range of M for which X will be significantly associated with Y. \n\nThanks so much for your help! :)",
        "created_utc": 1675763279,
        "upvote_ratio": 1.0
    },
    {
        "title": "Study: Hypothesis Testing or Regressions?",
        "author": "Hapzek",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vwqko/study_hypothesis_testing_or_regressions/",
        "text": "Hi there, guys. I'm new to statistics, I mean, I did one course of basic statistics through my EE major, but I never payed a lot of attention to it.\n\nA few months ago I discovered the beauty of statistics and I'm also changing my career path to Data Analytics.\n\nI had read and practice fundamentals of statistics, should I study Hypothesis Testing or Linear Regressions?\n\nCan you guys suggest me a beginner friendly textbook for statistics in general, please?",
        "created_utc": 1675758257,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to graph a three way anova?",
        "author": "Longjumping-Smell785",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vrmrf/how_to_graph_a_three_way_anova/",
        "text": "Hi folks, \n\nI am trying to conceptualize how to think about a three way anova with design 2 x 2 x 4. I don't know how to draw this out on paper. If someone could help with that,, that's be awesome and I can figure out the rest from there!",
        "created_utc": 1675741600,
        "upvote_ratio": 1.0
    },
    {
        "title": "What statistical analysis should I use for my data?",
        "author": "baisam",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vppia/what_statistical_analysis_should_i_use_for_my_data/",
        "text": "I have a research project with 20 game cameras collecting images of wildlife that wander by. These cameras are deployed in roughly five different forested habitats (we have forest structure data to connect to this as well). I have a running list of species identified from these images. We are also gathering daily weather data. We've been doing field work since starting the project, and while we are still gathering field data, we are beginning the analysis. I don't know how what analysis I should use for all this data and how to connect all of it. This may be a loaded question, I'm just new to data analysis and could use all the help I can get.",
        "created_utc": 1675736395,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I calculate the F-value based on the Pillai's Trace value?",
        "author": "thequandaviusdingle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vp221/how_do_i_calculate_the_fvalue_based_on_the/",
        "text": "Hello, I apologize if this is a rudimentary question, but I have been trying to understand the MANOVA test. I am a freshman in high school, and I finally understood most of it, and I calculated the Pillai's Trace for an example dataset that I was watching, off a video on youtube. However, I am not able to calculate the F-Statistic from my pillai's trace value. Would it be possible to give me the method on how to do that? If these values are calculated in a program, or some other sort of method, I would appreciate your time in helping me with this.\n\n&amp;#x200B;\n\nThank you so much!!!",
        "created_utc": 1675734705,
        "upvote_ratio": 1.0
    },
    {
        "title": "What process model to use?",
        "author": "kim_teddy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vor6r/what_process_model_to_use/",
        "text": "",
        "created_utc": 1675733903,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best way to analyse survey data?",
        "author": "AlternativePlenty983",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vn8h0/best_way_to_analyse_survey_data/",
        "text": "I conducted a survey that has 10 questions and got ~200 responses for my Master’s dissertation. The options for answers were strongly agree, agree, neutral, disagree, and strongly disagree. I am stumped on what statistical test to use to analyse the data, or if I even need to use one at all. Please help!",
        "created_utc": 1675729935,
        "upvote_ratio": 1.0
    },
    {
        "title": "on the use of variance or quantiles for an asymmetric(?) posterior distribution.",
        "author": "aveterotto",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vm8hn/on_the_use_of_variance_or_quantiles_for_an/",
        "text": "i know this is very specific but it's a statistical question and i think it's fits here. let me explain.\n\nImagine you have a set of data points (x\\_i,y\\_i) where the y\\_i are gaussian distributed around the true value and are affected by heteroskedastic error.\n\nNow suppose to train a  dense neural network with the maximum likelihood principle: the mlp predict for every point x the mean $\\\\mu(x)$  and the variance $\\\\sigma(x)$. The likelihood is the product of gaussians with the previous predicted mean and variance.\n\nNow, if I take multiple samples of y\\_pred at the same x, i know that they will be distributed normally around $\\\\mu(x)$ with a variance  $\\\\sigma(x)$ since the network gives the same mean and variance at each attempt. However, if i keep MC-dropout active during  the inference phase, i am not simply maximizing the MLE anymore but   maximizing the posterior distribution P(y|w)P(w) with the weight configuration that changes at every forward pass, thus giving different values fro the mean and the variance.\n\nMy question is , since mc-dropout changes the value of the distributional parameters at each run, will  the samples collected at the same point x still normally distributed but with a variance that is the sum of the aleatoric variance + the epistemic variance, or in general it is an asymmetrical distribution?  is it still ok to use the variance to describe the uncertainty since we are in a point of minimum? or should i use quantiles?",
        "created_utc": 1675727354,
        "upvote_ratio": 1.0
    },
    {
        "title": "Nursing Shift work Problem",
        "author": "RopeSad6008",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vgqes/nursing_shift_work_problem/",
        "text": "Hi everyone been trying to figure this one out for a few days. Stats was quite a few years ago in college so trying to figure this one out. In this system nurses work 3 days in a week. In this system you have no control over when the nurses schedule themselves. They can pick any day of the week Sunday through Saturday. Given the random nature of the scheduling system:\n\nA. How many nurses would you need employ to know that you will have at least one person there with a minimum of 97% certainty.\n\nB. How many nurses would you need to employ to have at least 4 Nurses there with at least 50% certainty.\n\nYou would help my sanity if you could help me figure this one out! Thanks!",
        "created_utc": 1675714266,
        "upvote_ratio": 1.0
    },
    {
        "title": "Unsure of whether to use T-test or Z-test",
        "author": "LilNesahCider",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vgmwm/unsure_of_whether_to_use_ttest_or_ztest/",
        "text": "Hi! I'm currently learning about confidence intervals and I was wondering a few things. (I take an intro).\n\nI've heard before that you're supposed to use a z-test if your sample size is above 30, and t-tests for below 30. Now I am hearing that sample size has practically nothing to do with which test to use and to instead see if the question is given a known standard deviation. \n\nSo which test are we to use if you have a small sample size but are given a population variance instead? But not a known standard deviation. \n\nMy thought process would be to take the square root of the variance instead and put it in place as the standard deviation in the CI formula. But I'm not quite sure if that is correct. \n\nAdditionally, when calculating alpha/2, what if the value is not present on the t-table? I've encountered issues like this before especially when dealing with confidence intervals that are not 90, 95, or 99.\n\nI appreciate any clarity. Thanks in advance!",
        "created_utc": 1675714048,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] How feasible is it to/how do I pursue a career in statistics, with a BSc Psychology degree?",
        "author": "permanenthouseguest",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vgm9u/q_how_feasible_is_it_tohow_do_i_pursue_a_career/",
        "text": "Hello! I’m almost completing BSc Psychology degree, where I’ve had to take 3 years of statistics modules. I came to realise that I really like statistics and working with R — it’s one of the few things which allow me to enter my flow state and block out the world for hours. \n\nFor some background, I’m now taking advanced statistics and have learnt to use R for things like data visualisation, simulation, MLMs, path models, etc. \n\nKnowing that there are degrees specific to data science and statistics, I was wondering how feasible it is for me to enter a career in this field. I have been scoring well (&gt;90%) on my statistics modules if that’s relevant, but again, I’m aware that there are people who dedicate their schooling years to statistics/math alone so I may not be able to compare to them. \n\nI’m not 100% certain about committing to a statistics-specific postgraduate degree, so I thought an option would be to find a statistics-related job, perhaps related to psychology as that’s where my background lies (I’m not interested in research/academia though).\n\nWould be really grateful to get some advice on this :)",
        "created_utc": 1675714007,
        "upvote_ratio": 1.0
    },
    {
        "title": "How Bayesian Regression is different from general regression method ?",
        "author": "Nazma2015",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vff3u/how_bayesian_regression_is_different_from_general/",
        "text": "",
        "created_utc": 1675711303,
        "upvote_ratio": 1.0
    },
    {
        "title": "Really confused about Maximum Ignorance Probabilities",
        "author": "Synaap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vcq4o/really_confused_about_maximum_ignorance/",
        "text": "I've been reading a bit on Bayesian probabilities and I ran into a question I couldn't wrap my head around - was wondering if anyone could help me understand?\n\n&amp;#x200B;\n\nUnder the probability of maximum ignorance, if you do not know something, the probability of something happening is 1/n, where n is the number of all probabilities. This is if you do not know anything something. \n\n&amp;#x200B;\n\nDoes this mean that the probability of a god existing is 50/50? Because it's either it exists or does not exist. I don't mean any god in particular - it can be one unknown to us for all we know, or one that defies all logic. \n\nBut then, you can reframe the question as, \"what is the probability that 0 god exist?\" from which there could be an infinite number of gods existing. so the probability would be 1/infinity, as any number of gods are possible. \n\nBut then, you can alsorethink the question further - if there is only one possibility of a 0 god, but infinite possiblities of 1 god, and infinite\\*infinite possibilities of 2 gods, etc, then it is vastly more likely that a combination of infinite gods is likely, as the different possible combinations of infinity\\^infinity.\n\n&amp;#x200B;\n\nWhat is the proper way to address this problem? Thank you.",
        "created_utc": 1675705177,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Do you know any references/books where Confidence Intervals and Predictions intervals are defined, or better, contrasted?",
        "author": "Hector_Her-Alo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vcjh1/q_do_you_know_any_referencesbooks_where/",
        "text": "Hi redditors!\n\nI  fitted a linear model for a research I am working on focused on forest  ecology. Then I did a graph to show the observed values, the regression  line and the prediction inverval (PI) instead of confidence interval,  and I did so because  the former is the accepted measure of  uncertainity. Howerver, a coauthor thinks we should reference the use of  PI and probably it is rigth as there is a lot of reseach that use  (wrongly imo) Confidence Intervals (CI).\n\nHow could I reference the use of PI rather than CI?",
        "created_utc": 1675704749,
        "upvote_ratio": 1.0
    },
    {
        "title": "Plotting the Conditional Indirect Effects",
        "author": "Hell-Walker-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10vb3e5/plotting_the_conditional_indirect_effects/",
        "text": "I used PROCESS Macro Model 59 for SPSS in testing conditional indirect effects. My moderator is a continuous variable. There are no significant interactions terms for each path of the mediation model but there are significant conditional indirect effects at low, moderate, and high levels of the moderator. Does anyone know to plot the conditional indirect effects?",
        "created_utc": 1675701397,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical test should I use to compare these two tables?",
        "author": "OphthoMDtn",
        "url": "https://www.reddit.com/gallery/10vas0a",
        "text": "Hi everyone,\nI’m having difficulty in which test to use in my thesis. Each table represents the number of patients according to the final visual acuity (last five columns). Each table uses a different score. I want to compare the two scores. \nThank you 🙏",
        "created_utc": 1675700648,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bon ferroni correction",
        "author": "rpranaviitk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10v8zwb/bon_ferroni_correction/",
        "text": "I do not understand when to use bon ferroni correction. If I have done lots of tests in an article let's say 20 or something and set alpha level as 0.05. Then even by randomness 1 of the test will pass the significance testing. So do I have to apply the bon ferroni correction based on the number of tests I have done in my paper or do I do the correction based on the number of tests I have done using a single dataset?",
        "created_utc": 1675696358,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] Best way to list the top features in a multiclass logistic regression m",
        "author": "SquareShock",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10v7149/question_best_way_to_list_the_top_features_in_a/",
        "text": "I am trying to use a multiclass logistic regression and would like to get an idea of which of the 10000+ features have the most positive impact on the model.\n\nI understand that the coefficients are a good way to get an idea of the impact on the model.\n\nHowever, for each feature, I have as many coefficients as classes. For each feature, can I sum up the absolute values to be able to easily classify my features ?\n\nTo be statistically robust, do I need to calculate a confidence interval for each coefficient ?\n\nIn my case, can a method like Ridge be interesting ?",
        "created_utc": 1675691296,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to find correlation between gender and Likert scale data?",
        "author": "CuterialC916",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10v5j0a/how_to_find_correlation_between_gender_and_likert/",
        "text": "Should I be using Spearman's rho or Ordinal Logistic Regression? And how exactly should I do it, I'm new to statistics and can't find a tutorial",
        "created_utc": 1675686888,
        "upvote_ratio": 1.0
    },
    {
        "title": "Regression analysis of non-linear response curves",
        "author": "musicalbiologist164",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10v37px/regression_analysis_of_nonlinear_response_curves/",
        "text": "Hi everyone,\n\nBeen  searching the internet for a good while now (and even asked ChatGPT)  but I haven't been able to find a solid resource for this problem so I  thought perhaps someone here can help.\n\nI  have a set of \\~500 response curves of different individuals, which  represent my \"data points\", with the major difference that they're not  single values, but quadratic functions. For each individual, I have a  linear coefficient and a quadratic coefficient.\n\nI  would like to conduct an analysis that tells me whether the  individual's body mass (and other traits) affect the shape of the  response curve. My idea would be to somehow include both coefficients as  linked response variables, and I came across multivariate regression,  but there seems to be some inconsistency with whether this means two  response variables or two predictors, and most of what I found ends up  being about multiple regression with several predictors.\n\nI use R, and I tried lm(cbind(linearcoefficient, quadraticcoefficient)' \\~ bodymass)  \n as my model, but I'm really unsure whether this is the right approach.\n\nReally appreciate any help!",
        "created_utc": 1675678713,
        "upvote_ratio": 1.0
    },
    {
        "title": "Beginner question: Is my interpretation and usage of t-test hypotheses correct?",
        "author": "J-Anvs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10v2pb8/beginner_question_is_my_interpretation_and_usage/",
        "text": "My sample yields the mean of 0.6, now i want to test If the population mean is no more than 0.6 with alpha level of 0.05.\n\n(1)\n\nI stated the hypotheses as: H0 -  pop mean = 0.6 and Ha: pop mean &lt; 0.6. Then I calculated a left-tailed t test and got a p-value of 0.99. I interpreted this as the null hypothesis is true and pop mean is equal to 0.6.\n\nI moved up to pop mean = 0.7 and got a p-value = 0.025, smaller than alpha level and therefore I can reject the null hypothesis and accepted the alternative that the population mean is smaller than 0.7.\n\nWhen moved down to pop mean = 0.5, I got a p-value = 1, thus the null hypothesis is true and that population mean is equal to 0.5.\n\n&amp;#x200B;\n\n(2)\n\nNow I want to test If the population mean is more than 0.6, also with an alpha level of 0.05. H0 -  pop mean = 0.6 and Ha: pop mean &gt; 0.6. The right-tailed test yielded 0.00078 and thus I must reject the null hypothesis and accepted the alternative that pop mean is greater than 0.6.\n\nMoved up to pop mean = 0.7, I got a p-value of 0.97 so accept null.\n\n&amp;#x200B;\n\nFrom (1) and (2), I can come to the conclusion that my population mean lies somewhere between 0.6 and 0.7. \n\nAre my intepretations of the results correct?\n\nThanks a lot.",
        "created_utc": 1675676634,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help ranking mean scores and frequency over groups",
        "author": "roadster24",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10uv82q/help_ranking_mean_scores_and_frequency_over_groups/",
        "text": "Not a statistician so I thought I will ask for help here.\n\nI have the mean Test score of students from around 500 schools and the number of students who took the test. I would like to find a way to rank these schools based on their mean and frequency of test-takers. Intuitively I can tell a school(40 students) with mean 920 is better performing than a school(20 students) with mean 925. But is there an approach to grok these two parameters and assign a score to each school so that they can be ranked.\n\ndisclaimer: \"better performing\" is in the context of text scores alone :)",
        "created_utc": 1675650782,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone explain what the criteria of a simple event is?",
        "author": "The_Boomis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10uto1x/can_someone_explain_what_the_criteria_of_a_simple/",
        "text": "Ive seen three separate definitions and I feel like all of them are conflicting. The one that makes the most sense to me so far is the set of simple events is made up of fixed probability events so like if the experiment were to flip a coin twice the set of simple events would be\n{{H,T},{T,H}\nUnfortunately I put this as the answer to my homework and it apparently was wrong so I have no idea whats correct now\nAn explanation in laymens terms would be greatly appreciated",
        "created_utc": 1675646485,
        "upvote_ratio": 1.0
    },
    {
        "title": "Random Number generator",
        "author": "learner_beginner",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10uqb8a/random_number_generator/",
        "text": "If you had a random number generator that produces only binary values (0 or 1) such that P(X = 1) = 0:5, then present the principles of how these could be used to produce approximately normally distributed \\[N(0; 1)\\] random variables.",
        "created_utc": 1675637948,
        "upvote_ratio": 1.0
    },
    {
        "title": "When performing a linear regression, can the outcome and predictor values be taken from two separate samples, or does each predictor, outcome pair need to come from the same observation?",
        "author": "helaaspk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10uoh75/when_performing_a_linear_regression_can_the/",
        "text": "",
        "created_utc": 1675633481,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this an example of p-hacking?",
        "author": "LearningRocket",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10unxzg/is_this_an_example_of_phacking/",
        "text": "At a company I used to work for a couple of senior data scientists were looking for reasons that Important Business Metric had decreased. After some amount of sleuthing and finding nothing, they instead decided to write a couple of for loops which would check ~100 features to see if there was a difference between groups within each of those features (e.g. Feature 1 might have levels A, B, C) using some kind of statistical test. If the p-value was less than a certain cutoff, they would consider if it made business sense that the feature had some impact on the metric, and if it did they would tell management 'here's a reason that Important Business Metric is decreasing!'.\n\n\nAt the time I asked if this was p-hacking, but they insisted because they weren’t going to accept just any of the low p-value results, and that there would have to be a reasonable explanation behind the low p-value, that it wasn’t p-hacking. \n\n\nI still think it was p-hacking though. Since you’re looking at so many features, eventually you will find one that is ‘significant’, even if you narrow your scope by saying that it must make business sense. What do you think?",
        "created_utc": 1675632188,
        "upvote_ratio": 1.0
    },
    {
        "title": "Pls help! Regression model not significant",
        "author": "HourDistinct9638",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10un1v8/pls_help_regression_model_not_significant/",
        "text": "R2 = 0.37, f(4,165) = 1.565, p= .18\nHow do i explain these results ? Do i just say that they the independent variables do not reliably predict the dv and therefore? What are some possible explanations of this ?\n\nThank you",
        "created_utc": 1675630089,
        "upvote_ratio": 0.99
    },
    {
        "title": "Graph",
        "author": "J314ql",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ulkvj/graph/",
        "text": " Hello,  \nI want to understand how to calculate the probability based on the distribution graph.  \nDuring class, I had a basic staff and I understood it (e.g. I  needed to calculate P(X   ∈  \\[-2,2)). Now I wonder how to calculate P(X  ∈  \\[-3,3\\])). I  really do not know how to do it because it is easy to see from the graph what is the probability of -2 and 2 but it is hard to read the exact probability for 3.  \nI also want to know how to calculate the first quartile and I completely do not know how to do it.  \nCould you please explain it to me?   \nThe graph is under the link:  [Graph](https://ibb.co/BVSndZm)",
        "created_utc": 1675626591,
        "upvote_ratio": 1.0
    },
    {
        "title": "Z critical value confidence vs significance",
        "author": "Adorable_Choice_7938",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ugu0k/z_critical_value_confidence_vs_significance/",
        "text": "I know this is stupid but can someone explain why\n\n1. Critical value of 0.95 confidence is 1.96\n2. Critical value of 0.05 significance is 1.645\n\nIf 0.95 confidence is equal to 0.05 significance should they not have the same critical z value?",
        "created_utc": 1675615237,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the type of research design for SEM (path analysis, or CFA)? Correlational research?",
        "author": "Teacher_Coder",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ufi14/what_is_the_type_of_research_design_for_sem_path/",
        "text": "Hello. I found that there are mainly four types of research design which are 1) descriptive, 2) correlational, 3) experimental, and 4) explanatory research design.\n\nFor my study, I want to look at relationships between variables using SEM (path analysis or confirmatory factor analysis). However, I found that these SEMs posit that there is somewhat causality between these variables. \n\nFor my study, I want to say something like \"x predict y...\" or \"in case of students have more x variables... they showed more y variables...\" \n\nIn this case, do I have to say the SEM is correlational research??? I am so confused because the correlational research design posits that there is no causality between variables...",
        "created_utc": 1675611903,
        "upvote_ratio": 1.0
    },
    {
        "title": "trouble discerning what statistical analysis to use",
        "author": "frescoj10",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10uendv/trouble_discerning_what_statistical_analysis_to/",
        "text": "I have a data set that is comprised of 6000 data points of performance ratings.  Performance ratings are on a uniform rating scale of 1 *(unsatisfactory)*  to 4 *(highest possible outcome).* The # of ratings a manager can make depends on the number of employees they supervise. The number of supervisees for each manager can range from 1 employee to 20 employees. \n\n I would like to analyze manager performance ratings of their supervises to look for harsh raters (to many 1s) vs. easy ratings (too many 4s) .  There are a number of other factors that may contribute to the overall scoring for example - tenure, job profile, etc.  \n\n&amp;#x200B;\n\nWould I utilize latent class profile for this?",
        "created_utc": 1675609697,
        "upvote_ratio": 1.0
    },
    {
        "title": "What test to use for 3 independent samples of dichtomous variables (before and after)?",
        "author": "KoksaCZ",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10udgr8/what_test_to_use_for_3_independent_samples_of/",
        "text": "The samples were tested for positive/negative. I then heated the samples, and tested again (in random order, so it is not paired data?). We know how many were positive before and after in the group. The goal is to see if heating affects positivity/negativity. This was done for 3 independent groups of samples (n=22, n=27, n=30). How should the data be tested to determine the effect of warming? Use a 2x2 contingency table and chi-square test? Don't know, thanks for answer!",
        "created_utc": 1675606576,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can anyone explain how I’m supposed to solve this? I know what a type I error is, but I just don’t get how to do this :/",
        "author": "eleeesa",
        "url": "https://i.redd.it/dinf9ca01ega1.jpg",
        "text": "",
        "created_utc": 1675592577,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Is this distribution in the exponential family and how to find the conjugate prior?",
        "author": "sonicking12",
        "url": "/r/statistics/comments/10tzhdf/q_is_this_distribution_in_the_exponential_family/",
        "text": "",
        "created_utc": 1675566882,
        "upvote_ratio": 1.0
    },
    {
        "title": "For proportion/binomial distribution, which way is the correct way to find the standard deviation",
        "author": "Adorable_Choice_7938",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10typok/for_proportionbinomial_distribution_which_way_is/",
        "text": " When googling this problem, two different responses come up.\n\nThe first formula: The standard deviation is the square root of np(1-p)\n\nThe second formula: σp = sqrt\\[ P \\* ( 1 - P ) / n \\]\n\nWhich one is correct? And why is there such conflicting information that is being listed? Am I correct in guessing the second one is right? I've been trying it out and it seems to give me end up giving me results that make sense. Thank you.",
        "created_utc": 1675564525,
        "upvote_ratio": 1.0
    },
    {
        "title": "Poisson distribution",
        "author": "jbr2811",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ty3mi/poisson_distribution/",
        "text": "I’m learning more about the Poisson Distribution and how it can help my fantasy hockey projections, but I think I’m using them wrong. For example: I project a player will have 2.9 shots on goal in a game. In backtesting, players with an xSOG of 2.9 have 2 or more shots 77% of the time. When I use poisson on the 2.9, it gives a probability of 55%. It’s like this through all of my backtest. \n\nDo I even need the Poisson distribution for this or should I just stick with my backtest?",
        "created_utc": 1675562774,
        "upvote_ratio": 1.0
    },
    {
        "title": "How does one create comparable metrics when the original metrics are not comparable?",
        "author": "brandojazz",
        "url": "https://stats.stackexchange.com/questions/604296/how-does-one-create-comparable-metrics-when-the-original-metrics-are-not-compara",
        "text": "",
        "created_utc": 1675551928,
        "upvote_ratio": 1.0
    },
    {
        "title": "It’s not significant or am I not understanding how to analyze my data?",
        "author": "Greedy-Resident3596",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10tspr7/its_not_significant_or_am_i_not_understanding_how/",
        "text": "I’m running a Chi Square on my data (using this calculator: [https://www.socscistatistics.com/tests/chisquare/default.aspx](https://www.socscistatistics.com/tests/chisquare/default.aspx)) but my data is all coming up not significant which I was not expecting at all. I’m wondering if I’m misunderstanding how to analyze my data. I’m comparing ages 65+ with &lt;65 to see if there is a significant difference between age. For example, here is one question:\n\nOthers are at greater risk than me, agree or disagree.\n\nOf those 65+, 10 agreed while 14 disagreed.\n\nOf those &lt;65, 65 agreed while 61 disagreed.\n\nWhen I ran it through the calculator, it came up with a chi square of 0.7937, p-value of .372998 which is not significant.\n\n&amp;#x200B;\n\nAnother question where respondents were asked to rate an activity as high risk or low risk:\n\n65+ — 12 answered high risk, 5 answered low risk\n\n&lt;65 — 88 answered high risk, 38 answered low risk\n\nThe calculator gave me a chi square of 0.004, p-value of .949731 which is not significant.\n\n&amp;#x200B;\n\nAm I analyzing correctly and there just isn’t anything in my data or am I misinterpreting how to analyze?",
        "created_utc": 1675548315,
        "upvote_ratio": 1.0
    },
    {
        "title": "which lme model to use?",
        "author": "dhs1041",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10trxnu/which_lme_model_to_use/",
        "text": "\nI am interested in learning whether a diet can help people lose weight. I randomly assigned  participants to receive the new diet. Weight was monitored at baseline  and again at the beginning of every month for 4 months following enrollment into the study.\n\nI want to investigate if the new diet affect one’s weight over time differently for those on the new diet versus those not receiving the new diet.\n\nI have the following data: id, treatment, age, outcome (the weight of a particular individual recorded at a given visit), and visitnumber.\n\nI am not able to understand which model is appropriate for my question specifically, If I should include baseline age as a time varying variable (level 1 predictor) or as a covariate.\n\nI have made the following model\n\nadding adding visit number as level 1 predictor, treatment as level 2 predictor , visittreatment  interaction, and baseline age as covariate in random intercept (by id) and random  slope model (by visit). Yij=b00 +b01X+ b10 tij+b1 X1 * tij+r0i+ r1i* tij + b2age+ eij\n\nb00 – average time 0 weight for Placebo group ( X= 0) b10 – average weight improvement for the placebo group for each visit ( X=0) b01 – average time 0 weight difference for treatment group patients b11 – average weight improvement difference for treatment patients for each visit r0i – individual deviation from average intercept r1i – individual deviation from average slope\n\ndoes this model look appropriate to my research question, or should I include age as level 2 predictor with treatment?\n\nI have age recorded as decimals ( for example 62.81341) as well as outcome recorded as (149.1255) will rounding them to 62 and 149.13 make any difference to my results?\n\nshould I include interaction term age*treatment in my proposed model ?",
        "created_utc": 1675546383,
        "upvote_ratio": 1.0
    }
]