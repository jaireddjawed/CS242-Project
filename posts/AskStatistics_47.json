[
    {
        "title": "How to test if test sites are different from each other in demographics or other potential explanatory variables",
        "author": "GaboGobbo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xti05/how_to_test_if_test_sites_are_different_from_each/",
        "text": "I am examining the incidence of GI symptoms in beachgoers exposed to coliform bacteria at six different beaches. I would first like to determine if the people going to the different beaches are the same in eg age, race, health status. A two-way test of any one beach population vs. the pooled pop of all beaches, or 1 beach pop vs. the other 5, seems wrong, as the chances that AT LEAST one of the sub-populations differs from the total population by some amount is much greater than the chance of a particular subpop not matching the overall means, so a pairwise test would give a much lower p value in testing that mean of Beach 1 =/= mean of total population, correct? In other words, I want to answer the question \"is this grouping of the total population into 6 subgroups consistent with a random division into groups of these sizes, or is there evidence that the subgroups are actually different for this parameter (which I think is the same as saying there is evidence they are sorted non-randomly).\nI have a couple of undergrad/grad stats courses in my distant past, so I can handle a little formal explanation but not much. ",
        "created_utc": 1518728332,
        "upvote_ratio": ""
    },
    {
        "title": "How to determine sample size for category agreement analysis?",
        "author": "ei8htohms",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xtcfs/how_to_determine_sample_size_for_category/",
        "text": "So I'm trying to establish how consistently various workers within an organization categorize a high variability input along two different axes.  Axis 1 might have 4 or 5 different categories to choose from, and Axis 2 has 3 different categories to choose from.  In fact, the Axis 2 categorization is in some way dependent on the Axis 1 category, but perhaps I'm already radically over-complicating the question? Anyway, I'd like to see how often (with what frequency?) workers choose the same categories when evaluating the same inputs.\n\nMy plan is to take a random sampling from the total population of inputs (actually, I'll only pull from the total population of the three most suspect categories along Axes 1), and have this sample data set evaluated by three \"auditors.\" Then I'll compare their agreement with each other and their agreement with the original worker's findings, ideally for both axes separately (to determine which axis is more critical for improvement efforts).  \n\nCan anyone share some advice about what kind of sample sizes I would need to examine (as a percentage of the total population size, or just raw numbers) in order to say something intelligible about the observed consistency of the outcomes? And I'll be sharing the results with folks that know even less about statistics than I do (assuming that's possible), so I need not go into incredible depth with details of the accuracy of the analysis.\n  \nThanks for any feedback or advice you'd care to share!",
        "created_utc": 1518727028,
        "upvote_ratio": ""
    },
    {
        "title": "2 Questions regarding probability of a starting hand.",
        "author": "thisisredditnigga",
        "url": "https://www.reddit.com/r/askmath/comments/7xsubb/2_questions_regarding_probability_of_a_starting/",
        "text": "",
        "created_utc": 1518723666,
        "upvote_ratio": ""
    },
    {
        "title": "Is it possible to chart/analyze growth if I didn't mark individuals?",
        "author": "Takeurvitamins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xs8d3/is_it_possible_to_chartanalyze_growth_if_i_didnt/",
        "text": "I did a survival study where I outplanted individual mussels to different places in the field (25 in each cage). \n\nI have a measure of how many survived, but I also measured their length before and after the experiment. Unfortunately, I didn't mark them, so I don't know which before-length goes with which after-length. Is there any way I can analyze these data or at least chart them to show that one site has more growth than another?\n\nThank you",
        "created_utc": 1518718082,
        "upvote_ratio": ""
    },
    {
        "title": "What Survey Tool should I use?",
        "author": "FregeIsMyDog",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xregf/what_survey_tool_should_i_use/",
        "text": "Hey guys, \n\n\nI have a couple of stupid question that I could really need your help with. I am conducting independent research and need your advice on a couple of things. \nWhat I want to do: I will have two condition groups that I divide on the basis of their occupational history (though all hold the same job). I will then give them a survey (that is largely a replication of another one. It has two parts, the first is entirely Likert-scales where they are asked to express attitudes towards certain actions. The second part is where they have to write in certain self-report behaviour (mostly numbers). \n\n\nNow my question: Which survey tool should I use for this? Given that I do not want the participants to self-identify, do I just send out two different surveys? \n\n\nThanks so much. If there's anything else that you think I got wrong, please let me know. \n",
        "created_utc": 1518711388,
        "upvote_ratio": ""
    },
    {
        "title": "Problem with computing means in SPSS, any thoughts on why it keeps giving a .0000 mean?",
        "author": "robertniels",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xr6n9/problem_with_computing_means_in_spss_any_thoughts/",
        "text": "Hi, can someone please help me figure out what's going wrong when computing means through SPSS? It keeps giving me a .00000 value although I know it shouldn't be zero. Tried different tests, the other results come out fine, but just the mean of all data entries stays an exact 0. \nThe data consists of numbers in between -0,05 till 0,05 and is normalized. If anyone has any idea what it could be, please let me know!",
        "created_utc": 1518709545,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating mean of a population",
        "author": "jon_eng",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xqtog/estimating_mean_of_a_population/",
        "text": "Given a large normally distributed set of data, is it possible to estimate the mean of that dataset without knowing the variance of the data or actually calculating the mean?  The data exist on paper so it is not possible to compute automatically.  The process has a target value of 13 and all data points are within +/- 5% of the target, e.g. 12.35-13.65. \n\nI'm looking for a way to justify averaging a smaller sample of that data set rather than finding the mean of the whole population. Would it be possible to get within 3% of the actual mean?",
        "created_utc": 1518706241,
        "upvote_ratio": ""
    },
    {
        "title": "Non-normality in Multiple regression",
        "author": "sassafrasfly76",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xqhj5/nonnormality_in_multiple_regression/",
        "text": "I can't get my data to work. \nHave run all assumption test for multiple regression. Got through most OK until I hit Normality of residuals. \nVisual inspection of histogram shows slight skew. P-P plot is off and the scatterplots are not randomly distributed. They are somewhat skewed.\nNot sure what I can do. I have removed one outlier and it did nothing. Sample size is N=122 , so decent size. \nCan I invoke central limit theorem in this situation? or is that only for ANOVA....  \nI transformed variables to z-scores but it also did nothing.",
        "created_utc": 1518702929,
        "upvote_ratio": ""
    },
    {
        "title": "Which statistical analysis method to use?(help)",
        "author": "Sorakaze",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xppd9/which_statistical_analysis_method_to_usehelp/",
        "text": "Hi guys, new person here. I’m a rookie in statistic analysis method so if someone could give me even a tiny hint, i would very much appreciate it.\n\nCurrently the study i am conducting for my project involves getting opinions from my participants regarding medical prescribing. I’m using questionnaire as a research method tool. My questionnaire comprises mostly of likert scale based questions and whether they agree or disagree to the list of statements i have included.\n\nWhat methods of analysis would be the most suitable for my likert scale? When researching online, most of the examples I see have two variables. I came across chi square and maan whitney-u test as a potential however i have no idea how i would go about.\n\nIf the question seems unclear, i really am sorry as i have pretty much zero knowledge on method analysis. ",
        "created_utc": 1518693219,
        "upvote_ratio": ""
    },
    {
        "title": "Which simple test in SPSS should I use?",
        "author": "cbildfell",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xopdd/which_simple_test_in_spss_should_i_use/",
        "text": "I have to perform analysis on this data for a sleep lab, but I haven't done statistics in years so I'm a little rusty. \n\nDependent variable– Whether or not you watch a lot of television (0 or 1)\n\nIndependent variable– Sleep latency (aka how long it takes you to get to sleep), which is continuous. \n\nMy goal is to figure out if there is a statistically significant difference between the two groups. I also have another variable, \"Average screen time\" which is continuous, and I might want to also run that against time it takes to fall asleep (sleep latency). Any help would be much appreciated!",
        "created_utc": 1518677193,
        "upvote_ratio": ""
    },
    {
        "title": "what are the odds?",
        "author": "TrickLeg",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xnkt3/what_are_the_odds/",
        "text": "The teacher gives each student in her class a number. There are 6 students. She then says that she will pick a random number from 1-6 and the student with that number has to give a 5 minute speech on a subject of her choosing. \n\nThere will be two rounds. If a student is picked - their number is removed from the pool of eligible candidates.\n\nWhat are the odds of being picked?\n\n\nI was expecting the answer to be (1/6 * 1/5) but my colleague is insisting it's 2/6. Which is it?",
        "created_utc": 1518663634,
        "upvote_ratio": ""
    },
    {
        "title": "Using ANOVA output for hypothesis testing?",
        "author": "andyyc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xmqf2/using_anova_output_for_hypothesis_testing/",
        "text": "Hey guys, im stuck on a homework question for my stats class and i'm not really sure where to start. \n\nThe information given is: Three replicated water samples were taken at each of four locations in a river to determine whether the quantity of dissolved oxygen, varied from one location to another. Location 1 was adjacent to the waste-water discharge point for a certain industrial plant. Locations 2, 3 and 4 were downstream sampling points that were 10, 20 and 30 miles from the discharge point.\n\nThe question: Is the mean dissolved oxygen content from location 4 higher than the combined mean of the first 3 locations? Carry out the most appropriate test statistic.\n\nWe are given 2 ANOVA output tables. one compares all 4 groups, the other compares location 1 with locations 2,3, and 4. I do not have the descriptives for the different locations, and therefore I do not know the individual means or standard deviations.  \n\nI am assuming I use the output that compares location 1 to locations 2,3, and 4 as it is a 1 mean model vs 2-mean model, just like in the question. However, I am comparing 4 to the rest so I do not know if I use this table all the same or how would I change the data so that I could use it? The question says I should be using the values from the output for my calculations, but wouldn't the different comparison mean I would have to use different values? Sorry if this seems like a stupid question, I am not strong in statistics. Thanks in advance!",
        "created_utc": 1518654986,
        "upvote_ratio": ""
    },
    {
        "title": "Negative confidence interval with logistic regression",
        "author": "Confused_Medic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xlmnu/negative_confidence_interval_with_logistic/",
        "text": "Hi all,\n\nAs a follow-on to my earlier [post](https://www.reddit.com/r/AskStatistics/comments/7tnd41/correct_regression_test_in_medical_research/), I have a quick question about confidence intervals.\n\nI have a model constructed including the relevant risk factors for our outcome (Maternal trauma), and this has given me a set of Odds Ratios, and 95% confidence intervals.\n\nI wanted to look at the effect of each independent variable on the outcome individually, i.e. without controlling for others. For example, I wanted to look at the effect of birthplace on maternal trauma, without adding in BMI, age, birthweight, etc.\n\nI've constructed the model as below\n\n    birthplacemodel.1 &lt;- glm(trauma ~ birthplace, data = trauma_data, model = binomial())\n    birthplace_confints &lt;- confint(birthplacemodel.1)\n\nWhen I look at the confidence interval, the lower bound is negative. I don't understand what this means? I assumed a value lower than 0 meant a decrease in risk, and greater than 1 an increase. How do I interpret a negative value? Birthplace in this instance is binary, either home or hospital, if that makes a difference.\n\nEDIT: I realize including the values may help.\n\n                   2.5 %     97.5 %\n    (Intercept)   -4.1357769 -3.9287235\n    BirthplaceHosp -0.3174743  0.4515041\n\nThanks in advance.",
        "created_utc": 1518644911,
        "upvote_ratio": ""
    },
    {
        "title": "Bonferroni correction correction",
        "author": "KasteTilbake",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xlc7e/bonferroni_correction_correction/",
        "text": "The Bonferroni correction for multiple comparisons seems to give too conservative a significance level, increasing Type II errors. For significance level α, the maximum p-value for rejecting the null hypothesis is α/n where n is the number of comparisons. The probability of incorrectly rejecting the null hypothesis at least once then becomes 1-(1-α/n)^n. This approaches 1-e^-α, which is slightly less than α. For instance, for α = 0.05, this approaches around 0.0488 for many comparisons. \n\nHowever, this should be easy to correct:\n\n1-e^-α' = α\n\ne^-α'= 1-α\n\ne^α' = 1/(1-α)\n\nα' = ln(1/(1-α))\n\n\n\n\n\nα' is the significance level we should use to achieve a real significance level of α.\n\n\n\n\nIs this correction used in statistics?\n\nIf not, why aren't we using this corrected version of the Bonferroni correction?\n\nIs it too much trouble for small gains?\n\nAre there some drawbacks I haven't considered? \n\nHave I made a mistake?\n",
        "created_utc": 1518642501,
        "upvote_ratio": ""
    },
    {
        "title": "How use Neural Networks to determine image filter used to blur images?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xl4yr/how_use_neural_networks_to_determine_image_filter/",
        "text": "[deleted]",
        "created_utc": 1518640874,
        "upvote_ratio": ""
    },
    {
        "title": "Propagating Uncertainties....Until Calculus Hits",
        "author": "aditya101099",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xk34w/propagating_uncertaintiesuntil_calculus_hits/",
        "text": "Hey everyone, \n\nSo I'm a high school senior with some data that I'm trying to propagate uncertainties for. I have a set of data points, each with its own different uncertainty (average values) that I've plotted in their respective error bars. However, I then use Excel to generate a model that creates a curve that fits the data points. The model is a quadratic function. What would the uncertainty of the function be? And how would I propagate the uncertainty while differentiating/integrating the function?\n\nThanks!\n",
        "created_utc": 1518632602,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical test/model river discharge and quality",
        "author": "acd9319",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xjrq8/statistical_testmodel_river_discharge_and_quality/",
        "text": "Hello, I have little or basically no experience with statistics. So I'm here for help, at least to know how to start! Here is the info: The river provides drinking water for people. There are 5 intake stations. River discharge is affected by the weather conditions (less precipitation, less discharge), and the water quality is affected by the discharge (when there is low discharge, less dilution of contaminants and thus more pollution). When there is sewage overflow more pollution in the river. When the quality parameters (pesticides, pharmaceuticals...) exceed the limit value, it's necessary to stop the intake of water temporarily.\n\nI have the data of precipitation, discharge, water quality, sewage overflow, and the intake stops of each station. For the stops, it says how long it lasted and what substances caused the stop. The water quality data is of at least 70 substances like glyphosate or pharmaceuticals. The data is collected daily over 10 years.\n\nNow, I must establish the relation of those variables with the frequency and period of the intake stops.\n\nHow can I do that statistically?",
        "created_utc": 1518630086,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to identify growth.",
        "author": "zetaphi938",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xjb8s/trying_to_identify_growth/",
        "text": "So, I have a data set which shows students from three different grade levels who have all taken the same test at two points in the year to determine if each student has increased their test score. \n\nThere are two independent variables - teacher and grade level. \nThere are two dependent variables - test score one and test score two. \n\nSo each data row would have the student's grade, teacher, first score and second score.\n\nWhat I am trying to understand is two-fold. Are the differences in test scores significant between grade levels and are the test scores significant between teachers? \n\nWould this be accomplished by running a One-Way ANOVA? Because there are two separate dependent variables - would this go beyond an independent t-test also because there are two independent variables would this go beyond a dependent t-test?\n\nThanks! \n\nEdit: Or would multivariate analysis (MANOVA) be the best option, even though the dependent variables are the same?",
        "created_utc": 1518626512,
        "upvote_ratio": ""
    },
    {
        "title": "Appropriate Representation using Control Charts",
        "author": "Brancher",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xj9qb/appropriate_representation_using_control_charts/",
        "text": "Howdy,\n\nQuick question, which I think I know the answer but want to check against a group.  If I'm analyzing a process which the goal is to be either 0% or 100% does using a control chart to represent the moving value work as an effective way to represent the data?\n\nExample - In a warehouse the goal is to scan out 100% of the items that come out of inventory.  If the scan out rate over a year period range from 75% to 99% does displaying this rate of scan out in a control chart add any value if the scan out goal is 100%? The control limit is always less than 100% so why show if a moving range is in control?  What would be the appropriate way to graphically represent this value?\n\nThanks for any help. ",
        "created_utc": 1518626181,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating population standard deviation given samples for t-test and ANOVA",
        "author": "ayeandone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xho7i/estimating_population_standard_deviation_given/",
        "text": "I've read conflicting stuff on the internet about this. My understanding of ANOVA test is that estimating population standard deviation is just square root of mean squares within groups, not between groups. Is this correct? Is it appropriate to say that the degrees of freedom for my standard deviation estimate is the denominator of the aforementioned mean square within groups?\n\nThe  appropriate  degrees  of  freedom  for  the  t-distribution  whenthe t-statistic is calculated in this way is given by the degrees of freedomof your estimate ofσfrom the ANOVA table.\n\nAnd then for t-test, estimating the standard deviation of the population is (((n1 - 1)s1^2 + (n2-1)s2^2 ) / (n1+n2-2))^2, where s1 and s2 are standard deviation of the samples?\n\nJust want to confirm because I've read conflicting stuff online. I've also seen a mention of standard error being relevant to my question.\n\n",
        "created_utc": 1518611581,
        "upvote_ratio": ""
    },
    {
        "title": "Tests for goodness of fit in a time series context with nonlinear independent variables",
        "author": "goodcleanchristianfu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xfo4q/tests_for_goodness_of_fit_in_a_time_series/",
        "text": "I'm working on an econometrics project but the class is primarily focused on cross sectional data while I'm working on a time series regression given by Y = B + B1*T + B2*T^2\n\nThat is, one regression with a constant, units of time, and units of time squared - just studying how Y has changed over time. I was wondering if F tests and adjusted R^2 were still valid in looking at goodness of fit in this sort of a model since I'm a bit outside my wheelhouse.",
        "created_utc": 1518584364,
        "upvote_ratio": ""
    },
    {
        "title": "Homework Help: Derive Unbiased Estimate",
        "author": "KittyBoopsAndToots",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xfayb/homework_help_derive_unbiased_estimate/",
        "text": "I am completely lost on this topic. For homework, it says:\n\nFor X1 ~ B(1,theta) and X2~B(1,theta), where X1 and X2 are independent, find an unbiased estimate for theta(1-theta).\n\nI understand conceptually that E(theta_hat) =theta then it's unbiased, but I don't understand how were supposed to derive it from two different samples. I got to E(theta)-E(theta^2 ) but just taking the E(theta(1-theta)), but I don't know what to do with this information. Does that mean the UE is some w(X1,X2) = X1 - g(X2) so that E(g(X2)) = theta^2 ?\n\nSorry if it's exceedingly ignorant; our professor has not been the best at connecting the text to the lecture to the homework, and I don't know where to begin.",
        "created_utc": 1518580301,
        "upvote_ratio": ""
    },
    {
        "title": "Question on performing EFA on different established scales",
        "author": "squarehipster",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xez97/question_on_performing_efa_on_different/",
        "text": "Hi,\nI'm hoping someone could provide some insight to a question I'm currently facing. I'm looking at self-reported measures of trust obtained from items adapted from several different established scales (11 in total, measuring constructs like trust in teammates, team competence, etc). Each of these scales was in some way expected to be related to subsequent levels of trusting behavior. My question is, after combining individual items into their respective scales, is there a reason that it wouldn't be appropriate to perform Factor Analysis on these to create a reduced set of predictors for the observed trusting behavior? ",
        "created_utc": 1518576976,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating the power when comparing 4 groups",
        "author": "mabrouss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xdrj5/calculating_the_power_when_comparing_4_groups/",
        "text": "Hi there,\n\nSorry if this is the wrong place to post this.\n\nThis is the question that I have:\n\n\"Researchers wish to compare the amount of wear sustained by 4 different types of fabric, by subjecting samples of each type to mechanical abrasion and measuring the weight loss. They wish to be able to detect differences in the means if their true values are mu_1 = 2.2, mu_2 = 2.7, mu_3 = 2.4 and mu_4 = 2.3. An estimate of the variance in weight loss is sigma2 = .05\n\na) What would the power be if the sample size for each fabric type is 5? b) How many samples of each type should be used in order to have a 90% probability of rejecting the null hypothesis of no difference among the fabrics using alpha=0.5\"\n\nWhat I thought to do was build the one-way anova table out of that and got an F values pf 3.733 and a P value of 0.033 but am now stuck. Any help would be greatly appreciated.\n\nThanks",
        "created_utc": 1518565361,
        "upvote_ratio": ""
    },
    {
        "title": "What kind of TTest would one do to compare two sets of data?",
        "author": "starwars101213",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xdnzt/what_kind_of_ttest_would_one_do_to_compare_two/",
        "text": "We did an experiment in class comparing whether gummy bears actually had different flavoring in each color. So we had two different experiments, one with named branded gummy bears and one with generic brand gummy bears.\n\nIn each of these two experiments, we had 3 different trials. Trial one was control, so we ate them and wrote down the right flavors. Trials two we were blindfolded and attempted to guess the flavors. Trial 3 was blindfolded and nose plugged and again, we tried to attempt to guess the flavors.\n\nSo my question is, what type of ttest can be used to compare the say trial 2 of the name brand with the trial two of the generic brand.\n\nI can clarify if this fails to make sense. Thanks.",
        "created_utc": 1518564430,
        "upvote_ratio": ""
    },
    {
        "title": "Need information: How to analyse preferences data",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7xbnlu/need_information_how_to_analyse_preferences_data/",
        "text": "[deleted]",
        "created_utc": 1518548003,
        "upvote_ratio": ""
    },
    {
        "title": "Help in sourcing what program this graph was made on?",
        "author": "NauticalTom",
        "url": "https://imgur.com/a/0RZCm",
        "text": "",
        "created_utc": 1518536017,
        "upvote_ratio": ""
    },
    {
        "title": "Does Canada produce enough food to feed itself?",
        "author": "SwimmingJohn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x9gfk/does_canada_produce_enough_food_to_feed_itself/",
        "text": ".",
        "created_utc": 1518529389,
        "upvote_ratio": ""
    },
    {
        "title": "Questions about exam conditions",
        "author": "MrMiez",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x8gpd/questions_about_exam_conditions/",
        "text": "Hello everyone\n\nFirst, apologies for my english, since its not my first language\n\nTomorrow, we have an exam about theory of action / action theory ( not sure what the exact translation is, sorry )\n\nExam works like this\n\nThere are 9 different texts of varying difficulties\n\nIt's an oral exam, you get asked question about 2 of the texts\n\nA random generator determines which text you get\n\nYou have 1 joker, meaning the text you got is removed and you get a new one ( joker removes the text for you forever )\n\nNow the thing is, you get tested in groups of three\n\nPerson 1 starts, then person 2, then person 3, then person 1 again, person 2 again, person 3\n\nIf a person is asked question to a certain text, the text is now removed from the pool, meaning no other person can get this text\n\nIf a text is assigned to a person, but this person chooses to use their joker on the text, the text is put back into the pool for the other persons, but not for the person who chose to remove it\n\nNow my question is, does any person have an advantage in this \"game\" ? My first thought was, that the person going first has an advantage, because if he gets one of the harder texts, he can deny it, and this would go on until until person 3 gets his second question, and the ratio between easy and hard texts is no longer the same as when the exam started ? Or is this just plain wrong, since everyone can deny the hardest text and it's now purely randomized ?",
        "created_utc": 1518516896,
        "upvote_ratio": ""
    },
    {
        "title": "Conjoint analysis with price as a function. Is it possible?",
        "author": "claralindam",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x8ed9/conjoint_analysis_with_price_as_a_function_is_it/",
        "text": "Hi everyone. I'm a M.sc in Economics and I have to use Conjoint Analysis in a survey about electric mobility. Sorry for silly questions but I'm not that much into Statistics.\n\nIt would be a choice about different cars with three attributes:  \n - battery range (200km, 250km....)\n - revenue model (buy, leasing, buy+rent battery\n - type of battery (which kind of lithium). \n\nEach configuration will have a different price at the beginning and a different monthly fee from an economic perspective.\n\nIs it possible to manage the price like a function of the levels instead that like an attribute with different levels? It makes sense form a statistic perspective?\n\nThank you!",
        "created_utc": 1518515965,
        "upvote_ratio": ""
    },
    {
        "title": "Bernoullis Random Variable",
        "author": "ut_33444",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x7ccq/bernoullis_random_variable/",
        "text": "In Bernoulli’s random variable, is there only 2 possible outcomes? ",
        "created_utc": 1518500923,
        "upvote_ratio": ""
    },
    {
        "title": "What exactly is irreducible error?",
        "author": "Jnk811",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x6zdw/what_exactly_is_irreducible_error/",
        "text": "In machine learning, when we talk about the bias-variance tradeoff, we know that the error (the difference between the actual observations and what our model predicts) can be split into two parts: reducible and irreducible error. I understand that irreducible error can be due to random chance or some unobservable factors. My question is, what exactly does irreducible error measure? Is it the distance between the observations and some kind of \"true\" model? If so, where does the \"true\" model come from and how is it different from the one that we fit based on the data we observe?",
        "created_utc": 1518496997,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a correlation between number/frequency of political town hall discussions, and their effectiveness?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x6zda/is_there_a_correlation_between_numberfrequency_of/",
        "text": "[deleted]",
        "created_utc": 1518496991,
        "upvote_ratio": ""
    },
    {
        "title": "Overlapping error bars",
        "author": "evolutionaryenergy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x6l09/overlapping_error_bars/",
        "text": "Hello all, \n\nMy error bars are overlapping, but spss still says that it is statistically significant. Can someone give me like an ELI 5 as to if and how that is possible?\n\nThanks so much :)\n\n",
        "created_utc": 1518492796,
        "upvote_ratio": ""
    },
    {
        "title": "A Vote For vs A Vote Against",
        "author": "tatterdermalion",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x4rp3/a_vote_for_vs_a_vote_against/",
        "text": "Imagine a 2 party election where each person  had the choice of either one vote for your favorite candidate or one vote against the one you wanted to lose the most votes (a negative vote which would subtract from the total of that candidate)    Obviously no difference for a strict choice between A or B.  But how would that play out in a system like ours where we had 2 favored parties but many small side ones?  What would the strategic choice turning points be?  If you wanted someone to lose, would you still be better off voting for the biggest party opposite them,  or voting directly against them?",
        "created_utc": 1518475731,
        "upvote_ratio": ""
    },
    {
        "title": "Correlation Analysis",
        "author": "ninafea86",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x4hf8/correlation_analysis/",
        "text": "Can a Pearson correlation be done with any type of variable?\n",
        "created_utc": 1518473349,
        "upvote_ratio": ""
    },
    {
        "title": "How do I scale the variance in a dataset?",
        "author": "TangoCJuliet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x43k9/how_do_i_scale_the_variance_in_a_dataset/",
        "text": "Let’s say that I know one source of error in my dataset corresponds to a 10% variance, and I want to remove it. How would the math work in this case? Is there a name for this procedure? \n\nThanks!",
        "created_utc": 1518470201,
        "upvote_ratio": ""
    },
    {
        "title": "\"Online\" correlation between two streaming data series",
        "author": "personalityson",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x2mub/online_correlation_between_two_streaming_data/",
        "text": "Hi,\nWhat options are there?\n\n\nIf I start with something like Pearson correlation coefficient, replace means with exp. running averages, lets say with factor a, I can then have ratio for each pair of values, which I again can average out exponentially with factor b?\n\n\n\nThanks,",
        "created_utc": 1518458610,
        "upvote_ratio": ""
    },
    {
        "title": "How do I find a statistical relationship between two large sets of data?",
        "author": "Zarniwoop123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x2cmw/how_do_i_find_a_statistical_relationship_between/",
        "text": "Hi there,\n\nAs part of my dissertation I’m looking to test the link between human development and colonialism. \n\nI have two very large data sets. One refers to European settlement and the other to human development (source: http://hdr.undp.org/en/content/human-development-index-hdi) \n\nCould anyone shed some light on how I might conduct a simple statistical test to prove a link between these two sets of data? \n\nWe aren’t expected to carry out any econometric analysis here, hence the desire for a simple statistical test. Also, I’m quite new to the field of statistics and although I welcome any suggestions going I think it’s important you’re made aware that I am not too confident in my statistical capabilities at the moment. \n\nThanks for reading. \n",
        "created_utc": 1518456437,
        "upvote_ratio": ""
    },
    {
        "title": "My randomization failed. What now?",
        "author": "iwaslostwithoutyou",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x1jtn/my_randomization_failed_what_now/",
        "text": "Hi guys! So I ran an experiment with two groups: experimental and control group. There was a pre- and posttest. The experimental group got a treatment, the control group did nothing. The treatment was supposed to raise self-compassion (IV) and thereby lower procrastination (DV). However, an independent-samples t-test shows that there was already a difference in self-compassion in the pre-test: the experimental group had significantly (p=,041) more self-compassion than the control group from the get-go. So the groups are not random.\n\n\nIn addition, my control group was impolite enough to also go and get more self-compassionate in the post-test. They have a slight gain in self-compassion and a slight drop in procrastination (copying the experimental group in a lesser fashion). It's not a significant effect, but I get the feeling it's messing up my MANOVA interaction effect (which is ns, ,219).\n\n\nThis is breaking my brain the more I think about it. Is there a way to compensate for the different levels of self-compassion in the pre-test? Is my control group useless, seeing as they were lower in self-compassion from the start, yet also managed to copy the experimental group in their development over time? Any creative analyses I can use to save this?\n\n\nThanks so much for all and any help!",
        "created_utc": 1518450009,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical inference, bivariate normal, problems (based on C&amp;B (casella &amp; berger)",
        "author": "mathstudent137",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x0z4r/statistical_inference_bivariate_normal_problems/",
        "text": "https://gyazo.com/283b3517def28cadfa35537edf6c4b9e\n\nOk, so I don't really understand any of the notation, and I don't understand what the questions are asking... Anything helps at this point, even if you only have a minor thing you can comment on.",
        "created_utc": 1518444801,
        "upvote_ratio": ""
    },
    {
        "title": "What are the common prerequisites for grad school?",
        "author": "kovlin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7x0aqx/what_are_the_common_prerequisites_for_grad_school/",
        "text": "I might be going to grad school for stats, and I want to get a general sense the coursework I'll need; my undergrad degree is in a different subject and I don't want to have to take remedial coursework.",
        "created_utc": 1518437243,
        "upvote_ratio": ""
    },
    {
        "title": "I have enough understanding of statistics, I want to learn more about business analytics. Where do I start?",
        "author": "PhantomTrouper",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wzc2p/i_have_enough_understanding_of_statistics_i_want/",
        "text": "I'm sorry if this is in the wrong place. I have a bachelor of business administration and around 20% of my background is statistical. During my students time I have worked in an analytical job for 1.5 years now where I research qualitative customer data. Now I'm trying to learn SQL and Python through Codeacademy, but I find it difficult without any practical use related to data analysis. \n\nAre there any online courses where I can get certificates at the end of the course. Something like University MSc level. I really don't have the money to follow a full-time study now. Currently I can do a lot with SPSS (Anova, T-test, Z-test, Chi square test, linear regression, multi linear regression, etc..) I know SQL basics, can use MS Access and I know Python basics. \n\nI really want this, but I'm getting no jobs because of my background. Is there any way I can still get into this world without starting a new full-time study? Any advice is appreciated.\n\n\nAny useful literature helps as well!",
        "created_utc": 1518423174,
        "upvote_ratio": ""
    },
    {
        "title": "I need some help with my homework",
        "author": "needsbeermoney",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wyw0s/i_need_some_help_with_my_homework/",
        "text": "Can anyone help me right now with this assignment I'm struggling with? I cant remember how to do #3\n\nYou are given a sample of 50 adult temperatures.\n\n \n\n \n\n98.8\n\n98.5\n\n98.5\n\n98\n\n98.4\n\n98.6\n\n98.7\n\n97.8\n\n99.2\n\n99\n\n97\n\n98.8\n\n98.4\n\n98.4\n\n98.4\n\n98.8\n\n98.6\n\n98.5\n\n99.5\n\n98.8\n\n98.9\n\n98\n\n99.4\n\n97.6\n\n99\n\n98.2\n\n97.9\n\n98.8\n\n97.3\n\n98.2\n\n98.2\n\n96.5\n\n99.4\n\n98.7\n\n98.4\n\n98.6\n\n99.2\n\n97.5\n\n98.2\n\n98.2\n\n98.2\n\n96.5\n\n99.6\n\n98.2\n\n98.6\n\n98.9\n\n97\n\n97.6\n\n98.4\n\n97.6\n\n \n\n1  Find the mean, median, and mode.\n\n2  Find the standard deviation.\n\n3  What is the highest temperature that would not be considered unusual? The lowest?\n\n4  Dogs have a mean temperature of 101.8 °F with a standard deviation of 0.4 ° Which of the following is the worse fever – a human with a 102 °F temperature or a dog with a temperature of 105 °F?\n\n5  Use a graphing tool to construct a histogram.\n\n6  Using the same horizontal axis as the histogram, create a box-and-whisker plot.\n\n7  How do the histogram and box-and-whisker plot compare?\n",
        "created_utc": 1518416793,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for ratios to measure subscriptions",
        "author": "nkj00b",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wyegh/looking_for_ratios_to_measure_subscriptions/",
        "text": "I'm looking for a good set of ratios that i could measure and benchmark a product which is based on subscriptions (mostly annual).  the following are the input variables and are provided each month : new subscriptions, deleted subscriptions, renewed subscriptions, total subscriptions.  What i'm looking for is a resource of ratios and methodologies i could use to analysis these variables. Understanding the relationship between the variables is also important and what the drivers to growth are.  any help appreciated!",
        "created_utc": 1518411101,
        "upvote_ratio": ""
    },
    {
        "title": "When using MCMC for Bayesian Inference, is the form of the likelihood function always obvious?",
        "author": "pottedspiderplant",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wvey4/when_using_mcmc_for_bayesian_inference_is_the/",
        "text": "I'm a physics grad student who somehow didn't get enough formal stats training, so I'm teaching myself. I've heard many talks where MCMC or nested sampling is treated as a magic black box that gives you posterior distributions. I think I understand MCMC now pretty well, but my problem is with the likelihood function. How do you choose the functional form? Is it always obvious? I work with non-gaussian data often...",
        "created_utc": 1518381703,
        "upvote_ratio": ""
    },
    {
        "title": "Multiple regression question",
        "author": "ladysyazwina",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wv513/multiple_regression_question/",
        "text": "(I apologise in advance for my ignorance)\n\nSo I’m running a multiple regression on SPSS and I have 138 participants and 3 predictor variables. When I do the analysis, I’m confused as to why the output is showing N = 137? When I remove one of the variables so there are only 2 predictors, the output now shows N = 138. Am I missing something?",
        "created_utc": 1518379343,
        "upvote_ratio": ""
    },
    {
        "title": "Kernel Density Estimation Variance",
        "author": "ThePengwyn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wv398/kernel_density_estimation_variance/",
        "text": "A kernel density estimation is a mixture distribution where the weightings of every partial kernel are equal. The variance of any mixture distribution is the weighted average of the variances of each partial kernel plus the weighted average of the variances of the means of each partial kernel (or equivalently the data points). Does this not then imply that the variance of any kernel density estimation is strictly greater than that of the data set?\nThanks",
        "created_utc": 1518378907,
        "upvote_ratio": ""
    },
    {
        "title": "Why would I make age a category variable not continuous?",
        "author": "goftar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wt7ny/why_would_i_make_age_a_category_variable_not/",
        "text": "I'm looking at an interaction between a particular variable and age. There's one paper that takes age as a continuous variable and finds just about significant evidence for this interaction. Another paper has a tiny sample size and age as a category variable: young adults and older adults, and doesn't confirm the effect. My supervisor wants me to basically repeat this second experiment with a larger sample size... But I'm unsure why I'd be using just these two categories instead of a continuous variable, since the age effect might well (probably does) vary within the bounds of the categories that I'd be using, 18 to 30 and 60 to 75. What's the advantage of doing it this way? Does it mean I'm more likely to get significant results relative to my sample size?",
        "created_utc": 1518361277,
        "upvote_ratio": ""
    },
    {
        "title": "How do you know if a distribution you derive has a name? (example in post)",
        "author": "rbkillea",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wq2fv/how_do_you_know_if_a_distribution_you_derive_has/",
        "text": "I've arrived at a rather simple distribution as the solution to a (bayesian) problem I've been tinkering with (1). My question is how do (bayesian) statisticians and people whose job revolves around probability find out if a distribution has been named? The usefulness of knowing a name, at least to me, is less-so knowing the easily derivable facts about the distribution and more determining connections with other areas, etc.\n\n(1) The distribution was p(x) = (e^-e^|x/s| )/(-2sEi(-1))",
        "created_utc": 1518315481,
        "upvote_ratio": ""
    },
    {
        "title": "How do we measure which country is the best at producing Olympic athletes?",
        "author": "4hedge2lyfe0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wozj7/how_do_we_measure_which_country_is_the_best_at/",
        "text": "Should this statistic be measured on an absolute basis (which country has won the most medals?) or does population need to be controlled for since we are interested in which country is *best* at producing Olympic athletes.\n\nIs gold medals per capita the best measure, or is there some other better way to measure this?",
        "created_utc": 1518304250,
        "upvote_ratio": ""
    },
    {
        "title": "If your calculated Chi squared value equals the table's value at p= 0.05, do you accept or reject the null hypothesis?",
        "author": "Hitlers_Gas_Bill",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wokbh/if_your_calculated_chi_squared_value_equals_the/",
        "text": "Usually, the Chi squared value ends up being greater or lesser than the table value, in which case, you accept or reject the NH (respectively). If your calculated value EQUALS the table value, what happens?",
        "created_utc": 1518300118,
        "upvote_ratio": ""
    },
    {
        "title": "Why does bayesian inference seem to not make sense?",
        "author": "50326991",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7woak4/why_does_bayesian_inference_seem_to_not_make_sense/",
        "text": "I am reading thinking fast and slow and I was stumped by the bayesian example, I went and searched it up and came across the cancer example widely used. If I go and take a cancer test that is 80% accurate and I get positive, to this individual situation it seems to me there's an 80% chance I have cancer with a 8% change of detecting false positives. So then it drops down to less than 10% accurate, my mind was blown and I just couldn't make sense of it. \n\n\n I don't consider myself a smart person so I apologize if it's a dumb question, I was really intrigued by this especially after thinking I understood the formula while it still didn't make sense to me (Obviously I don't understand it). \n\nI don't plan on studying statistics so what I truly want is just the big picture to get out of this so if one day I get a positive cancer test I do or don't stick to the less than 10% chance I have cancer. The answer might have to be in the format of ELI5 even though I already read a couple in that format and I'm still puzzled. Should I even try to understand this if I don't plan on studying something along these lines? I'm just really curious.\n\n\nThis is the example I'm referencing, \n\n1% of women have breast cancer (and therefore 99% do not).\n\n\n80% of mammograms detect breast cancer when it is there (and therefore 20% miss it).\n\n\n9.6% of mammograms detect breast cancer when it’s not there (and therefore 90.4% correctly return a negative result).\n\n\n**edit**: **Thanks** **for** **everyone's** **help**, I don't know if I'm just not very logical, rational, just not smart enough, or all three lol. So much that I have been looking for books to help me understand this stuff better, because I just have an extremely hard time grasping these concepts and it's pretty upsetting. Does anyone have a good book on the fundamentals of math because I'm starting to think all I truly \"understand\" is addition and subtraction(if that lol).\n\nDon't worry about trying to explain bayesian, I'm pretty sure that with all these helpful explanations I should've been able to understand by now if my fundamentals were good enough. Thanks again for all the explanations and I'll accept any recommendations on books for beginners or advice in general to improve my understanding of numbers, and logic.**",
        "created_utc": 1518297516,
        "upvote_ratio": ""
    },
    {
        "title": "What type of regression analysis should I run?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wna4c/what_type_of_regression_analysis_should_i_run/",
        "text": "[deleted]",
        "created_utc": 1518288190,
        "upvote_ratio": ""
    },
    {
        "title": "Simple question about a 1 in 100 chance",
        "author": "genericjames42",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wn47a/simple_question_about_a_1_in_100_chance/",
        "text": "I'm pretty sure this is a really dumb question but I can't wrap my head around it. If you have a 1 in 100 chance of something happening during some event, it means that each time the event occurs, there's a 1% chance that it will happen. But if the event occurs 100 times, what are the chances that it will actually happen 1 (or I guess 1 or more) of those times? I would think it was .01*100 but that would imply there was a 100% chance that the event occurs. Is it ~99%, 50/50, 1%, or something else? And how do you calculate it? Sorry again I'm sure this has a really simple answer but I'm not doing homework or anything this is just an irl statistics question.",
        "created_utc": 1518286690,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a difference between a Mixed ANOVA and a One-way ANOVA on change scores?",
        "author": "Kanunzorz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wlze5/is_there_a_difference_between_a_mixed_anova_and_a/",
        "text": "Hey guys,\n\nI've performed an experiment wherein participants were divided over three conditions and measured on the dependent variable over two time periods.\n\nMy hypotheses were that the scores of participants in a new intervention (intervention 1) would improve similarly to those in an already proven intervention (intervention 2). Furthermore, I expected that the scores of those in these interventions would have improved in greater amounts than the scores of those in the control condition (condition 3). \n\nNow my understanding is that since I am using within (time) and between (condition) factors, I should be using a mixed ANOVA. However, since my hypotheses state looking at the difference in scores I could also be looking at planned comparisons using a one-way ANOVA of change/gain scores (Pretest subtracted from posttest). \n\nNow, I have already ran both analyses and honestly I really like the profile plot of the mixed ANOVA. But I don't know what justification I would have for including the mixed ANOVA, because I can not tell the difference.\n\nIs anyone able to help me out? I could provide more details if necessary. ",
        "created_utc": 1518275942,
        "upvote_ratio": ""
    },
    {
        "title": "Most helpful videos, books, resources, etc. on how to interpret and read data/statistical analysis? So that anyone can become a bit less ignorant (more context inside)",
        "author": "curiousone6151",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wljrz/most_helpful_videos_books_resources_etc_on_how_to/",
        "text": "just like how they have pop books for the universe and astrophysics, pop books for evolution and life science, and pop-econ books,\n\n**what are the pop stats book for learning how to interpret and read data analysis?**\n\na pop stats book that also tells you what the 'main' methods in stats basically do in plain english would be very helpful also\n\n---\n\njust like you dont need to know math to understand how physics generally works (if it's being explained well)\n\nand just like you dont need to have have investigated biological evolution yourself\n\nand just like you dont need to be a pro tennis player to understand what are the main causal factors &amp; patterns &amp; principles that makes one players win significantly more than other players \n\nand just like you dont need to be an attonery to understand legal theory or traditions of jurisprudence\n\n**what are good pop stats/data books on how to interpret and read data analysis?** \n\nnot about doing stats or doing data analysis which is a completely separate topic from actually understanding how to interpret and read data analysis",
        "created_utc": 1518270930,
        "upvote_ratio": ""
    },
    {
        "title": "Model selection help!",
        "author": "insufferablemoron",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wljdp/model_selection_help/",
        "text": "Hello statisticians struggling to decide which model to choose for my analysis so I thought I would come to the experts.\n\nBasically I'm looking at a large dataset of birds and I want to see how female age affects her choice of partner age.\n\nBoth female and male age are naturally skewed (high counts for 1 year olds and getting progressively lower as they age) and so neither explanatory or response variables show a normal distribution.\n\n I was thinking about creating a GLMM with female age being the explanatory and male age being the response with various random effects added in to try and explain the variance in age choice however I think because assumptions of normality are violated I can't do this??\n\nSorry if this is a simple question but I find stats really hard and haven't found a clear answer online (although I'm sure there is one/loads). \n\nAny advice or even just pointing me in the direction of easy to understand online resources would be greatly appreciated!\n\nPlease and thank you \n\nEdit: just to clarify the age is given in years ",
        "created_utc": 1518270791,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical inference problems",
        "author": "mathstudent137",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wk7de/statistical_inference_problems/",
        "text": "This is given: https://gyazo.com/3257a522c1f482fe94acc04afc583224\n\nSo on a), they are asking you to sketch to the domain of variation in the x-y plane, but I'm not really sure what they're asking (i.e. what it means).. Also whether X and Y are independent or not, I suppose they are from this lemma: https://gyazo.com/a70c6977b261729da352c3823214bc50 (that's correct right=?), I ended up being able to do the same in 1c)( to show that they're independent.\n\n----------------------------------------------------------\n\nSo f(x,y) = 8xy if 0&lt;y&lt;x&lt;1, what does that mean? Is that the same as 0&lt;y&lt;x and y&lt;x&lt;1?. Ok so here is my issues with problem b), so I don't really understand how to find out what U and V are between? So when I solve for x and y in terms of U and V I get X=V and Y=UV. So if you look at what V is equal to X, does this then mean that you'd have y&lt;v&lt;1?, and now I have to find out what y is between etc? (I need this stuff to be able to calculate the marginal densities of U and V, since I need limit of integration (for the next problem). Also again, on b), they ask you to sketch the domain of variation... whatever that is.\n\n----------------------------------------------------------------\n\nHere's some more problems I'm wondering a bit about: https://gyazo.com/2c4653d1dcdcccb50566d2053caa24a5 , so I wasn't really able to solve for x and y on a) here, any advice on how I should do it? Seems pretty difficult to do algebraically. I started with the first one, and squared both sides so I get V^2 = -2lnX * cos^(2)(2piY), then I took e on both sides so I get e^V^2 and some other shit, but wasn't quite able to figure it out. The first one on b) was quite simple, since I just throw the expressions they got from a) straight into a derivative calculator, but I'm a bit unsure about the Jacobian of the inverse transformation (I'm a bit rusty on the in and outs of this stuff, like the implisict function theorem, inverse function theorem or w.e, and the general theory of what is happening). What are they doing here? How do they get 2pi/x for the inverse transformation?",
        "created_utc": 1518249515,
        "upvote_ratio": ""
    },
    {
        "title": "Measuring volatility",
        "author": "hsfrey",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wk12l/measuring_volatility/",
        "text": "A particularly timely problem:\n\nI'd like to have a quick measure of current volatility of the market at any time.\n\nI thought a convenient measure would be last week's hi/lo range divided by last year's hi/lo range, perhaps normalized to the respective means.\n\nAnnual Range is nice because Yahoo reports it daily, so I wouldn't have to download a year's worth of data to compute it myself.\nI'd only need 5 points to compute the weekly data.\n\nIs there some obvious way to determine a weekly value consistent with the yearly value, perhaps simply a factor of sqrt(52)?\n\nDoes that assume Normality? What if I thought the prices are better fitted by a Cauchy distribution?\n\n\n\n",
        "created_utc": 1518246624,
        "upvote_ratio": ""
    },
    {
        "title": "Sales/Customer. Static one year window vs rolling window from time of purchase.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wjxve/salescustomer_static_one_year_window_vs_rolling/",
        "text": "[deleted]",
        "created_utc": 1518245262,
        "upvote_ratio": ""
    },
    {
        "title": "does anyone know of any analysis or viz or graphs that shows what's the correlation between property and/or violent crime to per capita income? or just anything that shows anything insightful/informative",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wjn7z/does_anyone_know_of_any_analysis_or_viz_or_graphs/",
        "text": "[deleted]",
        "created_utc": 1518241066,
        "upvote_ratio": ""
    },
    {
        "title": "How 'equal' do variances have to be for t-tests?",
        "author": "ayeandone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wjapk/how_equal_do_variances_have_to_be_for_ttests/",
        "text": "For example, if one variance is 1.5 and the other variance is 1, is that 'close enough'? How do statisticians generally make that determination? ",
        "created_utc": 1518236799,
        "upvote_ratio": ""
    },
    {
        "title": "t-test fails to reject after transformation of data, rejects before transformation",
        "author": "ayeandone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wic8o/ttest_fails_to_reject_after_transformation_of/",
        "text": "I am studying whether or not Drug A or Drug B has an effect on recovery time. Drug A has an average recovery time of 10 days, Drug B 30 days. I did a t-test and got a .003 p-value. However, I realized I violated the assumptions that variances must be equal, so I did a transformation to make the variances equal.\n\nThe transformation made the means .15 and .10, and I got a p-value of .07, aka fail to reject.\n\nThis is kind of hard for me to understand intuitively. I realize that the first t-test I ran was invalid because of violated assumptions, but still it seems a mean of 10 and mean of 30 are very different, and a transformation shouldn't change that conclusion. I wouldn't feel comfortable saying there's no difference between Drug A and B, even though my transformed data and subsequent t-test reaches that conclusion.",
        "created_utc": 1518226334,
        "upvote_ratio": ""
    },
    {
        "title": "A numerical value used to summarize data calculated",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7whba7/a_numerical_value_used_to_summarize_data/",
        "text": "[deleted]",
        "created_utc": 1518216565,
        "upvote_ratio": ""
    },
    {
        "title": "Simulating the birthday paradox: How to estimate the probability of 2 or more people in a group of 30 sharing the same birthday, given the number of successful samples?",
        "author": "mirsss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wf8do/simulating_the_birthday_paradox_how_to_estimate/",
        "text": "We simulated the birthday paradox on R, by generating 30 random integers from 1 to 365. If there is at least one date with a count of greater than 1, then consider that sample to be a \"success.\" I had 5 successful samples. Tally report for one of my successful samples shown below:\n\ncounts:\n\nsample7\n\n 68  79  86  87 101 110 116 138 164 173 191 195 216 262 269 270 275 285 292 302 \n  1   1   3   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n316 319 322 324 326 341 359 \n  1   1   1   1   1   1   2\n\nThe question: How many of your ten trials were \"successes\"? Use this to estimate the probability of two or more people in a group of 30 sharing a birthday. Compare this to the theoretical answer of 0.71. \n\nI can't seem to wrap my head around it at all :(",
        "created_utc": 1518198928,
        "upvote_ratio": ""
    },
    {
        "title": "data format issue - collapse list of vars by ID and date",
        "author": "joot78",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wezft/data_format_issue_collapse_list_of_vars_by_id_and/",
        "text": "I received data in an unusual layout, with a row for every variable assessed. I want to reorganize the data so that I have a row for each subject-date.  \n\nEasier to visualize than to describe:  I receive the [data like this](https://imgur.com/5dS0f9v), but need  it in [format like this](https://imgur.com/eZk3cXh). There may be multiple time-points for a single subject (e.g. 111), and there may be missing data, which results in the absence of a row (e.g. 333). My actual dataset has 22 variables (vs 3 in the example here) and more than 500 subjects (vs 3 in the example).\n\nI have Excel, SPSS, SAS, and RedCap. I haven't figured out a way to do this that isn't absurdly convoluted.  I want to be able to update the database regularly, so I would like to avoid a cumbersome process, if possible.  Does anyone know a straightforward way to do this? \n\nThanks in advance!",
        "created_utc": 1518196954,
        "upvote_ratio": ""
    },
    {
        "title": "Need help from Stats genius with background in Marketing - Geo test for incrementality",
        "author": "richscottwill",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7we2am/need_help_from_stats_genius_with_background_in/",
        "text": "To start, I just want to say that my background is in Marketing, with little background in Stats. So any help would be amazing. I just ran a geo test to test for incrementality following [this text](http://www.unofficialgoogledatascience.com/2016/06/estimating-causal-effects-using-geo.html) as a guideline, and it's been hard reading the results for revenue. I used a linear regression model in R, with the following formula: y1,i=β0+β1y0,i+β2δi+ϵiy1,i=β0+β1y0,i+β2δi+ϵi (for geo i).\n\nMy actual input into R was this: lm_revenue_provs &lt;- lm(PostRevenue ~ PreRevenue + Spend, data = DataSheet, weights=1/PreRevenue)\n\n My question is regarding reading revenue. If we get an Estimate of .5018 for Revenue with a P-value of .00242, which of the below is true? \n\nA. For every $1 in ad spend, we are getting an incremental revenue of $0.5018 – a positive return of $0.50 for every dollar spent.\n\nB. For every $1 in ad spend, we are losing $0.4982 – a negative return of $0.50 for every dollar spent. To see incremental revenue the coefficient has to be higher than 1, and whatever is over 1 is incremental (e.g. a coefficient of 1.4 would mean $0.40 incremental revenue for every $1 in ad spend)\n\nThe confusion is there because if you're reading for something like Orders, A would be true. ($1 in, the Estimate Orders are incremental orders), but not sure about revenue.",
        "created_utc": 1518189256,
        "upvote_ratio": ""
    },
    {
        "title": "Finding the Expectation and Variance of a random variable that involves \"Mixing\"",
        "author": "aroach1995",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7wbi0s/finding_the_expectation_and_variance_of_a_random/",
        "text": "Hi, I am trying to compute the expectation and variance of a random variable that involves mixing. I am given a formula of how to find the variance/expectation of X using mixing, but I am confused by the notation in the textbook.\n\nThe random variable X~exp(\\Theta=\\theta) where the random variable \\Theta~Poisson(\\lambda=6)\n\nHere is a picture of everything that is going on with the problem in the top left, along with my incorrect attempts, and my reference on the right side: https://imgur.com/CCT393W\n\nPlease let me know what you can do to help.\n\nShould I try the answer: 6?\n\nEDIT: Tried 6, did not work. Then I looked at a similar example:\n\n**Following another example I found. It seems I should compute E[Var(X | \\Theta)] + Var[E(X | \\Theta)] =**\n\n**E[\\theta^2 ]+Var[\\theta] = E[\\theta^2 ] + E[\\theta^2 ] - E[\\theta]^2 = 42 + 42 - 6^2 = 48.**\n\n**Is this correct?**",
        "created_utc": 1518156230,
        "upvote_ratio": ""
    },
    {
        "title": "Composer looking to chart time spent composing and duration of music composed.",
        "author": "Reachcompletecare",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7waxto/composer_looking_to_chart_time_spent_composing/",
        "text": "Hello Askstatistics,\n\nI'm trying to figure out how to chart out some information and can't find a way to do it cleanly. There are three variables that I need to account for and I'm not sure how to put them all in one chart.\n\nHere's what I'm looking to do:\n\n1.) Show the time I spent composing music.\n\n2.) Show the duration of the music that was composed in that time frame.\n\n3.) Show the date on which this happened.\n\nAny recommendations would be appreciated.",
        "created_utc": 1518149888,
        "upvote_ratio": ""
    },
    {
        "title": "Need help getting started in statistics",
        "author": "dmalmer3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w9l01/need_help_getting_started_in_statistics/",
        "text": "I’m a sophomore in high school that’s really interested in getting into statistics, but I won’t be able to take a class on it until senior year so I was wondering if there are any online resources or books you suggest reading for a beginner in statistics. Thanks!",
        "created_utc": 1518136317,
        "upvote_ratio": ""
    },
    {
        "title": "Coursework help",
        "author": "Ephro",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w9325/coursework_help/",
        "text": "I'm fairly new to statistics and I'm writing this while having a minor meltdown. I'm in the middle of writing my coursework for my statistics exams and I feel that I've dug myself into a hole that keeps on giving to me. \n\nI've chosen to work on the advertising area of this unit and I've chosen to try to view the correlation of film trailer lengths to the full run time of the film. \n\nHere is my current set of spreadsheets:\nhttps://docs.google.com/spreadsheets/d/15dML04Q2_KYqYuvNb0Z3tq-KMyt5aLUZd_neewHCcSs/edit?usp=sharing\n\nCurrent write up:\nhttps://docs.google.com/document/d/1N3QO3tjSIk2_fnRwMEFjqlX1P0fhfsFiHk8IJTe1PhM/edit?usp=sharing\n\nMy current plan is to soon convert all the film minutes to seconds but I feel like this will offset the graphs I've created in the spreadsheets but I have the nagging feeling that if I leave them like they are they will not be correct. \n\nIf anybody has any ideas I'd be open to hearing them. In the worst case I'd have to scrap this idea and try for [another area such as the ones \n here](https://i.imgur.com/q5ADBHx.jpg).\n\nThank you /r/AskStatistics!",
        "created_utc": 1518131820,
        "upvote_ratio": ""
    },
    {
        "title": "Discriminate function analysis vs logistic regression?",
        "author": "slipstitchy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w89h6/discriminate_function_analysis_vs_logistic/",
        "text": "Hi, I'm trying to analyze some data (MRI) to examine the relationship between behavioural and anatomical factors (i.e. if decreased volume in regions x,y,z are related to behavioural measure 1). My group and I were originally going to look at this using discriminate function analysis, but we got the impression from our prof that simple logistic regression would be more reliable(?). We'd appreciate any insights you might have. TIA!",
        "created_utc": 1518124968,
        "upvote_ratio": ""
    },
    {
        "title": "Difficult estimation problem",
        "author": "pedrotheanalyst",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w7isu/difficult_estimation_problem/",
        "text": "I have a problem at work that I'd love to estimate a finer level of data than is currently available. \n\nI'll provide an example of the nature of the problem without getting into the technical components of my industry, so imagine I work in agriculture.\n\nLet's say I'm trying to figure out a good way to weight the productivity of various fruit pickers on an orchard.\n\nI know that worker W.1 spent X.1 hours, and I know that he picked apples, pears, and cherries. Worker W.2 spent X.2 hours picking apples, cherries, and mangoes. Worker W.3 spent X.3 hours picking pears and coconuts. [...] Worker W.y spent X.y hours picking ABCD combination of fruit.\n\nIs there a way to set up the data that would allow me to estimate the amount of time spent on these tasks, so that I could come up with a \"score\" for each one? Assume that picking any given fruit means picking approximately the same number of fruit and that it takes about the same amount of time. ",
        "created_utc": 1518119114,
        "upvote_ratio": ""
    },
    {
        "title": "Intro stats question: How many students received a score between..? Please check my work?",
        "author": "millystelescope96",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w78t8/intro_stats_question_how_many_students_received_a/",
        "text": "3000 entering freshmen at State University are given an entrance exam in mathematics. The scores are normally distributed with a mean of 100 and a standard deviation of 14.\n\nAny student earning above an 85 and below a 120 will be enrolled in a regular college math class. What is the percentage of students who will be enrolled in this regular college math class?\n\nFirst I found the z score of 120, which was 1.43, and its percentage from the mean was .4326.\n\nThen I found the z score of 85, which was -1.07, and its percentage from the mean was .3577.\n\n.3577 + .4236=.7846=78.46%. Is this correct? my classmates had different answers.",
        "created_utc": 1518116939,
        "upvote_ratio": ""
    },
    {
        "title": "How does the Y-intercept of a linear regression relate to accuracy?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/chemhelp/comments/7w6hud/how_does_the_yintercept_of_a_linear_regression/",
        "text": "[deleted]",
        "created_utc": 1518112766,
        "upvote_ratio": ""
    },
    {
        "title": "Can you use Chi-Square test even if one of your variables have more than 10 categories?",
        "author": "wizbrekkenej",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w53xv/can_you_use_chisquare_test_even_if_one_of_your/",
        "text": "",
        "created_utc": 1518099870,
        "upvote_ratio": ""
    },
    {
        "title": "How to analyze right skewed data with a continuous DV?",
        "author": "Sftik",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w4ltm/how_to_analyze_right_skewed_data_with_a/",
        "text": "I got my data from a questionnaire: group 1 had 30 individuals and group 2 also 30 individuals. They answered the same 6 questions where the opinions of others were exposed and after they could provide a final decision on those questions. Thus I could calculate weights on opinions (my DV) that range from 0 to 1 (continuous data), namely\n\nWOA = (final estimate − initial estimate )/(advice − initial estimate).\n\nDV = 0 means that participants stick to their initial estimates; DV = 0.5 means that they compromise; DV = 1 means that they adopt fully the other's opinion.\n\nAt the end, I got 6 different WOAs (per question).\n\nThe IV group is a dummy variable where 0 is group 1 and 1 is group 2.\n\nThe data look like this: In the 1st column - WOAs (6 rows) In the 2nd column - id (per 6 rows, it will be repeated id) In the 3d column - question (from 1 to 6) In the 4th column - group (per first 6 rows, group 1, per other 6 rows, group 2) and so on -&gt; at the end, 360 obs\n\nThe data area not normally distributed: most of the estimations are close to 0 (about 45% of the dataset).\n\nI have found from previous posts here that a generalised linear model with binomial family, logit link and robust standard errors would be the most appropriate for an analysis.\n\nBut since my DV is a continuous variable is then a fractional logit model the best? If yes, but I have multi-level data: individuals (30 - 30), group ( 2) and type of questions/treatments (6). Would it be then right to do a fractional logit if I do not control for the level (type) and plus my distribution is not normal?\n\nAccording to O'Hara, R. B. &amp; Kotze, D. J. 2010. Do not log‐transform count data it is better to use a GLM with negative binomial family.\n\nI am quite confused. I appreciate any help you can provide!\n\nPlus, a bit out of the regression topic, since the means do not tell me much about the data, would it be better to present in the analysis the medians of those WOAs as a descriptive analysis?",
        "created_utc": 1518094871,
        "upvote_ratio": ""
    },
    {
        "title": "Testing stationarity of non-autoregressive data",
        "author": "njoo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w3cqx/testing_stationarity_of_nonautoregressive_data/",
        "text": "Any suggestions apart from Kolmogorov-Smirnov?\n\nThe data distribution looks stationary, but I'm looking for something quantitative.\n\nExample data: https://imgur.com/d2xJTDx",
        "created_utc": 1518077360,
        "upvote_ratio": ""
    },
    {
        "title": "Intro Stats. Determining probability?",
        "author": "millystelescope96",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w2a67/intro_stats_determining_probability/",
        "text": "In basketball, the Golden State Warriors made 1790 free throw attempts during the 2015-2016 NBA season and were successful 1366 times. What was the team’s empirical probability of sinking a free throw? What was their theoretical probability?\nI determined the empirical probability to be .76. However, is that the same as the theoretical probability? I fail to see how, in this case, they are any different.",
        "created_utc": 1518063808,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing Separate Walks, Markov Chains",
        "author": "breunor7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7w01rk/comparing_separate_walks_markov_chains/",
        "text": "I currently am working with a time-homogeneous Markov Chain and was wondering if there was a simple way to compare two walks through the same Markov chain?\n\nFor example, assuming a chain with 10 nodes 1:10.\n\nw1 = {1,2,3,4}\nw2 = {1,2,3,5}\n\nIs there a way to say how 'similar' the above walks are (while taking into account the probability of transitions between nodes of course)? I feel like there is a simple approach that I'm just overlooking. \n\nContinuing with that line of thought, is there a way to do this when the length of each walk are not equal?\n\nw1 = {1,2,3,4}\nw2 = {1,2,3,4,4,2,1}\n\nI appreciate any ELI5 help I can get, I'm still very new to working with stochastic processes and want to make sure I build a good intuitive understanding.\n\nThanks!",
        "created_utc": 1518042685,
        "upvote_ratio": ""
    },
    {
        "title": "How to analyze a data set",
        "author": "yellowbirdlove",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vzur4/how_to_analyze_a_data_set/",
        "text": "I apologise greatly for my ignorance. I've been asked to analyze some data at work for a poster. I've read through my college textbook and I'm having a hard time with how I should proceed. In short, I work for a cord blood bank. We are asking the question- Does a high white blood cell count (wbc) correlate with positive bacterial contamination. I have all the WBC results for positive and negative units. I was going to make histograms for the frequency of WBC counts for both positive and negative. \nAn alternative question could be- should there be a cut off WBC count on which not to save a cord unit because it is probably contaminated. \nThanks for the help. Again, sorry for the ignorance. I'm learning. ",
        "created_utc": 1518041024,
        "upvote_ratio": ""
    },
    {
        "title": "Several questions concerning analysis of categorical and ordinal data",
        "author": "rainplop",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vyxyv/several_questions_concerning_analysis_of/",
        "text": "I have a survey that asked for preference of three specific attributes along with an overall preference. There were two options, so each question is binary. What test should I use to see which attributes if any were most correlated to predicting overall preference? My thought was a chi-square test for independence. \n\nI also asked for hedonic scores for these attributes and could also potentially test those scores against overall preference. Not sure at all which test would be best for that or if I should test each attribute individually or as a group. \n\nThank you!\n\nEDIT: Also interested in doing attribute agreement analysis, but I only have access to excel. Briefly read up on intraclass correlations, but haven't found anything specifically for ordinal data over categorical. ",
        "created_utc": 1518033807,
        "upvote_ratio": ""
    },
    {
        "title": "Struggling on whether a Cox regression is the appropriate test for a study which includes event (binary), time-to-event and total follow-up",
        "author": "ForzaMilano",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vyxp9/struggling_on_whether_a_cox_regression_is_the/",
        "text": "Hi everyone,\n\nI'm currently stuck on how to perform a Cox survival analysis.\n\nBasically, I have a big data set of patients and I'm studying heart attacks after starting a medication.\n\nI am looking at several variables, including smoking, cholesterol level, potassium level, etc.\n\nVariables such as:\n- total follow-up time after starting a medication (in months)\n- having a heart attack (binary)\n- time from starting medication to having a heart attack (in months)\n\nI'm trying to perform a Cox regression which includes the above mentioned 3 variables but also other variables such as cholesterol levels, potassium smoking, etc.\n\nThe issue arises because I have 2 time variables: total follow-up time after starting the medication as well as time-to-heart-attack after starting medication.\n\nI am unsure how to include both in my analysis. I use SPSS and I do not know which should be the time variable, which should be the status variable, etc.\n\nThank you!\n",
        "created_utc": 1518033748,
        "upvote_ratio": ""
    },
    {
        "title": "Standard deviation when using SVD to solve linear system of equations?",
        "author": "EngineeringHabitat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vywoz/standard_deviation_when_using_svd_to_solve_linear/",
        "text": "I can't find anything on what the standard deviation would be for each element in my solution vector, x. The equation is: x = (V) [diag(1/s_j)] (UT) (b) where s_j represents the singular values from SVD. Any help would be great!",
        "created_utc": 1518033528,
        "upvote_ratio": ""
    },
    {
        "title": "Repeated measures GLM/ one way ANOVA with non parametric data",
        "author": "YourFriendlySpidy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vyj8g/repeated_measures_glm_one_way_anova_with_non/",
        "text": "alternatively how do I normalize exponential data?",
        "created_utc": 1518030616,
        "upvote_ratio": ""
    },
    {
        "title": "Confident percentiles",
        "author": "Fairshot9312",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vx67a/confident_percentiles/",
        "text": "Say I predict a value using a model and get a confidence interval for that value. If I’m understanding correctly, the bounds of that interval corresponds to a percentile? So if I calculate a 90% confidence interval the lower bound is the 5th percentile and the upper bound is the 95th percentile. If I wanted to use the 5th percentile, and have a confidence interval around that, how would I go about calculating it? The idea is predict the 5th and 25th percentiles and to have a 95% confidence interval around both of them. I looked into quantile regression but it doesn’t seem to apply to my situation since my data set is small ",
        "created_utc": 1518020064,
        "upvote_ratio": ""
    },
    {
        "title": "Struggling with choosing and understanding my statistical significance tests (post-sidebar)",
        "author": "Diddly_eyed_Dipshite",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vwrw9/struggling_with_choosing_and_understanding_my/",
        "text": "I've been reading around my stats for a while now and can't seem to wrap my head around the type of variables that I have and subsequently which tests I should perform. \n\nBrief background to my experiment: \n\nI am examining the effect of UV-B on Pacific oysters the effect is measured with rate of mortality along other histological and pathogenic factors. I'm trying to figure out the stats relating to mortality. \n\n\nFor Trial 1: Three size classes (small, med., large) were exposed to  UV-B. There is an experimental group (exposed) and a control group (not exposed). Mortality was measured at 5 different time points. Trial 2 had two size classes but I figure it will be the same statistical analysis. \n\n\nThe dependant variable is mortality and is numeric. \nSize class and treatment (Expt/control) are both categorical and are both independent variables, I believe (?).\n\n\nWould a chi sq. test work here or should it be an ANOVA? \n\nI'm using both SPSS and R so if you are replying and have a preference then whichever is easiest for you. \n\nMany thanks in advance for your help. \n\n",
        "created_utc": 1518016820,
        "upvote_ratio": ""
    },
    {
        "title": "Proportional difference between population and sample",
        "author": "Lubok",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vv8v7/proportional_difference_between_population_and/",
        "text": "(apologies in advance for bad formulations or terminology misuse)\n\nSituation: so for instance a certain store customers usually consist of 3:7 - men to women. Later that store introduces new product and the buyers are 4:6 - men to women.\n\nIf the store customers were split evenly I could clearly see something like 6:4 on a product as \"men kinda like it more\" and the proportion is somewhat descriptive. But how would I go about to quantify clients preference towards this product when the population is imbalanced? Was thinking to just multiply by weights but that seems to be misleading. I know how to do chi squared test but I'm not sure how to interpret its values in this context. Would like to take sizes into account too if it's adequate here.\n\nEven just a direction in which to look will be highly appreciated.",
        "created_utc": 1517999902,
        "upvote_ratio": ""
    },
    {
        "title": "Time Sensitive: Beginner student trying to determine statistics tests &amp; how to perform. (tried unsuccessfully reading the side panel summary)",
        "author": "lukestarrrR",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vucy8/time_sensitive_beginner_student_trying_to/",
        "text": "Hi all,\nI know this is a last ditch effort, but I am really lost here.  I am doing some research in school and I am trying to determine if my data is statistically significant. I tried reading the side panel to determine which stat test I should use, but TBH, I couldn't even understand the \"nature of the independent variables\".  I have very minimal/nominal statistics knowledge and am feeling out of my league (with little support from my professors.)\n\nMy research consisted of testing 3 different modalities to disinfect a tooth. I wanted to test the efficacy of each modality in getting rid of bacterial, and after each tooth was cleaned, they were plated for 24 hours, and I counted the colonies that formed at various concentrations.\n\nThe end data that I have allows me to determine the average colony forming unit (CFU)/mL of broth we used. I tried using the log(10) of this number to make a chart of the data that is more manageable because the values ranged from 0 to 137million.\n\nUnfortunately, there are different numbers of samples for each modality... 2,3,3,and 10 (3 modalities + control w/out treatment). \n\nI've tried looking at research that is similar to my own, and found that they used Friedman's 2way analysis of variance by rank &amp; the kruskal-wallis procedure.  However, since my data does not have equal amounts of sample per group, I don't think I can do the same.  \n\nIf anyone could please help me, that would be great!  My professor is asking me to put together all this data for a presentation this weekend, but I am completely lost and overwhelmed... (Sorry if this is the wrong sub to be posting in, please let me know if I should have posted somewhere else or if i'm breaking any rules).  Thank you so much!",
        "created_utc": 1517987066,
        "upvote_ratio": ""
    },
    {
        "title": "Help designing a test for webpage changes",
        "author": "what_is_this_for",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vu4xy/help_designing_a_test_for_webpage_changes/",
        "text": "I looked at the link in the sidebar on which test to use but I can't quite figure out if I should use a paired t-test or a two independent sample t-test (or neither?).\n\nWhat I want to test is whether changing the format of a page's title tag will increase traffic, since the title tag is shown in Google search results. For simplicity, say the current format is \"Mr Robot's Electronics - PS4\" and I want to see if \"PS4 - Mr Robot's Electronics\" will increase traffic.\n\nIf I randomly select 1,000 product pages and apply that treatment to them (change title from \"[store name] - [product]\" to \"[product] - [store name]\"), I can see traffic before and after the treatment, and use a paired t-test. But what if the traffic changed because of seasonality? Should I compare those results against another paired t-test using a group of 1,000 randomly selected pages that did not receive the treatment?\n\nOr, is this a two independent sample t-test, where one sample is the 1,000 random pages that received treatment, the other sample is 1,000 random pages that did not receive the treatment, and I measure traffic for some period after the treatment occurred, say sum of traffic after 7 days?\n\nAnd a final question, is it a problem if page traffic does not follow a normal distribution? Ie. something like 80% of pages receive less than 10 visits a day, 15% will receive 10-100, and the other 5% get &gt;100. Or, does taking a large sample (1,000 samples) from the population of pages (possibly 1,000,000) and the central limit theorem, allow me to do those above tests?",
        "created_utc": 1517984145,
        "upvote_ratio": ""
    },
    {
        "title": "Accuracy of linear regression model using null hypotheses?",
        "author": "ColdPorridge",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vsrju/accuracy_of_linear_regression_model_using_null/",
        "text": "I'm working through a homework assignment right now and could use some help. I've determined a linear regression equation (slope and intercept) from my data using Python (sklearn), and have determined my MSE and R^2 for the data set vs the regression line. Now the question I have is to discuss the accuracy of the regression model using the following set of hypotheses:\n\n* H_0_: B_0_ = 0 and B_1_ = 0\n\n* H_0_: B_0_ = 0 (intercept = 0)\n\n* H_0_: B_1_ = 0 (slope = 0)\n\nI don't know how to even begin to formulate an informed discussion here or what calculations would be recommended to arrive at this conclusion. I do feel like I need a bit of handholding here as I am not very comfortable with some of the statistical definitions I'm reading online for this. Thanks in advance for any help!",
        "created_utc": 1517969723,
        "upvote_ratio": ""
    },
    {
        "title": "Question about the Gaussian distribution",
        "author": "GoopOnYaGrinch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vrifs/question_about_the_gaussian_distribution/",
        "text": "Hey All,\n\nSo to touch on my title, it would be helpful if you follow College Football. By no means a necessity, it would just be good additional color to have in understanding context.\n\n247Sports is the worlds most popular website for following College Football recruiting. Each recruit has a specific player rating, [as shown in this screenshot.](https://imgur.com/a/1rlZT) How those ratings are derived is not super relevant to this questions, so don't worry about it. \n\n[247Sports then ranks the strength of every teams recruiting class, which frequently becomes a highly discussed and controversial topic in the College Football world.](https://247sports.com/Season/2018-Football/CompositeTeamRankings) According to the website, it uses a Gaussian distribution formula to determine these rankings. Here's a snippet of the explanation they offer:\n\n&gt; In order to create the most comprehensive Team Recruiting Ranking without any notion of bias, 247Sports Team Recruiting Ranking is solely based on the 247Sports Composite Rating.\n\n&gt; Each recruit is weighted in the rankings according to a Gaussian distribution formula (a bell curve), where a team's best recruit is worth the most points. You can think of a team's point score as being the sum of ratings of all the team's commits where the best recruit is worth 100% of his rating value, the second best recruit is worth nearly 100% of his rating value, down to the last recruit who is worth a small fraction of his rating value. This formula ensures that all commits contribute at least some value to the team's score without heavily rewarding teams that have several more commitments than others.\n\n&gt; Readers familiar with the Gaussian distribution formula will note that we use a varying value for Ïƒ based on the standard deviation for the total number of commits between schools for the given sport. This standard deviation creates a bell curve with an inflection point near the average number of players recruited per team.\n\n&gt; [Below is a graphical representation of how our formula works. You can see that the area under the curve gets smaller both as the rating for a commit decreases and as the number of total commits for a school increases. The y-axis in this graph represents the percentage weight of the score that gets applied to an overall team ranking.](https://imgur.com/a/GcvuS)\n\nOkay, now that I've hopefully done an okay job in explaining the primer, I have some questions.\n\n1. Do you think this is a good methodology in grading out the quality of each schools recruiting class?\n\n2. Is there a different model you would use?\n\n3. What are the strengths and weaknesses of the Gaussian formula?\n\n4. If you look at the [team rankings page](https://247sports.com/Season/2018-Football/CompositeTeamRankings), you will see a column called Avg, which represents the average value of the player ratings in that class. To use an example, you will see that Georgia is ranked as the #1 class, and Ohio State at #2. This is despite the fact that they both have an equal quantity of players (22 each) but Ohio State has a higher average. Why does that happen under this specific statistical methodology?\n\nAnd just any other color or insight you might want to provide. I'm pretty statistically/data science literate, so you don't necessarily have to take an ELI5 approach in your answer.\n",
        "created_utc": 1517958336,
        "upvote_ratio": ""
    },
    {
        "title": "What do you think are the best statistics video series on youtube?",
        "author": "Gurung0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vq0jm/what_do_you_think_are_the_best_statistics_video/",
        "text": "",
        "created_utc": 1517946534,
        "upvote_ratio": ""
    },
    {
        "title": "Is this a good place to ask statistics homework questions?",
        "author": "figgitygoofedup",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vpctr/is_this_a_good_place_to_ask_statistics_homework/",
        "text": "I am really struggling with my homework and the tutors/tutoring center at my school are basically nonexistent. The class I am in is Math274. Thank you",
        "created_utc": 1517941367,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical Averaging - How to summarize data that stems from calculations? - part 2",
        "author": "Stryken101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vp80v/statistical_averaging_how_to_summarize_data_that/",
        "text": "Hello Everyone,\n\nA couple of days I wrote the following post (part 1)\n\nhttps://www.reddit.com/r/AskStatistics/comments/7ue451/statistical_averaging_how_would_you_summarize/\n\nI am confused about a similar situation.\n\nI am still seeking to understand how to represent calculations from sampling. I'll try my best to describe this newfound confusion below.\n\nSuppose we are trying to figure out mean Body Mass Index (BMI) for a the population of Texas. We sample multiple times from different cities (assume a complete random sampling and normal distribution). We sample height and weight from Dallas, Houston, and Austin.\n\nThe formula for BMI is... BMI = weight (kg) ÷ (height2 (m2))\n\nWould it be a more faithful representation of our data if we were to...\n\n(a) calculate the BMI for each individual sampled and then take the global mean of the BMI.\n\n(b) calculate the BMI for each individual sampled and then take the mean BMI for each city. Afterwards, take the mean BMI of each city.\n\n(c) find the average height and average weight of each city and then take BMI of each city. After, take the mean BMI.\n\n(d) another option I haven't thought of.\n\nBonus Question: What would be the name of this type of estimation that I could do more independent research into?\n\nThank you for your input ",
        "created_utc": 1517940316,
        "upvote_ratio": ""
    },
    {
        "title": "Help with determining dependent and independent variables",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vobnh/help_with_determining_dependent_and_independent/",
        "text": "[deleted]",
        "created_utc": 1517933203,
        "upvote_ratio": ""
    },
    {
        "title": "Effect size for a median Z-test?",
        "author": "Tjerk01030103",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vnb5j/effect_size_for_a_median_ztest/",
        "text": "Can anyone help me how to come up with the effect size for a Z value, that has come forth from a median comparison?",
        "created_utc": 1517923777,
        "upvote_ratio": ""
    },
    {
        "title": "Excel/Stats in MultiRegression Model Fitting with Less Input Variabes",
        "author": "kekcoke",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vl4jn/excelstats_in_multiregression_model_fitting_with/",
        "text": "I generated a multi-regression model using at least 7 independent variables; outcome concrete strength. Got a question... 4.Come up with regression formulas with 2, 3, and 4 independent variables that will closely match the linear regression formula that uses all the variables provided in the data.\n\nI initially thought of solving for the input and render coefficients constant and SAME. That was wrong since coefficients of the independent variables affect the variable independently and differently. I can just generate a multi-reg model selecting JUST the coefficient in this instances that greatly influenced my initial model, or but how convert my categorical variables into either 0 or 1 in generating the logit, odds, probability, and so on...\n\nAny other suggestions?",
        "created_utc": 1517894113,
        "upvote_ratio": ""
    },
    {
        "title": "Criteria for a function to be a MGF",
        "author": "justbeane",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7vl1qb/criteria_for_a_function_to_be_a_mgf/",
        "text": "I have two questions about Moment Generating Functions:\n\n1. If a function M(t) and all of its derivatives are defined in an interval around 0, and if M(0)=1, is M(t) necessarily the moment generating function for some random variable?\n2. Given a moment generating function M(t), is there any way to determine the support of the associated random variable? That is, without identifying the MGF as being associated with a known distribution?\n\nI have looked for the answers to these questions (particularly the first one) in a few different places, and have been surprised that I haven't been able to find an answer. \n\nThanks. ",
        "created_utc": 1517893234,
        "upvote_ratio": ""
    }
]