[
    {
        "title": "Response distribution in raosoft calculator",
        "author": "Mayazara",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114vzky/response_distribution_in_raosoft_calculator/",
        "text": "What should be the value of response distribution kept in sample size calculations from any online software e.g raosoft calculator?",
        "created_utc": 1676663878,
        "upvote_ratio": 1.0
    },
    {
        "title": "Simple survey",
        "author": "ANHOW17",
        "url": "https://docs.google.com/forms/d/e/1FAIpQLSeZEDApaONZcElJXKYxU3sFKokk2rnQpcMQdPtom2BOtSB2Ag/viewform",
        "text": "For a probability and statistics class I must gather data. I am required to gather as much random data as possible. The survey only takes about two minutes to complete and is 100% anonymous. Share if you would like, to increase the amount of data I gather. Thank you!",
        "created_utc": 1676660850,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best way to self-teach statistics?",
        "author": "Altruistic_Job8398",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114ueis/best_way_to_selfteach_statistics/",
        "text": "Hello,\n\nI'd like to teach myself statistics so I can understand research papers, assess statistical reasoning in public discourse, and develop statistics competency for personal and professional use. I'm not sure of the best way to do that, so I'd appreciate any advice you could offer on textbooks/methods/etc. Here's some information that might help out:\n\n* I currently teach precalculus at a public school, and I still have a good grasp of at least half of Calculus 1, so I'm mathematically literate and excited about math.\n* I'm familiar with basic descriptive stats, like mean, median, mode, and standard deviation, but I've realized that my familiarity with those rarely helps me understand most uses of statistics that I encounter in the wild.\n* I'm completely comfortable reading a textbook, as long as it has plenty of included homework and projects, ideally ones where I can check my answers.\n* I'm also ok with a course, though I would prefer one that is inexpensive and asynchronous.\n\nThank you again for your help!",
        "created_utc": 1676659846,
        "upvote_ratio": 1.0
    },
    {
        "title": "Improbability of something never happening (or not happening enough).",
        "author": "Never_Say_Never12345",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114tq8s/improbability_of_something_never_happening_or_not/",
        "text": "Let’s say I’m rolling a dice. The probability of rolling a 1 is 1/6.  At what point does it become improbable to not ever roll a 1?  For example, someone tells me they have rolled the die 200 times and didn’t get a 1 even once. How to I calculate that probability and determine if that highly unlikely or not?",
        "created_utc": 1676658134,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would it be statistically sound to use a z-test to compare proportions from the same sample?",
        "author": "TurboRadical",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114t6yq/would_it_be_statistically_sound_to_use_a_ztest_to/",
        "text": "Sorry for the confusing title, I wasn't sure how to word it succinctly.\n\nImagine you have a random variable with three different outcomes (A, B, C). After sampling the variable 100 times, you ended up with 43 A, 37 B, and 20 C.\n\nWould it be valid to then use a z-test to compare the proportion of A and the proportion of B? \n\nI'm only familiar with the idea of using a z-test to compare proportions from different samples, so this feels *really* dirty, but my stats knowledge isn't strong enough to conclusively say whether or not this is fine.",
        "created_utc": 1676656827,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone help me understand this paragraph about the differences between interval and ratio data? Why can't I say that 20C is twice the value of 10C?",
        "author": "sadmathguy2021",
        "url": "https://i.redd.it/2pgmh053ipia1.png",
        "text": "",
        "created_utc": 1676621263,
        "upvote_ratio": 1.0
    },
    {
        "title": "Chi-Square Odds Ratio",
        "author": "Slayer_of_Truth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114dn59/chisquare_odds_ratio/",
        "text": " \n\nI am looking at whether exposure to smoking is a risk factor for alcoholism. I have never done case-control. I know regression gives you OR but my supervisor told me to use this calculator:[https://ebm-tools.knowledgetranslation.net/calculator/case-control/](https://ebm-tools.knowledgetranslation.net/calculator/case-control/) and this gives me OR and chi-square... I double checked the chi-square with SPSS and they are the same so it seems like that its giving me accurate chi-square values but:\n\n1. is this normal for me to be reporting chi-square and OR? does that make sense?\n2. how would i report it? I can't say something like \"significant chi-square shows that smoking is a risk factor for alcoholism\" b/c chi-square only tells us if there is significant different between two groups? or do I say \"The OR shows that smoking is a risk factor for alcoholism\" instead? feels like its two totally different analyses jammed into one...\n3. I also have a p value that's nots significant when OR has a CI that has an upper bound of 0.95 but doesn't cross 1, how would I interpret that do I say the P-value is not significant ergo its not significant or do I say CI is significant ergo its significant?\n\nSorry for so many questions, this is such a simple analysis so I'm really getting frustrated that I a having such a hard time with it...",
        "created_utc": 1676619087,
        "upvote_ratio": 1.0
    },
    {
        "title": "Case-Control Study with Odds Ratio",
        "author": "Uber_Tr0ll",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114cqbb/casecontrol_study_with_odds_ratio/",
        "text": " I am looking at whether exposure to smoking is a risk factor for alcoholism. I have never done case-control. I know regression gives you OR but my supervisor told me to use this calculator:[https://ebm-tools.knowledgetranslation.net/calculator/case-control/](https://ebm-tools.knowledgetranslation.net/calculator/case-control/) and this gives me OR and chi-square... I double checked the chi-square with SPSS and they are the same so it seems like that its giving me accurate chi-square values but:\n\n1. is this normal for me to be reporting chi-square and OR? does that make sense?\n2. how would i report it? I can't say something like \"significant chi-square shows that smoking is a risk factor for alcoholism\" b/c chi-square only tells us if there is significant different between two groups? or do I say \"The OR shows that smoking is a risk factor for alcoholism\" instead? feels like its two totally different analyses jammed into one...\n3. I also have a p value that's nots significant when OR has a CI that has an upper bound of 0.95 but doesn't cross 1, how would I interpret that do I say the P-value is not significant ergo its not significant or do I say CI is significant ergo its significant?\n\nSorry for so many questions, this is such a simple analysis so I'm really getting frustrated that I a having such a hard time with it...",
        "created_utc": 1676615608,
        "upvote_ratio": 1.0
    },
    {
        "title": "Case-Control OR",
        "author": "Uber_Tr0ll",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114cfzh/casecontrol_or/",
        "text": "[removed]",
        "created_utc": 1676614550,
        "upvote_ratio": 1.0
    },
    {
        "title": "Thesis help - Multivariate Multilevel Modeling",
        "author": "Popular_Ladder_597",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1148w7a/thesis_help_multivariate_multilevel_modeling/",
        "text": "I am currently working on my master’s thesis and have bitten off a bit more than I can chew with my statistical analysis plan. My thesis is a diary study where I have 4 dependent variables. So it is longitudinal and multivariate. I also have individuals from different clinics so it’s a three level analysis. Could someone please explain to me how to run this analysis using IBM SPSS like I’m five? Or at least direct me to a resource that does so. I’m desperate at this point.",
        "created_utc": 1676602756,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Does a regression with a categorical variable have the same slopes?",
        "author": "bennettsaucyman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1147hxc/q_does_a_regression_with_a_categorical_variable/",
        "text": "I read somewhere that if you run a regression with a categorical variable that is just additive:\n\ny = b1x1 + b2x2 + b0\n\nwhere x1 is a continuous variable, and x2 is a categorical variable, then if you were to plot a graph, the intercepts would vary, but the slopes would be flat, and the same. And if you want the slopes to vary for each level of the categorical variable, then you must add in an interaction term. So if you dummy coded the variables, then the intercept is the mean value of the first level, then the beta of the other variable is the difference between the intercept (first level) and the second level. And that this would mean that the slopes are flat and the same, since you are only looking at the means of the groups.",
        "created_utc": 1676598563,
        "upvote_ratio": 1.0
    },
    {
        "title": "Distribution of percentile ranges of normally sampled gaussians?",
        "author": "Efficient_Phase_2549",
        "url": "https://www.reddit.com/r/AskStatistics/comments/114774k/distribution_of_percentile_ranges_of_normally/",
        "text": "I currently have a MC model capable of the following:\n1) it generates N samples from a normal distribution\n2) it creates a function of N gaussians of fixed sigma and uses the sampled values as means\n3) it calculates the interquartile range of this function (integrates to 25% of the total integral, same to 75%, takes the difference between the x values\n4) these steps are related to generate a probably distribution\n\nI know this is lethally inefficient, and I know that there's an analytic solution to this, but I've only been able to see solutions that work with discrete sampled points rather than a function using these points like in the above. Any advice?",
        "created_utc": 1676597670,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which stats test should I use?",
        "author": "Competitive-Rich-492",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1146br8/which_stats_test_should_i_use/",
        "text": "I have a dependent variable which is tree cover loss and independent variable which is temperature. Both variables have 20 groups/regions in a country in them and I want to do some kind of stats test/correlation test but am not sure which one to do? \n\nI have each region with the years from 2001-2021 but I also have just the mean of each region if I just need the mean.\n\nAny help would be much appreciated thanks!",
        "created_utc": 1676595135,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to compare two standard deviations (sd)?",
        "author": "rens713",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1140xj6/how_to_compare_two_standard_deviations_sd/",
        "text": "If I have two different variables (it's a silly example):\n\nX1: Number of guitars (unit: guitars)\n\nX2: Number of chords play by minute (unit: chords)\n\nI calculate the sd for each variable. How can I compare them? Because the sd measurement is in units of the variable (guitars, and chords, respectively). But if I want to compare what variable is more \"disperse\" how can I do that? Which is the more efficient/reliable way to do it?",
        "created_utc": 1676581169,
        "upvote_ratio": 1.0
    },
    {
        "title": "Performing One-way ANOVA test but variance is much greater than mean",
        "author": "imreadytolearn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1140i8m/performing_oneway_anova_test_but_variance_is_much/",
        "text": "Doing disinfection experiments and comparing each cleaning arm to determine if one cleaning method is more effective than another. Measured disinfection as colony-forming units (CFU). I ran a one-way ANOVA and did Bonferroni correction and found no statistical significance (p&gt;0.05) between the post-disinfection mean CFU. But since this is a disinfection experiment, I have many zeros in the dataset so wondering how to correct this. For example, the mean is 0.5 but the variance is like 40. Since many of the instruments ended up being cleaned after (CFU=0) and 1-3 samples had contamination around 10. I use SPSS for statistical analysis",
        "created_utc": 1676580068,
        "upvote_ratio": 1.0
    },
    {
        "title": "Navy Pilots not being honest",
        "author": "navydocRC12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113yg39/navy_pilots_not_being_honest/",
        "text": "I am a Navy Physician with very limited statistics background.   I did a research project where I asked pilots if they were hiding medical conditions from their flight surgeons.  The descriptive statistics are really interesting and not surprisingly the pilots are not disclosing medical conditions.   I have my data in SPSS and just need a little help. \n\nI have 2 groups: fixed wing and rotary wing pilots with variables junior, mid and senior pilot and whether they disclosed their condition or not (0-no, 1-yes).  I thought I would run a 2 tail t test to see if there is any statistical significance between type of aircraft,  how long served and disclosed or not disclosed.   Is a 2 trail t test the best test I should run on my data set or would you recommend a better test?",
        "created_utc": 1676574740,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why would my Nearest Neighbors be Gamma/Erlang/Chisq distributed?",
        "author": "jerbthehumanist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113ya7p/why_would_my_nearest_neighbors_be/",
        "text": "I am currently measuring distances between (different) proteins in a 2D. They definitely cluster more closely than uniformly-distributed proteins, we have tested simulated random proteins of equivalent density and these proteins have smaller nearest-neighbor distances.\n\nWhen fitting, the NN distances seem to follow a gamma distribution with shape parameter ≈2. I understand that this means it could also be Erlang/Chisq distributed.\n\nI’ve found that Erlang describes the sum of n exponential random variables, but I don’t know what this would have to do with distances. Any insight?",
        "created_utc": 1676574320,
        "upvote_ratio": 1.0
    },
    {
        "title": "How does the DW-NOMINATE scaling method determine \"conservative\" versus \"liberal\"",
        "author": "DukeMaximum",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113weja/how_does_the_dwnominate_scaling_method_determine/",
        "text": "I'm reposting this from elsewhere, and I apologize if my question seems elementary. I've taken two graduate statistics courses in my life, and I got a B- in both.\n\nI keep seeing DW-NOMINATE being used as the gold standard of political ideology measure, often being used to quantify how \"conservative\" or \"liberal\" an elected official is. But every explanation I'm seeing seems to explain that the method is really measuring different Congress members' distance from each other, rather than against some firm, quantified definition of \"conservative\" or \"liberal.\"\n\nAm I missing something here?",
        "created_utc": 1676569513,
        "upvote_ratio": 1.0
    },
    {
        "title": "how should I analyze my data?",
        "author": "Away-Performance-700",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113ufsw/how_should_i_analyze_my_data/",
        "text": "Hello everyone! I have conducted a study in which I wanted to compare the dyslexia knowledge of different school professionals. We had three groups: teachers, reading specialists, and speech pathologists. We administered a survey using Likert Scale formatting (Strongly Disagree-Strongly agree). We also added in a few questions about the professional's confidence working with children with dyslexia based on their informal and formal education. I initially used an ANOVA, but I feel that there are better ways to analyze this. I wanted to compare the knowledge of the groups, see if one group outperformed the others, and see if those who felt more confident about their education performed better in the questionnaire. Any advice?",
        "created_utc": 1676564499,
        "upvote_ratio": 1.0
    },
    {
        "title": "There are 250 data and they are not normally distributed. What statistical test do I employ to find the impact of days of exercise on another variable like cholesterol? The days range from 1 to 6000 days.",
        "author": "Future_Difference133",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113s08m/there_are_250_data_and_they_are_not_normally/",
        "text": "",
        "created_utc": 1676558058,
        "upvote_ratio": 1.0
    },
    {
        "title": "why would a uniform distribution over the real line results in an improper prior?",
        "author": "wlmai",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113pr1z/why_would_a_uniform_distribution_over_the_real/",
        "text": "I was wondering about [https://math.stackexchange.com/questions/791446/improper-uniform-prior-distribution](https://math.stackexchange.com/questions/791446/improper-uniform-prior-distribution)\n\nOne would integrate 1/(b-a) w.r.t theta (theta being the parameter to estimate), correct? Yet theta/(b-a) with b -&gt; oo, a -&gt; oo would let this term go to zero. Could you help me finding the wrong conclusion please?",
        "created_utc": 1676551418,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mann-Whitney U test with weights?",
        "author": "solresol",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113p7aa/mannwhitney_u_test_with_weights/",
        "text": "I have a data set of prediction market bids: each participant has spent some (possibly non-integer) number of tokens on their bids for various amounts.\n\ne.g. Participant A has bid 17.8 tokens on the final answer being 21. Participant B has bid 5.1 tokens on the answer being 20.5 and another 4.9 tokens on the answer being 27.1. I don't think I can claim that the bids are normally distributed; I'm not even confident that they will follow any common distribution. So any test I run has to be non-parametric. \n\nNow to my problem: I have a pair of related markets and I want to compare whether the bidding on these markets is different in a statistically significant way.\n\nIf each participant had bid the same number of tokens, I could just do a Mann-Whitney U.\n\nIf everybody had an integer number of tokens, I guess I could just pretend the data set had that bid multiple times.\n\nBut since the participants can bid a non-integer number of tokens, I need a weighted Mann-Whitney U I think. Does such a thing exist? Or is there another alternative?",
        "created_utc": 1676549610,
        "upvote_ratio": 1.0
    },
    {
        "title": "Normative sample - is my data analysis process correct?",
        "author": "oddsockx",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113nuhf/normative_sample_is_my_data_analysis_process/",
        "text": "Hello!\n\nFor my research project, I'm generating a normative sample based on responses for a Quality of Life (QOL) rating questionnaire. \n\nI wanted to check my method for data analysis is correct or how it could be improved. Statistics is not my strong suit but I've done what feels logical to me. I'm hoping I've selected the right kind of test to run.\n\nMy research Qs are:\n\n* how do respondents rate themselves on the questionnaire (i.e. overall average)\n* Are there significant group differences within the normative sample between variables (e.g. gender, ethnicity etc)?\n\nData analysis process:\n\n* excluding incomplete responses and those that do not meet the inclusion criteria\n* Use SPSS to check for anomalies\n* calculating the overall average and standard deviation per QOL statement\n* calculating the average and standard deviation according to variables (e.g. male v female, white v non-white)\n* calculating if there is a significant difference between variables using a one sample t-test\n* In the original study, there was a check for variance within the subsection of the questionnaire. I will also do this. I guess for the overall average and each demographic. \n* I am considering a regression analysis to check for correlation - is this a good idea?\n\nEven if it's just to say my steps are ok - please let me know!\n\nThank you!",
        "created_utc": 1676544662,
        "upvote_ratio": 1.0
    },
    {
        "title": "A Practical and applied book on Survey design methodolgy and specific case studies on probability sampling",
        "author": "Entire-Parsley-6035",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113lx92/a_practical_and_applied_book_on_survey_design/",
        "text": "Please I need recommendations on books about applied survey design methodology and practical  examples in probability sampling. I will love stuff with simulations in R or real world datasets . Thanks in advance.",
        "created_utc": 1676536705,
        "upvote_ratio": 1.0
    },
    {
        "title": "Pearson Correlation",
        "author": "mcnrenzo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113kpk3/pearson_correlation/",
        "text": "I am very much in need of help. How can I interpret a likert scale data? And correlate it to GPAs.",
        "created_utc": 1676531783,
        "upvote_ratio": 1.0
    },
    {
        "title": "Tests on skewed data?",
        "author": "vitis_rules",
        "url": "https://i.redd.it/loo72xw43iia1.png",
        "text": "",
        "created_utc": 1676531422,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interrupted Time Series Analysis - ARIMA vs Segmented Regression",
        "author": "AidenTheRoombaDog",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113fxwh/interrupted_time_series_analysis_arima_vs/",
        "text": "Hi, everyone! First off, a disclaimer, I am by no means an expert statistician. I'm an early career MD/MPH (not a biostats emphasis unfortunately!) graduating fellowship this summer. My interest is antimicrobial stewardship, a QI-esque field within the medical subspecialty of infectious disease. \n\n(TL;DR) My question is: what are the benefits and downsides to ARIMA versus a segmented regression? Is there a good tutorial on how to do a segmented regression in SPSS? \n\nSome background: I am evaluating rate of first-line antimicrobial prescribing before and after an intervention. Each of my data points were collected monthly with pre-intervention period totaling 45 months and post-intervention period totaling 12 months. Unfortunately, there is no control group. \n\nFor ARIMA (which I have played with): SPSS assigned an ARIMA model of (0,0,0), which I am reading is consistent with white noise. This seems to make sense to me when looking at a scatterplot as most pre-intervention data points vary between 65-80 per 100 with no (to the naked eye anyway) discernible trend in the pre-intervention data. In the post-intervention period, there is a 8.5 per 100 increase in first-line prescribing. Interestingly, I have also accounted for the recent Amoxicillin shortage in November to December of 2022 (where I stopped collecting data), and there is a 9 per 100 decrease in first-line prescribing. \n\nI would love to also be able to do a segmented regression in SPSS to compare to the two methods. Primarily, I am curious if the segmented regression model will have a very slight upward or downward trend in our first-line prescribing in the pre-intervention group when compared to the ARIMA which is attributing all pre-intervention variation to white noise... if that makes sense? However, I can't seem to find a good YouTube video or written tutorial on how to do a segmented regression in SPSS. Most that I see seem to be for R? \n\nIf you've made it this far, thanks so much for reading, and I would definitely appreciate any insight to the above questions. Happy to answer any questions to the best of my ability!",
        "created_utc": 1676515426,
        "upvote_ratio": 1.0
    },
    {
        "title": "Methodology for Selecting a Sample Size for QC",
        "author": "Weekly-Conflict-7845",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113fqcj/methodology_for_selecting_a_sample_size_for_qc/",
        "text": "Hi all.  I am trying to come up with a methodology for selecting a sample size to perform Quality Control.  \n\nI would be performing quality control on tasks performed by a mix of humans and automation.  These tasks have varying frequencies ranging from daily to annually.  \n\nMy goal is to find the sweet spot of performing proper QC, while being efficient with the resources performing the QC.  Ideally, this could be a simple and repeatable methodology, to avoid extra resources to calculate the sample size or introduce errors in selecting the sample sizes by the individuals performing the QC.  \n\nI have several different variables below, but they do not all have to be incorporated into the methodology and I would love to leverage a proven methodology vs something homegrown/unique.  \n\nHere are my variables:\n\n* Frequency \\- Tasks are varying frequencies with most occurring Daily, Weekly, Bi-weekly, Monthly, or Annually.\n* Risk - Each task has a risk rating (1-5) based on the impact of not performing the task correctly\n* Execution - Either will be manual/human or automated tasks\n* Prior performance - Ideally previously failed tasks would have additional scrutiny until they are consistently performed correctly for a period of time.  \n\nPlease let me know if you have any questions or ideas.",
        "created_utc": 1676514791,
        "upvote_ratio": 1.0
    },
    {
        "title": "Handling Skewed Data",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113c9sa/handling_skewed_data/",
        "text": "I have a simple data set (n = 600) that is relatively normal but skewed slightly and I would like more symmetry among the values. Log transform does not seem to work as much as I would like. Is it mathematically lawful to transform the lower half of the data but leave the upper half alone thus \"stretching\" the lower half to create a more symmetrical distribution? Any suggestions on how this can be done?",
        "created_utc": 1676504827,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a statistical test can I run on my data?",
        "author": "Len_Crafter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/113a7l6/is_there_a_statistical_test_can_i_run_on_my_data/",
        "text": "I am running an experiment to determine whether or not green light (520-565nm) contributes significantly to the rate of photosynthesis in spinach chloroplasts.\n\nThe rate of photosynthesis was measured indirectly by using DCPIP dye which turns from blue to colorless as it is reduced in the reaction. Using a spectrophotometer, the absorbance of each test tube containing a chloroplast suspension, buffer, and DCPIP dye was measured in 2-minute intervals. One test tube was exposed to full spectrum light, and one was exposed to the same light but with the green wavelengths filtered out.\n\nIs there a statistical test I can run to determine whether or not the photosynthetic rate over time is significantly different between the two tubes? I only ran one trial and recorded the absorbance in each tube for 14 minutes (8 total data points per tube). Or is there not enough data to run a quantitative analysis and can I only describe the difference in photosynthetic rates qualitatively?",
        "created_utc": 1676499467,
        "upvote_ratio": 1.0
    },
    {
        "title": "95% CIs with 0 in them but very significant p-values - Understanding check please?",
        "author": "AtomikRadio",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1131tmq/95_cis_with_0_in_them_but_very_significant/",
        "text": "I posted this in a different sub last night and got no replies, and in cross-posting it here I think I answered my original \"What don't I get here?\" question, but now just want to check understanding. Here is the original question:\n\n&gt;[Consider this table.](https://imgur.com/nfKJTa3)\n\n&gt;&gt;Treviño RP, Yin Z, Hernandez A, Hale DE, Garcia OA, Mobley C. Impact of the Bienestar School-Based Diabetes Mellitus Prevention Program on Fasting Capillary Glucose Levels: A Randomized Controlled Trial. Arch Pediatr Adolesc Med. 2004;158(9):911–917. doi:10.1001/archpedi.158.9.911\n\n&gt;In the abstract for the paper they state that:\n\n&gt;&gt;Mean fasting capillary glucose levels decreased in intervention schools and increased in control schools after adjusting for covariates (−2.24 mg/dL [0.12 mmol/L]; 95% confidence interval, −6.53 to 2.05 [–0.36 to 0.11 mmol/L]; P = .03). Fitness scores (P = .04) and dietary fiber intake (P = .009) significantly increased in intervention children and decreased in control children. Percentage of body fat (P = .56) and dietary saturated fat intake (P = .52) did not differ significantly between intervention and control children.\n\n&gt;However, in the abstract and throughout the table linked above virtually all of their 95% CIs for difference between groups straddle 0, ranging from a possible reduction to a possible increase in virtually every CI for every variable in the adjusted models. At first I thought that it was just bad formatting and the (-) from the first number was also meant to apply to the second number, but that would not make sense given the numbers for fiber in which they are certainly increasing so the second number in the CI should not also be negative.\n\nNow, as I look at this again int he light of day, I think I get it but would like to double-check before I misunderstand *again!* \n\nI believe that my first reading of the chart was interpreting the significant p-values as being evidence of a difference from baseline, when in fact they are representing a difference between the control and intervention. If this value *was* meant to interpret change from baseline it would *not* be significant because of 0 being within the CI, but because they are actually looking at differences between groups and the control group often \"went in the opposite direction\" than the intervention then that means the reference value (0 in the case of change from baseline, 1 in the case of things like risk ratios, etc.) that would need to appear in the CI for it to be insignificant is not 0 but in fact the point estimate for the other group?  Do I have that correct?\n\nAnd so, let us make up some tables to represent some value at baseline (BL) and at follow-up (FU) after some treatment so I can ask questions to check my understanding:\n\n##Table 1. No overlaps of values.\n\nGroup | Mean | 95% CI of Mean\n:--|:--:|--:\nControl at BL | 4.1 | 3.8 - 5.0\nControl at FU | 4.3 | 3.7 - 5.1\nIntervention at BL |  4.0 | 3.9 - 4.8\nIntervention at FU | 6.9 | 5.4 - 7.1\n\n##Table 2. Overlapping of CI with one another, not of point estimates.\n\nGroup | Mean | 95% CI of Mean\n:--|:--:|--:\nControl at BL | 4.1 | 3.8 - 5.0\nControl at FU | 4.3 | 3.7 - 5.1\nIntervention at BL |  4.0 | 3.9 - 4.8\nIntervention at FU | 6.9 | **5.0** - 7.1\n\n\n##Table 3. Overlap of CI of one with point estimate of another.\nGroup | Mean | 95% CI of Mean\n:--|:--:|--:\nControl at BL | 4.1 | 3.8 - 5.0\nControl at FU | 4.3 | 3.7 - 5.1\nIntervention at BL |  4.0 | 3.9 - 4.8\nIntervention at FU | 6.9 | **4.2** - 7.1\n\n##Table 4. Overlap of CIs, changes in control from BL.\nGroup | Mean | 95% CI of Mean\n:--|:--:|--:\nControl at BL | 4.1 | 3.8 - 5.0\nControl at FU | **3.4** | **2.1 - 5.4**\nIntervention at BL |  4.0 | 3.9 - 4.8\nIntervention at FU | 6.9 | 4.2 - 7.1\n\n\nAm I correct in saying that:\n\n* In Table 1 there will be a significant difference between group means at FU because nowhere in the CI of one is there overlap with CI of the other?\n\n* In Table 2 and Table 3 there will not be a significant difference between group means at FU because the intervention's CI for the mean at FU overlaps with the CI of the control's in Table 2 and the point estimate in Table 3, and though both values will not be statistically significant, table two will likely have a smaller p-value than table three?\n\n* In Table 4 there is not a significant difference in *means at follow up* between intervention and control because the CIs overlap, *but* there might be a significant result for the effect of treatment because we would be looking at significance in the difference, not significance in the means?\n\nDo I understand correctly, or might someone help me out?",
        "created_utc": 1676478174,
        "upvote_ratio": 1.0
    },
    {
        "title": "How accurately do Lagrange polynomials project future statistics?",
        "author": "InspiratorAG112",
        "url": "https://mathworld.wolfram.com/LagrangeInterpolatingPolynomial.html",
        "text": "",
        "created_utc": 1676473915,
        "upvote_ratio": 1.0
    },
    {
        "title": "Alternative to 2-way ANOVA?",
        "author": "Mediocre-Cicada8874",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112vy71/alternative_to_2way_anova/",
        "text": "Hi everyone, sorry if this is kind of a basic question, I’m no expert in stats. I am a PhD student (biologist) working on my thesis and I’ve been dealing with some complicated data that I obtained from an experiment. My animals are fruit flies and I want to study whether their “sleep minutes” change after a treatment (sleep deprivation) and if that depends on the genotype of the animal. I have three groups of animals (flies1, flies2, flies3). I take half of the flies in each genotype and I apply the treatment (sleep deprivation) during one night and the other half I use it as a control (I don't apply sleep deprivation during the same night). After that, I measure the amount of time they sleep during the first six hours of the day after that night of sleep deprivation or control (depending on the group). I have two data tables, one (named \"sleep\\_before\") for the sleep before the moment of sleep deprivation that has a column with the amount of sleep minutes during the first six hours of the day before the night of sleep deprivation, and the other table (named \"sleep\\_after\") which has a column with the amount of sleep minutes during the first six hours of the day after sleep deprivation. With that I calculate the numerical difference between the minutes of “sleep\\_after” and the minutes of “sleep before”, I call it “sleep\\_rebound”. For normal animals this data should be positive and in controls it should be 0 (though sometimes it is a small negative amount of minutes)\n\nFor each group of animals (flies1, flies2, flies3) I want to determine if the “sleep rebound” for treated animals is different than for control animals and if this shows a difference depending on the group (flies1, flies2, flies3). I naturally went to two-way ANOVA to try and study the interaction of group of flies\\*treatment besides the individual “group of flies” and “treatment” factors, but the residuals of my data don’t follow a normal distribution and they don’t meet the homoscedasticity assumption either. I inspected this visually by doing density plots, histograms and qq-plots of the residuals for the distribution and residuals vs. fitted values for the variances. I also performed the Shapiro-wilks test on the residuals for distribution and levene test for the variance, in both cases p&lt;0.005. I also tried to transform the data by adding a constant (to make all values positive, as I have some negative values in the data) and performing the sqrt (didn’t work), the log10 (didn’t work) and using the boxcox function in R (didn’t work). For all cases data still wasn’t normally distributed. I read there’s no non-parametric equivalent of 2-way anova as the non-parametric approach takes the values as “ranks” so it is impossible to test for interaction. My approach was then to take the data and define one factor that combines group\\_of\\_flies+treatment (for instance “flies1\\_SD”, SD stands for Sleep Deprived) and then compare all the groups by running a kruskal.wallis test with post hoc contrasts. I am pretty sure this is not the correct way to analyze this data, but I’ve been researching and I still can’t figure out another way. I know there’s the possibility of using some kind of mixed model but it falls a little out of the scope of my knowledge. Can someone help me with this please? I appreciate it very much.",
        "created_utc": 1676460733,
        "upvote_ratio": 1.0
    },
    {
        "title": "G*Power for repeated measures ANCOVA",
        "author": "VRCatPsych",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112vwn0/gpower_for_repeated_measures_ancova/",
        "text": "Hi everyone,\n\n&amp;#x200B;\n\nI need to calculate the sample size and the power of a Repeated Mesure ANCOVA. Do you know how? G\\*Power does not have an option for this.  Do you know of an alternative method? Do you know of another application/software that can help with such statistical analysis at priori?",
        "created_utc": 1676460585,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I go about selecting a representative sample from a subset of a population?",
        "author": "Plane-Tourist-6889",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112rszv/how_can_i_go_about_selecting_a_representative/",
        "text": " Hi all, I am trying to wrap my head around the statistics to see if this is even possible. But let's say we have a population of 100,000 people that drive to work everyday and we would like to get a representative average commute time. If we were to offer a subsidized device for $10 that measures commute times and gives us that data, and 2,000 folks buy it, is it possible to draw any conclusions from this or would the dataset be too skewed? Would I need to pick a random sample from the 100,000 population?\n\nAny help would be greatly appreciated. :)",
        "created_utc": 1676444074,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which Linear Regression to use?",
        "author": "jbr2811",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112mutc/which_linear_regression_to_use/",
        "text": "I’ve built 3 linear regression models in excel (due to variable size, hoping to move to python soon- ignore that part). So now I have 3 models, I thought I should combine them all since that will be the most comprehensive prediction, but the combined MSE is worse than the best model’s MSE. Should I just go with the model with the best MSE? Is MSE the end all be all answer of “which has the most predictive power?”",
        "created_utc": 1676427056,
        "upvote_ratio": 1.0
    },
    {
        "title": "Correct population and correct test question",
        "author": "frenchforkate",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112liu2/correct_population_and_correct_test_question/",
        "text": "I am trying to determine if Econ majors who participate in a professional development are more likely to do an internship in their field. I have two datasets from Career Services and am unsure which to use. \n\n1) I have a list of Econ majors who interned AND participated in the program. \n\n2) I also have a list of all Econ majors that includes those who both internship participants and non participants and both program participants and non participants. \n\nI’m wondering if I can use the list of interns only to show that participation in the career development program increases internship likelihood (using a t test) \n\nOR \n\nIf I need to use the population that includes all Econ majors and if a Chi Square test would be the correct method since the data has categorical variables and unequal sample sizes. Program participation would be IV and Internship would be DV.",
        "created_utc": 1676423001,
        "upvote_ratio": 1.0
    },
    {
        "title": "How should I go about analyzing my Likert data?",
        "author": "Valuable_Pass8843",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112hj9d/how_should_i_go_about_analyzing_my_likert_data/",
        "text": " Hello, I'm a student research assistant with a minimal stats background in a project involving new learning technology, so I'm having reaching deadlines because I do not know how to proceed with my data analysis.\n\nI have 70 student responses to 3 Surveys in which the S1 involved demographical questions (such as previous experience with the technology, perceptions on online/in-person/mixed learning, etc.), and then S2/S3 are identical 20 Likert-type questions (so two timepoints) categorized in Domains as well.\n\nI understand that I can analyze the Domains compiled (so 4 domains, sum of the questions for each participant would be averaged) as an Interval (average, stdev., ANOVA), or I can also analyze individual questions as Ordinal (median/mode, non-parametric – wouldn't this introduce multiple tests theorem? I have so many questions). What I do not know is simply which tests I should do, or what is commonly done for my type of data.\n\nAny help is welcome, ideally if I can be straight-up told the best path of analysis. So far I do have median, mode, histograms, domain averages, slopegraphs. But I have not done any statistical tests.",
        "created_utc": 1676412313,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I compare two percent change values?",
        "author": "MrPhoenix316",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112grlr/how_can_i_compare_two_percent_change_values/",
        "text": "If number X has an average annual percent increase of 1% and number Y has an average annual increase of 5% over 10 years. Is it fair to calculate the difference between them just as you would calculate the percent increase in the first place?\n\nIs it accurate to say number Y is increasing at 400% or 5 times faster than number X? If not, what's the best way to compare the two values?",
        "created_utc": 1676410341,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you help me find the name of an innovative randomized trial method I heard about a long time ago ?",
        "author": "throwawengineer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112fzsv/can_you_help_me_find_the_name_of_an_innovative/",
        "text": " I heard about it in a youtube video some years ago and I can't find it anything about it now that I would like to learn more and maybe apply it. The fact that I don't remember how it's called or how it worked exactly might have something to do with it.\n\nAll I know is that it was a new method that, instead of directly comparing an experimental group to a control group, randomly paired a person from the control group to a person from the experimental group. I think you repeated this random pairing several times and then combined the data. Or maybe was it that you compared each member from the experimental group to each member from the control group individually this way ? Either way, the gist of it was that instead of making one big trial, you used your populations to simulate a bunch of mini trials that brought together gave a more siginificant result. The video concluded that this method was prone to become the new gold standard once enought researchers learned about it. But I haven't heard about it since and can't find info on it so maybe it was flawed ?\n\nI'm sorry if this isn't enough info but this is all I can remember. Does this ring a bell to anyone ?",
        "created_utc": 1676408367,
        "upvote_ratio": 1.0
    },
    {
        "title": "ELI5: Why do we not always use exact tests?",
        "author": "Accomplished_Pain572",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112fhte/eli5_why_do_we_not_always_use_exact_tests/",
        "text": " This is probably a silly question, but I have worked myself into a bit of a rabbit hole and am wondering why we would ever not use an exact test. Is it because an exact test could take a very long time if the sample is rather large? If the sample is small, would the reasoning to not use an exact test be because using an exact test would not translate to other datasets as well? For example, using an exact test on a subset of a population would not necessarily translate to the rest of the population because it would be overfitted to the sample?",
        "created_utc": 1676407095,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to estimate population parameters for serial mediation without correlations from prior research?",
        "author": "PsychologyStudent31",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112fhbf/how_to_estimate_population_parameters_for_serial/",
        "text": "Hello,\n\nI want to calculate the sample size needed for .8 power in a serial mediation analysis (using the Schoemann Monte Carlo analysis for indirect effects app) and have correlations for all of the population parameters from prior research, except the b1 path, for which there is no existing prior research.\n\nI have heard that in this case I should input the smallest meaningful effect size according to my field (psychology) for the b1 path correlation. I am wondering please would the smallest meaningful effect size be in psychology, and would it be r2 for mediation analysis? And is there a reference available for this? I am also looking for a reference that says that this is what should be done in the absence of prior research. \n\nThank you!",
        "created_utc": 1676407062,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] When running a multiple regression with an interaction using a categorical variable (4 factors), do you run it multiple times with different reference variables?",
        "author": "bennettsaucyman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112ffp6/q_when_running_a_multiple_regression_with_an/",
        "text": "So I am running a multiple regression with an interaction, and I don't have any a-priori idea of what the reference variable should be. \n\nSo I have been running the regression multiple times with a different reference variable each time, and I'm not sure:\n\n1) How to report it\n\n2) Whether changing the reference variable multiple times increases the chance of a Type 1 error.",
        "created_utc": 1676406949,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Statistical Approaches For Animally Detection Over Time Across Many Related Measurements?",
        "author": "Aggressive3nthusiasm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112dsrg/q_statistical_approaches_for_animally_detection/",
        "text": " \n\nHello!\n\nI've been scratching my head about what way to approach a problem and am hoping for some direction. Like if someone could point me toward a paper or type of model I'd be appreciative.\n\nMy data comes from a machine that heats up and cools down as part of its operation. I'm trying to find and mark parts of a machine are getting hotter and or not getting as hot as its neighboring parts. So I'm basically looking for temperature anomalies over time. It would also be great if I could detect if the temperature anomalies start to grow, like they start to effect multiple neighboring chunks of the machine. Forgive my vagueness trying to be careful to not say too much, I'm dealing with NDA type things.\n\nFor every chunk of distance across this machine I have a Max, Min and Average temperature readings for every 10ish minutes across 6 months.\n\nAt first I thought it felt like a time serries problem, but I don't know how to account for the changes in temp across the whole machine as well as comparing to nearby chunks. I did some tests so I know the data is not stationary, I have not found a seasonal trend yet.\n\nAny guidance would be appreciated! I'll try to answer questions giving more context if I can. If there is a more relevant community to ask please let me know!",
        "created_utc": 1676402761,
        "upvote_ratio": 1.0
    },
    {
        "title": "I split a random variable according to two categories: how should i establish that the differenze of the mean Is significative?",
        "author": "MysteriousWealth2423",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112ce4y/i_split_a_random_variable_according_to_two/",
        "text": "That's a very easy question, but i'm new in this field. I would like ti know also if there are some assumptions that data must respect. Sorry also for the non appropriate terminology.",
        "created_utc": 1676399258,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I know the benefit by following the results of a hypothesis test for the difference in proportions",
        "author": "kharnaval",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1127d00/how_can_i_know_the_benefit_by_following_the/",
        "text": "Hi, I applied a hypothesis test for the difference in proportions of type 1 defects in process A and process B (which is the same to A but with a small difference).\n\nAlternative hypothesis is: Proportion of process 1 &gt; Proportion of process 2.\n\nI reject the null hypothesis with a power level of 90.52%.\n\nType 1 defects in B Is 0.25%.\n\nType 1 defects in A is 0.44%.\n\nNow that I know type 1 defects behave different in process B and process A, I would like to make an analysis on the cost-benefit of changing the whole process to act like process B.\n\nShould I expect a benefit close to the difference in proportions of each process?",
        "created_utc": 1676386668,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS manipulation",
        "author": "Accomplished_Dish109",
        "url": "https://www.reddit.com/r/AskStatistics/comments/112267i/spss_manipulation/",
        "text": "",
        "created_utc": 1676370837,
        "upvote_ratio": 1.0
    },
    {
        "title": "[question] is this ordinal data?",
        "author": "No_Seaweed_9483",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111saua/question_is_this_ordinal_data/",
        "text": "I’m looking at wait times in a patient satisfaction survey, and there are four possible option: 0-10 min, 11-20 min, 21-30 min, and 30+ min. Are wait times considered ordinal data in this example?",
        "created_utc": 1676344383,
        "upvote_ratio": 1.0
    },
    {
        "title": "Weibull analysis on heavily right-censored data.",
        "author": "PEPPAPIG122013",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111q26b/weibull_analysis_on_heavily_rightcensored_data/",
        "text": "Hello,\n\nI am working on a cost analysis on warranty coverage for a product. To do this, I want to model the # of failures over time to compare whether or not the warranty coverage is saving money or if we're better off going without warranty and buying new.\n\nThe historical population of products that have failed is approx. 4% over a 3 year period, so fitting a Weibull distribution on the survival time of the product is likely to be very biased since the vast majority of data points are censored.\n\nHow should I approach analyzing survival data when the vast majority of the population is still \"alive\"?\n\nDoes it make sense to fit two Weibull distribution on the following:\n\n1. The time-to-failure of the 4% that failed (to figure out the proportion that fail over a certain time frame given that it has failed)\n2. The overall population (to figure out the proportion of the population that will fail over time)?\n\nAnd make the following conclusions:\n\n1. If the CDF of the first distribution reaches \\~50% in 1 year and \\~100% in 3 years, can we say that given a product will fail, approx. 50% will fail under 1 year and almost all will fail under 3 years?\n2. If we produce 100,000 products total and the CDF of the second Weibull distribution reaches 4% in 3 years, can we claim that out of the 100,000 produced, approx. 4,000 will fail in 3 years, and out of the 4,000, approx. 50% of that (2,000) will fail within 1 year?",
        "created_utc": 1676337869,
        "upvote_ratio": 1.0
    },
    {
        "title": "Test for difference between proportions",
        "author": "Yaaaaaas321",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111ppyc/test_for_difference_between_proportions/",
        "text": "I’m in the midst of analysing election data, and I’m wondering if there is a way to test whether the proportion of votes each party got is significantly different from the proportion of parliamentary seats won by each party. \n\nAfter a little bit of researching, the chi-square goodness-of-fit test seems like it could work. But I’m wondering if there are any issues with the required assumptions for this test, namely independence. I feel like they aren’t not independent because the seats won kinda depends on the votes. \n\nIs the Chi-square goodness-of-fit test suitable for this scenario or is there a better option out there?",
        "created_utc": 1676336886,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Struggling to understand multiple regression output (long question).",
        "author": "bennettsaucyman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111npjf/q_struggling_to_understand_multiple_regression/",
        "text": "I have 4 groups of people in different \"relationship types\":\n\nAS/AS, AS/NA, NA/AS, and NA/NA (different assortments of autistic and nonautistic couples). \n\nThey completed 2 different questionnaires to measure 1) relationship satisfaction (RS), and 2) affectionate communication (ACI). \n\nI want to see whether the relationship between ACI and RS differ between the groups. I decided to run a multiple regression with an interaction term, because I wanted to measure:\n\n1) whether the slopes between the groups differ (significance of the interaction term)\n\n2) whether the slopes are 'higher' or 'lower' (different beta coefficients)\n\n3) whether their r/ R2 values are significantly different (not sure how to do this other than by running 4 different correlations. \n\n    model = smf.ols('CSI~ACI + Relationship_Type + Relationship_Type*ACI', df).fit()\n\nBut I am really struggling to interpret the output, since I've never interpreted an interaction before, let alone one that is categorical with multiple levels. It is below:\n\nhttps://preview.redd.it/5f3kw1eck1ia1.png?width=807&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d32ae1eecd6059aa5109ae91917330949c88730e",
        "created_utc": 1676331489,
        "upvote_ratio": 1.0
    },
    {
        "title": "probability of failure",
        "author": "bramecamp19",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111nonv/probability_of_failure/",
        "text": "Say two machines are doing the same task for years, using the same material, operators, software etc.\n\nHowever only one of the machines is producing a failure at a rate of 5 per year.\n\nI already analyzed both machines and the machine that is producing the failure had a bit difference in one of the parameters but I am sure that is not causing the failure.\n\nI want to understand if that is statistically expected since it's a highly unbalance dataset or not.\n\nAny thoughts?",
        "created_utc": 1676331424,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probability of two random numbers being within 10 of each other",
        "author": "Gwala_BKK",
        "url": "https://www.reddit.com/r/askmath/comments/111j79h/comment/j8f82z5/?context=3",
        "text": "",
        "created_utc": 1676327529,
        "upvote_ratio": 1.0
    },
    {
        "title": "Y is a dependent random variable of a known distribution, from the line of best-fit analysis we found \"Y estimated = aX+b\", is the distribution of 'Y estimated' related to the distribution of Y in any way?",
        "author": "AHM8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111gfan/y_is_a_dependent_random_variable_of_a_known/",
        "text": "Assume the relationship between Y &amp; X is not linear, so Y estimated =/= Y.\n\nNote that the independent random variable X is of an unknown distribution, we only know the distribution of Y.\n\n&amp;#x200B;\n\nSay Y is normally distributed, would 'Y estimated' be normally distributed as well?",
        "created_utc": 1676313540,
        "upvote_ratio": 1.0
    },
    {
        "title": "How would I go about evaluating the difference between two paired t-tests?",
        "author": "zxcccxz13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111gdt9/how_would_i_go_about_evaluating_the_difference/",
        "text": "My study has N identical participants that were randomly divided into three groups (A, B, and C), each receiving a different intervention. The response variable is measured pre-intervention (i.e., group A1) and post-intervention (i.e., group A2).\n\nThe hypotheses of the study were:\n\n1. Use of intervention B results in a greater change ((B2-B1)/B1)) than intervention A\n2. Use of intervention C results in a lesser change than the use of intervention B\n3. There is no statistically significant difference in the change observed in groups A vs. C\n\nTo compare the % change pre- and post-intervention, I used paired t-tests for each of groups A, B, and C. Each of these are statistically significant. Additionally, the percent change pre- and post-intervention is greater for group B (25%) than for group A (3%) and group C (11%). \n\nI was wondering what test can be used to statistically prove the above hypotheses (i.e., compare the difference between differences)?\n\nThanks for your help!",
        "created_utc": 1676313436,
        "upvote_ratio": 1.0
    },
    {
        "title": "Thesis Veracity and Depth",
        "author": "thebdup",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111g72p/thesis_veracity_and_depth/",
        "text": " I have two questions that could use the advice of an experienced statistician.\n\n1. I'm  able to use the USDA site to pull 100 years worth of crop data and I'll  be doing a time series analysis on it. I have a capstone project that  I'm working on concerning field crops and they're typically measured in  Bushels Per Acre (BPA), so I can query from the USDA database the  average amount of BPA harvested in a year. I can also query the amount  of acres harvested in that same year. But there does not seem to be a  total in either BPA or tonnage that I can use. **My  question is this: is it statistically sound to multiply average BPA for  a year and the acres harvested in the same year to arrive at total BPA  harvested for the year?** At first thought it seems to be a fine idea, but I'm concerned that I could be misrepresenting the data.\n2. My  project is to do a time series analysis of 100 years worth of field  crop data. At first I was concerned that the project would be too  complex and I've pared it down considerably. This project is just to  prove that you can independently execute a project, after all. But now  that I'm looking at the data, I'm concerned the project is too simple.  I'm thinking of adding additional data to it such as rainfall data and  perhaps looking at how weather impacted yields. **Is the project too simple? If so, what is your advice for a project that could use a bit more depth?**\n\nThank you.",
        "created_utc": 1676312995,
        "upvote_ratio": 1.0
    },
    {
        "title": "Meta-analysis and risk of bias assessment for \"before-after\" studies",
        "author": "DrPotash",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111g6l4/metaanalysis_and_risk_of_bias_assessment_for/",
        "text": "Hello,\n\nI am working on a meta-analysis. The intervention is a dietary plan, and the outcome of interest is a continuous variable. There are both randomized controlled trials and non-controlled, \"repeated-measures\" or \"before-after\" trials. In these \"before-after\" trials, a type of patients was studied before and after the intervention, but no control or placebo group was used. I have some questions concerning the meta-analysis and risk of bias assessment of this latter type of trials.\n\nIs it possible to perform a meta-analysis of before-after trials? Due to the scarcity of data, I would like to include these studies. In these studies, the variable of interest is measured before and after the intervention. I suppose it's not allowed to simply use the baseline data ('before data') as 'control' and the end of study data ('after data') as 'intervention' and insert them in the same analysis as the RCTs, is it? Is it somehow possible to perform a meta-analysis of this type of study and include them in the same meta-analysis as the RCTs?\n\nMy second question concerns the risk of bias assessment. I found the ROBINS-I tool, but I am not sure whether I can use this for this type of interventions. On [https://www.bmj.com/content/355/bmj.i4919](https://www.bmj.com/content/355/bmj.i4919) I read the following: \"Non-randomised studies of the effects of interventions (NRSI) are critical to many areas of healthcare evaluation. Designs of NRSI that can be used to evaluate the effects of interventions include observational studies such as cohort studies and case-control studies in which intervention groups are allocated during the course of usual treatment decisions, and quasi-randomised studies in which the method of allocation falls short of full randomisation.\"\n\nThus, this concerns observational studies and case-control studies, but the type of study I am talking about is neither. I couldn't find any other tool for the \"before-after\" studies. Is there a tool for risk of bias assessment for this type of studies?\n\nHope to hear from someone - thanks in advance!\n\nMartin",
        "created_utc": 1676312960,
        "upvote_ratio": 1.0
    },
    {
        "title": "[QUESTION] [HELP NEEDED] How to determine the number of stimuli for an experiment?",
        "author": "n2oc10h12c8h10n402",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111fdrb/question_help_needed_how_to_determine_the_number/",
        "text": "  Hello, everyone!\n\n&amp;#x200B;\n\nI recently joined this subreddit hoping someone could assist me. I have been assigned the responsibility of determining the required number of stimuli for an experiment I am conducting for my PhD study, as well as the number of participants necessary for a properly powered study. \n\nTo calculate the number of participants needed, I used the formula from Faul, Erdfelder, Lang, and Buchner's (2007) G\\*Power 3 software. Although I attempted to find an alternative method for determining the number of stimuli required, I was unsuccessful. Do you have any suggestions or recommendations?\n\nReference: Faul, F., Erdfelder, E., Lang, A.G., &amp; Buchner, A. (2007). G\\*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39(2),175-191.",
        "created_utc": 1676310963,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Which modelling technique would be most appropriate to examine trajectories of several measures and how they relate to a particular outcome?",
        "author": "HonestCap1924",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111dxn1/q_which_modelling_technique_would_be_most/",
        "text": "Hi there,\n\nI am going between several possible modelling techniques but I just don't think any of them are truly getting at the aim of what I am trying to achieve.\n\nEssentially, I have a series of continuous measures that represent some biological features of a clinical group, longitudinally. I am interested in understanding their trajectories and identifying a particular point at which these features result in the conversion of someone performing cognitively normally to what we would define as abnormal/impaired. I have continuous cognitive measures at the same intervals as the biological ones, although you could also convert this to a binary outcome measure of interest (unimpaired/impaired).\n\nThe two events are likely temporally incongruous, that is to say, changes in biology will occur well before changes in behaviour. What I would like to determine is whether there is a particular pattern in the biological features that determines if someone is going to become impaired in the near future and how far in advance that pattern emerges before we see the behavioural change.\n\nI've been considering time-to-event models and linear growth curve models but would appreciate a sanity check.\n\nThanks",
        "created_utc": 1676307306,
        "upvote_ratio": 1.0
    },
    {
        "title": "Farkle Statistics",
        "author": "Double_Grocery_9449",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111cycy/farkle_statistics/",
        "text": "I have been playing the game on my phone for a couple of weeks now and when I play the computer I notice something that I do not understand.\n\nA quick primer for scoring can be found quickly on google with some variants. What I want to focus on is rolling a 1 since it has the highest value.\n\n\nSuppose one rolls the following:\n1\n3\n1\n4\n5\n\nWhat the computer does take just one of the 1’s and then rolls again. \n\nWith the one locked in, let’s say the second roll is \n1\n1\n3\n4\n6\n\nThe computer will lock in one more 1 and then roll again\n\nOnly once it reaches four or more scoring dice in the turn, or Farkles, the computer it end its turn. \n\nIf in the same scenario on my turn. On the first roll I will take both of the 1’s, which in turn changes the second roll because there are now only four dice to roll in the second turn. \n\nSo my question is, does the computer have a better statistical chance than I do since there are also scoring options for 3 of a kind and other options for scoring as well? \n\nHere are my ataste a against the computer. Pretty close I think and only in the last games have I taken a lead, it has been close to 50/50. But, I am unclear if I have been lucky, or my method actually is working \n\nWins: 201\nLosses: 179",
        "created_utc": 1676304881,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to measure \"surprise\" in a population count?",
        "author": "TaedW",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111covc/how_to_measure_surprise_in_a_population_count/",
        "text": "I have the USA census data on surnames that tell me how common a surname is, ranging from a common SMITH (\\~850/100K) to a rare ALBINOWSKI (0.03/100K).\n\nI have a survey of an entire town (say 50,000 people) and I want to know how \"surprising\" each of the surnames is in that town.  Namely, if I get 10 SMITH and 1 ALBINOWSKI, which is more surprising?\n\nI currently calculate the rate compared to the census, which will tell me that my population has, for example, twice the rate of a given surname compared to the census.\n\nSurnameBoost\\[surname\\] = (SurnameCount\\[surname\\] \\* 100000) / (SurnameCensusRate\\[surname\\] \\* NumSurnameReferences);\n\nAnd then I take the results and calculate the mean and standard deviation, and then use the number of standard deviations from the mean as my \"surprise\".\n\nHowever, that really fails when there's only 1 or 2 samples of a rare surname in my town population.\n\nI'm sure there's a standard way of doing this due to population sampling, but I cannot figure it out.  It's been 35 years since my statistics course, so I'm guessing that I knew how to do this at one time!\n\nThank you for any advice!",
        "created_utc": 1676304233,
        "upvote_ratio": 1.0
    },
    {
        "title": "Am I eligible for apply in statistics phd degree?",
        "author": "Stat_with_BI",
        "url": "https://www.reddit.com/r/AskStatistics/comments/111a8nx/am_i_eligible_for_apply_in_statistics_phd_degree/",
        "text": "Hello everyone, I have a question about my eligibility.\n\nOne of my dream school have below requirements.\n\nMinimum Math Requirements\n\nAll applicants to the MS and PhD program should have the equivalent of approximately 30 or more quarter credits in mathematics and statistics, which must include:\n\napproximately three semesters or four quarters of calculus, which must include multivariate calculus\n\none course in linear algebra\n\none course in probability theory (calculus based)\n\n&amp;#x200B;\n\n**Unfortunately, I didn't take a probability theory. Should I at least send an email to admission community or should I just give up for applying this program?**\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nBelow are the subjects on my transcript.\n\nINTRODUCTION TO STATISTICS\n\nMATRIX THEORY FOR STATISTICS\n\nINTRODUCTION TO REGRESSION ANALYSIS\n\nBASIC STATISTICS I\n\nCALCULUS FOR STATISTICSⅠ\n\nINTRODUCTION TO MEDICAL AND PHARMACEUTIC\n\nBASIC STATISTICS II\n\nANALYSIS OF VARIANCE\n\nREGRESSION ANALYSIS\n\nINTRODUCTION TO PROGRAMMING\n\nMATHEMATICS FOR COMPUTER SCIENCE\n\nCALCULUS FOR STATISTICSⅡ\n\nMATHEMATICAL STATISTICS I\n\nSAMPLING THEORY\n\nEXPERIMENTAL DESIGN\n\nCOMPUTER USE FOR STATISTICS\n\nINTRODUCTION TO SOFTWARE\n\nSTATISTICAL MACHINE LEARNING\n\nMATHEMATICAL STATISTICS II\n\nSTATISTICAL SURVEY\n\nMULTIVARIATE STATISTICAL ANALYSIS\n\nCATEGORICAL DATA ANALYSIS\n\nDATA MINING\n\nSTOCHASTIC PROCESSES\n\nNUMERICAL ANALYSIS AND COMPUTER EXPERIME\n\nSTATISTICAL DATA SCIENCE I(English)\n\nTOPICS IN GEOMETRY\n\nPROBABILITY AND STATISTICS WITH LAB\n\nSTATISTICAL METHODOLOGY FOR DATA ANALYSIS\n\nSTATISTICAL METHODS FOR LINEAR MODEL\n\nINFERENTIAL STATISTICS(ENGLISH)\n\nThanks for reading my post.",
        "created_utc": 1676299079,
        "upvote_ratio": 1.0
    },
    {
        "title": "I have a data with two forms of intervention, 1 ,and 0. Each intervention has its own data/results. what type of descriptive statistics to do to, and how do I do it tell the effect of the intervention?",
        "author": "Eans_Riuk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1113hhr/i_have_a_data_with_two_forms_of_intervention_1/",
        "text": "",
        "created_utc": 1676277532,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dos &amp; don’ts of applying external information to estimating probability of an individual’s action?",
        "author": "cosifantuttelebelle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11100ku/dos_donts_of_applying_external_information_to/",
        "text": "Don’t know if there are any other Doughboys fans here — it’s a podcast that reviews chain restaurants. In a recent episode, they review Jeni’s ice cream, and the two hosts realize they got the same exact order independently. They jokingly ask, “hey one of our nerd listeners, figure out the odds of this happening!”\n\nFor fun, I calculated the odds for a simplified toy example: what are the odds that two people independently chose the same three different ice creams (for simplicity, assuming they won’t repeat any flavors in any given bowl) out of the 18 options? I get odds of 1 in 665,856 — (18! / 3!(18 - 3)!) * (18! / 3!(18 - 3)!)\n\nFirst question: that’s correct, right? And second question: could you calculate odds for this happening using something like [this survey on ice cream preferences in the US](https://today.yougov.com/topics/society/articles-reports/2022/07/19/favorite-ice-cream-flavors-yougov-poll-july-13-18-)? For example, both hosts are men and one is from the West Coast and one is from the East Coast, and they each ordered Butter Cake (let’s say Cake Batter to fit with the flavors of the survey), Chocolate and Chocolate Peanut Butter. Could you use proportions from the survey on favorite flavor + demographic combinations as probabilities of these flavors for each person (e.g. 4/112 or 0.0357 as the probability for a man from the West coast to pick Cake batter) to get more accurate odds of this event happening? How? What are the mechanics of how that works? What about if we had sales data from that Jeni’s location?\n\nThank you in advance for thoughts on this — I want to understand how to apply probability, but moving from toy examples and my intuition about the real world is a bit fuzzy (i.e. I don’t THINK each flavor has an equal probability of being chosen, so how do you incorporate that into calculating the overall probability of this event?)",
        "created_utc": 1676264639,
        "upvote_ratio": 1.0
    },
    {
        "title": "Flying blind. Did I go about this problem correctly?",
        "author": "lilgobblin",
        "url": "https://i.redd.it/boyfiafwewha1.jpg",
        "text": "Catching up on my notes from when I was unfortunately sick with Covid. Do I understand this correctly?",
        "created_utc": 1676251021,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trouble reading an equation",
        "author": "Lexicalyolk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110vcqc/trouble_reading_an_equation/",
        "text": "Is that a \"±\" symbol after the parenthesis on the right \n\n[Is that a \\\\\"±\\\\\" symbol after the parentheses on the right? or is it a subscript + raised to the -1\\/ξ ? I'm not sure in either case what the notation means exactly.](https://preview.redd.it/mkjwjw8ituha1.png?width=1564&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f48ab8927a11d933ab6e085afa6410ecb40614a7)",
        "created_utc": 1676249966,
        "upvote_ratio": 1.0
    },
    {
        "title": "Problems with standard deviation in Upper/Lower bounds",
        "author": "Important_Priority93",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110v1ak/problems_with_standard_deviation_in_upperlower/",
        "text": "Is the standard deviation statistically sound to find bounds or thresholds? I have modelled my issue of finding anomalies in HTTP responses with both of the following respectively: \n- Normal bounds: [mean -2stdev, mean +2stdev]\n- Poisson Distribution Bounds: [mean - sqrt(mean), mean + sqrt(mean)]\n\nIs this correct and how can I test whether a bound threshold is accurate? Are there any statistics I can use to measure the accuracy of such bounds perhaps?",
        "created_utc": 1676248962,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best books for learning statistics.",
        "author": "thereis_light",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110s40x/best_books_for_learning_statistics/",
        "text": "Hello everyone !\n\nI recently took an interest in statistics in clinical and psychological context.\n\nI would like to have a better understanding of the statistical tables as i may need to make statistics for a survey for my master's dissertation.\n\nAnyways, i would like to know what are the best books for learning statistics. BOnus points if it's about data from psychology or clinical studies (biostatistics basically).\n\nThanks in advance !",
        "created_utc": 1676240729,
        "upvote_ratio": 1.0
    },
    {
        "title": "Scale, location and shape of a log-normal distribution",
        "author": "muskagap2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110rprt/scale_location_and_shape_of_a_lognormal/",
        "text": "I'm a little bit confused about those parameters which define log-normal distribution. Given that we have some log-normal then μ is the location parameter and σ the scale parameter of the distribution. These two parameters should not be mistaken for mean or standard deviation from a normal distribution. When our log-normal data is transformed using logarithms our μ can then be viewed as the mean *(of the transformed data)* and σ as the standard deviation *(of the transformed data)*. \n\nSo a few questions regarding the above:\n\n1. location (of lognormal) = mean (of transformed). So those parameters are the same. Can we describe lognormal distribution using mean of transformed data? For example: transform lognormal data (to normal distr.), take a mean (e.g.mean=5) and say that lognormal mean=5?\n2. sometimes I read that lognormal is defined by 2 params: loc and scale, sometimes that by 3: loc, scale, shape. Is shape a parameter describing this distribution or not?\n3. what is exactly  a shape parameter of lognormal? Is it standard deviation of transfomed data? I'm a little confused with the relation between shape and scale - how do they correspond with each other?",
        "created_utc": 1676239691,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dice Stat Question",
        "author": "Double_Grocery_9449",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110ljt8/dice_stat_question/",
        "text": "I am terrible at math so please feel free to add roasting material \n\nIf I have a die, and I would like a 2 or a 4\n\nDo I say that I have\nA 1 :6 chance of a 2 and a 1:6 of a 4\nOr\nDo I say I have a 1:3 chance of a 2 or 4\n\nAre they they same mathematically? \n\nThanks in advance",
        "created_utc": 1676223964,
        "upvote_ratio": 1.0
    },
    {
        "title": "Difference-In-Difference when treatment (pre/post marker) is arbitrary?",
        "author": "Illustrious-Mind9435",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110kkqy/differenceindifference_when_treatment_prepost/",
        "text": "I have several years worth of individual level data on several thousand individuals and a corresponding measure. During those years several hundred individuals underwent an intervention with the hope of changing the result of this measure in aggregate- this intervention could occur on any day in that timespan.\n\nMy first approaching to analyzing this dataset was to look at the change in this measure before and after the intervention for those individuals; however, I wanted to incorporate those who did not receive an intervention into an analysis. My first thought was to conduct a Difference-In-Difference analysis (or the generalized form of it due to the intervention occurring on any one of several hundred dates); however, I am struggling with how to properly set up the control group since there is no single period in which to demarcate Pre/Post. Individuals can show up in the dataset at any point in that timespan and also leave at any point. Also, the intervention occurs somewhat randomly and is not necessarily related to when the individual enters the dataset - so some of the individuals who joined at the onset received an intervention many months later even after newer individuals received their intervention.\n\nDoes the nature of this treatment set-up preclude difference-in-difference since there are no specified treatment periods? Can I assign an distribution of \"intervention dates\" to the control group to replicate the distribution in those actually treated? Should I bucket individuals into treatment periods? What other options are available to set up the \"control\" group to compare my intervened individuals against?",
        "created_utc": 1676221547,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Dissertation Advice] What to do with ‘useless’ variable",
        "author": "paytontayylor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110i7zi/dissertation_advice_what_to_do_with_useless/",
        "text": "I’m doing my dissertation, but my supervisor is unavailable and will be until after it is due, so I am desperate for help!\n\nI did an experimental study (N= 24) and added two questions at the end that I added to provide a bit of clarity to my findings of the main questions being analysed. However, they haven’t told me much. \n\n&amp;#x200B;\n\nSo the first question asked the participant to indicate their **awareness** of the concept (5-point Likert scale), and then the second asked if they had **received any training or teaching** to deal with it (1 = Yes; 2 = No; 3 = Maybe; 4 = Don’t know/Don’t remember).\n\n**Of the 24 participants,** the responses I got for the awareness question were **N=18 saying “Definitely yes” and N=6 saying “probably yes”**. My initial thoughts were that it wouldn’t be worth analysing the impact of this on the dependent variables due to the response range being so small, so it won’t really tell us much. I tried analysing it using logistical regression, and the p-values were all pretty high, so I also tried changing it into a binary variable and got similar p-values. \n\nSimilarly, for the second question, N=1 said “yes”, N=3 said “maybe”, N=1 said “no” and **N=19 said “don’t know/remember”**. So again, I don’t think analysis would show much. \n\nI think I perhaps mention the frequency of responses to the awareness question by occupation (grouping variable) and use it anecdotally, but altogether scrap the training question. Or I’m considering scraping that section entirely as it wasn’t a key part of the study, but instead, I added it because I thought it might be helpful, which turned out not to be…\n\n**What approach should I take with this?**",
        "created_utc": 1676215532,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistic Index to normalize comparisons between different locations with different rates of business.",
        "author": "Worldly_Priority_444",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110ha09/statistic_index_to_normalize_comparisons_between/",
        "text": "I'm working on an index to level sale success rates for different locations with different rates of traffic. I've been adjusting an individual's number of sales based on the location's average vs the average for all locations. This works great except when someone has a bad day during the 1 day of the month they are in location G, this skews their results negatively since they can never reach the traffic to make up for it.\n\nBut when I adjust an individual's Daily Traffic to the average for all locations, a 1 day success of 3/3 for Location A results in results in unrealistic positive results since the average is adjusted to a much higher number giving them \"credit\" for a much higher number of sales than actual that skews the results too far in the positive direction.\n\nLocation examples:\n\n|Location|Daily Traffic|\n|:-|:-|\n|A|3|\n|B|80|\n|C|95|\n|D|100|\n|E|103|\n|F|107|\n|G|1000|\n\nWould the only way be not include the locations with small traffic sizes? If so, how would I be able to calculate what the minimum number of traffic would be?",
        "created_utc": 1676212998,
        "upvote_ratio": 1.0
    },
    {
        "title": "2^2 full factorial design with three center points",
        "author": "Smart-Firefighter509",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110h3rh/22_full_factorial_design_with_three_center_points/",
        "text": "Hi,\n\nI am working with MODDE in the pharmaceutical industry and am wondering whether it is valid to incorporate quadratic terms in a full factorial 2\\^2 model with three central points. Most of the sources I have found state that two level designs can only estimate quadratic effects and that there is a certain level of ambiguity when attempting to incorporate them in the model.  \nCould you please attempt to clear this up for me.\n\nThanks in advance,  \nI can of course provide additional info if necessary.",
        "created_utc": 1676212506,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing different starting hands in poker",
        "author": "raffyffyf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110g1kb/comparing_different_starting_hands_in_poker/",
        "text": "Hi everyone. \n\nI have been starting to work a little bit with pre-flop ranges (meaning which cards to enter in a hand). I have runned some equity calculations with these hands with 2 to 6 players on the table. I have kept hands which have &gt;50% equity with 2 players, &gt;33% with 3, and so on. Now that I have these hands, I would like to compare them in order to start to understand a little bit how the number of players in a hand affects how I should enter in it or not. For example, some hands like suited connectors (two cards like 8-9 of the same color, let say diamond) are overall stronger with more players because of their greater probability of making a strong combination (a straight or a flush). So now, I would like to draw some graphs with these percentages. The thing is that I feel some kind of transformation is needed in order to properly compare those, right? Imagine I would like to superimpose these graphs, it won't work because the percentages are not on the same \"scale\". Any thoughts around this? Thank you!",
        "created_utc": 1676209454,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing groups of repeated measurements",
        "author": "CaptainFoyle",
        "url": "https://i.redd.it/z5kxglo5yqha1.png",
        "text": "",
        "created_utc": 1676202863,
        "upvote_ratio": 1.0
    },
    {
        "title": "Simple meta-analysis conceptual question",
        "author": "AstralWolfer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110a5gj/simple_metaanalysis_conceptual_question/",
        "text": "Implying that the same test was repeated 100 times drawn from the same population, and an effect derived from each test… If we were to plot all our results on a forest plot,\n\n1) If I only include the significant results, this will give me a biased estimator of the mean, because the non significant results were not included\n\n2) Yet, if I include the non significant results in my forest plot, doesn’t that mean the long-run Type 1 error rate exceeds 5%? Doesnt the 5% number only hold for all the significant results?\n\nIs my line of thinking correct ? Will there always be a trade off between Type 1 error control and biased estimators? Is there an optimal method?",
        "created_utc": 1676187639,
        "upvote_ratio": 1.0
    },
    {
        "title": "I want to conduct a difference-difference analysis but I have 40+ dependent variables.",
        "author": "Sparklylattes3000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1107yy8/i_want_to_conduct_a_differencedifference_analysis/",
        "text": "Is there a more efficient or alternative way to go about this?",
        "created_utc": 1676179465,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Chi square test for association in Jamovi in inferential statistics",
        "author": "FlightFun9582",
        "url": "https://matchmaticians.com/questions/ebnw4x",
        "text": "",
        "created_utc": 1676173901,
        "upvote_ratio": 1.0
    },
    {
        "title": "i need help with jamovi and how to do a chi square test",
        "author": "FlightFun9582",
        "url": "https://www.reddit.com/r/AskStatistics/comments/110523j/i_need_help_with_jamovi_and_how_to_do_a_chi/",
        "text": "i really hope i dont sound too dumb lmao but i am writing my paper and i basically had to read a book and count the how often each word formation process was used (idk whether thats important) and now i have to do a chi square test of association with jamovi and my data set is the one in the photo. one column shows the word formation processes and how they are called and in the second one it says how often each process appeared in the book. and i have to find out the p-value i think lmao i have no idea how to put it into jamovi so it will give me a acceptable value (i tried but it always said &gt;0.05 so i dont think it makes any sense) maybe i have to change the layout in excel idk :/ help would be greatly appreciated &lt;3\n\n&amp;#x200B;\n\nhttps://preview.redd.it/uqfzx95a6oha1.png?width=626&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=440242c9444c178de4f2f160dc6dfcc625cfce0b",
        "created_utc": 1676169662,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are the essential definitions to statistics?",
        "author": "CoastieKid",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zzy8y/what_are_the_essential_definitions_to_statistics/",
        "text": "By essential, I mean descriptive.  Let's say terms like the following: \"normal\", \"deterministic\", \"stochastic\".\n\nIMO, the biggest failure of mathematics in a public education space is to define what these terms mean.  Although math is rooted in numbers, our language provides a description around mathematical concepts that are essential to both theoretical and applied fields.",
        "created_utc": 1676154859,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is a bachelors in statistics as useless as people make it sound?",
        "author": "Voldemort57",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zyk59/is_a_bachelors_in_statistics_as_useless_as_people/",
        "text": "I’m reading posts from this sub from a few years ago and most of the comments tell people to avoid majoring in statistics.\n\nI’m a UCLA third year with a major in stats and minor in data engineering, and kind of worried now. The stats major here includes a combination of statistical theory and programming (R, python, SAS, SQL primarily). I’ve also taken 2 quarters of linear algebra (one proof based), plus machine learning, algorithms, and a handful of other cs classes. \n\nI do intend to go to grad school for some form of stats/ds/analytics/biostats or something, and I’m feeling underprepared only because a bunch of responses from this sub and elsewhere say pure math is a better way to do this or that a bachelors in stats is a dead end…",
        "created_utc": 1676151193,
        "upvote_ratio": 1.0
    },
    {
        "title": "Non significant correlation level",
        "author": "Global_Pomelo2911",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zudsw/non_significant_correlation_level/",
        "text": " I am doing a mixed method case study with a sample of 19 participants. I want to do a correlation of the children's variable level against the variable level of the adults in that study. However, on SPSS, the correlation significance level (p value ) is on 0.2 which means that it is statistically insignificant? I've tried some novice bootstrapping with no significant results. Is there anything i can do to improve my correlation significance level to 0.05 or less and make my correlation level significant? Or should i just accept that my hypothesis has been rejected? I apologise if i messed up some statistics. Im figuring my way around spss.",
        "created_utc": 1676140308,
        "upvote_ratio": 1.0
    },
    {
        "title": "Differences-in-Differences Parallel Trends",
        "author": "Anonym1111273",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zswlj/differencesindifferences_parallel_trends/",
        "text": "I want to measure whether the impact of a company's headquarter country on my independent variable (goodwill paid) is stronger during recessions. After some researching, I found out that the differences-in-differences analysis could solve my problem. However, in the internet they always show a diagram (see example under: [https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.publichealth.columbia.edu%2Fresearch%2Fpopulation-health-methods%2Fdifference-difference-estimation&amp;psig=AOvVaw1yMN6knTtOEahZ9vstJpnV&amp;ust=1676208292554000&amp;source=images&amp;cd=vfe&amp;ved=0CAwQjRxqFwoTCLjbrNDIjf0CFQAAAAAdAAAAABAE](https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.publichealth.columbia.edu%2Fresearch%2Fpopulation-health-methods%2Fdifference-difference-estimation&amp;psig=AOvVaw1yMN6knTtOEahZ9vstJpnV&amp;ust=1676208292554000&amp;source=images&amp;cd=vfe&amp;ved=0CAwQjRxqFwoTCLjbrNDIjf0CFQAAAAAdAAAAABAE) ) with the \"treatment\" and \"parallel trends\". So two lines that increase or decrease in the same way until the treatment and then one line increase/decreases more than the other. My question now is what is my treatment and what is my control variable in my example? The treatment cannot be recessions because otherwise I just have the treatment group after the treatment and the control group before the recessions. If you think another statistical test may be better, I would be happy to consider that.\n\nFurthermore, I just want to make sure that I created my model correctly: Goodwil Paid=B0+B1*ressions+B2*Country+B3*ressions*Country Would that tell me whether the impact of the country is stronger during recessions?\n\nThanks a lot for your help.",
        "created_utc": 1676136526,
        "upvote_ratio": 1.0
    },
    {
        "title": "Shapley Value Formula",
        "author": "eternalmathstudent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zstib/shapley_value_formula/",
        "text": "In finding the Shapley value of a particular player. Why do we take the weighted mean of marginal contribution for coalitions of different size. Why not we take the plain mean? Also could you share any article or video that throws some light on different types of shap (for eg: kernel shap, tree shap etc)",
        "created_utc": 1676136301,
        "upvote_ratio": 1.0
    },
    {
        "title": "Stata Code?",
        "author": "Aggravating-Knee2512",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zrvae/stata_code/",
        "text": "Dear all,\n\nI am currently a student at a university in southern Italy and I am faced with the following Stata problem.\n\n There is a binary variable (which can take values of 0 and 1) and I want to look in the dataset to see if companies have changed their answer over the years, e.g. before they were no and now they are yes. Is there a way to do this in Stata?",
        "created_utc": 1676133857,
        "upvote_ratio": 1.0
    },
    {
        "title": "if X is normally distributed, and Y=ax+b, is Y necessarily normally distributed as well?",
        "author": "AHM8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zqbm7/if_x_is_normally_distributed_and_yaxb_is_y/",
        "text": "and what about other distributions?",
        "created_utc": 1676130203,
        "upvote_ratio": 1.0
    },
    {
        "title": "If you generate a number from 0-100 and reroll if you get 90-100 but using the range 0-10 which gets added back as 90 + the reroll give uniform distribution still?",
        "author": "Gutscazerk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zmvfh/if_you_generate_a_number_from_0100_and_reroll_if/",
        "text": "I hope the title makes sense, but I'll explain a bit more. I maintain a popular rng library in Rust and I've found a bug with generating very large numbers near float maxes. I think I've found a workaround but I would like to double check that I have not messed with the randomness.\n\n&amp;#x200B;\n\nLet's say my program works as follows:\n\n&amp;#x200B;\n\nRoll a number which can take any value between 0-100 inclusive. If my number is greater than or equal to 90, I will roll again in the range of 0-10. Now I will return the number 90 + the number you just rolled. \n\n&amp;#x200B;\n\n**Example 1:**\n\nroll1 = 67. Return 67.\n\n&amp;#x200B;\n\n**Example 2:**\n\nroll1 = 90. roll2 = 2. Return 92\n\n&amp;#x200B;\n\n**Example 3:**\n\nroll1 = 99. roll2 = 6. Return 96\n\n&amp;#x200B;\n\n**Example 4:**\n\nroll1 = 89. Return 89\n\n&amp;#x200B;\n\nIf this method will not produce uniform distribution I would be curious to hear of other ideas keeping in mind the issue is that as the numbers reach 64 bit float maxes, when scaling down by division it loses some of the high numbers (from my testing the top 127 numbers all are treated the same).",
        "created_utc": 1676119740,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with writing up/interpreting a LRT for random effects",
        "author": "niii27",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zlvlp/help_with_writing_upinterpreting_a_lrt_for_random/",
        "text": "Hi everyone! Really quick question - how do we report a loglikelihood ratio test for significance of variance in random effects?  What do we write up once we get the results?   \nI know how to interpret it  conceptually, but not how to write it up in APA, meaning - do i write \"the variance associated with the random effect was 3.23 and was statistically significant p = .005, or something else that includes the LRT, or?",
        "created_utc": 1676116285,
        "upvote_ratio": 1.0
    },
    {
        "title": "Understanding multiple comparisons - an example",
        "author": "throwaway8374939292",
        "url": "/r/biostatistics/comments/10zl8ut/understanding_multiple_comparisons_an_example/",
        "text": "",
        "created_utc": 1676114830,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statement of Interest",
        "author": "Plus_Broccoli_746",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10zfsut/statement_of_interest/",
        "text": "Hello!\n\nIm applying to a few biostatistics research programs. They require a statement of interest. I have the core of mine down at 300 words. I was wondering how long would be appropriate for something like this?",
        "created_utc": 1676102816,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting correlation coefficient for variable coded 1,2",
        "author": "sisofgenius",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10z9vvg/interpreting_correlation_coefficient_for_variable/",
        "text": "Hi, \n\nI'm in an intro psych stats class, and have to interpret the correlation coefficient, or r\\^2, for two variables and I'm struggling to understand if I am doing this correct because of the ways the variables are coded. \n\nThe two variables are extracted from the questions: \n\n1. \"What colour is this dress?\" (1 = white and gold, 2 = blue and black) \n2. \"What colour is a tennis ball?\" (1 = yellow; 2 = green) \n\nUsing the provided stats program, I got an r\\^2 of 0.08. I know this is a very weak/trivial positive correlation. I have to specifically interpret this with the variables, but I am confused about how to do this because it's not like a typically variable where it's ranging from least likely to most likely. Would I be correct to say: \"This correlation means that those who view the dress as white and gold are also more likely to view a tennis ball as being yellow.\"? \n\nAny clarification would be greatly appreciated!",
        "created_utc": 1676082478,
        "upvote_ratio": 1.0
    },
    {
        "title": "I have a question, how can i do this? If I use wincoxon test on every likert question, how will I generalize the change?",
        "author": "Dizzy_Care3972",
        "url": "https://i.redd.it/785a66jg9iha1.jpg",
        "text": "",
        "created_utc": 1676079694,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help on statistics problem, probably basic",
        "author": "maleviah1986",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10z7spf/help_on_statistics_problem_probably_basic/",
        "text": "I suck at statistics. Was wondering if someone could help me with statistics of a couple of games of MAgic the gathering I just played.  \n\nMy deck has 25 lands, out of 60 cards. Opening hand (7 cards all drawn at once) consisted of 1 land, 6 other cards.   \nWhat are the odds of that happening 4 times in a row?",
        "created_utc": 1676076161,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need help!",
        "author": "Busy-Plum4302",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10z5ywg/need_help/",
        "text": " \n\nHello guys, I am desperate for help!\n\nStatistics have never been an enjoyable part of my life (sadly) and now I have reencountered it. I am working on my thesis for my university and I am really struggling to decide/understand what kind of data analysis I need to use. I have tried reaching out to my professors but they take ages to reply and I am running out of time. I am doing a mixed method study, and my research is about the public's expectations when attending international mega-events. How important for them is that event has an opening ceremony, catering and so on. I did my descriptive analysis but it would only apply to my sample, while I would like for my results to be used in the future to manage such events better. I am not comparing anything. But I would like my finding to apply to the population. Should I do the inferential statistics? How? I am really lost and need some guidance. Thank you very much!",
        "created_utc": 1676071091,
        "upvote_ratio": 1.0
    },
    {
        "title": "Difficulty establishing Upper and Lower bounds where lower bound can never be negative",
        "author": "Important_Priority93",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10z5l6s/difficulty_establishing_upper_and_lower_bounds/",
        "text": " \n\nI am performing statistical analysis on HTTP request data and I need to establish upper and lower bounds for the number of HTTP requests.\n\nCurrently, I am using the following as my upper and lower bound formulae, respectively: Upper\\_Bound = Avg + (Std \\* Actual) Lower\\_Bound = Avg - (Std \\* Actual)\n\nHowever, in some instances, the current Lower\\_Bound formulae gives a negative number, which does not make sense as you cannot have a negative number of HTTP responses.\n\nWhat can I do to make sure the lower bound is never negative and are there better forumlae to compute the upper and lower bounds?",
        "created_utc": 1676070051,
        "upvote_ratio": 1.0
    },
    {
        "title": "Super Bowl Squares Winning Strategy problem",
        "author": "lcarter340",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10z2hrs/super_bowl_squares_winning_strategy_problem/",
        "text": "Hey all, I've got a debate at work over strategy on how to increase your chance of winning super bowl squares. We play it where everyone gets to choose 3 squares, and the numbers for the rows/columns are picked randomly AFTER everyone has chosen their squares. There are winnings for whoever gets the scores right at the end of each quarter, and all the other normal rules apply.  \n\n\nI argue that because the col/row numbers are decided after the squares are chosen, it doesn't matter where you place them because they all have an equal chance of happening. My coworkers are arguing that you should place your squares on the diagonals or place them in the corners, amongst other strategies. So in all, my question is this: Is there really a definitive strategy out there to increase your odds, or is it completely irrelevant where you place your squares?",
        "created_utc": 1676061784,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a massive multiple comparisons problem here?",
        "author": "throwaway8374939292",
        "url": "/r/biostatistics/comments/10yy6b6/is_there_a_massive_multiple_comparisons_problem/",
        "text": "",
        "created_utc": 1676051784,
        "upvote_ratio": 1.0
    },
    {
        "title": "Picking random card questions",
        "author": "Tricky-Volume1677",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10yxhn8/picking_random_card_questions/",
        "text": "My question is what are the odds of picking a random card if you try 52 times.\n\nSo the scenario is you pick a card put it back in the deck and i try and guess which one it is.\n\nI know if i do this once my odds are 1/52\n\nBut if I do it 52 times can I expect an average of being correct one time for each set of 52 attempts\n\nForgive me if this is ridiculously dense",
        "created_utc": 1676049684,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I know the sample size confidence level of my experiment with the following information?",
        "author": "kharnaval",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10yvxwi/how_do_i_know_the_sample_size_confidence_level_of/",
        "text": " I have process (A), from January 23 to February 2 it produced 249,597 items. But had some defects over those days;\n\n1130 of defect 1 = 0.45%\n\n1072 of defect 2 = 0.43%\n\n1893 of defect 3 = 0.76%\n\n  \n\nI ran an experiment with a small difference in the process, I produced 9202 items under that different condition and the defects behaved in this way;\n\n25 of defect 1 = 0.27%\n\n20 of defect 2 = 0.22%\n\n23 of defect 3 = 0.25%\n\n&amp;#x200B;\n\nHow can I know the confidence level of me saying that the whole process would improve in a set percentage with the information that I currently have?\n\nThank you for your time.",
        "created_utc": 1676045701,
        "upvote_ratio": 1.0
    }
]