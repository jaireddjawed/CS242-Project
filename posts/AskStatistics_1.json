[
    {
        "title": "The possibility of having both your children be gay?",
        "author": "MyLogDoesntJudge",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11o65im/the_possibility_of_having_both_your_children_be/",
        "text": "me and my brother are both gay and when i tell people that they either laugh or feel pity. i need to know if this is rare or common",
        "created_utc": 1678495518,
        "upvote_ratio": 1.0
    },
    {
        "title": "Willing to do stats homework for $15, no more than 25 questions. Lmk!",
        "author": "87James-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11o0ckb/willing_to_do_stats_homework_for_15_no_more_than/",
        "text": "",
        "created_utc": 1678481165,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using mean in regression?",
        "author": "Applesaucer18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11npv0t/using_mean_in_regression/",
        "text": "So I’m building a multiple regression model, and the dataset I’m using has different versions of the same variable.\n\nFor example, mean density, range of density, std.dev of density etc.\n\nMy first thought was to just use all the means, would this be suitable or is it breaking some assumption etc?",
        "created_utc": 1678456498,
        "upvote_ratio": 1.0
    },
    {
        "title": "Accounting for confounding variables with non-parametric data.",
        "author": "Ok_Journalist9924",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nnutc/accounting_for_confounding_variables_with/",
        "text": "I am a medical researcher relatively new to stats and I am currently working on a study in which I am looking for differences between 3 groups on &gt;300 variables. As my data is not normally distributed, I have conducted Kruskal Wallis tests (with FDR adjustment). The issue I am having is that my boss (who has limited statistics ability) seems to find the results strange and wants me to account for another variable that could be driving the differences in the groups rather than the current independent variable. After some reading, I was thinking about using an ANCOVA, with the confounding variable as an interaction term with my current IV.  \n\nMy question therefore is, does this sound like a reasonable approach or would there be better solutions? My other idea was to transform the data so that it fits a normal distribution and then I could perform parametric tests but I am not sure how logical this sounds.",
        "created_utc": 1678451326,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with Studydesign (multiple Regression)",
        "author": "Apfelstudel_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nldm6/help_with_studydesign_multiple_regression/",
        "text": "Hey everyone, I'm doing a study for my bachelor's thesis in psychology and wanted to get some feedback on my research design. I'm investigating the influence of various variables (gender, education, age, nightmare frequency, nightmare distress, and nightmare beliefs) on suicide risk among participants with depressive symptoms.\n\nTo do this, I plan to conduct a binary logistic regression analysis, with suicide risk (no = 0; yes =1) coded as the dependent variable, and the PHQ-9 (depressive symptoms) score included as a control variable to account for the influence of depressive symptoms.\n\nMy goal is to identify the factors that independently predict suicide risk, and I plan to use a backward stepwise approach to select significant predictors. I will use SPSS to analyze the data.\n\nDoes this study design make sense? Are there any additional tests I need to conduct to ensure the validity of my results? For example, do I need to conduct any post-hoc tests or checks for multicollinearity? Any feedback would be greatly appreciated as I'm not good with statistics.\n\nThanks in advance!",
        "created_utc": 1678443847,
        "upvote_ratio": 1.0
    },
    {
        "title": "Inter-rater reliability with Krippendorff's Alpha",
        "author": "ghosnius",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nkw4l/interrater_reliability_with_krippendorffs_alpha/",
        "text": "I'm conducting reliability analysis using a sample of 30 raters with a test survey (nominal data) whereby the inter rater reliability of the 30 raters was calculated using kalpha. I am wondering if it is viable to measure the reliability of the 30 raters against a \"perfect score\" of the survey (i.e., all 30 raters are simultaneously compared against a perfect score to produce a single kalpha value). I imagine this would require the use of averages across the 30 raters for each item rated however kalpha can accommodate ratio data. I am unsure however if this is a viable strategy. Alternatively each of the 30 raters could be individually compared against the perfect score however this would then produce 30 kalpha values - in this instance could an average kalpha value be used and reported, or is this unsuitable?",
        "created_utc": 1678442233,
        "upvote_ratio": 1.0
    },
    {
        "title": "H: Religious practice (spirituality) moderates the relation between relation between relative wealth and happiness.",
        "author": "enesra",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nhnzn/h_religious_practice_spirituality_moderates_the/",
        "text": "I'm using data from General Social Survey  \n\n\n\\- (« ATTEND ») Attend religious services (8 item ordinal)\u000b  \n\\- (« FINRELA ») Familial income (5 item ordinal)\u000b  \n\\- (« HAPPY ») General happiness   \n\n\nHow would you go about this in SPSS?",
        "created_utc": 1678431469,
        "upvote_ratio": 1.0
    },
    {
        "title": "Stat Sig Help",
        "author": "nickt2009",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nhej1/stat_sig_help/",
        "text": "If I have a TV viewer panel of 33M people that accounted for 10% of the population, and a purchase panel of [XX] people that accounted for .5% of purchasers.\n\nHow large does the purchase panel have to be in order to have 95% statistical significance to show correlation between the two datasets?\n\nThank you for your help 😄",
        "created_utc": 1678430645,
        "upvote_ratio": 1.0
    },
    {
        "title": "what happen to standard deviation and interquartile range when all the data values are increased by a certain percentage?",
        "author": "FaithlessnessOdd4390",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nbznd/what_happen_to_standard_deviation_and/",
        "text": "",
        "created_utc": 1678415381,
        "upvote_ratio": 1.0
    },
    {
        "title": "Having hard time calculating ARR/NNT",
        "author": "pinkpencilbox",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nan2k/having_hard_time_calculating_arrnnt/",
        "text": "If I have the RR, how do I calculate the NNT. I am having trouble trying to calculate the ARR. I know I need the ARR to calculate NNT (1/ARR). \n\nThis is the information I am given: \"The 2020 focused updates to the asthma management guidelines from the National Asthma Education and Prevention Program Coordinating Committee describes three large randomized controlled trials in which the efficacy for reducing asthma exacerbations of single maintenance and reliever therapy (SMART) was compared with inhaled corticosteroids (ICS) in children ages 12 years of age and older. A composite exacerbation score that included systemic corticosteroid use, hospitalizations, emergency department visits, increases in inhaled corticosteroid or other asthma medication dosages, and peak expiratory flow below 70% was used in two of the studies. A relative risk (RR) of 0.6 (range 0.53-0.68) in favor of SMART therapy was reported.\"",
        "created_utc": 1678411941,
        "upvote_ratio": 1.0
    },
    {
        "title": "PLEASE HELP thesis data",
        "author": "mccainallison",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11nabij/please_help_thesis_data/",
        "text": "Hey everyone! I am trying to interpret data for my thesis, and I'm struggling a bit with it. I'm using SPSS and I ran an ANOVA. I'm trying to understand effect size with eta squared, but I'm having a hard time understanding what is a significant vs insignificant effect size and can't find a straight answer anywhere. \n\nFor example, one of the values for eta-squared is .1, another is .272. Would those be considered large effect sizes?\n\nThank you so much in advance:)",
        "created_utc": 1678411119,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I interpret the slope of a random slope hierarchical model?",
        "author": "PsychRedditor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11n3llj/how_do_i_interpret_the_slope_of_a_random_slope/",
        "text": "I am trying to use a Bayesian random slope model to determine whether the hierarchical structure of the data is biasing my results. I am investigating the effect of IQ on test scores. I have three questions:  \n\n\n**I. Is the effect of IQ on test results substantially different for different schools?**   \n**II. If so, what is the corrected effect?**   \n**III. Can I interpret the overall slope as corrected for the hierarchy? If not, what's the correct interpretation?**\n\nThe regression model I used looks like this:  \nScore \\~ m.IQ + IQ + (IQ | School\\_Name)  \n\n\nScore = outcome variable, test score  \n[m.IQ](https://m.IQ)  = contextual/compositional effect - average IQ at school. To answer the question of how is the group-level construct associated with the outcome after controlling for the individual-level predictor.   \nIQ = intelligence quotient is a higher IQ associated with higher test scores.  \nSchool\\_Name = different schools (1-24)  \nI got the following results in R:  \n\n\n    Model Info:\n     function:     stan_glmer\n     family:       gaussian [identity]\n     formula:      Score ~ m.IQ + IQ + (IQ | School_Name)\n     algorithm:    sampling\n     sample:       4000 (posterior sample size)\n     priors:       see help('prior_summary')\n     observations: 359\n     groups:       School_Name (24)\n    \n    Estimates:\n                                                mean   sd   10%   50%   90%\n    (Intercept)                                0.0    0.0  0.0   0.0   0.0 \n    m.IQ                                       0.0    0.3 -0.4   0.0   0.4 \n    IQ                                         1.1    0.1  1.0   1.1   1.2 \n    b[(Intercept) School_Name:1]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:1]                       -0.2    0.1 -0.3  -0.2   0.0 \n    b[(Intercept) School_Name:2]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:2]                       -0.5    0.2 -0.7  -0.5  -0.3 \n    b[(Intercept) School_Name:3]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:3]                       -0.1    0.2 -0.4  -0.1   0.1 \n    b[(Intercept) School_Name:4]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:4]                       -0.2    0.2 -0.5  -0.2   0.2 \n    b[(Intercept) School_Name:5]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:5]                        0.3    0.1  0.1   0.3   0.4 \n    b[(Intercept) School_Name:6]               0.0    0.0 -0.1   0.0   0.0 \n    b[IQ School_Name:6]                        0.2    0.2 -0.1   0.2   0.5 \n    b[(Intercept) School_Name:7]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:7]                        1.0    0.2  0.7   1.0   1.2 \n    b[(Intercept) School_Name:8]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:8]                        0.1    0.2 -0.1   0.1   0.4 \n    b[(Intercept) School_Name:9]               0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:9]                        0.4    0.2  0.2   0.4   0.7 \n    b[(Intercept) School_Name:10]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:10]                       0.2    0.2 -0.2   0.2   0.5 \n    b[(Intercept) School_Name:11]              0.1    0.0  0.0   0.1   0.1 \n    b[IQ School_Name:11]                       0.1    0.2 -0.2   0.1   0.4 \n    b[(Intercept) School_Name:12]              0.1    0.0  0.0   0.1   0.1 \n    b[IQ School_Name:12]                       0.0    0.3 -0.3   0.0   0.4 \n    b[(Intercept) School_Name:13]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:13]                       0.1    0.2 -0.1   0.1   0.4 \n    b[(Intercept) School_Name:14]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:14]                       0.1    0.2 -0.3   0.1   0.4 \n    b[(Intercept) School_Name:15]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:15]                      -0.7    0.3 -1.1  -0.7  -0.4 \n    b[(Intercept) School_Name:16]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:16]                      -0.3    0.2 -0.6  -0.3   0.0 \n    b[(Intercept) School_Name:17]              0.1    0.0  0.0   0.1   0.1 \n    b[IQ School_Name:17]                       0.0    0.2 -0.3   0.0   0.2 \n    b[(Intercept) School_Name:18]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:18]                      -0.3    0.2 -0.5  -0.3   0.0 \n    b[(Intercept) School_Name:19]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:19]                       0.0    0.2 -0.3   0.0   0.2 \n    b[(Intercept) School_Name:20]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:20]                      -0.2    0.2 -0.4  -0.2   0.1 \n    b[(Intercept) School_Name:21]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:21]                      -0.2    0.2 -0.4  -0.2   0.0 \n    b[(Intercept) School_Name:22]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:22]                       0.1    0.2 -0.2   0.1   0.4 \n    b[(Intercept) School_Name:23]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:23]                       0.0    0.1 -0.2   0.0   0.2 \n    b[(Intercept) School_Name:24]              0.0    0.0  0.0   0.0   0.0 \n    b[IQ School_Name:24]                       0.1    0.2 -0.1   0.1   0.3 \n    sigma                                      0.0    0.0  0.0   0.0   0.0 \n    Sigma[School_Name:(Intercept),(Intercept)] 0.0    0.0  0.0   0.0   0.0 \n    Sigma[School_Name:IQ,(Intercept)]          0.0    0.0  0.0   0.0   0.0 \n    Sigma[School_Name:IQ,IQ]                   0.1    0.0  0.1   0.1   0.1 \n    \n    Fit Diagnostics:\n               mean   sd   10%   50%   90%\n    mean_PPD 0.0    0.0  0.0   0.0   0.0  \n    \n    The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg'))",
        "created_utc": 1678395551,
        "upvote_ratio": 1.0
    },
    {
        "title": "Forecast Accuracy",
        "author": "a_coupleofwavylines",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11n3esw/forecast_accuracy/",
        "text": "Question:\n\nMy company is looking at a different way to calculate forecast accuracy, which is great, because it will start incorporating WAPE instead of MAPE. This will solve for our low selling, high volatility items.\n\nAs one of the main users of this data, I can say that I HATE seeing accuracy that is outside the range of 0 - 100%. HATE IT. As a fix, I'm thinking about suggesting that we can make an adjustment to how the WAPE is calculated in the form of an IF statement.\n\nIF the SUM of all forecasts are less that the SUM of all the sales, then SUM(forecasts)/SUM(sales). This will ensure the accuracy is between 0 and 100. On the other hand, IF the SUM of all forecasts are grater than the SUM of all sales, then SUM(sales)/SUM(forecasts). This will also solve for weighted items and keep accuracy between 0 and 100%.\n\nCan anyone offer why this might be a bad idea?\n\nThank you kindly",
        "created_utc": 1678395105,
        "upvote_ratio": 1.0
    },
    {
        "title": "Measuring system performance based on irregular usage times.",
        "author": "rohtua3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11n33ho/measuring_system_performance_based_on_irregular/",
        "text": "\n\nHi, \n\nAt work we have a number of systems across several sites, each system will capture images. Each system is used for varying amounts of time each day.\n\nE.g. system 1 is used for 3 hours and records 200 events\n\nSystem 2 is used for 12 hours and records 2000 events\n\nI want to measure average performance on each system and create a baseline figure that I can compare against.\n\nAs the usage time is irregular I have worked out a pro rata figure for 24hrs use. \n\nTo do this I've taken the usage time of the system and worked out the ratio of usage time in 24hrs.\n\nE.g. 3hrs / 24hrs = 1/8\n\n\nI then multiply the number of events by the ratio to get a number for a full 24hrs use so for this example if it records 200 events in 3hrs I'll do 200×8 and say that for 24hrs the system would record 1600 events. \n\nI'm doing this for multiple days/systems and then taking the average of the results and saying that is the expected performance of the systems. \n\nI then want to use that average figure and compare the daily record rates against it and judge whether a system is underperforming (to identify faults) by again using ratios to account for the irregular usage times.\n\nDoes this make sense as a method? I ran it past some colleagues who've dismissed it as \"made up numbers\".",
        "created_utc": 1678394368,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help! What is the correct (regression) test to quantify changes of tumor volume of an inhomogeonus data set?",
        "author": "Curlous",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11n1k8t/help_what_is_the_correct_regression_test_to/",
        "text": "Hi,\n\nI    need some help. I am analysing a dataset of tumor volumes over time  of  a  tumor that ocurs in children. It is hypothesized that the tumors   shrink  over time and shrink faster the younger you are. The question  is  how  much the tumors decrease on average over time during different   ages. I  therefore thought I would need some regression and the slope   would be my  decrease of size over a certain time frame. However the   dataset is very  inhomogenous. Some tumours were measured several times   (e.g. 5 volume  measurements over time), other only twice, others 10   times. In addition  the measurements happened at different ages. This   means one tumor was  measured three times, e.g. at 4, 49 and 205 days of   age, another was  measured two times at 45 and 603 days of age and   another at 200 and 412,  415, 490 and 1700 days of age (just example   data). So the questions I  need to answer are:\n\n1. Do tumors shrink in volume over time, if yes, how much on average in my dataset over a certain timeframe.\n2. Do  tumors shrink faster when a patient is within e.g. his first year of  life compared to later years.I   first did a linear regression analysis  of different time frames and   used every measurement as individual  unrelated data point, but I think   this doesn´t make sense as  information of the measurements that are from   the same patient is lost  and patients and timeframes that have more   measurements are  overrepresented. But I am not sure if this is a   probleme. It probably  is...Other regression tests I looked into   required that every patient  has a value at the very same timepoint   (every patient needs a  measurment exactly at e.g. 0 days, 10 days, 20   days, 50 days, etc) and  if just one value was missing it gave an error.\n\nWhat is the correct test I need to do and can do with my type of data?\n\nThank you for your help and suggestions.",
        "created_utc": 1678390813,
        "upvote_ratio": 1.0
    },
    {
        "title": "Heteroskedasticity and Non-normality together; few solutions exist",
        "author": "Massive-Row8428",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11n0y7i/heteroskedasticity_and_nonnormality_together_few/",
        "text": "In linear regression, most of the methods that combat the two common problems of heteroskedasticity and non-normality are designed to counter the effect of one-problem only (either hetero or non-normality; not both in the same time). \n\nOne of the rare methods that combat both problems is one type of wild bootstrapping (it has an R-package for that). \n\nOn my searches for solution of the two problems, I found an R-package designed to do White-test (for heteroskedasticity) but conducted via bootstrapping (I guess package name is \"whitestrap\"). In many times when I try this package to obtain a bootstrap test of heteroskedasticity, I found good results (the bootstrap p-value of white test is generally larger than the conventional White test; this is under replications = 2000 or 1000). Once I increase the replications (B = 5000), the P-value goes towards non-significance, and usually once you try B = 10,000, non-signifiance is surely obtained (which is what we hope).\n\nAfter such experiments on bootstrap white test, can one say that doing a bootstrap regression (of a model contaminated with both non-normality and hetero) with high replications (say 10000) would solve both problems, non-normality and hetero?",
        "created_utc": 1678389430,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I get the value for an 85% C.I. of a given sample if the 95% C.I. is given?",
        "author": "SmashyInc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mzqee/how_do_i_get_the_value_for_an_85_ci_of_a_given/",
        "text": "Hey folks, I am a beginner and am studying for an upcoming exam. I stumbled upon a task which says:\n\ngiven is the 95% C.I. for a sample of monthly wage in country x \\[3122, 3856\\]\n\n**What is the upper threshhold for the 85% C.I?**\n\nThe correct answer is 3759. I have tried drawing it, cutting percentages by counting S.E. but it didnt amount to that answer. Help would be much appreciated!\n\n&amp;#x200B;\n\n\\[I also did post this on r/statistics and will delete either of the post if the other got answered, I am just stressed and want that answer very badly\\]",
        "created_utc": 1678386626,
        "upvote_ratio": 1.0
    },
    {
        "title": "What happens if both the t value (test statistic) and P value are higher than the critical (alpha) value for a Welch t-test?",
        "author": "Alive-Reaction-7266",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mzf7k/what_happens_if_both_the_t_value_test_statistic/",
        "text": "",
        "created_utc": 1678385916,
        "upvote_ratio": 1.0
    },
    {
        "title": "ROC analysis for binary outcome and binary predictor A and B. Can you use a P-value of difference between A's and B's AUC-value?",
        "author": "FlyLikeMcFly",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mxk0t/roc_analysis_for_binary_outcome_and_binary/",
        "text": "Using ROC analysis, I have calculated AUC values for Predictor A and Predictor B and both of these are binary predictors versus a binary outcome. The AUC value of Predictor A (0.64) is higher than B (0.59). \n\nBesides comparing the two AUC values (and confidence interval of these), how useful is it to calculate a P-value of the difference between the two AUC values? By just looking at AUC values I can say hey, A has a higher value which would indicate it having a stronger predictive value than B.",
        "created_utc": 1678381601,
        "upvote_ratio": 1.0
    },
    {
        "title": "Ordinal variables in linear regression: to dummy code or not to dummy code?",
        "author": "Rosie-daisy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mtba5/ordinal_variables_in_linear_regression_to_dummy/",
        "text": "Hi all,\n\nI am currently doing a study in which we try to predict psychological distress among individuals seeking fertility treatment, for which we decided to use multiple linear regression. One of our independent variables is “duration of fertility problems”, which is measured on an ordinal scale (0 – 6 months; 6 months – 1 year; 1 – 2 years; 2 - 3 years; 3 – 4 years; 4 – 5 years; &gt; 5 years).\n\nI've been taught that ordinal variables need to be dummy coded. However, I have read that sometimes, it is OK to treat ordinal variables as numerical variables in linear regression. I also notice that many researchers do so with, for example, level of education.\n\nDo you thnk it is acceptable to treat our ordinal variable as a continuous predictor? Our sample size is relatively small and we have other variables to include, so I thought it would increase power if we do, but I am not sure if this approach is acceptable / conventional.",
        "created_utc": 1678371370,
        "upvote_ratio": 1.0
    },
    {
        "title": "Error metric for non-negative prediction",
        "author": "autorayn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ms13f/error_metric_for_nonnegative_prediction/",
        "text": "I'm trying to predict precipitation (which is always non-negative), where I have ground truth labels and predicted values. For evaluation I want to compare them to understand how good the prediction works.\n\nThe most obvious metric is the absolute difference. However, it implies that predicting 2 mm/h of rain where it was actually zero is equally bad as predicting 12 instead of 10 mm/h, which is much less worse. The next obvious choice is relative error as percentage, however it's undefined at zero. Also, it assigns the same error to any prediction of zero, regardless the ground truth value. Additionally, predicting 1 instead of  2 mm/h is not as bad as 10 instead of 20 mm/h, so the absolute difference is still relevant.\n\nI though about using an angle difference as error, where an angle of 45° in a plot of ground truth/predicted precipitation yields an error of zero. However, it has the same issues except being defined for zeroes.  (side question: is there a well-known angle-based error metric?)\n\nI think I need to combine something like a relative/angle-based metric with a metric that adds a small punishment based on absolute values. Is there anything well-known for that?\n\nThanks in advance for your input.",
        "created_utc": 1678368066,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why use inverse (reciprocal) distribution?",
        "author": "JerryTheMaus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mkdzt/why_use_inverse_reciprocal_distribution/",
        "text": "Hi everyone. Sorry this is going to be a bit longwinded. Not a stats guru so I will try to provide all the info I think might be useful.\n\nI'm working with some eye tracker data and using the `saccades` R package to calculate fixations. Basically, when I feed it a data frame containing raw eye gaze coordinates and time data (so a row contains x coord, y coord, and time at which the recording occurred), it groups adjacent recordings among which movement was minimum and labels this group of rows as a fixation event. The output is a data frame that marks each event as a row and provides a start time, end time, and duration for each event. It looks something like this (the trial column denotes experimental trials, which has several in each participant):\n\n|participant|trial|event|start|end|dur|\n|:-|:-|:-|:-|:-|:-|\n|1|1|fixation|1.234|2.235|1.001|\n|1|1|fixation|3.567|4.567|1.000|\n|1|1|fixation|5.578|7.579|2.001|\n|1|1|fixation|8.123|9.234|1.111|\n|1|1|too short|9.567|9.678|0.111|\n|...|...|...|...|...|...|\n\nTheoretically, one would also want to exclude grouped time points that are of too short a time span because it is more likely a coincidence than an actual fixation event. This is where I had some problem comprehending. When I was looking at this part of the source code, this is what they did:\n\n    dur &lt;- 1/fixations$dur\n    median.dur &lt;- stats::median(dur, na.rm=TRUE)\n    mad.dur &lt;- stats::mad(dur, na.rm=TRUE)\n    \n    threshold.dur &lt;- median.dur + mad.dur * 5\n    event &lt;- ifelse(event!=\"blink\" &amp; dur &gt; threshold.dur, \"too short\", event)\n\nWhere `fixations` denotes the data frame. It seems like they took the inverse of each value in the column `dur` that contains the duration of each event, stored it in a vector, and calculated median &amp; median absolute deviation (MAD) of the vector. They then defined a threshold as 5 MAD away from the  median and labeled every event with the *inverse* of its duration larger than this threshold value as being `too short`. I understood the comparison to the extent where taking the inverse would make larger values smaller and vice versa. **My question is why they made it an inverse distribution in the first place, as opposed to defining a threshold using some combination of mean, median, SD, MAD, on the original data.** I tried to compare the histogram between the inverse vector and the original vector, and the former appeared to be more spread out numerically than the latter.\n\nFor those interested, the code is from [https://github.com/tmalsburg/saccades/blob/master/saccades/R/saccade\\_recognition.R](https://github.com/tmalsburg/saccades/blob/master/saccades/R/saccade_recognition.R) from line 194 - 201.\n\nThanks in advance!",
        "created_utc": 1678341542,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing Groups of Grade Distributions",
        "author": "YonderMTN",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mjeyu/comparing_groups_of_grade_distributions/",
        "text": "So here's what we have: Grade distributions counts collapsed to A-Range, B-Range, C-Range. We have these data by year, term, course, and instructor.\n\nSo here's the question: What can we use in SPSS to statistically determine if Professor A grades MATH100 much easier than Professor B grades MATH100? Other than just looking at the distributions side by side and visually looking at the comparisons.\n\nI'm thinking Z-Test....am I on the right track? What would you suggest. Thank you!",
        "created_utc": 1678338409,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statista: A little bit of help",
        "author": "Rjhell",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mfq9h/statista_a_little_bit_of_help/",
        "text": "It's a long shot but hopefully someone can help!\n\nDoes anyone have access to Business content on Statista?\n\nI could really do with some data from here but I 100% can't afford to pay for it:\nhttps://www.statista.com/outlook/amo/advertising/out-of-home-advertising/hong-kong#ad-spending\n\nIf anyone is able to download the data in xls and DM me I will be so appreciative\n\nThanks everyone!",
        "created_utc": 1678327912,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can a latent factor load onto different types of observed variables?",
        "author": "AnotherDayDream",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mcfl0/can_a_latent_factor_load_onto_different_types_of/",
        "text": "If I wanted to run a confirmatory factor analysis with latent factor F1 and observed variables X1, X2, X3 and so on, would these observed variables need to be the same type as each other? In other words, would the model work if some of the observed variables were continuous and some were binary for example? If so, would this require specific estimation methods?",
        "created_utc": 1678319428,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sample size calculator for continuous data",
        "author": "Large-Papaya-5519",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mbjgg/sample_size_calculator_for_continuous_data/",
        "text": "Currently taking a class and was provided the website raosoft to identify sample sizes for discrete data. Is there a similar calculator available online for continuous data? Thanks in advance!",
        "created_utc": 1678317333,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the standard deviation of a Likert-type item impacted by the number of response options?",
        "author": "Soothsayerslayer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11mbiwb/is_the_standard_deviation_of_a_likerttype_item/",
        "text": "As an example, let’s say that a Likert-type item was developed with 5 response options, and another researcher used the same item in their own study but provided only 4 response options. All things being equal, would item standard deviations across the studies? Context is item-level meta-analysis.",
        "created_utc": 1678317297,
        "upvote_ratio": 1.0
    },
    {
        "title": "Unsupervised clustering - I have three strongly left-skewed variables and all the items are in the same region of the 3D space that they build up. What is the best way to work in this situation?",
        "author": "MysteriousWealth2423",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m9m07/unsupervised_clustering_i_have_three_strongly/",
        "text": "",
        "created_utc": 1678312920,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question on using a t-test for email marketing data.",
        "author": "SunscreenTea",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m8uq9/question_on_using_a_ttest_for_email_marketing_data/",
        "text": " If we want to compare two different email campaigns, we have how many  emails were sent out for A: 1500 and how many emails sent out for B:  4000 . For campaign A, there were 1600 page views while campaign B  produced 3250 page views. So in other words Campaign A yielded  about  1.06 views per email while campaign B yielded .81 views per email. Would  a t-test work for comparing the means? I have no other data given to me  so I am not sure about standard deviation. The goal is basically to see  if campaign A performed significantly better than Campaign B. What  would you do? Thanks in advance!",
        "created_utc": 1678311143,
        "upvote_ratio": 1.0
    },
    {
        "title": "Many things we measure are all positive definite quantities yet we treat them as Gaussian, what are they really?",
        "author": "ggrieves",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m7bfv/many_things_we_measure_are_all_positive_definite/",
        "text": "\nWe treat these with means, standard deviations, etc. as if they were normally distributed. But for small values, they can't become negative and so they bunch up near zero.   \n\nIn the data I have it seems to be fit well by a Poisson, because of the nice property that Poisson converges to Gaussian when the mean is large.  But I have no idea whether these theoretically are Poisson processes.  Is that true?\n\nThe data I have are live arrival delays, how many hours did a packet take to travel.  So my data is always less than about 24-48, and often it's near 1-2.",
        "created_utc": 1678307630,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multiple partial correlation coefficient for a regression model",
        "author": "Alicia-Emily",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m4jmu/multiple_partial_correlation_coefficient_for_a/",
        "text": "I am working on my Master's thesis and currently trying to perform a post-hoc Power analysis for my (full) linear regression model in SPSS. I have to enter a multiple partial correlation coefficient as the effect size, but I'm not exactly sure what this means/ how to obtain that value (is it the same as r\\^2, adjusted r\\^2, f\\^2, or…). Can someone please provide an explanation?\n\nI know g\\*power might be a better option, since it works with f\\^2/r\\^2, but I am unable to do so because it is not compatible with the screen reader software I am dependent on.",
        "created_utc": 1678301446,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate the confidence interval of marginal means of a linear mixed model?",
        "author": "pashtun92",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m3teh/how_to_calculate_the_confidence_interval_of/",
        "text": "The context of the question is posted here: https://stats.stackexchange.com/questions/608646/calculating-confidence-intervals-of-marginal-means-for-a-linear-mixed-model-usin \n\nBut havent received any responses so far, so trying it here. Basically, I have the marginal means, standard error and degrees of freedom and would like to estimate the 95% CI. Am looking for a formula. The model was built using a linear mixed model. Appreciate any help.",
        "created_utc": 1678299765,
        "upvote_ratio": 1.0
    },
    {
        "title": "What’s the probability of...",
        "author": "Therapyisnotbad",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m2s65/whats_the_probability_of/",
        "text": "What’s the probability of someone passing a qualification exam, like the bar, if they failed the first four attempts and passed on the fifth?  Assume the pass fail rate is the same from test to test? \n\nWhat if they failed the first four, then passed, then passed a second consecutive test in a different region (with the same assumptions)? So four fails, then consecutive passes",
        "created_utc": 1678297419,
        "upvote_ratio": 1.0
    },
    {
        "title": "what is normal distribution, could someone help and explain.",
        "author": "The_Unkn0wn_-_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m2nfa/what_is_normal_distribution_could_someone_help/",
        "text": "I understood  standard deviation but ND is not clearly understood",
        "created_utc": 1678297124,
        "upvote_ratio": 1.0
    },
    {
        "title": "Correlating nominal / continuous / ordinal data: which statistical tests do I use?",
        "author": "Round_Mushroom_8515",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m26gi/correlating_nominal_continuous_ordinal_data_which/",
        "text": "  \n\nHi, for my master's study I would like to:\n\n1. correlate a nominal variable (e.g: native language: English, French, Dutch or German) with a binary nominal variable (e.g: gender: male or female). Is the chi-square test the best (and least complicated) statistical test for this?\n\n2. correlate a continuous variable (e.g: age) with a binary nominal variable (e.g: gender: male or female). Is the Spearman Rank-correlation the best (and least complicated) statistical test for this?\n\n3. correlate a continuous variable (e.g: age) with an ordinal variable (e.g: the extent to which a person was satisfied with a service: score 0-3). Is the Spearman Rank-correlation the best (and least complicated) statistical test for this?\n\nI will use SPSS for this.\n\nThank you in advance!",
        "created_utc": 1678296064,
        "upvote_ratio": 1.0
    },
    {
        "title": "Given the data below, how can I justify there is no statistical difference?",
        "author": "acidsyzygy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m1cch/given_the_data_below_how_can_i_justify_there_is/",
        "text": "&amp;#x200B;\n\nhttps://preview.redd.it/ce3vrvymojma1.png?width=950&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bb10658d2ccf4d2dc8fb0aa1e13b0f84da0a34c3",
        "created_utc": 1678294191,
        "upvote_ratio": 1.0
    },
    {
        "title": "Alpha value correction for \"unplanned interim analysis\"",
        "author": "TK-710",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11m16m3/alpha_value_correction_for_unplanned_interim/",
        "text": "Hello,\n\nWe planned a survey and conducted an a priori power analysis that gave us a sample size of about 100. Once we had gotten 100 responses, we started looking at the data and realized that a number of people had started, but not filled out, the survey. For this reason, we thought it might be a good idea to collect more data to hit 100 completed surveys. My colleague is under the impression that looking at the data means this counts as a interim analysis and we need to adjust the alpha value we use to eventually evaluate the results. I'm familiar with some of the interim analysis rules used in clinical trials (e.g. the O'Brien-Flemming procedure), but those rules seem to be meant for situations where 1) the interim analysis was planned, and 2) the interim analysis would be used to determine whether the study should be stopped early. In our case, the analysis was not planned and the effect-size/p-values were not used to determine whether we needed more data, only the fact that we didn't actually have 100 responses was. Do we need to change our final alpha value and, if so, is there a procedure that is appropriate for this situation?\n\nThanks!",
        "created_utc": 1678293830,
        "upvote_ratio": 1.0
    },
    {
        "title": "Good example of bad use of statistics in recent News?",
        "author": "eddiesteady99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lyezc/good_example_of_bad_use_of_statistics_in_recent/",
        "text": "I’m helping a high school teacher make an assignment for a high school class where they are to try to identify misuse (intentionally or unintentionally) of statistics in news.\n\nDo any of you have any good examples of recent news stories where the headlines, or the story, misleads the reader by misunderstanding statistics or using misleading visualizations?  \n\n\n(The students are Norwegian, but any English language media will do. Preferably about a topic that can be understood internationally)\n\nI have looked around, but couldn’t find any really good examples, but thought this group might have seen some particularly jarring examples recently. Thanks in advance!",
        "created_utc": 1678287404,
        "upvote_ratio": 1.0
    },
    {
        "title": "Homogeneity of the variances assumption not met after transformation",
        "author": "malik__ergin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lvcn3/homogeneity_of_the_variances_assumption_not_met/",
        "text": "Hi, \n\nI've transformed data to meet assumptions of ANOVA. I've achieved normality but variances are still heterogen. I want to show affects of transformation on data but heterogeneity is a problem. How can I explain ?",
        "created_utc": 1678279218,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are increasing percentages and amounts mean here?",
        "author": "Chromelikeaos",
        "url": "https://i.redd.it/gzik4app5gma1.png",
        "text": "",
        "created_utc": 1678251498,
        "upvote_ratio": 1.0
    },
    {
        "title": "What qualifies as being “proficient” at STATA?",
        "author": "givenpriornotice",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lkqir/what_qualifies_as_being_proficient_at_stata/",
        "text": "At what point could one consider themselves proficient in STATA? \n\nThere is a job that I am applying to that requires proficiency in STATA, and if I got the job, would start in late summer. I am currently trying to self-teach and I think I am getting the hang of it but unsure of whether I am doing enough.",
        "created_utc": 1678245613,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the minimum time it takes to complete the Penn State University Graduate Certificate in Applied Statistics?",
        "author": "Worth_Schedule_6441",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lio80/what_is_the_minimum_time_it_takes_to_complete_the/",
        "text": "I would like to hear from anyone who has experienced the Penn State Graduate Certificate in Applied Statistics. Is it possible to complete this program in 2 or 3 semesters? Or does it typically take more than a year?\n\nAlso, I did not take math as an undergraduate, were there any prerequisites for this program?\n\nThank you.",
        "created_utc": 1678240296,
        "upvote_ratio": 1.0
    },
    {
        "title": "What type of statistical analysis can I use to determine if specific factors strongly correlate to someone becoming a customer at my business?",
        "author": "Intrepid-Monitor-642",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11li2fm/what_type_of_statistical_analysis_can_i_use_to/",
        "text": "I am trying to figure out if there is a simplistic approach to determining if certain variables strongly correlate to someone becoming a customer at my business.\n\nFor my customer data set, I have data on them such as their income levels, frequency they visited my storefront before becoming a customer, number of times they called before becoming a customer, etc.   I'm giving basic examples, the real dimensions are much more robust.  \n\n\nWhat kind of statistical analysis/visualization can I do on this set of data to see which factors have a strong correlation to someone becoming a paid customer? \n\nThanks in advance!",
        "created_utc": 1678238717,
        "upvote_ratio": 1.0
    },
    {
        "title": "Please Help!",
        "author": "CommercialMistake498",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lfygd/please_help/",
        "text": "Hi,\nI am working with school drop out data of 12 years to see if an intervention has made a difference in drop out rate.\nI have drop out numbers of 12 years. Can someone please help me how I can do interrupted time series analysis? \nDo I have to run regression? Sorry, I am terrible with stats.",
        "created_utc": 1678233453,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mediation model question",
        "author": "Glad_Chip_5917",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lf2et/mediation_model_question/",
        "text": "Hello!\n\nI ran a parallel multiple mediation model using the PROCESS macro, with 4 mediators. My total effect was positive (significant), but my direct effect was negative and non significant. 3/4 of my mediators were positive (2 sig, 1 non-sig), and the 4th was negative (non significant). \n\nMy mediators are correlated (small to medium). When I run a correlation between the IV and the 4th mediator, it is positive. When I run a correlation between the mediator and the DV, it is also positive. \n\nQuestions\n\n1. Why is it that when including the mediators in the model, the relationship between the fourth mediator and the DV becomes negative? Does the positive correlation rely on the correlation/relationship to the other mediators, which would be removed in the model? \n2. Why is it that my total effects and direct effects have different signs? Is the 4th mediator a surpressor variable? \n\nThank you!!",
        "created_utc": 1678231330,
        "upvote_ratio": 1.0
    },
    {
        "title": "Method for approximating year of birth from SSN data?",
        "author": "PeripheralVisions",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lddm7/method_for_approximating_year_of_birth_from_ssn/",
        "text": "I'm working with a large, panel data set of de-identified SSNs (US social security numbers) and quarterly wages in Texas from 1995-2021. Under (even greater) lock and key, we also have the true SSNs. It is based on employer reported unemployment insurance data. I learned that it is possible to somewhat accurately predict the month-year in which an individual originally applied for their SSN, which I'd like to use to generate a proxy for age prior to assigning a de-identified SSN ID.  [I know that researchers have cracked the code](https://dataprivacylab.org/dataprivacy/projects/ssnwatch/samples.html) with varying accuracy based on time and place, and [this paper's appendix](https://www.pnas.org/doi/10.1073/pnas.0904891106#supplementary-materials) seems to have some useful information, although it would be quite time consuming to reverse engineer it (they are doing the opposite of what I want).\n\nIs anyone aware of resource or documented method for predicting this? I was hoping to find a \"worked example\" on GitHub or an economic methods paper with specific information but have not had any luck so far. Any advice is welcomed.\n\nI apologize in advance if this question is too specific or inappropriate for this sub.",
        "created_utc": 1678227622,
        "upvote_ratio": 1.0
    },
    {
        "title": "ELI5: Infinite Parameter Estimates in Logistic Regression",
        "author": "-curious-cheese-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lcqh1/eli5_infinite_parameter_estimates_in_logistic/",
        "text": "The book I am reading says the maximum likelihood estimate for an explanatory variable, x, is infinite when the x values where y=0 are completely separate from the x values where y=1. It goes on to explain that a perfect fit for completely separated data has π\\^ of 0 or π\\^ of 1, based on the value of x. The book states, \"A sequence of logistic curves that approaches this ideal results from letting  β\\^ increase without limit, with α\\^=-50\\* β\\^. In fact, the likelihood function keeps increasing as we take such a sequence, and the ML estimate of beta is β\\^=∞.\" Can someone please help me understand this? I understand what complete separation means, but I do not understand why that would cause the estimates of β to increase infinitely. Also, can someone explain why this is a problem? Wouldn't it be ideal to have a model where y can always be predicted with 100% accuracy based on x? TYIA!",
        "created_utc": 1678226232,
        "upvote_ratio": 1.0
    },
    {
        "title": "Working with fold changes",
        "author": "researchmonkey22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lcbvn/working_with_fold_changes/",
        "text": "First off, thank you in advanced for your replies! \n\nI’d like to test see if there was a statistically significant difference for the change in a given variable between two groups (such as a lab test). \n\nFor example, stratifying by clinical outcomes, if the change in glucose levels between two time points was different for each group. \n\nIs it possible to do hypothesis testing using  fold changes? Does it make sense to use fold changes over the average “raw” changes (“b-a”)? \n\nThanks again!",
        "created_utc": 1678225338,
        "upvote_ratio": 1.0
    },
    {
        "title": "Desperate need of statistics career options (MS/Job market?)",
        "author": "darkfuture74",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lbm7x/desperate_need_of_statistics_career_options_msjob/",
        "text": " \n\nI am an undergraduate in one of the top 30 universities. I am double majoring in economics and statistics. I found myself interested in learning statistics and wanted to apply for the masters program after the graduation recently. If I apply for the graduate school, it will be at the end of this year.\n\nOne thing I am worried about, is that I feel like I am too late. I am a junior now, and planning to graduate after this Spring semester (yes, graduating early). I do not have any internship experience at this point. I tried to get my internship during my first two years in the college but it didn't work out well. All I have is working experience of undergraduate learning assistant in college courses, tutoring experience, and attending summer economics program in Chicago.\n\nMy GPA is 3.6. I only have few A's in statistics. Most of my course grades are A-/B+, which is the reason I am worried about my recommendation letters, since I heard that most professors will only write the letters for the students who received A.\n\nAbout my programming skills, I've been trying my best throughout the college years. I learned good amount of knowledge in Python and R with several courses. I've been self-studying SQR as I saw good amount of companies are requiring SQR for the jobs.\n\nThis has been a major and stressing concern for me in these days. I am judging whether I should still go apply for masters, or go straight to search jobs in the job market. Or delay my graduation for a semester or an year to get more experience. Do you have any recommendation or advice to this situation?\n\nHow high are my chances to get into good statistics masters program, based on my profile? What about job market?\n\nAnd lastly, how worth is statistics masters in the job market?\n\nThanks for reading the post. I haven't had (or known) opportunities to learn more about this statistics field, which is why I am struggling and asking these vague questions. If you can answer my questions that would be very awesome, but any other recommendation comments would be very nice too. I just need more information to learn about statistics, because I recently made my decision to delve in statistics.\n\nI am willing to provide more information in comments if needed!",
        "created_utc": 1678223743,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone explain the difference between a ordered Logit model and an ordered Probit model within ordinal regression?",
        "author": "Silent-Thund3r",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11lbg3q/can_someone_explain_the_difference_between_a/",
        "text": "Hi there, I'm doing some statistical analysis between 2 ordinal variables and I wanted to ask what is the different between an logit model and probit model within ordinal regression. The first ordinal varaible is 5 age categories and the second one is level of knowledge ranging from \"I Know a Great Amount\" to \"I Know Nothing\".",
        "created_utc": 1678223373,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why am I getting this wrong? (pictures attached)",
        "author": "Adopted_Jaguar",
        "url": "https://www.reddit.com/gallery/11lai3o",
        "text": "",
        "created_utc": 1678221319,
        "upvote_ratio": 1.0
    },
    {
        "title": "International Beerio Kart Championships of the World: Power Ranking Development Help!",
        "author": "zakarm22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11la96v/international_beerio_kart_championships_of_the/",
        "text": " \n\n**TL;DR: My friends and I have a stupid hobby that's getting out of control and I need your help spiraling it further. Please help me create a fair power rankings system (using the attached spreadsheet for reference) for the Beerio Kart tournaments we host.**\n\n[**https://docs.google.com/spreadsheets/d/1CS5pWnmgS8wIZAvFQL4cc\\_jHWbTZ\\_khS/edit?usp=sharing&amp;ouid=114408781303577995971&amp;rtpof=true&amp;sd=true**](https://docs.google.com/spreadsheets/d/1CS5pWnmgS8wIZAvFQL4cc_jHWbTZ_khS/edit?usp=sharing&amp;ouid=114408781303577995971&amp;rtpof=true&amp;sd=true)\n\nDear members of the Statistics community,\n\nI call humbly upon the statisticians, mathematicians, programming aficionados, excel experts, sports analysts, and power rankings enthusiasts of this great community to assist me with a vital task -- creating a fair and representative power ranking formula for the International Beerio Kart Championships of the World.\n\nA little background: my buddies and I were trapped at home Thanksgiving of '21 for a fourteen day COVID quarantine. We were saddened by a missed opportunity to see our families, but with competitive spirit running through our veins and a surplus of leftover PBR from a party we threw (which was undoubtedly what gave us COVID), we found solace in roughly two weeks straight of fierce competition in the best drinking/video game pair to ever exist: Beerio Kart. For the uninitiated: Beerio Kart is Mario Kart, however, you need to finish your beer before the end of each race, and you can't drink and drive (i.e. chug and control your character simultaneously). Our version of the game has many extra rules and sub-rules, however, that's the basic premise of the game.\n\nAfter two weeks of this, we needed an outlet to determine who was truly the best of us, and thusly the International Beerio Kart Championships of the World were born. It started with a modest eight competitors, but interest has increased steadily over the past three years and in recent events we've had as many as 58 competitors fighting to compete in a 32 person bracket (surplus competitors play in Play-in Prix's for entry into the main bracket). We've now had 75 people play in official brackets and obtain power rankings, and close to 100 participate in the events overall. For a little context into how the tournaments are run, four competitors participate in each Grand Prix, and the top two competitors advance from each round until the championship. In the preliminary rounds, players must drink a beer on races two and four of each Grand Prix, and in the finals all four races are drinking rounds, thusly the final four competitors must drink a minimum of 10 beers to win the tournament.\n\nAs tournaments got larger and more intricate (and people started complaining that they were seeded unfairly), we realized we needed an objective ranking system to seed players so that the Prix's leading up to the championship were fair and quantitative. This background brings me to the hallowed undertaking I beseech your help with: **please help me figure out how to do this.**\n\nWe've tried a few formulas, but we are but amateur statisticians and none have felt like they effectively capture a player's skill level.\n\nFirst we tried the following formula: ibkc power ranking = 0.33t/60n + 0.33z/60 + 0.33y/60, where:\n\n1. 60 = the maximum number of possible points scored in any given grand prix\n2. t = total points accrued over all past tournaments attended\n3. n = total number of grand prix’ held in all official tournaments\n4. z = average points scored per prix, per tournament, in all tournaments attended\n5. y = average points scored per prix, per tournament, in all tournaments attended this calendar year\n\nIt was a good start, but it unfairly biased players who had played in more tournaments, and wasn't an accurate reflection of *current* skill level. It would be like baseball power rankings putting the Yankees are at the top because they're an ancient ball club and have won 27 World Series', even though the last time they won was 2009, or the Astros low down on the power rankings because they didn't win their first Series until 2017, even though they've won twice in the past 5 years.\n\nWe then created a formula based on Pythagorean expectation, where a players skill level is calculated by averaging their (points accrued in a prix)/(points accrued in a prix + total number of possible points in a prix). Each round of a tournament was weighted heavier than the last, and tournaments with four rounds carry more weight than tournaments with three rounds. The player's Pythagorean expectation was then averaged over all tournaments they've participated in, averaged over the last four tournaments held, and averaged over the last two tournaments held. Their power score was then calculated by averaging these three numbers together with the intention that more recent tournaments would be weighted heavier than older ones. **This is the formula that the attached spreadsheet uses.**\n\nThis new formula was better than the first but has an inverse problem -- it weighs recent tournaments too heavily and doesn't account for any rank decay from missing tournaments. For example, you can see that BAT has won 6 of 8 tournaments, but after a huge upset in the semi's, BAT did not make the finals of the last tournament, and was booted from first place overall to third. All the while, Squirt4Boyz advanced from second place overall to first, even though Squirt4Boyz didn't even participate in the last tournament.\n\nThere's all sorts of hidden columns and rows and whatnot in this spreadsheet so please dm me with any questions you might have, but please, I beg of you fine and glorious proprietors of the world's most stressful game, help me create a ranking system that makes sense. **Ultimately we need a system that reflects how many points a player is expected to score, considers that player's tournament wins, podium finishes and finals appearances, accounts for rank decay, and like in global tennis or golf rankings, has some bias for recent events.**\n\nThank you, friends.\n\nYour servant,\n\nThe International Beerio Kart Championships of the World League Commissioner",
        "created_utc": 1678220782,
        "upvote_ratio": 1.0
    },
    {
        "title": "Want to examine effect of both specific covariates and their sum in regression",
        "author": "tchaikswhore",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11la6w2/want_to_examine_effect_of_both_specific/",
        "text": "I have a regression model that I am using for explanatory purposes. I have a set of binary covariates which are specific comorbid diseases. I also have a variable which is the sum of comorbid diseases (categorical). I would like to examine the effects of both on an outcome.\n\nI ran the 2 models separately and the results of all other covariates are nearly identical. My initial thought is to report the results of model 1 (individual diseases) and then also include the CIs of the sum variable, with an asterisk indicating how the model was ran twice. Perhaps averaging the results of other covariates between the 2 models.\n\nIs there a standard methodology for this scenario? I want to report this in a single table and am having trouble researching this specifically.",
        "created_utc": 1678220632,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Bayesian networks - how have you presented your inference?",
        "author": "grackith",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11l8kg7/q_bayesian_networks_how_have_you_presented_your/",
        "text": "**\\[BACKGROUND - useful\\]**\n\nDiving deep into Bayesian networks for a PhD project. In this case, these **results need to be approachable for an audience of DOTs and state policy officials.** I have a background in statistics, however I wouldn't say I'm experienced in Bayesian network formulation/inference. Over the past year I have developed two Bayesian networks (dynamic &amp; static) to address my RQs. DAGs were a combination of structure learning (stochastic hill-climbing \\[shogun\\]) and domain knowledge. **It would be important to know that while the RQs are focused on a specific \\*dependent\\* variable, there are 6 other variables of interest (specified a priori) that may be either static or dynamic in nature depending on the observation. Final DAGs were 16,18 nodes/30,35 arcs.**\n\n**\\[PROBLEM\\]**\n\nI'm having a hard time presenting network results to the funding agency. They vetted and supported this methodology, but seem very tied to traditional regression results/inference - which makes it hard to grasp the Bayesian network results. In my humble interpretation, the power of using BNs is to view \\~all dependence/independence relationships (considering conditionals) without having to specify direction and definition of predictors w.r.t. an outcome variable.\n\nI have shown them DAGs, prediction accuracies, conditional probability tables; I have queried specific nodes of interest and explained how posterior distributions of an interest variable changes w.r.t. evidence given. **Nothing seems to stick! They keep asking for an 'ahh ha' moment.**\n\n**\\[QUESTION\\]**\n\nFor those that have experience with Bayesian network inference, how have you presented or formalized your results? Did you have success with DAG visualization \\[mine just seem crowded\\]? or other result visualization?\n\nAny advice on bridging the gap between tradition regression inference and BN inference? I was thinking I could show some non-linear relationships present in the BN that were insignificant in the traditional regression model \\[this is the case\\].\n\nBecause I am also relatively new to the application, perhaps I'm not thinking of all the interpretations/inference of these networks.\n\nFor reference - working in R, comfortable with python - but in general just want to get advice.\n\n**\\[This is NOT casual inference\\]**\n\n**\\[tldr\\] If you have experience with Bayesian network inference and have presented these results to non-statisticians, how have you done it \\[visually/numerically/useful interpretations/bridging the gap from regression\\]?**",
        "created_utc": 1678217027,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you get 90% confidence interval &amp;. 10% level of significance? Do you use t-test?",
        "author": "feellikepooping",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11l5137/how_do_you_get_90_confidence_interval_10_level_of/",
        "text": "",
        "created_utc": 1678209086,
        "upvote_ratio": 1.0
    },
    {
        "title": "In spss, how do you convert continuous variable into 3 categories (making categroical) low, medium, high?",
        "author": "Exact_Comparison93",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11l4rsj/in_spss_how_do_you_convert_continuous_variable/",
        "text": "I have data with 3 scales. Essentially 2 of the scales are my dependant variables and the other 1 is my iv, I have said my iv has 3 levels; low, medium and high. I have seen how you split into 2 categories using a median split and recoding variables. But struggling to make it work into 3 groups",
        "created_utc": 1678208518,
        "upvote_ratio": 1.0
    },
    {
        "title": "Culture survey participation rate",
        "author": "garbagemonsta",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11l486j/culture_survey_participation_rate/",
        "text": "I want to know what an acceptable response rate is for a company doing an organizational culture survey. I'd like to calculate an overall goal for participation as well as for each department. Is Yamane's formula appropriate for this? A colleague has used it in the past but I feel like in this case it justifies a 30% response rate. I don't believe this is acceptable when we're talking about culture surveys because sub-cultures exist within the broader culture.",
        "created_utc": 1678207306,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need Reddit wisdom. This is a Eurostat data “Distribution of income by quantiles”. Question: if it’s quintiles, why is it not 20% each? What is a quintile in this case? (I read their explanation, but they are unclear to me). Would GREATLY appreciate the answers, thank you!!",
        "author": "imwaalkinghere",
        "url": "https://i.redd.it/54v1omxstdma1.jpg",
        "text": "",
        "created_utc": 1678205242,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which are the statistical methodologies to consider when examining study group death rates but without considering time to death?",
        "author": "Ridyot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11l2edj/which_are_the_statistical_methodologies_to/",
        "text": "I have a dataset for a group of 66,000 subjects diagnosed with a dangerous condition, and the time it takes for death to occur (the “event”) or to not occur (survival or “censored”). I am pursuing survival analysis using R. However, what if I am not interested in time-to-event, but just want to study the end of period death/survival rate with the objective of deriving a probability distribution of death at year 5 for another group of subjects facing the same diagnosis? I’d like to explore other statistical options where time to event is not a consideration. Is this a matter of choosing a distribution (such as binomial since in this case the subjects either live or die) and running simulations using my group’s parameters? \n\nIn the case of my study group: \n\n* Death rate (over 5 years) is 70.5% \n* SDEV (population or sample) is 0.4562 based on calculating SDEV on patient status at study period end where 1 = death and 0 = alive or “censored” \n* SERR is 0.0018",
        "created_utc": 1678203137,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone help me get an answer for this",
        "author": "rdhthenerd",
        "url": "https://i.redd.it/g1ruahqlldma1.png",
        "text": "",
        "created_utc": 1678202488,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should/Can I report significance for a Spearman correlation test if data is not normally distributed?",
        "author": "vulpecula05",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kyqpn/shouldcan_i_report_significance_for_a_spearman/",
        "text": "Quick info: I'm working with JASP and have no formal statistical education. Sorry if this is a somewhat stupid question!\n\nI've run a Spearmans rho correlation test in JASP to determine whether my non-normally distributed variables correlate. JASP reported significance as well (H1 = variables correlate) and some correlations were marked as significant, some p &lt; .001. Now,  I've read online that you shouldn't report p-values for non-normal distributions, so I wanted to make sure whether this applies in this case as well. \n\nFurthermore, if I shouldn't mention significance/p-values, how should I report the findings on correlations instead? Are there any other conventions which determine what correlations are worth discussing if not in terms of significance? \n\nThanks!",
        "created_utc": 1678194040,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing analytical methods to determine correlation",
        "author": "1Josh1234",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kxpn2/comparing_analytical_methods_to_determine/",
        "text": "Hey all, \n\nI'm looking at comparing several different tests in laboratories to see if their results correlate. I have data from around 250 patients who all had their blood analysed in a two year period, and each test was taken at the same time for each person. I have ranked these results into 5 categories (from 1-5 based on reference ranges and available research) but am finding it difficult to present this data in a descriptive way. There is too much data for histograms, dual scatter plots look strange due to the limited range (1-5), and don't really describe the data. \n\nI have decided to use Spearman's rank correlation coefficient to measure the similarities in two groups at a time, to show that the data is closely related (or not so). Is there any other way of showing this data off? \n\nThanks for your help!",
        "created_utc": 1678191200,
        "upvote_ratio": 1.0
    },
    {
        "title": "Vanishing twin syndrome",
        "author": "Dirlens",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kw0w0/vanishing_twin_syndrome/",
        "text": "If identical twins births happens in 0.3% of all pregnancies worldwide then the vanishing twin syndrome accounts within those 0.3% births and the number of identical twins will be lower because that syndrome occurs at about 36% of all identical twins pregnancies or that syndrome doesn't account within and 0.3% of all pregnancies will be identical twins that doesn't were affected by syndrome hence the identical twins pairs pregnancies would be 36% more frequent worldwide per 1000 births but the syndrome occurs?",
        "created_utc": 1678186231,
        "upvote_ratio": 1.0
    },
    {
        "title": "Two way mixed ANOVAs",
        "author": "Ruadj",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kvv98/two_way_mixed_anovas/",
        "text": "Hi All, \nI am performing two separate two way mixed ANOVAs for my thesis. What would be the best tables and/or figures to present in APA style? Any help at all would be greatly appreciated. Thank you",
        "created_utc": 1678185732,
        "upvote_ratio": 1.0
    },
    {
        "title": "Moderation with a multicategorial IV, a multicategorial Moderator and continuous DV",
        "author": "Smileysmile_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kus6e/moderation_with_a_multicategorial_iv_a/",
        "text": "Hi, I'm trying to do a moderation with a multicategorial independent variable with four categorial items, a moderator with three categorial items and a continuous dependent variable. I want to do moderation with Hayes Process Macro. Therefore I put my variable in it and chose \"Multicategorial indicator\" for IV and Moderator. SPSS creates then automatically dummy variables for me. However, it seems that I lose the \"first item\" of each variable in the analysis. So for my IV I get X1, X2 and X3 (but initially my variable had four items) and the same for my moderator which is now W1 and W3 (even though I had three groups). Has anyone done sth. similar before?",
        "created_utc": 1678182158,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with Statista Premium Account Access",
        "author": "giorgiocerve",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kup3b/help_with_statista_premium_account_access/",
        "text": " Hello guys, Can someone share their Statista account details with me? I am working on a research project and need some help downloading some reports on the cost of cybercrime: \n\n[https://www.statista.com/outlook/tmo/cybersecurity/worldwide#cost](https://www.statista.com/outlook/tmo/cybersecurity/worldwide#cost)\n\n[https://www.statista.com/statistics/1280009/cost-cybercrime-worldwide/](https://www.statista.com/statistics/1280009/cost-cybercrime-worldwide/)",
        "created_utc": 1678181841,
        "upvote_ratio": 1.0
    },
    {
        "title": "I'm studying about linear regression and I had a question about the normal equation of simple linear regressions",
        "author": "lilchink88",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ku1w4/im_studying_about_linear_regression_and_i_had_a/",
        "text": "Does the equation in the picture below hold? Or is it a misprint? I thought it should be B1 and B0 hat, and not B1 and B0. If it is true for B1 and B0 hat, would that mean it would be true for B1 and B0? My thought is that it shouldn't be but I would like to confirm.\n\nhttps://preview.redd.it/z5o3nc3h7ama1.png?width=407&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=74b46dd4c1630af111e8c81be8bff507b5a050bc",
        "created_utc": 1678179419,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need help lol",
        "author": "No_Text263",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ks8vw/i_need_help_lol/",
        "text": " \n\nSo I'm doing a research. I'm not that equipped for statistics so I need help.\n\nI have a lot of nominal/ordinal data. They're demographics, if they felt pain (if yes, how long have they been feeling it), other questions about pain and symptoms of certain illnesses, or the level of functionality of their body parts, etc.\n\nI used test for independence (some needed to use Fisher's, some used Likelihood Ratio for when they violate chi-square's assumption), are there any other tests that would be significantly helpful?\n\nThank you. :3",
        "created_utc": 1678172824,
        "upvote_ratio": 1.0
    },
    {
        "title": "A basic (?) question about interpretation of statistics in DNA testing.",
        "author": "Adjectivenounnumb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kq38v/a_basic_question_about_interpretation_of/",
        "text": "I’m not a student or particularly good at math.\n\nA podcast I listen to will often make statements like this:\n\n“The DNA comparison showed that the odds that the DNA sample came from anyone but the accused are more than 1 in 9 billion. And since there are only 8 billion people on the planet, it had to be him.”\n\nThis … doesn’t sound like it’s quite the right way to apply the test results to the situation, but like I said, poor math skills on my part. \n\nThanks in advance.",
        "created_utc": 1678166163,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question regarding statistical test",
        "author": "theanonymousanalyst2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kmqjj/question_regarding_statistical_test/",
        "text": "I am looking to perform a statistical test of the correlation between a personality score (numerical) and a likert scale 1-6.  What statistical analysis would I use?",
        "created_utc": 1678156982,
        "upvote_ratio": 1.0
    },
    {
        "title": "Indirect Feedback Loop in a Non-recursive Model (SEM)",
        "author": "Comprehensive-Yam259",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kkyb3/indirect_feedback_loop_in_a_nonrecursive_model_sem/",
        "text": "Hi everyone,\n\nI asked a question here last year about non-recursive models and, based on the answers, saved myself a lot of headaches down the line, so thank you to this community.\n\nI have reworked my model a lot and it now looks like the model (b) Kline uses to illustrate indirect feedback loops (2016, p. 151). The three endogenous variables have an A--&gt;B--&gt;C--&gt;A relationship. There are also three (instrumental) exogenous variables which affect only one endogenous variable each, so my model is identified.\n\nI know one of the most difficult parts of non-recursive models is their a priori specification based on sound theory, but I think I can give it a good shot. I'm also aware that there is some criticism directed towards the use of non-recursive models with cross-sectional data, but I feel it can be justified in this case.\n\nI have read Paxton et al.'s (2011) monograph on the subject which is great but is more focused on direct feedback loops (A-&gt;B-&gt;A). I have also read quite a few papers in the social sciences utilising direct feedback loops.\n\nI was wondering if anyone could direct me to sources which would help with the practical aspects of analysing this model. For example, Kline explains the distinction between recursive/non-recursive models as being based upon path models. Is it possible to run EFAs/CFAs for non-recursive models? Could this analysis be run in AMOS or would I need more specialist software?\n\nI appreciate any and all feedback on this topic.\n\nThanks!\n\n Kline, R. B. (2016). *Principles and practice of structural equation modeling*. Guilford publications. \n\nPaxton, P., Hipp, J. R., &amp; Marquart-Pyatt, S. T. (2011). Nonrecursive models: Endogeneity, reciprocal relationships, and feedback loops. Thousand Oaks, CA: Sage.",
        "created_utc": 1678152564,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Statistical Test]",
        "author": "Trying2FigureItOut16",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kjga1/statistical_test/",
        "text": "I need help to try to figure out the best test to use for this statistical issue.\n\nIn terms of each of the assigned health measures (weight, cholesterol level, fat intake, cholesterol\n\nintake, calories from fat), is there enough evidence to conclude that the program is a success?",
        "created_utc": 1678148882,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sorting an unknown group based on data from known group",
        "author": "underexpressing",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kje49/sorting_an_unknown_group_based_on_data_from_known/",
        "text": "I have a pure sample 1 that contains 100% A and a sample 2 that contains an unknown mixture of A and B. I don’t have a pure sample of B alone. \n\nI already know that sample 1 and sample 2 are significantly different, but I’d like to determine a good threshold to use to differentiate A from B for future unknown samples. I’ve thought to use the sample 1 average + std dev as the threshold. Is there a better statistical method of thresholding?",
        "created_utc": 1678148733,
        "upvote_ratio": 1.0
    },
    {
        "title": "correlations with multiple entries per participant",
        "author": "majorcatlover",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11khdpm/correlations_with_multiple_entries_per_participant/",
        "text": "Is it a problem when a participant contributes multiple entries to a correlation? e.g., if we were looking at the relationship between weight and exercise in cats and some participants provide data for their multiple pets, will that be an issue since some of the data points are nested within that individual?",
        "created_utc": 1678144141,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dice Statistics Question",
        "author": "Bushka123456",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kfo8z/dice_statistics_question/",
        "text": "Hi!   I've been reading a book going over some stats and they present a  formula I don't understand.  I think my college stats was too long ago!   Here's the problem:\n\nLet's say you and an opponent are each going to roll 2 die.  The first one to roll a 6  wins (not 4+2, etc, but an actual 6 on one dice and any number on the  other).  If you roll first you have an advantage because you can win right away.  We know the chances of rolling at least one 6 are 11/36 while the chances of not doing so are 25/36.  But, what is the actual  probability that you will win the game when you roll first?  How do you  calculate it?  Here's the formula they provide:\n\nLet p = probability that the first roller will win the game.\n\np = (11/36) + (1-p) (25/36)\n\nDoing the math, p = .59 so you have a 59% chance to win the game by rolling first.\n\nI  don't understand where the formula comes from or what it means conceptually.\n\nEnlightenment would be much appreciated!",
        "created_utc": 1678140317,
        "upvote_ratio": 1.0
    },
    {
        "title": "grades predicting grades",
        "author": "FearlessHead8689",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11ke4m8/grades_predicting_grades/",
        "text": "At a school I work at we give grades (which could be converted to gpa obvi) about halfway through the semester. These are just updates not official transcript grades. I now have both these grades and the semester grades.\n\nHow would I go about verifying if these progress report grades do/don't predict semester grades?",
        "created_utc": 1678137033,
        "upvote_ratio": 1.0
    },
    {
        "title": "Basic question comparing two experiments",
        "author": "solycarne",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kdqk6/basic_question_comparing_two_experiments/",
        "text": "HI redditors, \n\nI want to compare the same variable - amount of acids - from two experiments that have a normal distribution. One of the experiments is the \"precise\" one which is expensive and time-consuming, and the other experiment is fast and cheap but \"imprecise\". \n\nI want to see how they differ and to evaluate the applicability of using the imprecise experiment to save money and time.  \n\nTo do this, I have used a linear regression comparing the amount of acid in accurate vs. inaccurate experiment with sample size, n:16. I have an extremely good R (0.93, p:1.6E-9), mainly because I have two outsider values, but if I remove them it remains R 0.68 (p: 0.016).\n\nMy question is, is linear regression the right method? can I say that the imprecise experiment can be used instead of the precise one, or should I apply some other test?\n\nthank you in advance!",
        "created_utc": 1678136231,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I interpret multiple regression coefficients for a multiple regression model?",
        "author": "h9ppygurl29",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kcmpi/how_do_i_interpret_multiple_regression/",
        "text": "I am struggling to interpret the estimated coefficients for this model. I know how I would interpret this model as a simple linear regression but is the same. Can someone please help with an example interpretation of this model? \n\nSigned a struggling student",
        "created_utc": 1678133912,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to compute the arithmetic mean of time-series data?",
        "author": "NextTumbleweed8159",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kc1e5/how_to_compute_the_arithmetic_mean_of_timeseries/",
        "text": "I have a time-series consisting out of a data-set (x,y) with times in seconds (equally spaced with data-points always being 3 seconds apart) and corresponding pollutant concentrations to which I am fitting a Gaussian model curve. To perform the least-squares fitting in Python I provide an estimation of model parameters by computing for the data-set (x,y) the amplitude and location of the peak and the standard deviation. To compute the standard deviation, I use the following equations:\n\n   mean = sum(x\\_{i}\\*y\\_{i})/sum(y)\n\n   standard\\_deviation = squareroot(sum((x\\_{i}-mean)\\*\\*2 \\* y)/sum(y))\n\nThe equation for the mean and standard deviation I obtained from this link: \n\n[https://stackoverflow.com/questions/19206332/gaussian-fit-for-python](https://stackoverflow.com/questions/19206332/gaussian-fit-for-python)\n\nHowever, can someone provide a link to a source (e.g. textbook or paper) in which it is specified that the mean can be computed this way? Or do you think the definition to compute the mean this way is wrong? I have little knowledge about statistics, but textbooks usually state that the mean is computed as 1/n \\* sum(x\\_{i}) with n the number of datapoints. However, this does not work for time-series data as there are 2 dimensions. \n\nSmall example data-set for which I want to compute the mean and (using this) the standard deviation:\n\nx = \\[537, 540, 543, 546, 549, 552, 555, 558, 561, 564, 567, 570, 573, 576, 579\\]\n\ny = \\[1.87844626,  1.46226636,  0.13306075,  9.0276285 , 50.5930491, 43.0010514 , 34.629264  , 53.9516939 , 44.5387266 , 53.4610397 , 40.2893107 , 29.7825935 , 20.1958528 ,  5.98440421,  5.43533879\\]",
        "created_utc": 1678132659,
        "upvote_ratio": 1.0
    },
    {
        "title": "Correlating nominal / continuous / ordinal data: which statistical tests do I use?",
        "author": "Round_Mushroom_8515",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kboeq/correlating_nominal_continuous_ordinal_data_which/",
        "text": "Hi, for my master's study I would like to:\n\n1. correlate a nominal variable (e.g: native language: English, French, Dutch or German) with a binary nominal variable (e.g: gender: male or female). Is the chi-square test the best (and least complicated) statistical test for this?\n2. correlate a continuous variable (e.g: age) with a binary nominal variable (e.g: gender: male or female). Is the Spearman Rank-correlation the best (and least complicated) statistical test for this?\n3. correlate a continuous variable (e.g: age) with an ordinal variable (e.g: the extent to which a person was satisfied with a service: score 0-3). Is the Spearman Rank-correlation the best (and least complicated) statistical test for this?\n\nI will use SPSS for this.\n\nThank you in advance!",
        "created_utc": 1678131881,
        "upvote_ratio": 1.0
    },
    {
        "title": "Helpdesk and supervisor give opposing advice",
        "author": "SintChristoffel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11kbf6k/helpdesk_and_supervisor_give_opposing_advice/",
        "text": "I need to look at differences of a variable, or actually two, on two different times, pre- and postoperative. The statistics helpdesk at school said to create new variables with \"seizuretypes\". 1 being no seizures pre and post, 2 being seizures pre but not post, 3 is not pre only post, 4 is both pre and post. Then take these variables up in your regression on your dependent variables. \n\nNow my supervisor just said \"No. Mixes up analysis.\" \n\n???",
        "created_utc": 1678131335,
        "upvote_ratio": 1.0
    },
    {
        "title": "Target Encoding",
        "author": "eternalmathstudent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11katun/target_encoding/",
        "text": "Let's say we need to use features such as \"day of the week\" or \"week of the year\" or \"month of the year\" in a DL model for predicting sales of a product (there are a bunch of other variables present as well). I would naturally incline towards using OHE (Yes, I'm aware that it increases dimensionality). I've had someone suggest that we can apply Target Encoding so that it won't increase the dimensions. My first thought was that it'll lead to target leakage (I've looked it up as well. It indeed happens and there seems to be some work around). I would immensely appreciate it if you can help me pick one of the above approaches with good rigorous argument supporting it. Or if you have another approach apart from the above two.",
        "created_utc": 1678130058,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bayesian Data Analysis",
        "author": "lostman89",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k7q5u/bayesian_data_analysis/",
        "text": " I was recently accepted into the [BDU](https://avehtari.github.io/BDA_course_Aalto/gsu2023.html) course, which is the most challenging course I have ever taken. While I lacked experience with R, I quickly realized its importance and studied some tutorials to prepare. Although the first assignment was easy, I encountered some problems for the second one. With some help, I completed it and hope to do great in upcoming sessions. The third assignment, worth nine points, seems daunting compared to the previous two, and I am concerned about my ability to complete it. I found the BDA book difficult to follow, but I have seen recommendations for Rethinking Statistics from this [sub](https://www.reddit.com/r/statistics/comments/pjq5dm/d_statistical_rethinking_vs_bayesian_data/), which I have yet to take seriously. At the same time most yassignments are related closely to the book. I did not find Aki Vehtari's lectures engaging, as there were only presentations and no whiteboard sessions. Meanwhile I am big fan of [STAT110](https://www.youtube.com/watch?v=KbB0FjPg0mw&amp;list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo&amp;ab_channel=HarvardUniversity). The lectures are great and professor’s way of teaching concepts using story really interests me a lot. What are things I can do to improve and do well in this course. Also what are the expectations I can have especially regarding getting job?",
        "created_utc": 1678125818,
        "upvote_ratio": 1.0
    },
    {
        "title": "Binary Logistic Regression Not Making Sense to Me",
        "author": "navydocRC12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k6swn/binary_logistic_regression_not_making_sense_to_me/",
        "text": "So I am not ready to publish my data, so as not to confuse the subject, I changed the variables in my research to a simple example of candy.  \n\nI ran a binary logistic regression among males and females who ate candy in the last 24 hours.  If they ate the type of candy indicated they answered yes or no.  The realy study is much more complicated, but I am not very good with statistics so I am having difficulty understanding the concepts.\n\nThe null hypothesis is that there is not statistical significance between females or males when choosing candy. After running the test it appears that as my coefficients are less than or close to zero and my significance level is greater than .05 in all cases except for Mike and Ikes which is .025 among females, there is no statistical significance between females and males who chose the type of candy and I cannot reject my null hypothesis except with regards to Mike and Ikes.\n\nAssuming I ran my Binary Logistic Regression correctly (which I admit I may have messed up), there is no statistical significance between the males and females (except mike and ikes), however if you look at the percentage graph there are other candies who have similar percentages and they are not significant. \n\nFor example the mike and ikes at 2/7 male and 5/7 female, but the licorice is 5/18 male and 13/18 female and the results are not significant but both have a percentage of 27-28% male and 72-73% female.   Does this seem correct, or did I mess up my statistics?  Does my table look correct?\n\nFYI: Many Reddit members suggested going to the local university and speaking to the Biostats professor to discuss my research and I have an appointment in a couple weeks, but I am trying to get my head around the concepts before I go and embarras myself.",
        "created_utc": 1678124779,
        "upvote_ratio": 1.0
    },
    {
        "title": "Percentiles related question",
        "author": "Lacks-discip1ine",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k50gv/percentiles_related_question/",
        "text": "I read in the textbook that given some data, percentiles are the 99 numbers that divide the data into 100 equally sized groups.\n\nLet's say I have a list of numbers starting from 1 till 100 that is 1,2,3,...,97,98,99,100. \n\nWhat are the 1st,  50th and 100th percentiles here?\n\nIs there anything such as a 0th percentile?",
        "created_utc": 1678122729,
        "upvote_ratio": 1.0
    },
    {
        "title": "OLS regression with fixed effects when dependent variables are measured differently by group",
        "author": "spicysnake333",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k2pm2/ols_regression_with_fixed_effects_when_dependent/",
        "text": "I have data where each observation is a region within a country, and each observation falls into one of two large groups (each group makes up half the dataset). Each group corresponds to a seven-year funding period for a fiscal transfer program, and my independent variable is the percentage of total allocations that a region was able to spend within a seven-year timeframe.\n\nFor the first group, I have data on all seven years of the funding program, but for the second, I have data just on the first five years. Thus, for the observations in the first group, my dependent variable is fund absorption after seven years; for the second group, I can only get fund absorption after five years as a DV. As expected, the dependent variables for the second funding period are all much lower than for the first.\n\nOther than the fact that the DVs are measured differently and are calculated after different years, they are fairly comparable. Would I be able to run a single OLS regression with funding period fixed effects, or should I run two separate regressions?",
        "created_utc": 1678118430,
        "upvote_ratio": 1.0
    },
    {
        "title": "Derivative cumulative multivariate Gaussian distribution",
        "author": "Margaux408",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k2mp5/derivative_cumulative_multivariate_gaussian/",
        "text": " \n\nI am looking for the derivative with respect to the variable s of the following function\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4j9jv0gk55ma1.png?width=799&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=055c7a86317652d165fa9c5b5862bd4fbb2fde5c\n\nIn other words, I have a cumulative normal distribution, where both the mean (f(s)) and the variance (g(s)) depends on s. f'(s) depicts the transpose of f(s). I don't see how I can apply the chain rule or the product rule here, can anybody help me out?",
        "created_utc": 1678118250,
        "upvote_ratio": 1.0
    },
    {
        "title": "Analysis of mutual information score and vif score",
        "author": "TelephoneStunning572",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k25ar/analysis_of_mutual_information_score_and_vif_score/",
        "text": "I am working on a dataframe to conclude something and i have around 2400 columns and i want to reduce that number and for that i am calculating mutual information score and after that calculating VIF score.  \nThough mutual information score is changing when i am shuffling the dataframe rows but how is that possible because the values in all columns are still the same and the relative ordering among columns is not changing then why after shuffling the rows the mathematical calculations are somehow changing and i am getting a different mutual information score every time i run the code.",
        "created_utc": 1678117134,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does \"standard error of the mean\" represent in this context?",
        "author": "Ridyot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k1pc9/what_does_standard_error_of_the_mean_represent_in/",
        "text": "I’m running a survival analysis using R package `survival` and I have question regarding how to interpret the standard error of the mean that is returned when running the summary function of the fit survival curve (`summary(fit())$table`) against my data, as shown in the image at the bottom of this post. Is there a simple, common-sense way to describe the standard error? Or is this instead a standard deviation?\n\nI know the standard error definition is more or less “a quantification of how far the sample mean is likely to be from the true population mean”, whereas standard deviation is “a quantification of the variability of the individual data values to the mean of the entire dataset”. I ran this analysis on an initial population (group) of 60,836 subjects for which I have data and track the number of subjects that either die each month or are censored. I don’t see where “sampling” comes into play and if standard error is what applies. Isn’t the “population” the 60,836 subjects? Or in this case, when we say sample in defining SE, are my 60,936 subjects a \"sample\", a subset of the population as a whole that these subjects derive from (such as, for sake of example, all of the 1 million persons diagnosed with illness X at time 0 (for which I don't have the data), of which my 60,836 subjects are a subset (for which I do have the data))?\n\nhttps://preview.redd.it/oasg722ry4ma1.png?width=705&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=78f85b2dce4d15de5c603988b8be4c07d4b762ec",
        "created_utc": 1678116142,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about something being a general regression model",
        "author": "IgodZero",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11k1je7/question_about_something_being_a_general/",
        "text": "How is   Yi = log10(β1Xi1) + β2Xi2 + εi a general regression model? My professor says it is, but I don't understand how this model is linear within the parameters if one of the parameters in being logged. Do logs just not count for something being linear within the parameters?",
        "created_utc": 1678115774,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical tests do I use? Help a master's student who's terrible with statistics out!",
        "author": "Round_Mushroom_8515",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jvt55/which_statistical_tests_do_i_use_help_a_masters/",
        "text": "Hi, for my master's study I would like to:\n\n1. correlate a nominal variable (e.g: native language: English, French, Dutch or German) with a binary nominal variable (e.g: gender: male or female). Is the chi-square test the best (and least complicated) statistical test for this?\n2. correlate a continuous variable (e.g: age) with a binary nominal variable (e.g: gender: male or female). Is the Spearman Rank-correlation the best (and least complicated) statistical test for this?\n3. correlate a continuous variable (e.g: age) with an ordinal variable (e.g: the extent to which a person was satisfied with a service: score 0-3). Is the Spearman Rank-correlation the best (and least complicated) statistical test for this?\n\nI will use SPSS for this.\n\nThank you in advance!",
        "created_utc": 1678100659,
        "upvote_ratio": 1.0
    },
    {
        "title": "Difference between splitting data according to gender versus adding gender as a moderator?",
        "author": "Life_Sprinkles_7958",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jvpw0/difference_between_splitting_data_according_to/",
        "text": "Hi all, my supervisor wants me to split my data file according to gender and conduct my analyses on men and women separately to see whether findings are different. However, I have already conducted analyses on the complete dataset with the participant gender as a moderator. Doesn't this do the same thing or am I wrong? \nThanks!",
        "created_utc": 1678100415,
        "upvote_ratio": 1.0
    },
    {
        "title": "hypothesis test for not necessarily i.i.d data of unequal sample sizes",
        "author": "achsoNchaos",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jsp49/hypothesis_test_for_not_necessarily_iid_data_of/",
        "text": "Hey there,\n\nI'd like to perform a hypothesis test to compare the means of two samples and test at significance level 0.05 whether the mean increases. The samples are of size 6193 and 6065. It's unclear whether they follow a specific distribution or even are independent. \n\nI tried to perform Python's `stats.ttest_rel` test (t test for correlated samples) yet this doesn't work since samples must have same sizes.\n\nWhich options remain? Is there a way to test for independence s.t. one can at least perform a t-test. I'd be super grateful for any help!",
        "created_utc": 1678090344,
        "upvote_ratio": 1.0
    },
    {
        "title": "Python: how do I get the t value for a t test using scipy.stats",
        "author": "wlmai",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jrsav/python_how_do_i_get_the_t_value_for_a_t_test/",
        "text": "Hi there,\n\nSorry I know this is a super basic question:\nI want to  know the t-value for the rejection region of a t-test. Im my case I have 6301 degrees of freedom and a significance level alpha = 0.05.\nI tried `stats.t.pdf(x = 0.95, df=6301)` but this results in `0.2540389` which seems way to small. Could you give me a hint pls?",
        "created_utc": 1678087333,
        "upvote_ratio": 1.0
    },
    {
        "title": "Linear mixed effects model-in which order should I enter variables?",
        "author": "birbebur",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jri1k/linear_mixed_effects_modelin_which_order_should_i/",
        "text": "Hello all!\n\nI'm analyzing fMRI data and I want to use linear mixed effects modeling, including a random intercept for \"participant\" in my models.\n\nThis is how the variables looks like:\n\n\\-'accuracy' on a behavioral task - this is the outcome variable\n\n\\-age\n\n\\-gender\n\n\\-x metric for braingregion1 (x1)\n\n\\-x metric for brainregion2 (x2)\n\n\\-x metric for brainregion3 (x3)\n\n\\-y metric for brain region1 (y1)\n\n\\-y metric for brainregion2 (y2)\n\n\\-y metric for brainregion3 (y3)\n\nMy question is, is it okay to enter all variables at once and then decide on the final model with the 'significant' variables? Or should I enter them one by one based on apriori theoretical knowledge/the hypotheses? The results or values change depending on the approach I use.\n\nI've seen people suggesting both of these approaches and I cannot be sure what my decision criteria should be.\n\nDoes anyone have suggestions about this? I'd appreciate any help. Thanks in advance!",
        "created_utc": 1678086442,
        "upvote_ratio": 1.0
    },
    {
        "title": "Tutoring",
        "author": "Stacoopsky",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jqo3p/tutoring/",
        "text": "Hey guys I'm taking a statistics course wondering if anyone can tutor, willing to negotiate pay.",
        "created_utc": 1678083868,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] My Paired Samples Results are Significant -- but my Independent Samples are not; please help me understand why",
        "author": "Alive_Slice6257",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jprp7/question_my_paired_samples_results_are/",
        "text": "[removed]",
        "created_utc": 1678081159,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] What exactly is a Hypergeometric and Binomial Distribution?",
        "author": "Acrobatic_Sample_352",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jnlbh/question_what_exactly_is_a_hypergeometric_and/",
        "text": "Hi guys, sorry if I’m violating any rules here as this is my first post, but I was wondering if anyone could explain to me in simple terms what exactly a Binomial Distribution and Hypergeometric Distribution are. \n\nFrom what it seems, people are telling me the latter is essentially the first but without replacement, however I’m a bit confused on what it means to be “Binomial”. \n\nDoes it just indicate that the outcome space only has two possible values?",
        "created_utc": 1678074857,
        "upvote_ratio": 1.0
    },
    {
        "title": "Want to know about scolership exams",
        "author": "_LoNe_SoLe_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jnk1i/want_to_know_about_scolership_exams/",
        "text": "I am currently studying B.Sc(Stat),I want to pursue higher studies abroad in this field after completing Masters .\nCan you give me information about exams,and scholership regarding it ?!",
        "created_utc": 1678074753,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the statistical indicator to prove that the choice of the subject wasn't due to low number of choices?",
        "author": "GladAstronomer548",
        "url": "https://www.reddit.com/r/AskStatistics/comments/11jeqcq/what_is_the_statistical_indicator_to_prove_that/",
        "text": " in a pair choosing social experiment: What is the % or amount of choices, to prove that the subjects' choice was not coerced by the lack of enough variety ?",
        "created_utc": 1678052389,
        "upvote_ratio": 1.0
    }
]