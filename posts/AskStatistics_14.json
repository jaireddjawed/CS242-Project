[
    {
        "title": "Calculating statistical power for a Cox prop-hazards regression with a single continuous predictor at varying hazard ratios.",
        "author": "SlackWi12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpxftr/calculating_statistical_power_for_a_cox/",
        "text": "I have a fixed sample with event data, I would like to calculate the statistical power I have for a Cox proportional-hazards model at varying hazard ratios with a continuous predictor variable. Anyone know how? Thanks",
        "created_utc": 1671470180,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the minimum and maximum sample size for the Anderson‚ÄìDarling test",
        "author": "Ill-Ad-106",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpx84q/what_is_the_minimum_and_maximum_sample_size_for/",
        "text": "Can we use it for as high as N=5000? Or only smaller sample sizes.\n\nAnd can we use it for N=1 or N=2?",
        "created_utc": 1671469714,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical hypothesis testing for data with non-normal distribution",
        "author": "Ill-Ad-106",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpwfi7/statistical_hypothesis_testing_for_data_with/",
        "text": "For data with non-normal distribution, what do we use for statistical hypothesis testing?¬†Do we use non-parametric statistical tests such as Wilcoxon Rank Sum and Signed Rank¬†Tests?",
        "created_utc": 1671467986,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any suggestions on methods to try when MCMC is not converging to a high log-likelihood value?",
        "author": "MundaneYard1800",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpsqk4/any_suggestions_on_methods_to_try_when_mcmc_is/",
        "text": "",
        "created_utc": 1671459332,
        "upvote_ratio": 1.0
    },
    {
        "title": "Lottery ticket with a reasonable return rate at 4%?",
        "author": "tiberiummonster",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpslp1/lottery_ticket_with_a_reasonable_return_rate_at_4/",
        "text": "So I was bored at the gas station and bought some scratch lottery tickets, then at home mapped their odds in an excel sheet and was surprised to see that at the top there was the following ticket:\n\n\\- Maximum Win: $450,000  \n\\- Probability of max. win: 1:1.2 million = 0.000083% --&gt; Estimated win: $450k / 1.2m = $0.38 (right?)  \n\\- Price of ticket: $10\n\n\\--&gt; Isn't the estimated return on this lottery ticket $0.38 / $10 = 4% ?   \n\n\nWhich is not a too bad estimated return in low-interest times (which are over pretty much, but still)?  \n\n\nWould be very grateful for any input on what the problem in my thinking is!",
        "created_utc": 1671459015,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question: I am conducting research and need stats help please",
        "author": "AmbitiousStable2666",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zppyyf/question_i_am_conducting_research_and_need_stats/",
        "text": "If I have 1 interval dependent variable and multiple (5) independent categorical variables. These IVs are things such as region, exam board, etc (educational research). The DV is the participants' score from 0-14 on a questionnaire. I want to analyse whether the IVs affect the DV in terms of whether the exam board affects the score etc. Which test would I do? Many thanks!",
        "created_utc": 1671452018,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to figure out a win likelihood",
        "author": "sullyC17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpm8hd/how_to_figure_out_a_win_likelihood/",
        "text": "Hello there!\n\nI am a bit of a math dummy. I mostly got C‚Äôs and D‚Äôs in math in high-school so forgive me if this question either doesn‚Äôt make sense or is simple.\n\nIf I have lets say 2 teams going up against each other.\n\nTeam A has won 40% of their games\n\nTeam B has one 57% of their games\n\nIs there a way to figure out who is more likely to win and by how much? \n\nMy thought was take the difference and add 50 to the team with a higher win rate so in this case team B has a 67% chance of winning but I just pulled that out of thin air.\n\nIf there is a way to figure this out I‚Äôd appreciate any help!",
        "created_utc": 1671439028,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question: How can I calculate an appropriate sample size for a multiple logistic regression?",
        "author": "copernicanrevolution",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpjkya/question_how_can_i_calculate_an_appropriate/",
        "text": " \n\nHi, can I please get some advice. \n\nI need help with making sure my sample size is large enough. Here are (briefly) the details that I think are needed:\n\nBinary response variable.\n\nTwo-tailed hypothesis.\n\nLevel of significance = 0.05\n\nPower 0.8\n\nEffect size = needs to be able to detect very small effects.\n\nTWO predictor/explanatory variables. Both are categorical.\n\n\\- X1 has 5 levels\n\n\\- X2 has 3 levels\n\nI have some data from a pilot study which gives a success probability of \\~ 0.35\n\nI do not  have an R-squared value.\n\nThe   model I will be using is a mixed effects model with two nested random   effects (I'm not sure if that is relevant to how the sample size is   calculated)\n\nAre there any other details needed to do the calculation?\n\nThanks.",
        "created_utc": 1671429555,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics question",
        "author": "tinydancer147",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpi9b3/statistics_question/",
        "text": "Hi everyone.\n\nI‚Äôm doing a stats unit for my psychology degree \n\nIf my figures are \nR= -.072*\nP = .025\n* correlation is significant at the .05 level (2-tailed) \n\nIs this correlation significant ?\n\n\nAm I correct in saying it‚Äôs a weak negative correlation ?\n\nthank you!!",
        "created_utc": 1671425313,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using Statistics to determine if there is a difference in the teams seeded 3-6 in fantasy football",
        "author": "Wutever789",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpgaus/using_statistics_to_determine_if_there_is_a/",
        "text": " \n\nI was having an argument with my friend over the seeds in fantasy football. I argued that the seeds 3 -6 were interchangeable and wanted to perform ANOVA using the points scored by each seed, each week during the regular season.\n\nMy friend pointed out that the data is not independent though. Is this a serious issue with using ANOVA for this?\n\nIf this is an issue, what process could I use to determine if there is a significant difference between the points scored for these play off seeds?",
        "created_utc": 1671419400,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best way to measure consistency between experimental trials",
        "author": "steve2118ace",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpej85/best_way_to_measure_consistency_between/",
        "text": "I'm trying to measure the consistency of data between multiple presentations of stimuli to an animal in an experiment. \n\nEach stimuli is presented 10-12 times, and during each stimuli period we collect some neural imaging data from multiple cells in the animal. The data collected is basically brightness values ranging from *-n to n*. I end up with 10-12 rows of brightness data for each stimuli-cell pairing. So, if we had ten cells, there would be ten sets of 10-12 rows. \n\nWhat I'm looking to do is look at the consistency/similarity of the 10-12 rows of data, for each pairing of stimuli and cell. So there should be a measurement for every cell-stimuli pairing letting me know whether all 10-12 presentations had similar measurements, or the measurements were random/very variable.\n\nI'm not sure if I used the correct terminology, as I haven't taken a true stats course in over five years and am trying to learn more by working on this since my mentor isn't available this week. Let me know if some examples of the data would be helpful for steering me towards a test/measurement.\n\nIf anyone could provide some resources to point me in the right direction I would appreciate it!\n\nThanks!",
        "created_utc": 1671414329,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question on adjusting a survey with published statistical analysis",
        "author": "44cprs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpcxpt/question_on_adjusting_a_survey_with_published/",
        "text": "Scenario: there's a common survey that has publications on it analyzing results. From reading academic journals, I can get the average results and st dev for each question and also in aggregate. The average of 3.2 and standard deviation of 1.15. \n\nI want to use the survey (it's in public domain) but cut the survey down from 18 to 12 questions. \n\nI have a publication showing the averages and st dev's for each question, so I can recalculate the average with the 12 questions I'll use. But I want to know the st dev of my new 12 question set in order to break results into percentiles. \n\nI used to know this, but it's been 20 years. Wondering if my logic is generally OK.\n\nThe questions each have stdev's in the 1.6ish territory. Since the 18 question set has a stdev of 1.15, I'm thinking I could use some kind of non-linear mapping of \n\nX Y\n\n1, 1.6\n\n12, ??\n\n18, 1.15\n\nI might guess somewhere around 1.18? \n\nAm I thinking about this the right way?\n\nAnother question, I got the question stdev and the total stdev from different journal publications, but I'm assuming I'm interpreting it correctly. Does everything I say here make reasonable sense?",
        "created_utc": 1671409719,
        "upvote_ratio": 1.0
    },
    {
        "title": "How is my cumulative GPA more with a B than an A?",
        "author": "Civil_End_4863",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpc6ru/how_is_my_cumulative_gpa_more_with_a_b_than_an_a/",
        "text": "I calculated my cumulative GPA between 3 different junior colleges. I just got a B in chemistry and an A in precalculus, most recently. My current cumulative GPA is 3.629.  \n\nHad I gotten an A in this recent chemistry class, my GPA would be only 3.6. \n\nThis is ironic. How is my GPA slightly higher (.029) with a B in chemistry rather than an A? \n\nIn total, I've taken 27 classes. 19 A's, 6 B's, and 2 C's. That is 98/27=3.629  Had I gotten a A in chemistry, it would have been 99/27=3.6",
        "created_utc": 1671407656,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there way to have a dummy/binary variable for a Fixed Effect model (Panel Dataset)",
        "author": "Silent-Thund3r",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zp5cgt/is_there_way_to_have_a_dummybinary_variable_for_a/",
        "text": "Hi, I have a panel dataset which consists of 3 High-Income (UK, USA, Germany) and 2 Middle-Income Countries (China and Mexico), with data spanning from 1991 to 2019. I'm trying to analyze the relationship between unemployment, infant mortality rate, fertility rate and adolescent fertility rate (unemployment is the y-variable).  On Stata, I've tried to run a Fixed effects model, with a dummy variable, as to compare coefficients between High and middle income countries, but its omitted due to collinearity.\n\nIs there a way to overcome this. What kind of variables or models can I run.",
        "created_utc": 1671389524,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are simplified inputs in SHAP, LIME?",
        "author": "eternalmathstudent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zp39k2/what_are_simplified_inputs_in_shap_lime/",
        "text": "I've been reading the original papers of a few model explainability techniques such as SHAP, LIME. I believe that I got the gist of those concepts except one thing. They mention simplified input X' corresponding to the actual input X. Could you please explain what it means for a normal tabular dataset?",
        "created_utc": 1671383512,
        "upvote_ratio": 1.0
    },
    {
        "title": "What approach is best for analysing response times?",
        "author": "_siggy__",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zp2zm9/what_approach_is_best_for_analysing_response_times/",
        "text": "I've been thinking about human response times and the distribution and I'm a bit stuck.\n\nResponse times (RT) are not normally distributed, and there seems to be some debate about which distribution fits RTs best. (Is there a clear answer to this?) In this context I'm talking only about simple RT, e.g. to make a response as quickly as possible whenever a stimulus occurs.\n\nClearly one cannot have a negative RT, so there is a lower bound of 0 ms. Often the response period will be capped as well, enforcing an upper bound (generally a second or two). Either of these negates the normal distribution (I think), and on top of this they often have a positive skew.\n\nIs it really correct to log-transform these in order to analyse them, and then continue as usual since there is some idea that RT is log-normal? This was what my friend said but I am not sure if it is accurate.\n\nAnother related question, if we were to take the median as a robust estimate, then keep doing that for all subjects in our hypothetical experiment, would the mean of medians be normally distributed?\n\nI am not interested in a specific recommendation for a test, I'm more interested in understanding how to approach the problem itself (of analysing this data with an unknown distribution).",
        "created_utc": 1671382740,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing ordinal data across multiple categories?",
        "author": "Aust-SuggestedName",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zp1wgz/comparing_ordinal_data_across_multiple_categories/",
        "text": "I have two ordinal variables of interest and I want to see whether they differ based on a factor with &gt;2 categories. What kind of analyzes exist for this purpose? Is the only option for this ordinal regression? Do I then just do ANOVA o nfeh coefficients associated with my factor?",
        "created_utc": 1671379666,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best way to explain 2 Way ANOVA vs MANOVA difference?",
        "author": "HistoryBuffLakeland",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zp1k0h/best_way_to_explain_2_way_anova_vs_manova/",
        "text": "I have a simple question. Would it be correct to define the difference between the 2 way ANOVA and a MANOVA as the follow\n\n1. 2-way ANOVA is one DV and 2 IVs\n2. MANOVA is two DVS and two IVs\n\nMany thanks in advance.",
        "created_utc": 1671378718,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does anybody have a good explanation for contrasts in repeated measures ANOVA",
        "author": "karlparker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zoyamy/does_anybody_have_a_good_explanation_for/",
        "text": "I am having some difficulty fully understanding the value r. My professor described it as the correlation between going from a certain treatment level to the next and the corresponding shift in the dependent variable. However, I don't really understand the number I get. What does it actually mean?",
        "created_utc": 1671369033,
        "upvote_ratio": 1.0
    },
    {
        "title": "Non-overlapping Subjective Ratings from Annotators - normalize, standardize, do nothing?",
        "author": "afg500",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zopzsm/nonoverlapping_subjective_ratings_from_annotators/",
        "text": "Hello community,\n\nIn short my situation is that I have a bunch of annotators establishing ratings for music performance audio files (no repetition due to low amount of annotators). Each set of files given to annotators contains X conditions under study, equally represented. Rating scale 1-10.\n\nI notice a certain degree of bias in ratings, what transformation can I apply so that I can have more comparable data? I understand there is no perfect post-processing solution but consider that **I am not interested in the absolute ratings of the performances, but rather in the relative change across conditions.**\n\nThese are the possible adjustment actions I have in mind\n\n1. Normalize each set of ratings to have min\\_rating = 0, max\\_rating = 10\n2. Standardize each annotator's ratings\n3. Encode ratings as \"deviation from median\" \n4. Include \"Evaluator ID\" as random effect in mixed models\n5. ???  \n\n\nI appreciate any input",
        "created_utc": 1671337992,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to interpret latent growth slope?",
        "author": "duckiedokie",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zomlmi/how_to_interpret_latent_growth_slope/",
        "text": "Hi everyone, \n\nI am running a latent growth curve model and found a declining growth slope (declining depression over time). Then I added in a covariate (support) which has a positive association with the slope. Am I right in interpreting that higher support is associated with more rapid decreasing slopes of depression? I saw some papers that interpreted the other way (higher support predicts slower declining slopes)\n\nMuch appreciated if you can point me to the right sources\n\nThanks",
        "created_utc": 1671327455,
        "upvote_ratio": 1.0
    },
    {
        "title": "Predictions based on a binomial looking curve?",
        "author": "ASmileyMan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zoljqc/predictions_based_on_a_binomial_looking_curve/",
        "text": "Hi there, I have a basic question but I can‚Äôt seem to think of a way to solve this.\n\nBased on past events, there is a 2/20 chance the value is -5, 3/20 chance the value is -2, 5/20 chance the value is 0, 3/20 chance the value is 2, and 2/20 chance the value is 0. What‚Äôs the probability that a 1.5 will occur?\n\nI thought this would be a binomial distribution based on the probabilities but I‚Äôm not sure that‚Äôs correct? The next part involves 2 models with similar distribution patterns, also shapes like binomial and asks to combine them to calculate the probability of an event. Any clue how to approach this?",
        "created_utc": 1671324612,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it possible to do a mediation analysis with a categorical IV, a continuous MV and three continuous DV's and two covariates?",
        "author": "PastSelect",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zo8qo7/is_it_possible_to_do_a_mediation_analysis_with_a/",
        "text": "I am currently working on my master thesis and I ran into this statistical problem. Hopefully one of you can help me, because so far I can only see that a mediation analysis with a MANCOVA isn't possible.",
        "created_utc": 1671291180,
        "upvote_ratio": 1.0
    },
    {
        "title": "What type of regression analysis to use for 1 categorical variable and 1 continuous?",
        "author": "SurgMMA",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zo3hsx/what_type_of_regression_analysis_to_use_for_1/",
        "text": "Apologies for the stupid question, I'm so confused as I've found conflicting data online.\n\nI'm trying to do regression analysis if female gender is associated with worse outcomes as measured by a scoring tool with a scale from 0-100 (0 = bad score, 100 = good score). Do I do linear or logistic regression for this?",
        "created_utc": 1671272776,
        "upvote_ratio": 1.0
    },
    {
        "title": "Discover optimal interaction terms with large impact on dependent variable in regression analysis?",
        "author": "tiko844",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zo3g3d/discover_optimal_interaction_terms_with_large/",
        "text": "Hi, I'm wondering if there are techniques or tools to determine optimal interaction terms in large set of independent variables. When implementing linear regression in e.g. R, I can set them manually and see what is the impact. But how could I determine automatically them? Thanks!",
        "created_utc": 1671272579,
        "upvote_ratio": 1.0
    },
    {
        "title": "Combining ‚Äúscreen size‚Äù and ‚Äúresolution‚Äù",
        "author": "UnluckyForSome",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zo1wp8/combining_screen_size_and_resolution/",
        "text": "Hi! üëã  stats newbie here looking for some very basic help hehe\n\nI have two variables, screen size (in inches) and resolution (in total pixels). I want to combine them to create a ‚Äúdisplay quality‚Äù score I can use for further data analysis against other variables (I know there are other factors but just for the sake of argument)‚Ä¶ What‚Äôs the best way to do this which gives equal weighting to both variables? At first I thought just multiplying them together, but apparently this won‚Äôt be a fair and equal weighting. Is my best option to subtract the mean and divide by the standard deviation for each variable?\n\nThanks!",
        "created_utc": 1671266088,
        "upvote_ratio": 1.0
    },
    {
        "title": "Distribution of the difference of two non-independent poisson distributed random variables.",
        "author": "Sti302fuso",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zo12ho/distribution_of_the_difference_of_two/",
        "text": "I know that the distribution of the difference of two independent poisson random variables follows a Skellam distribution. However, I have two poisson distributed random variables which are not independent, i.e. if one has a higher value, the expected value of the other slightly increases. \n\nWhat I get for the difference appears to be a distribution which very much resembles a Skellam distribution, but with too high kurtosis for it to be one. \n\nI wish to find the pmf and cmf of the difference. How do I start going about this?",
        "created_utc": 1671262618,
        "upvote_ratio": 1.0
    },
    {
        "title": "looking for help in these",
        "author": "Background-Buy-5267",
        "url": "https://i.redd.it/tc6k9o0wuf6a1.jpg",
        "text": "",
        "created_utc": 1671256253,
        "upvote_ratio": 1.0
    },
    {
        "title": "What's the mathematical intuition for this statement?",
        "author": "Strange-Bar8952",
        "url": "https://i.redd.it/herm6mc12f6a1.png",
        "text": "",
        "created_utc": 1671246548,
        "upvote_ratio": 1.0
    },
    {
        "title": "[R] How do i calculate the target sample size for a SURVEY",
        "author": "throwawaysadxx",
        "url": "/r/statistics/comments/znuk1t/r_how_do_i_calculate_the_target_sample_size_for_a/",
        "text": "",
        "created_utc": 1671240046,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question",
        "author": "P1atinumS0ccer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znreoc/question/",
        "text": "What test would be used to compare the size of two numbers, for example is the population of New York vs the population of Georgia? I am not sure what to use since a t test requires multiple samples",
        "created_utc": 1671230807,
        "upvote_ratio": 1.0
    },
    {
        "title": "AIC and AUROC for model comparison",
        "author": "throwaway19916026",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znqe41/aic_and_auroc_for_model_comparison/",
        "text": "Hey all,  \n  \nI'm building my first logistic regression model using a sample dataset of credit default data. Long story short, I am at the part where I am running the logistic regression.  \n  \nNow I have a two models.  \nA - very simple model with 3 variables.  \nB - Much more complicated model with ~20 variables.  \n(For context train size is ~55k obs with ~1k 'bads'.   \n  \nNow for both models, all variables are statistically significant with p values well below 0.05. However, I'm using AIC and AUROC as a means of comparing the two models. My understanding is the higher AIC, the better and same for AUROC.  \n  \nThis is where I am getting mixed signals. Model A has an AIC in excess of Model B (10000 Vs 9500) but Model B has a higher AUROC (0.82 Vs 0.74).  \n  \nWhich is better to use to interpret the two models? Also, is there any other tests which could help?  \n  \nThis is all new to me so if it seems obvious to you it probably isn't to me, I'm just looking to learn!  \n  \nAs a side note I have run a likelihood ratio test too which seems to indicate Model B is superior.  \n  \nBonus question (is that allowed?) Is there a limit to how many variables I should be using in my model? 20 seems like a lot.  \n  \nThanks!",
        "created_utc": 1671228160,
        "upvote_ratio": 1.0
    },
    {
        "title": "I am confused. This is the formula for transforming logit regression coefficients to odds ratios right? If this looks familiar to you. Sorry for my stupidity in advance",
        "author": "Holiday_Snow_2734",
        "url": "https://i.redd.it/gihyhtrpmb6a1.png",
        "text": "",
        "created_utc": 1671223146,
        "upvote_ratio": 1.0
    },
    {
        "title": "How was Bonferroni's correction formula made?",
        "author": "FruitPopsicle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znlqfb/how_was_bonferronis_correction_formula_made/",
        "text": "How did he come to the conclusion that he should divide alpha by the number of tests.",
        "created_utc": 1671215957,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I avoid p-values correction using Bootstrap instead?",
        "author": "nirvana5b",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znjhy5/can_i_avoid_pvalues_correction_using_bootstrap/",
        "text": "In a multiple comparison context it's fair to correct p-values since type I error may be inflated.\n\nBut what if I run my analysis several times resampling it with bootstrap methods, wouldn't it give me a better notion about the p-values' SEs?\n\nTnx in advance!",
        "created_utc": 1671210169,
        "upvote_ratio": 1.0
    },
    {
        "title": "Marathon stats: Gun time vs Net time",
        "author": "ChemicalWar908",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znjgzx/marathon_stats_gun_time_vs_net_time/",
        "text": "I am trying to determine if there is a difference in race results for runners in a marathon if the gun time (time the race starts to crossing the finish line) or net time (time the runner crosses the start line and finish line). \n\nAny ideas where to start? A paired t-test?",
        "created_utc": 1671210100,
        "upvote_ratio": 1.0
    },
    {
        "title": "Repeat Measures Correction After Friedman Test",
        "author": "Odd-Importance4513",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znje8k/repeat_measures_correction_after_friedman_test/",
        "text": "I did a Friedman test to determine if there were statistical differences, but I want to know where the differences are. I used a paired Wilcox rank test, but I have many ties. Is there a different test that would be recommended?",
        "created_utc": 1671209921,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Looking for a Recommendation for a Stat Methods/Design of Experiments Book",
        "author": "pmorri",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zngvug/q_looking_for_a_recommendation_for_a_stat/",
        "text": "Hello, I have my capstone statistics project next semester and need to review methods and experiments. Does anyone have a book recommendation, one that would be appropriate for a last year undergrad in Statistics.",
        "created_utc": 1671203516,
        "upvote_ratio": 1.0
    },
    {
        "title": "Analysis question",
        "author": "ariesgirly7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zngi6e/analysis_question/",
        "text": "Hi everyone,\n\nI completed a longitudinal study assessing clinician comfort, confidence, knowledge, and intent to counsel patients experiencing suicidal ideation after completing a training. I have multiple questions in my survey assessing each area. For example, there are three questions assessing comfort, three assessing confidence, three assessing intent to counsel and eight assessing knowledge. I am stuck on how to synthesize the information from each question to summarize levels of confidence, comfort, knowledge and intent to counsel at T1, T2, and T3. Can anyone provide some advice? Thanks so much.",
        "created_utc": 1671202510,
        "upvote_ratio": 1.0
    },
    {
        "title": "What method will show connections between items in a list?",
        "author": "likeanoceanankledeep",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zngeqk/what_method_will_show_connections_between_items/",
        "text": " I  am working on a project where I want to show the overlap between  recipes in a database and show which ingredients overlap or commonly  occur together. For example, focaccia bread has rosemary, salt, flour,  and water, and roasted potatoes has rosemary, salt, and potatoes, so the  link would be rosemary and salt occur together.\n\nI  am not necessarily looking for a statistical analysis here but rather a  method of graphically displaying the connections between the items in a  list. The closest thing I know of is either factor analysis, PCA, or  pathway analysis, but these show the statistical relationship between  variables. I also thought of co-occurrence analysis but I don't think  this is correct either.\n\nThanks in advance.",
        "created_utc": 1671202247,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing quantitative assays?",
        "author": "Successful_Gur_8528",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znfm3p/comparing_quantitative_assays/",
        "text": "We're replacing an assay in our lab with a new manufacturer. We have two new ones to choose from, but we need to justify our choice by retesting samples with all three assays (two new, and our old assay), and finding the one that's the best match to the old assay across the range of samples.\n\nAm I right in thinking using bland Altman plots is right for this? And what value should I be looking at to determine which is best?",
        "created_utc": 1671200016,
        "upvote_ratio": 1.0
    },
    {
        "title": "I couldn't even decide how to approach this test, anyone cares to help? I could pay too.",
        "author": "Wolv_95",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zneuou/i_couldnt_even_decide_how_to_approach_this_test/",
        "text": " So, the information that I had was a population of 120 students, where you are getting a sample of their marks (out of a 100) in a test of 10 students, their marks were 2, 33, 34, 41, 47, 57, 87, 91, 97.   \n1) What's the average of THE CLASS, is the average below 60?   \n2) A new sample of 60 people's marks is divided into quartiles with values of 30, 55, 80 and 97 what is the proportion of the student population with marks between 55 and 80? is this proportion smaller than 0.30?\n\nMy mistakes that I would like to be explained too were. First I thought it was a discrete variable because they were all integers and perfectly countable. Second I don't know what to do about the average I have a lot of ways of calculating an average, discrete, continuous, normal, not normal?, population mean with large sample, with small, population proportion with large sample...  \nand since this proportion is not large enough NP&lt;15 then is not normal, but what does it mean for me? what am I missing? I would love to read the logic behind the answers instead of just the answer, thank you.",
        "created_utc": 1671197778,
        "upvote_ratio": 1.0
    },
    {
        "title": "Non-parametric mixed-model the most appropriate?",
        "author": "abcdeze",
        "url": "https://www.reddit.com/r/AskStatistics/comments/znb8a8/nonparametric_mixedmodel_the_most_appropriate/",
        "text": "Hi, \n\nI'm having a little trouble determining whether I'm on the right track (and my biostatistician is away for the holidays!). Our retrospective cohort study is looking at surgeries conducted on patients on either low dose/high dose of a certain medication and determining whether a continuous outcome variable differs between these two groups. I have two issues. Firstly - the outcome variable is not normally distributed - OK no big deal, I'm fine with non-parametric tests. However, I've noted a number of the surgeries were conducted in the same patient in both groups (they had multiple), which means there are technically some repeated measures in the dataset that could introduce bias (but not all, the majority of the outcome data is from unique patients).\n\nAm I right in thinking we need to be using a non-parametric mixed-model test here? Any pointer in the right direction would be much appreciated.",
        "created_utc": 1671185110,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are the highlighted areas in this graph? What is this graph called?",
        "author": "OisinWard",
        "url": "https://i.redd.it/0n21r9poe86a1.png",
        "text": "",
        "created_utc": 1671184068,
        "upvote_ratio": 1.0
    },
    {
        "title": "Getting the average value of a lootbox",
        "author": "Jvc94",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zna7vq/getting_the_average_value_of_a_lootbox/",
        "text": "Hi, \n\nI'd like to ask if my calculation is correct in getting the average value of a lootbox. Thank you\n\nLootbox items\n\n* A: 10 gold\n* B: 5 gold\n* C: 1 gold\n\nBonus Item:\n\n* BB: 10 gold\n\nLootbox rules:\n\n1. Item A, B, and C has equal chances to show up in the Lootbox\n2. Lootbox has equal chance to contain 2 or 3 items  \n3. There's a 10% chance that bonus item \"BB\" is included whenever you open a Lootbox\n\n&amp;#x200B;\n\nMy formula:\n\n((A\\*(1/3) + B\\*(1/3) + C\\*(1/3))\\*2 + (A\\*(1/3) + B\\*(1/3) + C\\*(1/3))\\*3)/2 + BB\\*(1/10)\n\n= 14.33",
        "created_utc": 1671180902,
        "upvote_ratio": 1.0
    },
    {
        "title": "Null Hypothesis for Resampling Tests",
        "author": "AnAsianKiddo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zn930i/null_hypothesis_for_resampling_tests/",
        "text": "Hi everyone,\n\nIn my last stats class, we learned about a bunch of nonparametric alternatives to the t-test, like the permutation test and bootstrapping. However, I am getting a little hung up on the framing of the null hypothesis for these tests.\n\nIn most of the examples I've seen, the null and alternative hypotheses are no different from those used in the t-test: the null is equality of population means, while the alternative hypothesis is that they aren't equal/one is larger than the other. However, it seems to me that these resampling techniques operate under the assumption that the two groups being compared not only have equal means, but are drawn from the same underlying distribution. In bootstrapping, we repeatedly resample from the pooled data, and in permutation tests, our reshuffling is motivated by the null assumption that the two groups could've been arbitrarily picked from the same population. \n\nGiven this, I don't understand how it's possible to use these tests to solely assess a difference in means unless we assume the variances of the two populations are equal, which then seems contrary to the definition of a nonparametric test. It feels like the only hypothesis these resampling tests are equipped for is that of different population distributions, yet I've read that bootstrapping can be leveraged to calculate p-values for almost any summary statistic. Anyone who can help clarify this discrepancy would be greatly appreciated!",
        "created_utc": 1671176109,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistics test?",
        "author": "P1atinumS0ccer",
        "url": "https://i.redd.it/y983d9guj86a1.jpg",
        "text": "Which statistics test would I use to analyze the association between habitat and color here?",
        "created_utc": 1671167789,
        "upvote_ratio": 0.5
    },
    {
        "title": "Newbie ggplot2 issue",
        "author": "AidanRM5",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zn5b2q/newbie_ggplot2_issue/",
        "text": " \n\nHi all, I hope you can excuse a very newbie question.\n\nI will be collecting data from a repeated measures design where participants complete a survey battery at two timepoints before and after an intervention. I have calculated two further variables that are an aggregate of several items, one for each time point. So there is agg\\_t1 and agg\\_t2 for each participant.\n\nWhat I am trying to produce is a plot with the mean of each aggregate variable on the Y axis, split by time point on the X, with a line displaying the change in mean value across the two time points.\n\nMost of the documentation I've read assumes a single aggregate variable with timepoint as a factor, plotted something like this:\n\nggplot(df, aes(x=timepoint, y=agg\\_var))+  \ngeom\\_point()+  \ngeom\\_line()\n\nThis approach does not seem to work with my data. Do I need to restructure it or is there a different method I should use?\n\nHuge thanks in advance",
        "created_utc": 1671162706,
        "upvote_ratio": 1.0
    },
    {
        "title": "Jamovi: Trend line equation",
        "author": "marcthemyth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zn51ut/jamovi_trend_line_equation/",
        "text": "I've plotted a scatter plot and displayed the trend line/line of best fit for the date using the ‚Äùscatr‚Äù module, but I'm not sure how to view the actual equation for the line of best fit. Does anyone know how?",
        "created_utc": 1671161897,
        "upvote_ratio": 1.0
    },
    {
        "title": "object 'Survival' not found",
        "author": "sakhrabdelsalam",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zn0iwz/object_survival_not_found/",
        "text": "when in run this code :\n\ncox &lt;- coxph(Surv(Survival, as.numeric(Vital)), data = mydata)\n\nr shows me this error :\n\nError in Surv(Survival, as.numeric(Vital)) : object 'Survival' not found\n\nhow can I solve this problem ?",
        "created_utc": 1671148565,
        "upvote_ratio": 1.0
    },
    {
        "title": "How many possible 11 digit password can be created with only numbers?",
        "author": "skyReact",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmykqf/how_many_possible_11_digit_password_can_be/",
        "text": "",
        "created_utc": 1671144060,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this missing at random or missing not at random?",
        "author": "n23_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmu9se/is_this_missing_at_random_or_missing_not_at_random/",
        "text": "In a trial I am involved with, not all patients had their primary outcome (remission) which consisted of a few components fully measured. This was during the height of the pandemic and if all the parts of the outcome that we could assess by phone were good, we did not force patients to come in to take blood for the final component of the outcome which was a lab value. \n\nTherefore we end up with data where the component 5 is selectively missing in patients in whom components 1-4 are quite favourable. We considered this missing at random of component 5 depending on the values of component 1-4 and imputed the missing values.\n\nThe statistical reviewer (at a good enough journal that I am starting to doubt our interpretation) responds that \"missingness is selective and systematic rather than random.\" I feel like the reviewer doesn't seem to use the standard MAR/MCAR/MNAR terms here and says that the missingness isn't random, which it isn't by definition under MAR.\n\nBecause of this they want us to primarily do a complete case analysis, which IMO is biased because it excludes the patients who were doing best (and because they were mostly in the treatment group, which makes sense if a treatment actually works better than placebo, this reduces the effect size). Our pre-specified protocol also had the imputed analysis as primary for this reason. \n\nSo, do I have a point or am I just completely off here? We'll probably do as they ask anyway since its just a question of which is primary and which is secondary in the results, but I just want to understand.",
        "created_utc": 1671133877,
        "upvote_ratio": 1.0
    },
    {
        "title": "i need help with this",
        "author": "helperkh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmu27x/i_need_help_with_this/",
        "text": " if I invest 120000 every year with a 10% profit and add the 10% profit of the current year to next years  \n120000 how long will take to reach one million and 10 million",
        "created_utc": 1671133365,
        "upvote_ratio": 1.0
    },
    {
        "title": "im being told by a amateur statistician this is not \"clean\"",
        "author": "tedybear123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmtm9j/im_being_told_by_a_amateur_statistician_this_is/",
        "text": "&amp;#x200B;\n\n Is this comparison fair? because I don't think so why? 1. Does the number 95 refer to an individual? Or are you referring to couples? because if it is pairs they already started wrong the result is 47.5 , it is not closed. 2. How much time did those who did this study spend collecting this information on gay families compared to straight families? 3. Are the economic and social conditions of gay families the same as those of heterosexuals? because they are not mentioned anywhere. ***I don't think you can compare 5 against 5 million*** \n\n&amp;#x200B;\n\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6309949/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6309949/)",
        "created_utc": 1671132276,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I interpret SD and Variance correctly?",
        "author": "thedissapointedwife",
        "url": "https://i.redd.it/5y13ojybk56a1.jpg",
        "text": "",
        "created_utc": 1671131637,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Calculate standard deviation change in one dependent variable based one standard deviation move of a single independent variable - can I just run a regression on standard deviations between the independent and dependent variables?",
        "author": "collarsandchains",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmt8ox/q_calculate_standard_deviation_change_in_one/",
        "text": " \n\nCalculate standard deviation change in one dependent variable based one standard deviation move of a single independent variable - can I just run a regression on standard deviations between the independent and dependent variables?",
        "created_utc": 1671131352,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Calculate standard deviation change in one dependent variable based one standard deviation move of a single independent variable - can I just run a regression on standard deviations between the independent and dependent variables?",
        "author": "acascuse-me-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmt6jh/q_calculate_standard_deviation_change_in_one/",
        "text": "Calculate standard deviation change in one dependent variable based one standard deviation move of a single independent variable - can I just run a regression on standard deviations between the independent and dependent variables?",
        "created_utc": 1671131197,
        "upvote_ratio": 1.0
    },
    {
        "title": "Employee Reporting Patterns Help",
        "author": "AltruisticSea",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmpv2e/employee_reporting_patterns_help/",
        "text": "Not sure if this is more appropriate here or in /r/excel, but since I'm asking about the math, I thought I'd start here.\n\nI have an employee that habitually shows up late for work or calls out with little notice. I've had discussions with them previously, but it doesn't seem to be sinking in, so I hoped to provide myself and the employee with some concrete numbers by tracking when they come in.\n\nThat resulted in a dataset that looks like [this](https://imgur.com/sMMCRRi).\n\nI have a hypothesis that this employee is more likely to show up on time or before lunch on Mondays, Wednesday, and Fridays, when their friend is here. I would like to see (using statistics rather than gut feeling) if that hypothesis is true. I'd also like to see a variant where I don't include Fridays because there's another reason that the employee might show up on time on Fridays and I'd like to eliminate that variable by also only looking at Mondays and Wednesdays.\n\nWhat is the appropriate test for this kind of hypothesis and data? Can anyone help with the excel formula (I'm reasonably good with excel, but certainly not a wizard)?\n\nThank you!",
        "created_utc": 1671123057,
        "upvote_ratio": 1.0
    },
    {
        "title": "What type of data do screening tools produce?",
        "author": "Arrhythmania",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmpmk3/what_type_of_data_do_screening_tools_produce/",
        "text": "I‚Äôm using the hospital anxiety and depression scale as part of my project and want to conduct a correlation test on the results I get. However, I‚Äôm struggling to work out what type of data this would give me.\n\nMy other variable will be accuracy expressed, most likely expressed as a percentage.",
        "created_utc": 1671122484,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does it mean when a study is \"powered\" to show a 25% reduction in adverse effects",
        "author": "miamiredo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmofst/what_does_it_mean_when_a_study_is_powered_to_show/",
        "text": "",
        "created_utc": 1671119573,
        "upvote_ratio": 1.0
    },
    {
        "title": "how to find the mean of this Random varible? - is my way ok?",
        "author": "Sen_7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmleji/how_to_find_the_mean_of_this_random_varible_is_my/",
        "text": "so I have\n\nY = min(X, T/2)\n\nwhen  X\\~U(0,T) and T&gt;0 and Im supposed to find E\\[Y\\]\n\nmy idea\n\nso Y will be just \"normal\" uniform varible if X is between 0 and T/2\n\nif X is bigger than T/2 Y will just be the constant T/2\n\n&amp;#x200B;\n\nso I wanted to do and Integral from 0 to T/2 of the unifrom distribution and then from T/2 to T Integral on the constant T/2\n\nwill that be right?",
        "created_utc": 1671111872,
        "upvote_ratio": 1.0
    },
    {
        "title": "what's the name of this church",
        "author": "nabillalali97",
        "url": "https://i.redd.it/dnh3iry3s36a1.jpg",
        "text": "",
        "created_utc": 1671110036,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I account for temporal autocorrelation in a LMM when individual ID is unknown?",
        "author": "Technical-Ad6396",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmkcqt/how_do_i_account_for_temporal_autocorrelation_in/",
        "text": "Hello,\n\nI have some skin surface temperature of urban house mice collected via a thermal imaging camera. Data were collected for 4 day blocks, with a 3 day break, every week for 6 months. I am trying to test through a LMM (lme4 in R currently) whether skin temperature predicts the time, relative to sunset, that the mice leave their den to forage. Unfortunately as the individuals are not tagged I cannot use \"individual ID\" as a random effect. Would using \"date\" suffice? Or should I use Month or week? I have tried nesting date in week, but have convergence/singularity warnings. I thought about posting this question after I've saw a very similar question to this posted in another statistics sub, but they could identify individuals so temporal autocorrelation could be accounted for. \n\nAny thoughts are appreciated!",
        "created_utc": 1671108722,
        "upvote_ratio": 1.0
    },
    {
        "title": "sampling distribution of Person's R",
        "author": "2000aden2000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmiej1/sampling_distribution_of_persons_r/",
        "text": "Can somebody explain why it only takes into a account the number of subject or the number of data points to calculate how accurate the correlation is? I would guess the chance of getting the right correlation in a study with 10 people out of a population of 11 would be much higher than getting the right correlation with a study of 10 people out of a population of 1000. \nI'm sure I'm missing something so it would be great if somebody can point that out",
        "created_utc": 1671102013,
        "upvote_ratio": 1.0
    },
    {
        "title": "hello, is there a way to verify the answers regarding the confidence intervals without knowing the z table?",
        "author": "Purple-Surround2640",
        "url": "https://i.redd.it/g25sg0v6l26a1.jpg",
        "text": "",
        "created_utc": 1671095601,
        "upvote_ratio": 1.0
    },
    {
        "title": "Do I need to account for autocorrelation in paired weather data?",
        "author": "bomberbomberjayjay",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmdx8c/do_i_need_to_account_for_autocorrelation_in/",
        "text": "I'm working on a project for work where I need to establish a relationship that predicts hourly barometric pressure data at one weather station based on the pressure at another. I have \\~8000 rows of paired sequential, hourly data from station1 and station2.\n\nI know I can use a simple linear model to do the job adequately, but might I do a better job with a generalized least squares model? To be more specific, I'm a little stuck on whether or not autocorrelation matters in this case. The data is clearly a time series, and is heteroskedastic, but I'm not sure that the order of the pair matters since I'm deriving a model based with pressure on both the x and y axis, so maybe I don't need to account for autocorrelation?",
        "created_utc": 1671084720,
        "upvote_ratio": 1.0
    },
    {
        "title": "How is this interpretation of a study accurate?",
        "author": "myneighbortotohoe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmdn7q/how_is_this_interpretation_of_a_study_accurate/",
        "text": "[This](https://www.case.org/awards/circle-excellence/2021/dealing-loneliness-college-during-pandemic) interpretation says:\n\n&gt;We cited a number of recent studies that looked at the pandemic‚Äôs effect on young people‚Äôs mental health, including a survey of more than 18,000 college students in the United States conducted by the Healthy Minds Network that **found that 80 percent report that COVID-19 has negatively impacted their mental health**.\n\nIs this accurate according to pages 10 and 11 from the [HMN study](https://healthymindsnetwork.org/wp-content/uploads/2020/07/Healthy_Minds_NCHA_COVID_Survey_Report_FINAL.pdf)? Sorry if this question doesn't belong in this sub but I'm not literate in statistics.",
        "created_utc": 1671083781,
        "upvote_ratio": 1.0
    },
    {
        "title": "Please fill out this quick survey for my statistics class",
        "author": "Zhethon",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zmcgce/please_fill_out_this_quick_survey_for_my/",
        "text": "pleeeease [https://forms.gle/BcLd3i4XYZ5XXHGu9](https://forms.gle/BcLd3i4XYZ5XXHGu9)",
        "created_utc": 1671079793,
        "upvote_ratio": 1.0
    },
    {
        "title": "Risk Statistics ‚Äú100 year‚Äù events 1% annual chance",
        "author": "guitarngineer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm9gx1/risk_statistics_100_year_events_1_annual_chance/",
        "text": "If a 100-year event (rainfall, storm, fire, etc.) is also defined by a 1% annual chance, what is the probility of two 1% annual chances in a 28 year period? \n\nFor instance, if a hurricane of a certain magnitude is defined by a 1% annual chance in a certain area and it did occur what is the probability a hurricane of same magnitude happens again in a 28 yr period?",
        "created_utc": 1671070700,
        "upvote_ratio": 1.0
    },
    {
        "title": "Poker odds question",
        "author": "toraai117",
        "url": "https://i.redd.it/tf39j59w906a1.jpg",
        "text": "I saw a poker hand today and I‚Äôm curious what the odds are.\n\nSo here is the basic scenario:\nYou draw 9 cards from a standard 52 card deck, out of those 9 cards, there is a 2 of kind, 3 of a kind, and 4 of a kind.\nThat may be an oversimplification, especially since there was likely more than just two players at the table who received cards, plus cards are burnt periodically, however, I‚Äôm curious what the odds of this standoff occurring.\n\nI attached a picture for reference.\n\nPlease delete if not appropriate for this sub. Im just curious and don‚Äôt have the time or energy to learn this myself (I‚Äôm lazy)\n\nThanks",
        "created_utc": 1671067589,
        "upvote_ratio": 1.0
    },
    {
        "title": "I tried to verify if this estimator of the mean is correct, efficient and consistent, did i proceed in the right way?",
        "author": "Gendobus99",
        "url": "https://www.reddit.com/gallery/zm7zhn",
        "text": "",
        "created_utc": 1671066569,
        "upvote_ratio": 1.0
    },
    {
        "title": "SEM lavaan with categorical endogenous variable",
        "author": "Dragonfruit-4855",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm6vcq/sem_lavaan_with_categorical_endogenous_variable/",
        "text": "Hi!\n\nI'm estimating a path model in lavaan with a mediating variable that is binary. I have binary and continuous exogenous variables. I have more variables, but this is a simplified version of my syntax in lavaan:\n\npath\\_model = '\n\nln\\_A \\~ b1\\*ln\\_B + b2\\*ln\\_C + b3\\*D + b4\\*E + b5\\*F\n\nD \\~ b6\\*E + b7\\*F\n\n\\# indirect effects\n\nindirect\\_E := b3\\*b6\n\nindirect\\_F := b3\\*b7\n\n\\# total effects\n\ntotal\\_E := b4 + (b3\\*b6)\n\ntotal\\_F := b5 + (b3\\*b7)\n\n\\# Fit the models\n\nfit = sem(path\\_model, data = df, ordered = \"D\")\n\nBecause D is ordered, lavaan uses the DWLS estimator. I wanted to add covariances between the exogenous variables, but I get an error message when I try to do it in R. It seems that it's because I have an ordered variable which makes fixed.x = TRUE and conditional.x = TRUE. I tried changing to fixed.x = FALSE, but then I also have to change to conditional.x = FALSE. When I do this I get a negative variance, so I know that the estimation is incorrect. Two variables are highly collinear (because one is an interaction term of the other variable), so at first I thought that multicollinearity might be the issue. But even when I remove the two collinear variables I get the same error message.\n\nI would highly appreciate help!\n\nThis is the error message that I get in R when I try to add covariances:\n\nError in tmp\\[cbind(REP$row\\[idx\\], REP$col\\[idx\\])\\] &lt;- lavpartable$free\\[idx\\] :\n\nNAs are not allowed in subscripted assignments\n\nIn addition: Warning message:\n\nIn lav\\_partable\\_vnames(FLAT, \"ov.x\", warn = TRUE) : lavaan WARNING:\n\nmodel syntax contains variance/covariance/intercept formulas\n\ninvolving (an) exogenous variable(s): \\[B C\\]; These variables\n\nwill now be treated as random introducing additional free\n\nparameters. If you wish to treat those variables as fixed, remove\n\nthese formulas from the model syntax. Otherwise, consider adding\n\nthe fixed.x = FALSE option.",
        "created_utc": 1671063552,
        "upvote_ratio": 1.0
    },
    {
        "title": "How viable would a statistics major be if I‚Äôm bad at math?",
        "author": "Euthyphros_father",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm6q8x/how_viable_would_a_statistics_major_be_if_im_bad/",
        "text": "I‚Äôm a second-year student, and after taking a few statistics courses (The ‚Äú101‚Äù and Introductory Econometrics), I‚Äôve discovered that I‚Äôve been having a lot of fun with them. Playing around with R is super interesting, and learning about its framework and statistical concepts gets better and better every day. I‚Äôve never felt this way about something before, and I‚Äôd dive into it if I wasn‚Äôt so bad at math. \n\nAs for math, I‚Äôve never seriously studied it until this year. High school math was pretty manageable, and I didn‚Äôt have to do much to do well in my college‚Äôs Calculus 1 class. With Calculus II this year, though, I‚Äôm struggling quite a bit (around an average class score of a B+ where grades are curved to a B/B+)‚ÄîI have no problem with the concepts, but I can‚Äôt seem to translate that understanding into test scores. I make lots of errors here and there due to my carelessness, and I blank even on problems similar to ones I‚Äôve solved easily. It‚Äôs extremely frustrating, and it‚Äôs making think that I‚Äôm not cut out for statistics. \n\nFor people who work with/study statistics at a high level, what has your relationship with math been like? Is it normal to struggle this much with math?",
        "created_utc": 1671063177,
        "upvote_ratio": 1.0
    },
    {
        "title": "Testing for curvilinearity and interaction in multiple regression",
        "author": "stats-help-lol",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm5mt9/testing_for_curvilinearity_and_interaction_in/",
        "text": "When testing for curvilinearity and interaction in a multiple regression do I include all my IV's in my regression or just the ones relevant to the interaction term? And if I include all of them do I just assume values for them when graphing to test for curvilinearity?\n\nHere they are if it helps  \nDependent: \n\npartyid: 0-6, 0 strong dem, 6 strong rep\n\nIndependent:\n\nage: 18-89 scale\n\nmale: 0 or 1\n\nwhite: 0 or 1\n\nmarried: 0 or 1\n\nincome: 1-12\n\nThanks!",
        "created_utc": 1671060325,
        "upvote_ratio": 1.0
    },
    {
        "title": "Am I employable with my degree?",
        "author": "OrganicBluebird9464",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm3nrd/am_i_employable_with_my_degree/",
        "text": "I have a bachelors in Statistics with a minor in math and computer science. Would I be able to find good jobs without a masters or am I screwed?",
        "created_utc": 1671055461,
        "upvote_ratio": 1.0
    },
    {
        "title": "Selecting an appropriate test for image response comparison",
        "author": "little_fall_of_rain",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm2qxd/selecting_an_appropriate_test_for_image_response/",
        "text": "  \n\nI am trying to see if image characteristics impact how different image subjects are viewed by survey respondents.  \n\nEach survey respondent will be shown a set of 6 images, each of the 6 images featuring a different subject but with the same visual context (i.e., each respondent is exposed to only one visual context condition, but all of the different subjects pictured in that condition). They will be asked a series of questions about the 6 images. \n\nThe visual context for each set will be based on the interaction of 4 different backgrounds and the presence or absence of a specific object‚Äîso, 8 different potential visual contexts in total. \n\nThis is just for student-level research, so my sample size will likely be small (\\~30-50 respondents).\n\nI am trying to figure out what kinds of statistical analyses would be appropriate to determine if visual context impacts viewers' perception of each subject. For example, if I have survey takers respond via a Likert-type scale to a statement like ‚ÄúI trust the pictured individual‚Äù for each of the 6 images they see, and want to know if (1) subjects in certain visual contexts were generally perceived as more trustworthy and (2) if certain subjects were perceived as more trustworthy regardless of visual context. \n\nI have seen ordinal logistic regressions suggested for similar research questions, but I am not sure if this is best and I am getting confused about how this would apply to a mixed design like mine.\n\nMy program has no related instruction/resources available, so any help is appreciated!",
        "created_utc": 1671053276,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can dependent predictors be included in a logistic regression estimating propensity scores?",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm27tf/can_dependent_predictors_be_included_in_a/",
        "text": "E.g., a measurement at baseline and at 24 hours, which is expected to be highly correlated?\n\n\n\nIf the treatment and outcome both have high SMD for both the baseline and 24 hr measurement, and I want to only include true confounders (that are associated with both outcome and treatment) in my ps model. Should I include both the baseline and 24 hour measurement in my ps model or only the baseline measurement?",
        "created_utc": 1671051988,
        "upvote_ratio": 1.0
    },
    {
        "title": "If this is the distribution of how far a ball will roll, how do I find the probability it will go from one number to another. For example what is the chance it will continue from 20 to 30",
        "author": "Calebkeller2",
        "url": "https://i.redd.it/a4kg0wlhwy5a1.jpg",
        "text": "",
        "created_utc": 1671050973,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the advantage of building a score based on linear regression vs structural components?",
        "author": "Naj_md",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zm1nl3/what_is_the_advantage_of_building_a_score_based/",
        "text": "I was reading this article, [https://pubmed.ncbi.nlm.nih.gov/18945283/](https://pubmed.ncbi.nlm.nih.gov/18945283/) which made a score for transplantation outcomes prediction. Instead of using structural component analysis (SCA), they used logistic regression to predict 3-month outcomes. Based on OR direction, they would assign a number to compose an overall score. How is this different from SCA, and will it ever be better (simplicity?) \n\n&amp;#x200B;\n\nThanks",
        "created_utc": 1671050575,
        "upvote_ratio": 1.0
    },
    {
        "title": "HELP. Hypothesis testing 1-8?",
        "author": "StudentGrad2025",
        "url": "https://i.redd.it/jft1yx7hly5a1.jpg",
        "text": "Hey, this is not homework. I am studying for my final, but I don‚Äôt know how to do this. Can you help me with steps 1-8?",
        "created_utc": 1671047271,
        "upvote_ratio": 1.0
    },
    {
        "title": "I'm a UI/UX designer &amp; I'm learning about A/B Testing Statistics, I have question regarding z-tests",
        "author": "cronerd",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zlzhrn/im_a_uiux_designer_im_learning_about_ab_testing/",
        "text": "So, based on my understanding, z-tests use variance in their equation in a 2-sample test. But how does one the variance when you don't have access to all the data. Example:\n\nVisitors on Page A (Sample Size) = 5000\n\nAverage Transaction Value (mean) on Page A = $50\n\n\\-\n\nVisitors on Page B (Sample Size) = 5000\n\nAverage Transaction Value (mean) on Page B = $60\n\n&amp;#x200B;\n\nIn this case, I want to see if the Average Transaction Value on Page B is significantly more than Page A.  (confidence = 95%). How do I find out the z-value considering I don't know the variance at all??\n\n&amp;#x200B;\n\nPS. I don't have a mathematical background, so I'm really sorry if my question is stupid",
        "created_utc": 1671045092,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to use a post-stratification weight on a dummy variable",
        "author": "Specialk3533",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zlwzly/how_to_use_a_poststratification_weight_on_a_dummy/",
        "text": "Hello,\n\nI'm currently working on a datafile that comes with post-stratification weights, but I am struggling to put them to use.\n\nI understand the theory behind weighting, and I think I'd also know how to use the weights in some cases. For example, if all respondents gave their opinion on some issue on a scale from 1-10, the unstratified mean would simply be the sum of all responses divided by n. But for a \"true\" or truer mean, I'd multiply each individual response by each individual's weight, and then calculate the mean. Please correct me if I'm already wrong at this point.\n\nBut what I want to do now is something different. I have three items that I use to assign a dummy variable to individuals. The items probe support for a generous welfare state, and for those individuals who express a strong opinion (i.e. agree or disagree strongly) consistently across these three items, I create a new variable that assigns them a \"1\", otherwise a \"0\". There are then two of these dummy variables, one for those strongly in favor of generous social policy, one for those against. Then I want to calculate the proportions of individuals with a \"1\" in the whole sample, by country, by education, by employment status, etc.\n\nBut I don't know what to do with the weight now. All individuals with a \"0\" would still have a zero after multiplication with the weight, whereas the \"1\"s would then have their weight as their value. Is the post-stratified proportion of individuals with quality \"1\" then simple the mean of that corrected dummy variable?\n\nThanks in advance!",
        "created_utc": 1671039125,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] How to report Bayes factor results?",
        "author": "named999",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zlvw6l/q_how_to_report_bayes_factor_results/",
        "text": "I am writing a paper and in order to see if I can support the null hypothesis I ran a bayes regression in R. the bayes factor is 0.05743579  ¬±0%  but I do not how to properly report this, is just \"BF10=0.06\" or is there another thing that  should be added? Can someone give an example or send me a paper with bayes factor reports ?",
        "created_utc": 1671036539,
        "upvote_ratio": 1.0
    },
    {
        "title": "Political science student in need of help from a statistician",
        "author": "fabbe25",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zlu7e7/political_science_student_in_need_of_help_from_a/",
        "text": "Hello Reddit! I am a political science student from Sweden writing a paper using statistical methods. Since I am not that well versed in statistical methods, I am having some troubles understanding the results.\n\nIn this paper I am comparing people's trust in institutions and trust in other people before and after the pandemic. The data is based on a survey where people gave a score to their trust in institutions and their trust in other people on a scale from 1-10.¬†\n\nTo analyze this I used SPSS. I first compared means which showed me that trust in other people, after the pandemic, increased from 4.79 to 5.01 while trust in institutions decreased from 4.25 to 4.20. After this I did a correlation analysis between trust in other people and institutional trust. Before the pandemic the correlation between the two was 0.272\\*\\* and after the pandemic the correlation was 0.371\\*\\*.¬†\n\nSo the thing I have trouble understanding is how the correlation increased after the pandemic. If institutional trust decreased and trust in people increased, wouldn‚Äôt that lead to the correlation decreasing, since the different types of trust are going in different directions. Would someone please help explain this to me.\n\nThanks in advance.",
        "created_utc": 1671032416,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Evaluating bond yields via Excel regression outputs, looking for stats advice on this topic, general brushing up, and life advice. Already deleted facebook, lawyered up, and hit the gym.",
        "author": "acascuse-me",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zlr0z2/q_evaluating_bond_yields_via_excel_regression/",
        "text": " \n\nFor context: I'm an analyst who took Stats101 a decade ago and is now being pushed to do statistical analysis. I would appreciate any help/guidance.\n\n0) Are there any particularly good reference texts for brushing up on stats, particularly simple two variable regressions and also multivariable regressions?\n\n1. Is excel respectable for this use case or should I be using R (or an alternative?)\n2. With respect to the Excel regression output appended, would p-values for the dependent and intercept use the respective dependent and intercept t-values and degrees of freedom values?\n3. What is the difference between F and Significance F? Is one of them critical F or am I just remembering the wrong things?\n4. From a statistical perspective if you are looking at bond yields/how they influence each other what metrics would you focus on? Are they included below, are some irrelevant in your view, and are there others you would include?\n\nThank you.\n\n**Excel's regression output gives the following**\n\nIndependent Variable Name\n\nDependent Variable Name\n\nK\n\nBlanks \\[Dates Pulled-Observations\\]\n\nMultiple\\_R\n\nR\\_Square\n\nAdjusted\\_R\\_Square\n\nRegression-Standard Error\n\nObservations\n\nRegression-dF\n\nRegression-SSE\n\nRegression-MSE\n\nRegression-F Value\n\nRegression-F Critical\n\nResidual-dF\n\nResidual-SSE\n\nResidual-MSE\n\nTotal-dF\n\nTotal-SSE\n\nIntercept-Coefficient\n\nIntercept-Standard Error\n\nIntercept-tStat\n\nIntercept-pValue\n\nIntercept-Coefficient (-95%)\n\nIntercept-Coefficient (+95%)\n\nDependent-Coefficient\n\nDependent-Standard Error\n\nDependent-tStat\n\nDependent-pValue-\n\nDependent-Coefficient (-95%)\n\nDependent-Coefficient (+95%)",
        "created_utc": 1671024739,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are infinite series ever useful in representing probability distributions?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zlejbd/are_infinite_series_ever_useful_in_representing/",
        "text": "Specifically, I was thinking about how, in mathematics, a large class of non-elementary functions can be represented as an infinite sum of either polynomials (via Taylor series) or trig/complex exponential functions (via Fourier series and the Fourier transform).  When I took a course on probability theory this past summer, we spent some time on convolution integrals and related integral transforms that made certain calculations a variable's PDF/CDF easier (can't remember the terminology for the type of transforms) which seemed quite analogous to using the Fourier and Laplace transforms to convert between the time and frequency domains in electrical engineering.  \n\nSo now I'm wondering, do we ever try to represent empirical distributions (i.e. the distribution of an actual data set that doesn't match any of the common distributions) as infinite sums of the common distributions (e.g. normal distribution, exponential, poisson, etc.)?",
        "created_utc": 1670984075,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question regarding: semi-partial correlations",
        "author": "kungfu_baku",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zldy2b/question_regarding_semipartial_correlations/",
        "text": "Can you perform a semi-partial correlation on baseline corrected data while controlling for the baseline measurements?\nI have 2 data sets. One is %change from baseline in proteins and the other is change in subjective effects. So can I perform a semi-partial correlation on these datasets while controlling for baseline measurements in the subjective effects?\nWould I be double correcting the subjective effects?\nThank you in advance. :)",
        "created_utc": 1670982517,
        "upvote_ratio": 1.0
    },
    {
        "title": "Standardizing ordinal variables",
        "author": "Fair-One3754",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zld31e/standardizing_ordinal_variables/",
        "text": "Hello all‚ÄîI am looking for statistical books or literature that discusses standardizing ordinal variables. I have two variables that are ordinal in nature (Julian date and the days since I started my surveys) and I‚Äôm trying to find sources that support or refute scaling ordinal variables. Thank you in advance for the help!",
        "created_utc": 1670980265,
        "upvote_ratio": 1.0
    },
    {
        "title": "If something is indexed for inflation twice a year (CPI) will it be higher after a year than if it was once a year?",
        "author": "Reporter3874",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zlbvcy/if_something_is_indexed_for_inflation_twice_a/",
        "text": "",
        "created_utc": 1670977181,
        "upvote_ratio": 1.0
    },
    {
        "title": "Find a distribution that built entirely from data points",
        "author": "HavenAWilliams",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl6dry/find_a_distribution_that_built_entirely_from_data/",
        "text": "Hello,\n\nI'm pretty sure I'm asking an ML question, but I was wondering what tools I should be using to get a distribution from data that is not described by any popular distribution.\n\nThe data are bounded between 0 and 1. There is essentially a \"nike swoosh\" where output values are high toward the low end, there's a trough toward the middle of the distribution, and another peak closer to 1. I want to conduct point estimations to predict where the output value is going to be based on where my data lies--between that 0 and 1--but I don't know what test to use for that.\n\nThank you!",
        "created_utc": 1670964240,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you calculate the correlation between a correlation value (Kendall‚Äôs Tau) and a ordinal variable (explanation in text)",
        "author": "VeressNeedle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl617w/can_you_calculate_the_correlation_between_a/",
        "text": "Hi there,\n\nI hope I can make this question make sense as trying to find an answer online has been difficult. \n\nI have a survey that basically  determines how an individual would rank 10 values in order of personal importance. I want to compare the similarity of an individual‚Äôs ranking of their values with the way an organization ranks these same values. Initially, I am using Kendall‚Äôs tau to determine this. \n\n\nBut what I really want to do is compare the resulting Kendall‚Äôs tau for each individual to their overall ranking on a selection list (#1 is the top pick, down to about 70 people.)\n\nMy question is: could you use spearman‚Äôs P to assess the correlation between the Kendall‚Äôs tau value and increasing position on the selection list? (Correlation of a correlation measure[kendall‚Äôs tau] and rank position?)\n\nIs there a better way to do this?\n\nI am very grateful in advance for any pointers/suggestions.",
        "created_utc": 1670963415,
        "upvote_ratio": 1.0
    },
    {
        "title": "Method of Moments and Max Likelihood Estimators",
        "author": "ImprovementLower8475",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl5uzd/method_of_moments_and_max_likelihood_estimators/",
        "text": "can someone explain to me, like i'm a 5 year old, what method of moments and max likelyhood estimators are + how to find them? thanks üôè",
        "created_utc": 1670963003,
        "upvote_ratio": 1.0
    },
    {
        "title": "Scipy Mann-Whitney Function Results Interpretation",
        "author": "AioilPGrBacce",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl5tiq/scipy_mannwhitney_function_results_interpretation/",
        "text": "I am performing a Mann-Whitney test in a dataset from molecular  simulations with \\~770 variables. I am comparing a group with 4 samples  versus a group with 8 samples. For a two-sided level of significance  Œ±=0.05, my U critical is 4. However, Scipy *mannwhithneyu* function is returning U values of 4 with p value &gt; 0.05:\n\n|Index|Min U Value|P Value|\n|:-|:-|:-|\n|458|0|0.00847480189215383|\n|485|0|0.00824673789986071|\n|657|0|0.00802184705268784|\n|61|2|0.0202627693866874|\n|135|2|0.0207132850335062|\n|168|2|0.0174305985159386|\n|366|2|0.0216256142161174|\n|624|2|0.0218560090983137|\n|718|2|0.0216256142161174|\n|113|3|0.0337522229559557|\n|216|3|0.0337522229559557|\n|220|3|0.0337522229559557|\n|375|3|0.0337522229559557|\n|459|3|0.0337522229559557|\n|578|3|0.0337522229559557|\n|620|3|0.0337522229559557|\n|634|3|0.0337522229559557|\n|665|3|0.0337522229559557|\n|0|3.5|0.0411841631553894|\n|513|3.5|0.0411841631553894|\n|682|3.5|0.0366275349095895|\n|125|4|0.0507985207388399|\n|179|4|0.0507985207388399|\n|404|4|0.0103556732947481|\n|428|4|0.0499907406064739|\n|541|4|0.0499907406064739|\n|544|4|0.0507985207388399|\n|632|4|0.0507985207388399|\n|677|4|0.0507985207388399|\n|205|4.5|0.0612713952988295|\n|215|4.5|0.0567468164893814|\n|597|4.5|0.0262142426679836|\n\nSo, I am questioning if I understood correctly the meaning of U value  and p value that Scipy function returns. How does the function calculate  the p value? Is the p value already the answer for the question \"Is  group 1 different from group 2?\"? If that's the correct interpretation,  why is It returning p values &gt; 0,05 for U values &lt;= U critical?  How a U value &gt; U critical can return a p value &lt; 0,05 (index  597)?",
        "created_utc": 1670962903,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would radical life extension be possible with a VR brain chip, that alters our perception of time so that every second felt like a year in VR?",
        "author": "RattyRusty1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl5kdk/would_radical_life_extension_be_possible_with_a/",
        "text": "So instead of extending life by increasing the amount of years we live, we reduce the rate at which we perceive time (whilst in a virtually simulated reality from a sensory chip within our brain)... Is this at all possible?",
        "created_utc": 1670962308,
        "upvote_ratio": 1.0
    },
    {
        "title": "Null and Alternative Hypothesis Question",
        "author": "Educational-House382",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl52o8/null_and_alternative_hypothesis_question/",
        "text": "Hi everyone, I am having trouble trying to figure out what the null hypothesis would be in this case. I know it should be no change but I'm not exactly sure what it would be for this. Below is the research question. I'd appreciate any suggestions\n\nThe federal limit for lead in soil in the United States is 400 ppm. Some rubber crumb materials (like that used in playground) have tested above the federal limit. Variables to use: avgPb (a quantitative variable) Does the mean amount of lead in rubber playground surfaces in Boston exceed the federal limit of 400 ppm?",
        "created_utc": 1670961144,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to I add the mean to my histogram?",
        "author": "Educational-House382",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl4c7e/how_to_i_add_the_mean_to_my_histogram/",
        "text": "Hi everyone, I cant seem to figure out how to add the mean to my histogram below is my coding for my histogram. I know the mean should be about 19 with my data but i tried a few solutions i found online and it was putting it in, in the negatives. any help would be great\n\nhist(playgrounds\\_new$avgPb\\[playgrounds\\_new$SampleType==\"rubber\"\\])",
        "created_utc": 1670959383,
        "upvote_ratio": 1.0
    },
    {
        "title": "Retrain a model when testing on a subset?",
        "author": "YourWelcomeOrMine",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl2590/retrain_a_model_when_testing_on_a_subset/",
        "text": "I  am training a model based on keystroke timing features. My raw data is a set of keystrokes. As a feature example, if 4 keystrokes have pauses of  1, 2, 3, and 4 seconds, then a feature would be ‚Äúmean pause,‚Äù which  would be 2.5 in this case.\n\nHowever,  I want to see how well a model based on a random subset of original  data performs as compared to the original model. So in this case I might  only sample the keystrokes with 1 and 3 second pauses, making my ‚Äúmean  pause‚Äù = 2.\n\nIn both cases, full  and random subset, I have the same number of cases, and the same feature  set. The only difference is the data that the features are based on.\n\nIn  order to compare the two models, should I also retrain a model on the  subsetted data, or should I test the subset using the same parameters as  the original model?",
        "created_utc": 1670954283,
        "upvote_ratio": 1.0
    },
    {
        "title": "toy manufacturers",
        "author": "SDK24SCP",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zl1k2h/toy_manufacturers/",
        "text": "Hi! I need a statistic about the toy manufacturing companies like a top 20 or top 50 by popularity",
        "created_utc": 1670952916,
        "upvote_ratio": 1.0
    },
    {
        "title": "Regression Models: Negative Slope with Categorical Variables",
        "author": "Just-aLittleStitious",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zkzu4z/regression_models_negative_slope_with_categorical/",
        "text": "I've been trying to interpret the effects of specific predictors in multiple linear regression models, but seem to be hitting a wall with categorical variables. If the slope of one predictor, gender for example, is negative, what would this tell you with respect to the response?\n\nI'd greatly appreciate any insight.",
        "created_utc": 1670948935,
        "upvote_ratio": 1.0
    },
    {
        "title": "Illumina: can I use it on my laptop?",
        "author": "Adorable-Ear-947",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zkziqh/illumina_can_i_use_it_on_my_laptop/",
        "text": "We had RNA seq data which had been processed for alignment and TPM generation using DRAGEN pipeline from Illumina.\n\nNow we have more data from a few additional samples, and my prof asked me if I can process the FastQ files into ReadCounts on my laptop itself? I tried finding but couldn‚Äôt figure it out, can I use Illumina on my laptop? (As I‚Äôd need data processing homogeneity I‚Äôd have to process it using Illumina itself).\n\nSorry for the basic question, I‚Äôm new to this field. Thank you for your help.",
        "created_utc": 1670948209,
        "upvote_ratio": 1.0
    }
]