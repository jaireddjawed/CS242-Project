[
    {
        "title": "A random sample of",
        "author": "Tiniest_ATINY",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7f2ym/a_random_sample_of/",
        "text": " \n\nA random sample of 82 African American students has finished an average of 13.5 years of\n\nformal education with a standard deviation of 1.7 years of formal training. The US national\n\naverage is 12.4 years.\n\n&amp;#x200B;\n\nIs the z score 5.86? Is it statistically significant if the confidence level is 95%?",
        "created_utc": 1669684667,
        "upvote_ratio": 1.0
    },
    {
        "title": "What exactly is Mechanistic Analysis, and what statistical tools does it involve?",
        "author": "MaBrowser",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7b4ce/what_exactly_is_mechanistic_analysis_and_what/",
        "text": "My understanding is that this type of analysis is usually applied by big data scientists, in circumstances that necessitate accuracy and very little room for mistakes (e.g. the medical industry). It's used to for measuring the exact changes in variables that lead to other changes in other variables.\n\nI guess I just don't understand how is this any different to Predictive Analysis? What type of statistical tools / methods would this type of analysis use?\n\nThanks",
        "created_utc": 1669675194,
        "upvote_ratio": 1.0
    },
    {
        "title": "Paired t-test with uneven variable size",
        "author": "Mountain-Wrongdoer-8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z7awuk/paired_ttest_with_uneven_variable_size/",
        "text": "I’m working on a project to figure out if student scores started improving after joining a program. \n\nBasically I have exam scores per month and I know the date they joined the program. \nI thought I would calculate the average score before the join date and the average score after the join date and run a paired t-test but the issue is that not every student has data for each month. \n\nFor example, in some cases theres only data on one exam before the join date but multiple exams after. \n\nIn some cases theres only one month of data after but multiple months of data before.\n\nThe goal is to figure out if joining the program improved scores but i’m not so sure if comparing averages would be the best idea.\n\nAny guidance would be appreciated.",
        "created_utc": 1669674719,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about adding additional values to Rolling Average in projections",
        "author": "ForeignConcept4823",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z79uyt/question_about_adding_additional_values_to/",
        "text": "Hi all,\n\nFirst I want to make clear that I do not have a background in stats so apologies ahead of time for my explanation…\n\nIn my role I was tasked with making a projections table for 2023 using rolling historical data from the past 12 months. I am getting feedback about building in another table that will add additional customers to the projections ahead of time. For example, a stakeholder anticipates that a customer will go-live in March of 2023 and add 85 more clients to our projections so that additional table adds 85 more customers to the current March 2023 projection.\n\nIt was brought to my attention that this method may result in double counting so to speak down the line. Would that be the case? And if so, how do I add them ahead of time without double counting? Would a weighted average help?",
        "created_utc": 1669672449,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics Help!! Please!!!",
        "author": "regressedintofreud",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z79prq/statistics_help_please/",
        "text": "Hi all! \n\nFor my project, I am attempting to understand the correlation between parenting styles (Likert), current negative mental health (Likert), and psychological treatment seeking behaviors. \n\nI have tried several tests already through SPSS, but I am not quite sure of what test to use. \n\nAny help would be greatly appreciated!!!",
        "created_utc": 1669672140,
        "upvote_ratio": 1.0
    },
    {
        "title": "Kaplan-Meier plot for 2 groups with different follow-up periods",
        "author": "matzoh_ball",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z785xb/kaplanmeier_plot_for_2_groups_with_different/",
        "text": "I have two groups of people. Group A was treated in year 1 and the Group B was treated in year 2. I have data for each group until the end of year 4, which means I have a longer time period for  Group A than for Group B.\n\nI want to create a sensible Kaplan Meier plot that shows the survival curves for each group, but I'm not sure what time cutoff I'm supposed to use. \n\nBasically, I'm considering the following options: \n\n1. I calculate survival time for everyone for as long as possible (i.e. for up to 4 years for Group A and up to 3 years for Group B), and then I use that survival-time variable to create the Kaplan-Meier plot, cutting the plot off at 2 years for both groups. \n2. I calculate survival time for everyone for 2 years from treatment, which means I censor Group A cases more than Group B (since I have more data regarding survival times for Group A). Then I create a Kaplan-Meier plot, cutting it off at 2 years for both groups. \n\nThe first plot will show a survival curve for Group A that does not go to 0% by the end of the time period. The second plot, in contrast, shows survival curves that go to 0% for both groups. \n\nWhich plot is \"better\" (i.e. more informative, less misleading, etc.)?",
        "created_utc": 1669668805,
        "upvote_ratio": 1.0
    },
    {
        "title": "how to interpret this mediation result?",
        "author": "thedarkb1ue",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z77btu/how_to_interpret_this_mediation_result/",
        "text": "So I have three variables:\n\nx: Autonomy (lower scores indicate higher autonomy and higher scores indicate low autonomy)\n\nmediator : burnout (low scores indicate low burnout and high score high burnout)\n\nY : Job satisfaction ( high scores indicate high satisfaction )\n\ni ran a mediation analysis and this was what I found (all significant)\n\nAutonomy to burnout (-0.8266) \n\nburnout to job satisfaction (-0.3649) \n\nautonomy to job satisfaction (total -1.004, direct -0.8266) \n\nthe indirect relationship was significant\n\nHow do I interpret these results? I think I'm confused by the directions\n\nthank you",
        "created_utc": 1669666991,
        "upvote_ratio": 1.0
    },
    {
        "title": "In which order should I take the Khan Academy statistics courses?",
        "author": "Science_is_Greatness",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z76dtq/in_which_order_should_i_take_the_khan_academy/",
        "text": "Is the correct order from the three courses available:\n\n1. High school statistics\n\n2. Statistics &amp; Probability\n\n3. AP College Statistics",
        "created_utc": 1669664959,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I interpret these Johansen Cointegration Test results?",
        "author": "JAPersonalTraining",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z756t3/how_do_i_interpret_these_johansen_cointegration/",
        "text": "Firstly, thank you to u/Efrique for help in my last post finding this route.\n\nI have run this test to support my correlation in mean compensation data over 10 years against EPS for the S&amp;P 500. The test, based on the description, matches exactly what I need to support my Spearman's rank correlation test results.\n\nThe issue I'm having is that the results come out mostly true depending on the years and I'm uncertain I'm interpreting them right.\n\nI would assume that there IS cointegration based on 2/3 cases being true dependent on the timeframe.\n\nI performed the test using the NumXL addon for excel.\n\nhttps://preview.redd.it/2urfq0zrpq2a1.png?width=891&amp;format=png&amp;auto=webp&amp;s=66ef7f46f6464e6e0bfa4c1982ad5567450f5395",
        "created_utc": 1669662338,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can i use the same sample for 3 different correlations?",
        "author": "Prize-Candidate7187",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z73t9h/can_i_use_the_same_sample_for_3_different/",
        "text": "Hello everyone, i am a student and i am conducting a psychology study. I have 4 variables and 3 hypotheses. H1: variable a is correlated with variable b. H2: variable a is correlated with variable c. H3: variable a is correlated with variable d. I plan on doing independent pearson correlations (bivariate) for each of these hypotheses. But I can't decide if I can do it in the same sample size. Should I use a different sample for each variable that is correlated with variable a? Or can I use the same sample? I can't determine sample size because of this problem too. Please help.",
        "created_utc": 1669659350,
        "upvote_ratio": 1.0
    },
    {
        "title": "Beginner, need guidance on what I need and how to do it in excel",
        "author": "dummythicccx",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z721us/beginner_need_guidance_on_what_i_need_and_how_to/",
        "text": "\nSorry I am a beginner with stats, I took some classes in college but I’m not sure what I need here. I have likert scales on questionnaires on 2 groups of people, one who is diagnosed with a particular syndrome and a control group (my controls are 32 and my positive are 25-28 depending on the questionnaire). I have excel, and I want to figure out what variables between the 2 groups are significant and then with the positive patients find out if there are any correlating answers between the questionnaires. Can someone walk me through what I need to be looking for and any good applicable resources? \nThanks",
        "created_utc": 1669655427,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question is in body.",
        "author": "ZealousidealRatio475",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6x6w2/question_is_in_body/",
        "text": "the distribution of SAT mathematics scores for college-bound male seniors in 2016 has a mean of 524 and a standard deviation of 126. The distribution of SAT mathematics scores for college-bound female seniors in 2016 has a mean of 494 and a standard deviation of 116. One male and one female are randomly selected. Assume their scores are independent. (Source: The College Board)\n41. What is the average sum of their scores? What is the average difference of their scores?\n42. What is the standard deviation of the difference of their scores?",
        "created_utc": 1669643675,
        "upvote_ratio": 1.0
    },
    {
        "title": "The starting lineup for a baseball team in an international match is 12 players. How many different batting order are possible using the starting lineup?",
        "author": "ZealousidealRatio475",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6x1mv/the_starting_lineup_for_a_baseball_team_in_an/",
        "text": "I’m going to be honest, I don’t watch baseball and don’t know what batting orders are but I would guess the answer is 12 factorial.",
        "created_utc": 1669643286,
        "upvote_ratio": 1.0
    },
    {
        "title": "Small sample-size to large sample estimation?",
        "author": "ripanarapakeka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6vz18/small_samplesize_to_large_sample_estimation/",
        "text": "Hi!\n\nI'm looking for a method to somehow emulate a larger number of samples from a small number. The example I was given is something like:\n\nLet's say I have some infinite population. I'd like a CI of 95%, a 5% margin of error with 20.000 random samples. However, it's impractical to take 20.000 samples, so we were looking to get 1.000 samples instead. Can we estimate the population mean and standard deviation as if we had 20.000 samples?\n\nI was told that there is a method to do this, but was given no more information... and I was hoping that someone on here might either explain how to get to this OR know where the appropriate place to ask this is.\n\nI'm assuming there is a diminishing returns of sorts here: if I have 1.000 samples, 100 extra ones will have a larger impact than if I had 20.000 and added an extra 100.\n\nI understand that this is somewhat small information to work on, but I hope someone more statistically enlightened could share some insights on it. TIA!",
        "created_utc": 1669640418,
        "upvote_ratio": 1.0
    },
    {
        "title": "Estimating Sample Size for Binary Logistic Regression Study",
        "author": "GogaReborn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6vht3/estimating_sample_size_for_binary_logistic/",
        "text": "Hi, in conducting a case-control study with binary outcome I have one main independent variable and say 9 co-variates in my mind to account for confounding. Making total independent variables to 10.\n\nNow calculating sample size for a regression model.\n\n1) How do I measure the effect size of regression model from relevant studies? How to measure the effect size of each variable in regression model generally?\n\n2) I decide on a clinically significant effect size and a power of 80% how do I calculate the required sample size? Forexample I decide on a hypothetical small effect size, 80% power and calculate sample size, but that would be for \"one\" covariate? So I need a minimum of that number \"positive\" for each co-variate? Forexample one co-variate is rare? Onle occurs once or twice? I can't include that then?\n\nA little guidance would be much appreciated. Thankyou.",
        "created_utc": 1669639108,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mixed effect regressions and individual quitting",
        "author": "UnusualTranslator319",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6u1vd/mixed_effect_regressions_and_individual_quitting/",
        "text": "Hi, I am running a mixed effect regression on the main determinants of CEO remuneration during the pandemic. \nThe regression follows this basic formula; total remuneration = A + B1*performance + B2*tenure + B3*firm size… + (1|Company) + (1|Year) + (1|Sector) ; where (1| Random factor)\n\nI have been asked how many ceo quit during the pandemic and if this would affect my main findings. What would you think it would be the answer? I don’t think it would affect mixed effect regression under the assumption that we still must to have 1 CEO in the firm.",
        "created_utc": 1669634643,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you use Spearman's Rho for ordinal data and discrete data?",
        "author": "xenos97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6t64r/can_you_use_spearmans_rho_for_ordinal_data_and/",
        "text": "We are going to be using a Likert scale with school grades (98, 99, 87, etc.) being the dependent variable. Is it ok to use Spearman's? If so, how?",
        "created_utc": 1669631523,
        "upvote_ratio": 1.0
    },
    {
        "title": "Literally so confused, please help with this t-test.",
        "author": "No_Acanthocephala510",
        "url": "https://i.redd.it/hx9l2iy7so2a1.jpg",
        "text": "",
        "created_utc": 1669620925,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Is there a way to compute for standard deviation using only the mean difference and sample size, n, without the data points?",
        "author": "astrophantom2001",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6pqhg/q_is_there_a_way_to_compute_for_standard/",
        "text": "Hi, I was unfortunately not given the complete data set for the papers that I will be using for a meta-analysis, and only have the data for the mean, mean difference, interquartile range, and median. Is there a way for me to compute for the standard deviation using any of these available data?\n\nThanks!",
        "created_utc": 1669619251,
        "upvote_ratio": 1.0
    },
    {
        "title": "Testing for heteroskedasticity, non-normal residuals",
        "author": "ZBlckMamba",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6ok63/testing_for_heteroskedasticity_nonnormal_residuals/",
        "text": "I did a linear regression on these two variables - exchange rate and balance of trade.\n\ni have got my residuals and would want to test for heteroskedasticity of these residuals. However, it isn't normally distributed, so I can't use the  Breusch-Pagan Test.\n\n&amp;#x200B;\n\nWhat test should I use?\n\n&amp;#x200B;\n\nThank you!",
        "created_utc": 1669615390,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mediation analysis",
        "author": "CharleneBae",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6n6gq/mediation_analysis/",
        "text": "Is it possible to use mediation analysis when: \n\nIndependent variable: continuous\nMediating variable: categorical\nDependent variable: continuous",
        "created_utc": 1669611185,
        "upvote_ratio": 1.0
    },
    {
        "title": "This is driving me crazy!!!! Why does R studio give me a different value than my calculator, when calculating natural logarithm of the odds?",
        "author": "Gorillasdontshave",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6lxzn/this_is_driving_me_crazy_why_does_r_studio_give/",
        "text": "I am trying to convert probabilities to log odds in R, using the \"qlogis\" function. However, when I carry out the calculations by hand I get a different value. This is driving me nuts. Can anyone explain or point out what I'm doing wrong?\n\nUsing R:\n\n\\&gt; qlogis(0.077)\n\n\\[1\\] -2.483824\n\nBy hand:\n\nlog(0.077/1-0.077) = -1.078",
        "created_utc": 1669607458,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which test should I use to compare Control experimental data to Test experimental data when there is a variable involved?",
        "author": "Tiedtomythoughts",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6ltst/which_test_should_i_use_to_compare_control/",
        "text": " \n\nI am confused about my experimental data. I studied a bit of statistics a while ago (and found it confusing). I want to compare the Control experiment to test experiment.\n\nI am testing a new piece of instrument and want to determine if this piece impacts the background emission of peaks at three different wavelengths. I will test the emission for each peak individually, so a same statistical test will be used for each of the three peaks separately. I have two sets of data, one for control experiment and one for test experiment. The control experiment was conducted for 5 different integration times (20ms, 40ms, 60ms, 80ms and 100ms) and 3 trials were conducted for each integration time. For each trial, the emission intensity of the OH peak was recorded (along with others). Same goes for the test experiment. I inserted the new part into the same (existing) instrument and conducted the same experiment. So, there are two variables - Control Vs Test and the Integration time.\n\nArranging the data on the table, the average for each on integration time will be on the rows and the Control/test will be in two columns.\n\nI was thinking about using the Paired T-test but I am not sure if this will give a good result as the integration time increases sequentially (down the column). Please suggest which test should I use. \n\nThanks.",
        "created_utc": 1669607121,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics about Egypt",
        "author": "alyhassan10",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6kyw9/statistics_about_egypt/",
        "text": "I am trying to create a small academic project about readers in Egypt.\n\nAnd it seems very difficult to find any statistics about how, what people in egypt like to read.\n\nThank you in Advance :)",
        "created_utc": 1669604560,
        "upvote_ratio": 1.0
    },
    {
        "title": "if i haver the average occurance of something happening in a day of the week what distribution do i use to calculate if it happens more or less than average",
        "author": "ForeskinPenisEnvy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6jayk/if_i_haver_the_average_occurance_of_something/",
        "text": "binomial right?\n\nsuppose I fart once a day on average... whats the likeliehood i fart 3 times a day on wednesday\n\nWhat formula would I use",
        "created_utc": 1669599791,
        "upvote_ratio": 1.0
    },
    {
        "title": "is this probability situation possible in r?",
        "author": "Happy_Bahman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6ifbx/is_this_probability_situation_possible_in_r/",
        "text": " So suppose I have an auto repair company, more accidents are registered in May and June than the rest of the year. monthly average is say 4 accidents, what kind of probability formula would I use to find the probability of 7 accidents in may? or no more than 3 accidents will occur in May and June combined? does the fact that may and June have more accidents come into the formula or is this useless information?  Also, is there an easier way to solve in r",
        "created_utc": 1669597424,
        "upvote_ratio": 1.0
    },
    {
        "title": "please help",
        "author": "Infinite_Trash2884",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6ie1j/please_help/",
        "text": "I will pay someone to complete the quiz and discussion I have due tonight 11/27 by 11pm central time with a passing grade.I will also pay someone to finish the final two weeks of the class with a passing grade of a C. I have a 68% D and it is going to cost me so much more if I fail. my mental health has hit an all time low because of this class and I cannot do it. I don't have the capacity to understand it",
        "created_utc": 1669597327,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why does T-test and ANOVA gives me the same results for my data set!",
        "author": "Aggravating_Hope2390",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6hszg/why_does_ttest_and_anova_gives_me_the_same/",
        "text": "Just exploring some data sets for supplementary reading and currently so confused!",
        "created_utc": 1669595753,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate the probability of the product of several dice rolls being over a specific value.",
        "author": "Yerland",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6hits/how_to_calculate_the_probability_of_the_product/",
        "text": "Funnily enough I'm applying this to a tabletop rpg game. We've got an ability which does d12*d12... damage, with the number of d12s equal to how many turns the ability is active. I'm trying to figure out the risk of the damage passing certain danger thresholds on certain turns. Unfortunately I haven't done much math in the past decade and this has proven a tad difficult to quickly learn via Google.",
        "created_utc": 1669594990,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bias of KDE and taylor series.",
        "author": "MySecret44Account3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6g3x6/bias_of_kde_and_taylor_series/",
        "text": "Hello, \n\nI had a question about bias of kernel density estimator and using taylor series/mean value theorem. \n\nYou can see the question here: [https://stats.stackexchange.com/questions/597137/kernel-density-estimator-misunderstanding-in-taylor-series-and-the-bias-of-kde](https://stats.stackexchange.com/questions/597137/kernel-density-estimator-misunderstanding-in-taylor-series-and-the-bias-of-kde)\n\n&amp;#x200B;\n\nThank you.",
        "created_utc": 1669591387,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why does chi square test give the following p values?",
        "author": "EmergentPhysics",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6frhy/why_does_chi_square_test_give_the_following_p/",
        "text": "Let say you have column A:92,169,239 and column B:90,165,245.\n\nIf you calculated chi square test, you would get p value around 0.93. Incredibly high, considering there is an obvious relationship.\n\nYet on the other hand, column A:152,138,210 and column B:243,147,110 would give you a p&lt;0.001, even though there is no obvious relationship.\n\nWhy? What am I misunderstanding? Shouldn't the first one have very low p value, while the second one should have a fairly high p value?",
        "created_utc": 1669590549,
        "upvote_ratio": 1.0
    },
    {
        "title": "Differences between sexes",
        "author": "cchhiicchhaarriittoo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6ey60/differences_between_sexes/",
        "text": "Hi,\nFor comparing differences between Males and Females, for example with a shitty hypothesis “Males in the study will be taller than females in the study”\n\nI can see that I can use both Independent sample t-test, but also one-way ANOVA.\n\nIs one more preferred than the other or does it not really matter?\n\nCheers",
        "created_utc": 1669588598,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can we estimate type 1 error rates in a field’s journal by the reproducibility of rate of its articles?",
        "author": "ragold",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6dpu5/can_we_estimate_type_1_error_rates_in_a_fields/",
        "text": "Let’s say psych journal X uses a p standard of .05 but only 50% of randomly selected published studies can be reproduced. Can we say anything about the rate of type 1 errors?",
        "created_utc": 1669585780,
        "upvote_ratio": 1.0
    },
    {
        "title": "just some questions",
        "author": "Low-Love-479",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6c7rh/just_some_questions/",
        "text": "Me and my friend are disagreeing on this problem on our take home test, we've already tried looking them up, no results. I think the first one is A and the second is E, she thinks it's C and A. Please help if you can, here are the problems:\n\nIn a certain region in Africa, 94 percent of the elephants mate for life. Suppose a group of 30 elephants from the region are selected at random. Let the random variable C represent the number of elephants in the sample who do not mate for life. Random variable C follows a binomial distribution with a mean of 4.8. Which of the following is the best interpretation of the mean?\n\n\n(A) For all groups of 30 elephants, the average number of elephants who do not mate for life is 4.8. \n\n(B) Every group of 30 elephants will have 4.8 elephants who mate for life.\n\n(C) Every group of 30 elephants will have 4.8 elephants who do not mate for life.\n\n(D) On average, 4.8 elephants are selected until finding one who mates for life. \n\n(E) On average, 4.8 elephants are selected until finding one who mates for life.\n\n\nThe probability of winning a game at Main Event is 0.7. If a person wins 80 percent or more of the games in a series of n games, the player gets a free hour of bowling. If the possible choices for n are 20, 50, and 100, which value of n should the player choose in order to maximize the probability of winning a prize?\n\n(A)n-20 only\n\n(B) n=50 only\n\n(C)n-100 only\n\n(D)n-20 or n=50 only; the probabilities are the same\n\n (E) n=20, 50, or 100; the probabilities are the same",
        "created_utc": 1669582265,
        "upvote_ratio": 1.0
    },
    {
        "title": "Basic tool for correlation",
        "author": "88080808088",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6bhev/basic_tool_for_correlation/",
        "text": "Hello. I've been tracking 15 of my daily habits as part of a self-improvement strategy. Each day there's a binary yes/no response for each one. I was hoping there is a simple analysis I could perform that would show me which of these habits correlate most with each other. I have little knowledge of statistics so I was hoping someone could recommend me a quick and dirty tool that would perform this. Thank you!",
        "created_utc": 1669580504,
        "upvote_ratio": 1.0
    },
    {
        "title": "Finding Level of Measurement and Central Tendency",
        "author": "Haskz-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6ba01/finding_level_of_measurement_and_central_tendency/",
        "text": "Hi all,\n\nI'm interested in analyzing a question from the Canadian CES to find its central tendency and have run into a bit of trouble. The question: \"On average, how much time do you usually spend watching, reading, and listening to the news each day? Would you say ...\"❍ 1 None❍ 2 1 to 10 minutes❍ 3 11 to 30 minutes❍ 4 31 to 60 minutes❍ 5 Between 1 and 2 hours❍ 6 More than 2 hours❑ -8 Refused❑ -9 Don't know\n\nI know this should be a ratio level variable with a central tendency being the mean of 3.88, however Im finding difficulty in understandings what the 3.88 means in terms of numeric value. What would be the central tendency and level of measurement for this question? I thought ratio because of the presence of an absolute zero being the\"none\" response category\n\nThanks for the help",
        "created_utc": 1669580028,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you decide which data to exclude before analyzing a dataset?",
        "author": "DrLeftCrRight",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6ahwy/how_do_you_decide_which_data_to_exclude_before/",
        "text": "There's a website called PSN Profiles. PlayStation has a system for scoring different types of trophies against each other, and this website maintains a leaderboard for its accounts. I wanted to take this data and measure how much more work it would take to go up one percent in the rankings. Before doing that, it's obvious to me that not all of the data is good. Out of the roughly 31,000 pages of accounts, the first 1,000 pages have four or less trophies. Is there a general rule for deciding how much to cut out? I would assume it's anything beyond 2 SDs, but that 1,000 pages is already beyond 2 SDs to the left (I think).",
        "created_utc": 1669578229,
        "upvote_ratio": 1.0
    },
    {
        "title": "When is E(X) = 0",
        "author": "Easy-Echidna-7497",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6a0xr/when_is_ex_0/",
        "text": "Title. I don't think I've encountered a p.m.f where the expectation is 0. Does anyone have an example? I'm really curious.",
        "created_utc": 1669577103,
        "upvote_ratio": 1.0
    },
    {
        "title": "Finding estimator of a function of parameters",
        "author": "Practical_Kangaroo_1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z692ju/finding_estimator_of_a_function_of_parameters/",
        "text": "Hi. If I have the coefficient of variation (theta=sigma/mu) and I want to obtain its estimator (by method of moments and MLE) what would be the correct process to do that? My first thought was to obtain mu and sigma by method of moments, so theta hat would be sigma\\_mm/mu\\_mm. And do the equivalent for MLE. I am not sure if this is valid, or if it's valid for both approaches (moments and mle). Thank you. :)",
        "created_utc": 1669574874,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why isn't instrumental variables estimation more popular (among non-Economists)?",
        "author": "Various_Ad_1067d",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z68g9y/why_isnt_instrumental_variables_estimation_more/",
        "text": "",
        "created_utc": 1669573409,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help needed",
        "author": "Haiado",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z68ehi/help_needed/",
        "text": "Hello. I would like to ask for advice.\n\nI am trying to do an OLS model, trying to see relationships between different variables of countries of EU(27 countries). However, not all variables are from the same year, for example GDP is from 2021 but health statistic is from year 2020 and number of cigarette smokers is from 2019.\n\nIs this a problem? Does it pose an obstacle or am I good if I mention it in my work?",
        "created_utc": 1669573289,
        "upvote_ratio": 1.0
    },
    {
        "title": "Distribution to use for events in a computer application error log",
        "author": "Early-Evening-Soup",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z67eu5/distribution_to_use_for_events_in_a_computer/",
        "text": "I am working on a system to find abnormal stuff in a computer application error log. The log has thousands of log entries per hour. A lot of it is harmless noise so it’s hard to see when actual bad things are happening. \n\nSo what I want to do is track how often each error happens (e.g. 20 “A” errors and 50 “B” errors a day) to get my distribution of what happens normally. And then I can see when something abnormal starts happening (e.g. new code gets released and all the sudden we have 40 “A” errors per day). \n\nThis seems like poisson? But I’m confused about the histogram shape vs the means I am seeing. For example I get an error with a mean of over 100 occurrences per day. But most days it has no errors or very few and then there  are a few days with thousands so the histogram looks like poisson with a very low mean like .5 or 1 (see purple line in link below). \n\nhttps://calcworkshop.com/wp-content/uploads/poisson-distribution-curve.png\n\nThis is probably because certain areas of the app get hit heavily on certain days and not on others. I’m guessing this breaks the poisson assumption of things occurring at random in the interval….\n\nSo any suggestions on how I could do this?",
        "created_utc": 1669570988,
        "upvote_ratio": 1.0
    },
    {
        "title": "A statistics question",
        "author": "pathology_mcqs",
        "url": "https://i.redd.it/yefu1bd5ck2a1.jpg",
        "text": "",
        "created_utc": 1669567387,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it A conditional logit model or THE conditional logit model? Is there a standard model or are there variations?",
        "author": "miliseconds",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z65481/is_it_a_conditional_logit_model_or_the/",
        "text": "Can I write \"**A** conditional logit model (CL) and **a** random parameter logit model (RPL) were used for estimations in the analysis of....\"?  \n\n\nOr is it a standard model/method without variations?",
        "created_utc": 1669565661,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the T-Test appropriate when looking at the relationship between Compensation and Company Earnings-Per-Share?",
        "author": "JAPersonalTraining",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z650tz/is_the_ttest_appropriate_when_looking_at_the/",
        "text": "I am currently completing a study for my master's dissertation and I am at a slight loss of where to go from here (this test goes beyond the requirements of my course but I am pushing to do well).\n\nI have found Pearson's Correlation Coefficient of 0.95 which is fantastic between the yearly Mean of EPS and Compensation for all companies in the S&amp;P 500 but need a statistical analysis test to back this correlation up.\n\nMy plan is to use the T-Test, but I think I may have misunderstood its purpose in this situation. The question I am trying to answer is whether EPS can be used as a performance indicator for CEOs when compared to their compensation.\n\nWould the T-Test help back this up? Or is there a more appropriate test?\n\nAn example of the data is below.\n\n|Year|Mean Compensation|Mean EPS|\n|:-|:-|:-|\n|2012| $9,662,636.80  |$2.66|\n|2013| $10,669,936.73  |$3.37|\n|2014| $11,337,638.80   |$3.65|\n|2015|$11,116,841.62   |$3.51|\n|2016|$11,988,157.68     |$3.93|\n|2017|$12,150,536.92   |$4.64|\n|2018|$13,617,045.04   |$5.42|\n|2019|$13,506,279.79   |$6.21|\n|2020|$13,998,486.83   |$4.71|\n|2021|$16,588,185.54|$8.06|",
        "created_utc": 1669565436,
        "upvote_ratio": 1.0
    },
    {
        "title": "Basketball game combinations/permutation",
        "author": "Aggravating-Bit-2404",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z6342t/basketball_game_combinationspermutation/",
        "text": "Hi all, \n\nLooking to get some guidance on a simple question.\n\nIf there are 6 basketball games, each with 7 players on the court at once - how many total combinations would there be?\n\nFor example -  the Brooklyn Portland game the 7 players would be (Nurkic, Durant, Grant, Irving, Claxton, Simons, Simmons). That is one game, between two teams. The remaining 5 games would also have 7 players. \n\nPlease let me know the total amount of combos and how it’d be figured out. Thank you.",
        "created_utc": 1669560825,
        "upvote_ratio": 1.0
    },
    {
        "title": "Index of Moderated Mediation Missing on SPSS Output",
        "author": "dkmaustin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z62cjj/index_of_moderated_mediation_missing_on_spss/",
        "text": "Hi Guys - I am using Hayes Models 76 and 92 and the output from SPSS does not list an index? Is there a reason for that? Don't I need to report it?",
        "created_utc": 1669558785,
        "upvote_ratio": 1.0
    },
    {
        "title": "Asking Statistical treatment",
        "author": "CharleneBae",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5ztcs/asking_statistical_treatment/",
        "text": "Asking Statistical treatment\n\nI am confuse because our campus statistician said it's not possible to correlate three variables. \n\nI would like to ask if there's a statistical treatment should we use when I want to know significant relationship of 2 continuous variable with respect to their demographic profile (sex, age). \n\nI hope you can help me..",
        "created_utc": 1669551350,
        "upvote_ratio": 1.0
    },
    {
        "title": "What test should I use?",
        "author": "Jekyll2003",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5yh9r/what_test_should_i_use/",
        "text": "I have a group of students who I've tested once before applying a certain study method and twice 1 month after applying said method. Should I use paired t test or two way anova without replication?",
        "created_utc": 1669546831,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the standard error for sample proportion correct in this study?",
        "author": "GogaReborn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5woin/is_the_standard_error_for_sample_proportion/",
        "text": "Hi, I'm following a study that used the NSDUH dataset to find the prevalence of cannabis use among pregnant women in the US.\n\nThe study includes 2069 pregnant women, The prevalence found in the study is 5.9%. the standard error written in the study is 0.5 (I'll attach an image maybe I'm interpreting wrong)\n\nWhen I calculate the standard error. I get:\n\nVariance = 0.059 (1-0.059) = 0.0555\n\nS.D = square root (variance) = 0.235\n\nStandard error = S.D / square root (n) = 0.235/ square root (2069) = 0.00518\n\nthat not even a minor difference 0.5 vs 0.00518? maybe they just multiplied by 100? but that's not correct is it? if they wanted to change it to relative standard error they would divide it by the prevalence and multiply by 100? am I wrong here? \n\n&amp;#x200B;\n\nhttps://preview.redd.it/3d3mx4u8og2a1.png?width=794&amp;format=png&amp;auto=webp&amp;s=eb6b1bc40e64714c5d712995ff9ce3569cd15272",
        "created_utc": 1669540764,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this standard error of a proportion calculation correct in this article?",
        "author": "GogaReborn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5vry6/is_this_standard_error_of_a_proportion/",
        "text": "Hi, I'm following an article that used the NSDUH data set to find the prevalence of cannabis among pregnant women in the US.\n\n&amp;#x200B;\n\nThe total pregnant women in the study are 2069. The prevalence found is 5.9% and the article writes standard error in brackets as (0.5)\n\n&amp;#x200B;\n\nhowever, when I try to calculate standard error myself. i.e variance = p(1-p) = 0.59(1-0.59) = 0.2419\n\nthen Standard deviation = square root (variance) = 0.4918\n\nand standard error = std deviation/ square root (n) = 0.4918 / square root (2069) = 0.01\n\nAm i wrong here? or is the reference article wrong? The article has a standard error written for every proportion yet none of the ones I calculate matches with theirs and they have not mentioned how they calculated those standard errors in materials and methods.",
        "created_utc": 1669537528,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trying to analyse profit/loss in an 'all you can eat' buffet",
        "author": "nick__2440",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5u2ih/trying_to_analyse_profitloss_in_an_all_you_can/",
        "text": "Hi, I am trying to model a typical situation in a buffet where people can come in and eat any amount of food they want, for a fixed price. My assumptions:\n\n* Number of people eating per day is Poisson distributed, with mean 200: X \\~ Po(r = 200)\n* The value of the food (in $) taken per customer at the buffet is Normally distributed with mean 14.5 and standard deviation 2.5: Y \\~ N(m = 14.5, s\\^2 = 6.25)\n* The price of the buffet per person is set at $18 (fixed): p = 18.\n\nI am trying to work out the mean and variance in the profits taken per day, which I would then use to maybe change to a more suitable price using confidence intervals etc. My approach was was follows:\n\n\\~\\~\\~\n\nvalue of food taken per day, C = (people per day) \\* (value of food per person) = X \\* Y\n\nincome from people per day, I = (people per day) \\* (price of buffet) = p \\* X\n\nprofits per day, P = I - C = p \\* X - X \\* Y\n\nexpected profits: E\\[P\\] = E\\[pX - XY\\] = p E\\[X\\] - E\\[X\\] E\\[Y\\]   (X and Y are independent)\n\n= pr - rc = (p - m)r = $700  (seems reasonable)\n\nvariance in profits: Var\\[P\\] = Var\\[pX - XY\\] = Var\\[pX\\] + Var\\[XY\\] - 2 Cov\\[pX, XY\\]\n\n= p\\^2 Var\\[X\\] + (Var\\[X\\] + E\\[X\\]\\^2)(Var\\[Y\\] + E\\[Y\\]\\^2) - 2p Var\\[X\\] E\\[Y\\]\n\n= p\\^2 r + r(r + 1)(s\\^2 + m\\^2) - 2prm = 8663700 (dollars squared)\n\nThis variance seems totally unreasonable, implying a std dev of $2943, which, assuming the profit distribution is normal (I know it is not, but it should still be kind of close), gives about 40% chance of losing profit. This seems way too high.\n\n\\~\\~\\~\n\nTo check the answers, I ran a simulation in Python:\n\n`from scipy.stats import norm, poisson`  \n`import numpy as np`\n\n`p = 18 # price of buffet (per customer)                            # p`  \n`r = 200 # mean number of people arriving per day                    # E(X), Var(X)`  \n`m = 14.5 # mean cost of the buffet taken (per customer)              # E(Y)`  \n`s2 = 2.5 ** 2 # variance of the cost of the buffet taken (per customer)   # Var(Y)`\n\n`def profit_day():`  \n `num_customers = poisson.rvs(mu=r)`  \n `loss_day = sum(norm.rvs(loc=m, scale=np.sqrt(s2), size=num_customers))`  \n `income_day = num_customers * p`  \n `return income_day - loss_day`\n\n`samples = np.array([profit_day() for _ in range(50000)])`  \n`print(f'Simulation: mean profit = {np.mean(samples)}, variance profit = {np.var(samples)}')`  \n`mean_profit = (p - m) * r`  \n`var_profit = p ** 2 * r + r * (1 + r) * (s2 + m ** 2) - 2 * p * m * r`  \n`print(f'Theoretical: mean profit = {mean_profit}, variance profit = {var_profit}')`\n\nThe simulation produces a much more reasonable number in my opinion, and I do think it is correct because when considering the case of s = 0 (each person takes a fixed amount of food), the variance is clearly just (p - m)\\^2 \\* r, which matches the simulation but not my theoretical model.\n\nCan anyone help to see what's gone wrong in my theory? Thankyou.",
        "created_utc": 1669531500,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using a t-test where some values fall exactly on the \"cutoff\" between groups?",
        "author": "grnmtgrl",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5tcjh/using_a_ttest_where_some_values_fall_exactly_on/",
        "text": "Hello statisticians, I would like to run a two-tailed independent samples t-test. I am comparing fitness level percentile to hours exercised per week and would like to see if there is a difference in mean hours exercised between a \"high fit\" and \"low fit\" group. I want to use 50th percentile as my \"cutoff\" between the groups (the range is 10-99th percentile). My problem is that I have some data with 50 as their exact percentile.\n\nIs a t-test appropriate here? What group do I put the 50th percentile data into, high or low? Neither? Both? Choose a different cutoff? (44.5th percentile is the distance between 10 and 99 and no data have that value).\n\nThank you!",
        "created_utc": 1669529052,
        "upvote_ratio": 1.0
    },
    {
        "title": "Suggestions for MS Statistics programs in Canada.",
        "author": "VastDragonfruit847",
        "url": "/r/gradadmissions/comments/z5t04g/suggestions_for_ms_statistics_programs_in_canada/",
        "text": "",
        "created_utc": 1669528219,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trying to statistically prove that after a certain year, management of a surgical complication was conservative depending on size",
        "author": "Front_Employee_3088",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5q52s/trying_to_statistically_prove_that_after_a/",
        "text": "Hello everyone, a bit in over my head with regards to this so hoping for some guidance. Apologies in advance for elementary questions\n\nI have a 100 patient sample size. 33 from before 2000, 67 from after 2000. Of the 33, 20 had a complication that was &lt;2cm in size, while for the 67 after 2000, 36 had a complication &lt;2cm in size. \n\nThe 20 smaller ones had 17 operated on, while the 36 had only 10 that were operated on - while guidelines have not been written on this, physicians are trending towards conservative management of smaller complications. \n\nWhat stats analysis would I use on this? I have it broken into &lt;1cm, 1-1.99cm, 2-2.99cm, 3-3.99cm, 4+cm and Pre-2000 vs. Post-2000. Wasn't sure of t-test vs. Chi vs Z-test vs. f-test for this... had done this all a long time ago and it's a bit over my head now. Would me dof be n-1 = 100-1 or would it be a different dof for each size (one for the 33 patients pre-2000 and one other for the 67 post-2000?)\n\n&amp;#x200B;\n\nOpen to any recs and pointers! Thanks!",
        "created_utc": 1669519153,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a difference between order of differentiation and order of integration?",
        "author": "No-Policy3368",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5ptbl/is_there_a_difference_between_order_of/",
        "text": "An undergrad here working on his thesis that will use an error correction model, cointegration. Next week, I will report my findings to my adviser.\n\nAccording to the Engle Granger Representation Theorem I would need variables to be stationary at the same order of integration. I have two variables: energy demand, and GDP (not to be too specific).\n\nWhen I run the test in Python (this is the only stat software I am familiar with) to change the order of integration using np.cumsum(). The graph smoothens and when tested with ADF the p-value got worse. But when I used np.diff() to turn the data to first order difference, the graph became more stationary and passed ADF test. Most importantly, with coint() that uses engle-granger approach, the null hypothesis was rejected, and the variables are said to be cointegrated. The OLS was great as well, significant, and R-Squares are not that high.\n\nI am having doubts because the method I used was engle-grangers and it specifically states same order of integration. Am I on the right path here?",
        "created_utc": 1669518194,
        "upvote_ratio": 1.0
    },
    {
        "title": "Given the odds of something happening in one year, how do I extrapolate that to different time periods?",
        "author": "jamie5892",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5odlz/given_the_odds_of_something_happening_in_one_year/",
        "text": "If something has a .32% chance of happening each year, what are the odds it will happen at least once over the course of ten years? Or 100? Or even a day? And what's the formula for figuring this out?\n\nThank you.",
        "created_utc": 1669514001,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can effect size be smaller than the minimum effect size detectable by study (for given power, sample size, alpha) still have a significant p-value [Beginner Question]",
        "author": "GogaReborn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5buow/can_effect_size_be_smaller_than_the_minimum/",
        "text": "The question says it all, I'm referring to both univariate and multivariate analysis if that helps\n\nIf for example, for my sample size, power and alpha value, for aa independent sample t-test, I find that the minimum detectable effect size is 0.5 or up.\n\nIs there a possibility that I get a significant p value between two groups but with effect size less than 0.5? if I do, do i have to reject that?\n\nSorry if it is a stupid question, I think its just a yes/no answer?",
        "created_utc": 1669481042,
        "upvote_ratio": 1.0
    },
    {
        "title": "I am having hard time understanding random variables.",
        "author": "sanju_3108",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z5broq/i_am_having_hard_time_understanding_random/",
        "text": "Can anyone recommend a book or online lecture where it is explained in layman terms. Any help will be highly appreciated.",
        "created_utc": 1669480825,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does last line mean?",
        "author": "Suspicious-Tea-6914",
        "url": "https://i.redd.it/lz7gf120hb2a1.png",
        "text": "",
        "created_utc": 1669477768,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is t test appropriate here?",
        "author": "Such_Cranberry5751",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z59oia/is_t_test_appropriate_here/",
        "text": "Hi guys, I am just starting biostatistics as a nursing student and doing an analysis on a survey I did. I am very noob and looking for expert advice. Just wanted to know if I am wrong doing t test here for significance testing.\n\n-I have a survey of ordinal data (always, sometimes, never ) asking staff about different activities related to wellbeing at work (do you get enough breaks, do you get breakfast, do you get toilet break when you need it... Etc) then I have 2 groups of doctors and nurses that I compare. \n-I am only familiar with t test so far and I wanted to test significance only for the always outcome and left the sometimes and never.\n-So I made 2 columns one for doctors and one for nurses. The values were the percentage of each group (doctors number 36 and nurses 20 but I used percentages) saying always (40% of surveyed doctors who said they get breakfast paired with 30% of nurses that said they always get breakfast)\n-I used Rstudio to generate the difference and made a t-test on that and got significance interval and pvalue\n\n-is this appropriate?\n-What test would be appropriate for similar data?\n\nThanks in advance for those able to help",
        "created_utc": 1669475335,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] What kind of statistical analysis is fitting to this data and research question",
        "author": "Chaosido20",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z55esc/q_what_kind_of_statistical_analysis_is_fitting_to/",
        "text": " \n\nHello,\n\nI've been thinking about a future research setup and I'm not the most well versed in this area, so I thought I'd just throw the question out here. I have panel data of people's symptom scores (psychological) and am curious in the temporal precedence of certain symptoms over another, e.g. does rumination always precede lack of motivation. I'd like to ideally cluster the data in certain groups, e.g. a group where you see that certain symptoms always precede others and another group with a different pattern.\n\nI could imagine that this kind of analysis is often similarly done in weather forecast or economic research.\n\nIs anyone able to help me in the right direction? Right now I'm looking at temporal network modelling or some sort of autoregression",
        "created_utc": 1669462257,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dependent or Independent",
        "author": "IAmMLADS",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z50qy1/dependent_or_independent/",
        "text": "I am tabulating the prices of gasoline and diesel among 8 gas stations in our community. Are the prices dependent or independent?",
        "created_utc": 1669445698,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which test to go for and why?",
        "author": "Hour_Woodpecker_906",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4ujsc/which_test_to_go_for_and_why/",
        "text": "So I had conducted an experiment. It had two IV with two levels each And DV which is dichotomous\n\nAt first I thought Two way ANOVA would be fine but a friend mentioned it can't be done. I Friedman test is what I should go for instead,?",
        "created_utc": 1669426766,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does R^2 actually mean for non-linear regression models?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4uhia/what_does_r2_actually_mean_for_nonlinear/",
        "text": "For linear regression models, it's simple and easy to understand-- it's just the average proportion of the variation in the dependent variable(s) that can be explained/predicted using the model.   And, if I understand correctly, the more general definition of R\\^2 is just \n\nR\\^2 = 1-MSE(model))/var(dependent variable(s))\n\nAt least, that's what Desmos says here: [https://help.desmos.com/hc/en-us/articles/202529139-Why-am-I-seeing-a-negative-R-2-value-](https://help.desmos.com/hc/en-us/articles/202529139-Why-am-I-seeing-a-negative-R-2-value-) \n\nIt also says, \n\n&gt; Since there is no limit to how bad a model’s predictions can be—and thus no limit to how big the errors can get—it’s possible for this ratio to become arbitrarily large, and 11 minus a large value is negative.   \n&gt;  \n&gt;In practice, R\\^2 will be negative whenever your model’s predictions are worse than a constant function that always predicts the mean of the data. \n\nAnd OK, that does make sense based on that formula for R\\^2.  But my question is, *why* is that the formula, and why, in the case of linear regression, is it equal to the average proportion of the variation in the dependent variable(s) that can be explained/predicted using the model but not in general?  That makes it seem like, in the case of linear regression, MSE(model))/var(dependent variable(s)) is the proportion of the variance shared between the actual values and the predicted values, but wouldn't that just be var(predicted)/var(actual)?  And why is it different in the case of linear/least-squares regression and regression more generally?",
        "created_utc": 1669426581,
        "upvote_ratio": 1.0
    },
    {
        "title": "MCAR question",
        "author": "lenddjs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4u9c9/mcar_question/",
        "text": "Can the MCAR assumption hold when the sample is non random but the missing observations are missing at random?",
        "created_utc": 1669425937,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is any of Gary Kleck’s (author of many statistical studies on gun control) data good?",
        "author": "SocialActuality",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4q5s8/is_any_of_gary_klecks_author_of_many_statistical/",
        "text": "I’m wondering if Gary Kleck has been totally discredited by now or not. Contemporary academics on one side of the gun control debate are still citing some of his work, but there’s academics on the other side who have panned his stats work. \n\nI’m inclined to believe the latter, given what I’ve read so far, but Kleck is still active AFAIK so maybe he cleaned his work up at some point.\n\nAdjacent question - how can someone not trained in stats and who’s generally poor at math make a reasonable determination of the integrity of a given study? Are they any “tells” that tend to suggest a study is bad/good?",
        "created_utc": 1669415158,
        "upvote_ratio": 1.0
    },
    {
        "title": "Textbooks/Approaches to Linear Model Theory",
        "author": "Safe_Drop6122",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4oky6/textbooksapproaches_to_linear_model_theory/",
        "text": "I'm interested in learning Linear Model Theory. As far as my math background goes, I'm fairly comfortable with probability theory and statistics at the level one would get in a basic upper-undergraduate/intro-graduate two-semester series. I've never taken a formal course in linear algebra, but I've learned a decent amount of it on my own (looking through the various appendices in linear model textbooks that go over the necessary linear algebra stuff, basically all of it looks familiar). However, I should note: I have never taken any sort of proofs-based course and have basically no idea what I'm doing with respect to proof methods. That said:\n\nCan somebody\n\na) recommend a good, introductory-level textbook for Linear Model Theory?\n\nb) try and explain in a way a non-mathematician would understand what these prefaces are talking about when they're advocating a \"projective\" approach to linear models vs a \"coordinate free\" approach? what are the respective strengths/weaknesses? are there other approaches I've missed (I think I've seen a \"geometric\" approach talked about but it wasn't clear if that was the same thing as \"projective.\"",
        "created_utc": 1669411297,
        "upvote_ratio": 1.0
    },
    {
        "title": "Data processing conundrum and questions",
        "author": "ChemistCapy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4ets3/data_processing_conundrum_and_questions/",
        "text": "im doing a statics project where I'm ranking the best players at each club in the premier league.\n\nHere is a link to all the polls to show how i have put them together (or if you want to fill one in): [https://www.reddit.com/r/PremierLeague/comments/z4ai6y/mid\\_season\\_review\\_for\\_each\\_club/?utm\\_source=share&amp;utm\\_medium=web2x&amp;context=3](https://www.reddit.com/r/PremierLeague/comments/z4ai6y/mid_season_review_for_each_club/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\nIm starting to think about how to process the data and in my trial run i have come into a problem. i am having to manually count the number of votes per player in each category (best, second best, and third best player which are all weighted differently) and then add them manually. Is there a way i can make sheets count the number of times it reads a name? i have shown some of the data below for context \n\n[sample data sets ](https://preview.redd.it/0k3l0ovhu32a1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=0ae62ace4c2f58b1690dbd0afbc132756d33ae0f)\n\nOther than this i am quite worried about the weighting of points. Currently i am planning to allocate 5 points to the best player, 3 points to the second and 1 to the third best. However when looking at the ranked list (above in the sample data)  and compare it to the voting (shown below in graph 1). the voting appears a lot more evenly distributed in graph 1 t than the table shown in the sample data set. Is there a way i can find the \"perfect\" multiplier/weighting\" or is it a case of trial and error unitl i feel the table represents the charts?.\n\n \n\n[Graph 1](https://preview.redd.it/d5sdezl5y32a1.png?width=716&amp;format=png&amp;auto=webp&amp;s=2dfe487e7623356d6779620f2a52d7f12719b371)\n\nFinally as shown bellow in graph 2 i am recording fans average ratings to see if there is a relationship between best players and higher rating (which i suspect will be the case) but also more interesting trend such as the relationship between points accumulated and standard deviation, etc. Are there other trends ideas that come to mind that could be investigated?\n\nhttps://preview.redd.it/zvguiokfw32a1.png?width=1510&amp;format=png&amp;auto=webp&amp;s=919e495555dd519aaff305718494c4decf063e3e",
        "created_utc": 1669386882,
        "upvote_ratio": 1.0
    },
    {
        "title": "sampling adjustment Rstudio",
        "author": "Disastrous-Nature-22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4ee8r/sampling_adjustment_rstudio/",
        "text": "Hello,\n\nIn my dataset, I had significant difference in age between my three groups. Going to see what was going on in the data, I see that the participants in group B are over-represented for the 35-59 age categories compared to the other two groups. I would like to be able to perform a deletion adjustment on the data, but I can't find a script on RStudio.\n\nCould someone help me?",
        "created_utc": 1669385699,
        "upvote_ratio": 1.0
    },
    {
        "title": "Industry 4.0 problem",
        "author": "Hairy-Preference3159",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4csic/industry_40_problem/",
        "text": "I am responsible for data analytics in a factory. In a dataset we collected in AWS, there is information about the parameter values of a production process (example: 40 degrees temperature, 500kg press, vibration, oil / quantitative values) and whether the product of that process is scrap. Here, the dependent variable is whether the product is scrap or not. The independent variable is the parameters of the process. How can I determine the parameters that affect the scrap products here? For example, how can I make analyzes such as this product was scrapped because the temperature rose to 60 degrees? I have close to 200 parameters.\nso I need a statistical method or a machine learning method.",
        "created_utc": 1669381139,
        "upvote_ratio": 1.0
    },
    {
        "title": "statistics mentor",
        "author": "RobinAzarath",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z4a9m6/statistics_mentor/",
        "text": "Hi, \n\nI'm looking for a statistics mentor, I'm learning it from scratch. And I just need someone or maybe some group to share my progress with and consult for questions.",
        "created_utc": 1669372636,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to run cosinor analysis",
        "author": "k25vm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z475dl/how_to_run_cosinor_analysis/",
        "text": " \n\nHi, I'm working on a research project for which I've collected google trends data and need to do cosinor analysis to study seasonality. This project is similar to this previous work- [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9498888/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9498888/)\n\nHowever, I'm not familiar with R and have previously used SPSS or JMP for the analysis. I've downloaded R and the cosinor package but not sure about running the scripts.\n\nCan anyone please share any video links or any detailed guide on how to proceed? Your help is highly appreciated. Thank you so much.",
        "created_utc": 1669361252,
        "upvote_ratio": 1.0
    },
    {
        "title": "Heteroscedastic or Homescedastic",
        "author": "Flying_Pesta",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z46knk/heteroscedastic_or_homescedastic/",
        "text": "Can't understand the difference between two, I have been looking at the different models/examples and still can't get it\n\nWhat about this?\n\n[https://imgur.com/a/13XSYMT](https://imgur.com/a/13XSYMT)",
        "created_utc": 1669359259,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing 4 different groups on cross sectional data",
        "author": "boniver07",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z44qtk/comparing_4_different_groups_on_cross_sectional/",
        "text": "Hi, so I am interested on analysing the impact of internet usage on income, and dividing it into 4 groups, internet user at rural area, non-internet user at rural area, internet user at urban area, and non-internet user at urban area. I only have cross-sectional data. Which statistical methods should I use? Previous studies about the subject uses propensity score matching (psm). Suppose that I have the result from both group (internet usage on urban and rural area), how do I compare these number to know which area benefitted more from internet usage? \nThanks in advance.",
        "created_utc": 1669353308,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which type of regression analysis should I use?",
        "author": "Chordaee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z42pid/which_type_of_regression_analysis_should_i_use/",
        "text": "My independent variables are frequency and absorption coefficient. My dependent variable, however, is a sequence of 7 Fibonacci numbers. \n\nThese 7 numbers are fixed (i.e. 1,1,2,3,5,8,13), but the dependent variable can give a certain arrangement/sequence of these numbers (e.g. for a frequency of 50 Hz and coefficient of 0.4, the sequence would be 3,5,1,2,13,8,1.) \n\nI wish to input the 2 independent variables and obtain the sequence as an output from the trained model, which I will use a dataset to train it. How can I go about doing this?\n\nThanks in advance!",
        "created_utc": 1669347069,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this left skewed?",
        "author": "Proper_Potential_192",
        "url": "https://i.redd.it/sa1fsx07u12a1.jpg",
        "text": "",
        "created_utc": 1669343130,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is preferable: One study with a lot of participants or more studies with an overall equal amount of participants?",
        "author": "Endokinet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3yb8m/what_is_preferable_one_study_with_a_lot_of/",
        "text": "What is better in terms of evidence for an effect?\n\nOne study with say, N=500 VS five studies with 100 participants each.\n\nAssuming everything equal between one study vs the many studies: experimental design, research conduct, analysis, same effect size, etc. \n\nSE would differ between one smaller study and the bigger one, I think, but does that ultimately matter?",
        "created_utc": 1669333925,
        "upvote_ratio": 1.0
    },
    {
        "title": "ISO Mac version of Graph Pad Prism to finish Thesis by Nov. 30!",
        "author": "sayoheyo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3y82j/iso_mac_version_of_graph_pad_prism_to_finish/",
        "text": "Does anyone have a free copy of GraphPad Prism for Mac?\n\nMy Master's thesis is due November 30 and I haven't finished the analysis of my data because I am a peak procrastinator--\n\nbut my trial expired recently and I don't have another computer to do another free trial on.\n\nI would buy a license but I don't need a year-long license just for me to not use it after this week (plus let's be real I'm on student money at the moment)\n\nI really appreciate it and whoever shares can be acknowledged in my thesis paper and possible publication for helping if you would like!\n\nFor the context of acknowledgments in my paper-- This is the title of it if you're interested! “SEX DIFFERENCES IN OXYCODONE DEPENDENCE IN MICE: CHARACTERIZATION AND POSSIBLE MECHANISMS”\n\n**Thank you and happy thanksgiving to all!**",
        "created_utc": 1669333663,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are the odds?",
        "author": "Aaasteve",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3uwvn/what_are_the_odds/",
        "text": "\nThrough 9 weeks of my fantasy football league, I have had the largest number of points scored against me during those 9 weeks. My opponents’ scores against me are on average 15% higher than their season averages, not a single opponent has scored at or below their season average against me - it’s almost as if they’re saving their best weeks for me. \n\nI know it’s not a giant conspiracy against me, it’s just worked out that way. \n\nMy question: is there a way of calculating the odds of this happening?  And if so, how would I do so?",
        "created_utc": 1669324777,
        "upvote_ratio": 1.0
    },
    {
        "title": "Understanding results from Dunn's test in R studio",
        "author": "LazerChrome",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3t5sg/understanding_results_from_dunns_test_in_r_studio/",
        "text": "I'm currently looking at biodiversity across 3 different habitats. I calculated a diversity index using Simpon and was left with a load of figures. I then did a kruskal Wallis test on the data and got the results:\n\nchi-squared = 6.1011, df = 2, p-value = 0.04733\n\nI therefore reject the null hypothesis that 'there is no difference in biodiversity between the 3 sites'\n\nI then do a Dunn's test using that data in R but I am a bit confused to what my results mean. I watched a YouTube video on it but my results format is different to the video.\n\n(this is what they look like in R studio)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5bg8w01efy1a1.png?width=954&amp;format=png&amp;auto=webp&amp;s=e03c8e70fb4840eb43394b83d24ced6ada0386fb\n\nhttps://preview.redd.it/azpqna58fy1a1.png?width=570&amp;format=png&amp;auto=webp&amp;s=950591fa463338dbc553daaf31576b0a62efa177",
        "created_utc": 1669319932,
        "upvote_ratio": 1.0
    },
    {
        "title": "understanding results of Dunn's test in R studio",
        "author": "LazerChrome",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3t2z4/understanding_results_of_dunns_test_in_r_studio/",
        "text": "I'm currently looking at biodiversity across 3 different habitats. I calculated a diversity index using Simpon and was left with a load of figures. I then did a kruskal Wallis test on the data and got the results:\n\nchi-squared = 6.1011, df = 2, p-value = 0.04733\n\nI therefore reject the null hypothesis that 'there is no difference in biodiversity between the 3 sites'\n\nI then do a Dunn's test using that data in R but I am a bit confused to what my results mean. I watched a YouTube video on it but my results format is different to the video.\n\n(this is what they look like in R studio)\n\nCol Mean-|\n\nRow Mean | 1                           2\n\n\\---------+---------------------------------------\n\n2 | -2.102629\n\n| 0.0177\\*\n\n|\n\n3 | 0.583997              2.331176\n\n| 0.2796                  0.0099\\*",
        "created_utc": 1669319724,
        "upvote_ratio": 1.0
    },
    {
        "title": "Degrees of freedom in two sample unpaired t test",
        "author": "League_Of_Noobs10",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3qcmw/degrees_of_freedom_in_two_sample_unpaired_t_test/",
        "text": "How do i calculate the degrees of freedom when im trying to work out the p value from the t.\nSome places tell me the smallest sample subtracted 1, other places tell me add both samples together and subtract 2.",
        "created_utc": 1669312572,
        "upvote_ratio": 1.0
    },
    {
        "title": "95% CI using the 'delta method' formula?",
        "author": "ar_604",
        "url": "/r/rstats/comments/z3ppc3/95_ci_using_the_delta_method_formula/",
        "text": "",
        "created_utc": 1669311338,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why study tableau if I know R graphics, Excel and Python Graphics?",
        "author": "Various_Ad_1067d",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3oq6j/why_study_tableau_if_i_know_r_graphics_excel_and/",
        "text": "",
        "created_utc": 1669308434,
        "upvote_ratio": 1.0
    },
    {
        "title": "Difference between log(y)~x and y~exp(x) in R",
        "author": "Low_Teach7736",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3nz6t/difference_between_logyx_and_yexpx_in_r/",
        "text": "I am trying to make an exponential regression model in R using the basic lm function but I get different coefficients and R2 values when I log the y variable or exp the x variable:\n\nlm(log(y) \\~ x, data = df)\n\nvs\n\nlm(y \\~ exp(x), data = df)\n\nI thought they would have given me the same answers, does anyone know what the differences are?",
        "created_utc": 1669306575,
        "upvote_ratio": 1.0
    },
    {
        "title": "Ordered regression and the proportional odds test rationale",
        "author": "nirvana5b",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3m5r3/ordered_regression_and_the_proportional_odds_test/",
        "text": "Hi all,\n\nI'm running an ordered logistic regression and have stumbled upon the proportional odds assumption. Looking for a practical way to test it on my sample I've found this post on stackoverflow: [Testing the Proportional Odds Assumption in R](https://stackoverflow.com/questions/37016215/testing-the-proportional-odds-assumption-in-r)\n\nThe idea here is to test the goodness of fit between an ordered and a multinomial, is that it?\n\nWhat's the null hypothesis here exactly and how it's related to the proportional odds assumption?\n\nThanks in advance!",
        "created_utc": 1669302151,
        "upvote_ratio": 1.0
    },
    {
        "title": "Alternative ways to show significant differences, or describe over/underestimations? More details in comments - picture is an example of the data",
        "author": "IndicationOwn4144",
        "url": "https://i.redd.it/3zyhq2j64y1a1.jpg",
        "text": "",
        "created_utc": 1669298066,
        "upvote_ratio": 1.0
    },
    {
        "title": "Numpy intro in just 2 minutes",
        "author": "jredrose",
        "url": "https://youtu.be/99ne6gq5fbE",
        "text": "",
        "created_utc": 1669289227,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why does large residual variance indicates large differences within-groups and not between-groups (multilevel modelling)",
        "author": "luchins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z3cyl0/why_does_large_residual_variance_indicates_large/",
        "text": "*In multilevel modeling,* ***residual variance is a reflection of the*** ***within-groups*** ***effect*** *(Garson, 2019).* ***Large residual variance coefficients indicate large differences within-groups*** *(Xie, 2009). In ANOVA,* *Within-group variation* *is synonymous with residual variance.*\n\nWhy does large residual variance indicates large differences within-groups and not between-groups instead? Can someone explain me please? If we are looking at the total model (wich accounts for all the groups and fits the best regression line) then why do large residual variance does it mean larhe within group variance and not between groups?",
        "created_utc": 1669272940,
        "upvote_ratio": 1.0
    },
    {
        "title": "In the perceptron classification algorithm, is the bias term simply the decision threshold?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z369em/in_the_perceptron_classification_algorithm_is_the/",
        "text": "So I just started doing Brilliant.org's course on artificial neural networks and I'm on the section \"Perceptrons as Linear Classifiers\".  They introduced the perceptron algorithm as a binary classification algorithm and said that, for a vector of data, x, and a vector of weights, w, the output is 1 i.f.f. w*x + b ≥ 0, and is 0 otherwise, where b is called the bias.  \n\nThey didn't define what bias is in this context, but they gave the example of two candidates in an election with 101 voters, and say that, in that example, b=-50.5.  That sort makes sense if b is just the decision threshold, since, assuming a first-past-the-post voting system, the candidate to get more than half the votes would win.  But why -50.5 specifically?  Why not -50.1, or even -50.01?  Clearly it's not because you can't have a fraction of a vote, since -50.5 would correspond to 50 1/2 votes.\n\nAnd finally, is the bias term in this algorithm generally representative of the chosen threhold?  That is, is it simply that any particular value of x, after being multipled by the probability vector, is greater than b, it goes in whatever the \"positive\" category is, otherwise it goes in the \"negative\" category?  That's seems pretty obviously the case, but the reason I'm unsure is that, if I understand correctly, usually in statistics and ML, \"bias\" refers to the difference between the estimated value of a parameter and the actual parameter.  I've had some difficultly lately with understanding the relationship between bias and variance in regards to predictive and classification algorithms, as my post history certainly shows, so I want to make sure I'm misunderstanding something here.  Is this just a completely use of the term \"bias\" or is there someone more going on with this algorithm that means the bias term is bias in the usual, statistical sense of the term?",
        "created_utc": 1669252272,
        "upvote_ratio": 1.0
    },
    {
        "title": "Appropriate model for likert scale data",
        "author": "kwisatch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z35bx6/appropriate_model_for_likert_scale_data/",
        "text": "Hi All,\n\nI am an intermediate statistics student currently working on a project which involves analysis of likert scale data(range 1-5, 1 being \"Completely Disagree or Least Likely). The dataset contains a few other customer specific information like race, gender, age, location, income group etc. The remaining columns are the responses of customers to a survey questionnaire in 1-5 likert scale. There are total 25 questions and therefore 25 likert columns. \n\nThe end objective is to understand customers attitude towards a product , based on  survey responses. \n1.How do I go about modelling such a project? Someone in my group suggested using hierarchical models ( not sure weather the frequentist or Bayesian hierarchical model). What would be the response variable in this case, since I have 25 likert columns that could be treated as the response variable.\n2. How do I measure the effect of one variable (say income group) on the customers attitude towards the product? \nI have already performed clustering techniques to segment the customers and develop a profile. I have also performed Kruskal Wallis and Mann Whitney U test to see if there exists a  difference in attitude based on income group for each survey response.\n\n3. If I decide to model using Bayesian hierarchical model, how would I choose a prior for the likert data points? One likelihood that comes to mind is Xij-1~Binomial(4,pj) where Xij is the survey response of the ith customer for the jth question and pj is the prior parameter for jth question. However, I don't think this would be correct as pj will lose its interpretability. Is there any other way I can specify a likelihood and a prior?\n\nAny help in this regard will be appreciated!",
        "created_utc": 1669249635,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with SPSS , Spearman Test, P value &lt;0.001 ??",
        "author": "Lumpy-Champion7938",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z350yo/help_with_spss_spearman_test_p_value_0001/",
        "text": "Hi, I have an assignment due and have only recently learned how to use SPSS. I am quite confused on how to interpret the results of my Spearman test, for a scale variable (dependant) and an ordinal variable (independent) \n\nI can’t seem to post a link or photo however the results say\n\nCorrelation coefficient: 1.000|.092**\nSig. (2-tailed): &lt;0.001\nN: 2213|2170\n\nCorrelation coefficient: .092**|1.000\nSig. (2-tailed): &lt;0.001\nN: 2170|2173\n\nFrom the material I found online, I have concluded that the P value is less than 0.001 and the null hypothesis should be rejected ? \n\nI am really struggling with this part of the assignment and I would appreciate any help as I cannot find much help online or within my lecture slides !",
        "created_utc": 1669248805,
        "upvote_ratio": 1.0
    },
    {
        "title": "In prediction modeling, do you report estimates from the training or test sets?",
        "author": "Naj_md",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z33t45/in_prediction_modeling_do_you_report_estimates/",
        "text": "I report AUC and other metrics for my training and test sets, but I want to report estimates, which one would it be (they have different VIMP and estimates)",
        "created_utc": 1669245576,
        "upvote_ratio": 1.0
    },
    {
        "title": "How Do You Generate a Score for a Risk Assessment Variable Using Weights Derived from Logit Coefficients?",
        "author": "squidward69s",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z2y6kb/how_do_you_generate_a_score_for_a_risk_assessment/",
        "text": "I am validating an 8-item risk assessment tool designed to classify inmates to custody levels and predict institutional misconduct within prisons (0 = No misconduct; 1 = Misconduct). I would like to propose modifications to the scoring of the tool such that point totals for each variable on the tool reflect the true comparative predictive power of each variable (e.g., age should have a higher point value range than something like within-prison drug trafficking, as the latter weakly predicts misconduct whereas age is highly predictive). Total scores on the original risk assessment were generated by adding point totals taken from the 8 different variables; point totals were initially decided by expert judgement and, thus, were not grounded in estimates of comparative predictive power beyond casual intuition.\n\nSo, for instance, one of the variables in the original risk assessment is age which takes on the following point values: \\[-2 = 45 +, -1 = 35 - 44; 0 = 26-34, 1 = 21-25; 2 = &lt;=21\\]. Another variable is history of previous misconduct taking on the following point values: \\[0 = None; 2 = minor infraction in prior six months; 5 = Major infractions in prior six months\\]. Higher scores on the tool imply a higher likelihood of engaging in misconduct. If age more strongly predicts misconduct than history of prior misconduct, I would like to find a methodologically-appropriate way to upweight the point total for that variable.\n\nI have validated the existing tool with validation metrics (e.g., Brier scores; AUC; Cohen's D) and have used backward elimination procedures after binarizing the variables on the assessment (i.e., stripping items on the tool of the arbitrariness of their point totals to only indicate presence/absence of the feature) to assess variable importance. However, I am stuck on the question of how to generate the specific scores for each variable on the revised version of the tool on the basis of each item's predictive power and have not been able to find much literature which speaks to this question.\n\nOne approach I've seen to developing assessment scores is to run bivariate logistic regression on random samples of the data with each variable in the assessment (predicting the outcome), average the series of unstandardized beta coefficients for the item across the samples, and then multiply the averaged beta coefficient by the \"item rating\" to generate scores. However, I am not really sure what is meant by item rating using this definition: what precisely is it that is being multiplied by the weight? For reference, please see the following paper: [https://journals.sagepub.com/doi/full/10.1177/00328855211069150](https://journals.sagepub.com/doi/full/10.1177/00328855211069150)\n\nAny clarification on the strategy used in the referenced paper or any resources on how to develop the specific scores for the revised tool would be helpful! Thank you :)",
        "created_utc": 1669231857,
        "upvote_ratio": 1.0
    },
    {
        "title": "(Question) Three-way mixed Anova... AND two-way Anova?",
        "author": "Independent_Rest_644",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z2xybe/question_threeway_mixed_anova_and_twoway_anova/",
        "text": "I'm analyzing data collected in three time periods (repeated measures), with two independent variables (A and B). I'm using SPSS. I have five dependent variables which I want to analyze separately.\n\nAt first, I ran two-way Anovas on data from each period separately, with follow-up tests of simple main effects (if the two-way interaction effect was significant) or main effects (if it wasn't). However, I also want to analyze temporal variations in my dependent variables and check if these variations depend on the levels of A and B. I ran three-way mixed Anova and didn't get statistically significant three-way interactions for any dependent variable. However, for several dependent variables there are statistically significant two-way interactions (time\\*A and/or time\\*B). The effect of time itself is also highly statistically significant in all cases.\n\nFrom what I understand, simple two-way interactions (at each time step separately) are only recommended when there's a significant three-way interaction, which I don't have. In case of non-significant three-way interactions, it's recommended to investigate two-way interactions (in all three time periods combined) instead. How do I analyze variations in time then? Since I have already run two-way Anova (before deciding to run three-way mixed Anova), I already know that the effects of A, B and A\\*B often vary between the periods. Do I need to exclude those results because of the non-significant three-way interactions, or does it make sense to report results from both two-way Anova and three-way Anova?\n\nI hope this makes sense. Many thanks in advance for help!",
        "created_utc": 1669231309,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best way to handle otherwise continuous variables that have one bin?",
        "author": "DrStuffy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z2x4wo/best_way_to_handle_otherwise_continuous_variables/",
        "text": "For example, I have a dataset that includes age as a variable and is continuous over the range 55-80, but anyone older than 80 is just assigned \"&gt;80\" (and they represent about 10% of my sample). I would like to perform a regression and know the difference in my outcome per 1-year increment in age. \n\nMy first instinct would be to treat it as ordinal without further binning (i.e. 1-year bins), but a few ages are not represented in this sample. I'm not sure how best to preserve the information in this scenario. Thank you for any advice!",
        "created_utc": 1669229358,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need help to choose a statistical test for an one sample, unpaired, non-parametric scenario. Is Wilcoxon the best option? Context is given in post body text.",
        "author": "torta-de-frango",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z2upe0/i_need_help_to_choose_a_statistical_test_for_an/",
        "text": "# Context\n\nIn my experiment, 15 people answered questions through Likert scales about perceived ease of use, perceived usefullness and low time cost of a software that does educational controlled experiments.\n\nThe Likert scales have 4 alternatives, going from strongly disagree associated with the number 1 to strongly agree associated with the number 4.\n\nThe scales are analyzed at the interval measurement scale as instructed in Boone, Harry N., and Deborah A. Boone. \"Analyzing likert data.\" *Journal of extension* 50.2 (2012): 1-5. \n\n# My Hypothesis\n\nThe alternative hypothesis is that the number of positive answers associated with each metric is higher than the number of negative answers. So I can say that the software is easy to use, useful and has a usage low time cost.\n\n# How I'm doing the tests\n\nI choose this test based on Nayak, Barun K., and Avijit Hazra. \"How to choose the right statistical test?.\" *Indian journal of ophthalmology* 59.2 (2011): 85. \n\nI'm using one sample Wilcoxon signed-rank test to compare each metric sample median with a hypothesized value. In this case, 2. If the median is higher than 2, there is more positive answers than negative answers and vice-versa.\n\n[Utilidade\\_Percebida means perceived usefullness, Facilidade\\_Uso\\_Percebida means perceived ease of use and Baixo\\_Custo\\_Tempo means low time cost.](https://preview.redd.it/ntx5zy2dgq1a1.png?width=720&amp;format=png&amp;auto=webp&amp;s=4c66d826dff9132dc7f48ddffed75ad545d3250e)\n\n# The questions\n\nAm I doing this right?\n\nI would like to say the median is higher than 2. But I can't change the alternative hypothesis in SPSS. But I know is is higher because of this graph. Can I use it to say that the median is higher or do I need an one-sided test?\n\nhttps://preview.redd.it/y1ryfpjrgq1a1.png?width=853&amp;format=png&amp;auto=webp&amp;s=50bc23900a812c290138d14392eab228d60fd078\n\nShould I use Kruskall-Wallis because I have 3 metrics so it is 3 samples? How would I compare with a hypothesized value in this case?\n\nI'm actually learning inferential statistics as I'm writing this article for my graduation thesis so I don't have much knowledge about it. I really appreciate any opinion you can give me! Sorry for the long text!",
        "created_utc": 1669223667,
        "upvote_ratio": 1.0
    },
    {
        "title": "HELP: not sure which stats test to perform on my data (and how)",
        "author": "misha_park_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z2ujxg/help_not_sure_which_stats_test_to_perform_on_my/",
        "text": "Hi, I'm trying to perform a statistical test on a dataset I have collected.\n\nI am measuring reaction time in participants\n\nThe participants are split into 2 groups; gamers and non-gamers\n\nthey each take a reaction time test in the morning and evening and record their average over 5 attempts at each time, it is these average values that are in the table.\n\nI then calculated an average for each combination of variables, and put these values in a 2x2 square (shown with picture)\n\nHow do I do the test ???\n\nI have performed ANOVA without repetition and obtained a p-value, but I don't think this is the correct test, as the AM/PM variable is not independent\n\nIs it ANOVA with repetition????? (as the same subject is measured twice, am and pm), if so how would I go about doing this test (preferably in excel)\n\nAny help would be much appreciated",
        "created_utc": 1669223306,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS lab signature assignment?",
        "author": "PianistOwn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/z2u4li/spss_lab_signature_assignment/",
        "text": "I have a signature assignment to do using SPSS but I have no idea how to use SPSS really. It’s due tonight. Can anyone give headers or something so I’m not turning in a complete zero",
        "created_utc": 1669222311,
        "upvote_ratio": 1.0
    }
]