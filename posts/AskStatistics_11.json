[
    {
        "title": "Simple probability question",
        "author": "RanjeetThePajeet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108m5gg/simple_probability_question/",
        "text": "Hello, I had a quick question about the odds of a specific scenario occurring and I was never good at probability calculations so figured I’d outsource the problem. It’s fairly straightforward I think.  Hopefully this is the right sub for such a question. \n\nGiven a well shuffled set of 6 decks of 52 cards each, what would the odds be of being dealt 8 straight aces?  Suit doesn’t matter and the 8 cards came straight off the top one after the next.  \n\nI know the odds change after each one dealt (24/312, 23/311, 22,310, etc.) but I can’t remember exactly how you combine those odds into a final probability.  \n\nFYI this isn’t a homework question lol, my Dad was telling me a story of when this happened to him back in the day and he always wondered exactly how lucky he was.  Appreciate any help!",
        "created_utc": 1673387479,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why are Kaplan Meier and Nelson Aalen curves similar?",
        "author": "ceo-of-earth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108m0iz/why_are_kaplan_meier_and_nelson_aalen_curves/",
        "text": "I have noticed them being similar but not straight up similar, why is that? Aren't they different?",
        "created_utc": 1673387162,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can a hazard ratio be used when the outcome is continuous?",
        "author": "Theobviouschild11",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108j9kw/can_a_hazard_ratio_be_used_when_the_outcome_is/",
        "text": "For example, I'm looking are risk factor effect on length of survival. Could hazard ratio be used to describe that or does the outcome have to be categorical?",
        "created_utc": 1673380789,
        "upvote_ratio": 1.0
    },
    {
        "title": "How is that the answer for 2? What is the method used?",
        "author": "BethStubbs",
        "url": "https://i.redd.it/bwm8eiqftaba1.jpg",
        "text": "",
        "created_utc": 1673374469,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multivariate Normal Distribution definition",
        "author": "ihavenoopi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108f7pd/multivariate_normal_distribution_definition/",
        "text": "Wiki lists some convoluted definitions of a normal random vector. But I think it can be done in a more simple way. It seems to me that every element of a random vector being a normal variable is a sufficient and necessary condition for the vector having a multivariate normal distribution.\n\nThis directly follows from one of the alternative definitions of MND: any linear combination of the vector's variables must be a normally distributed variable. \n\nThe condition stated above is sufficient, because any linear combination of normal variables will be a sum of normal variables - which is itself a normal variable. \n\nIt is the necessary condition too, because if any of the elements of the vector is not normally distributed, say X1, then the linear combination L = 1\\*X1 + 0\\*X2 + 0\\*... will not be normally distributed. \n\nThus, the multivariate normal distribution is simply a collection of normally distributed variables. But I have my doubts, because, if it is so, then why use all those more complicated definitions? Is it possible for a normal random vector to not be a collection of normal variables?",
        "created_utc": 1673371056,
        "upvote_ratio": 1.0
    },
    {
        "title": "T-test with covariates",
        "author": "Relative_Credit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108ep61/ttest_with_covariates/",
        "text": "I am working with a dataset of 90 patients where we measured cytokine concentrations in plasma. I recently learned that the type of blood tube used to collect the plasma changed half way through the study, so roughly half the patients had samples collected in EDTA tubes and the other half in Heparin tubes. It looks like there is an effect of blood tube on the concentrations of some of the cytokine concentrations in my dataset, and literature supports that the type of blood tube used can affect cytokine concentrations. \n\nBasically, I would like to include the type of blood tube as a covariate in my analysis. I am primarily interested in comparing the cytokine concentrations between patients who survived or died. It looks like an ANCOVA might be the best option for me to do this, but I wanted to see if this sub has any insight/suggestions on what I should do.",
        "created_utc": 1673369863,
        "upvote_ratio": 1.0
    },
    {
        "title": "What’s the significance level: one player in 50 years with life-threatening heart ailment, compared to two players in a single season?",
        "author": "RGregoryClark",
        "url": "https://www.reddit.com/r/AskStatistics/comments/108chzu/whats_the_significance_level_one_player_in_50/",
        "text": " In the wake of the Damar Hamlin cardiac arrest medical authorities have claimed the number of athletes with heart conditions is “normal”. However, previously there was one player with a life-threatening heart issue 50 years ago, who tragically died. Now, this one season in addition to Damar,  there was celebrated defensive player JJ Watt who had to be put on a defibrillator to shock his heart back into rhythm.  \n\n What’s the p-value against this being just “normal”?",
        "created_utc": 1673364404,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for a textbook",
        "author": "Consistent_Win9239",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10896ua/looking_for_a_textbook/",
        "text": "[removed]",
        "created_utc": 1673355439,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for a statistics textbook",
        "author": "Consistent_Win9239",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10895qq/looking_for_a_statistics_textbook/",
        "text": "Hello guys, I am looking for a specific textbook from which is the problem on the picture. Does anyone recognize it/ can anyone help? Thank you!",
        "created_utc": 1673355342,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question : how could I calculate s and t if they're not given?",
        "author": "Daniel_Mathieu",
        "url": "https://i.redd.it/k96mcus2o8ba1.jpg",
        "text": "",
        "created_utc": 1673348457,
        "upvote_ratio": 1.0
    },
    {
        "title": "What model do I use?",
        "author": "minimal-pancakes-98",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1081rnv/what_model_do_i_use/",
        "text": "If I have a continuous dependent variable (distance) and many different categorical independent variables (e.g. gender, age group, dwelling type, purpose of trip, and mode of transport), what model should I use if I want to find out the effects of independent variables on the dependent?",
        "created_utc": 1673328948,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is bootstrapping a logistic regression appropriate when sample size is reduced after weights are implemented?",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107z8qn/is_bootstrapping_a_logistic_regression/",
        "text": "Hi all, I have an unweighted logistic regression with a sample size of 171 which is reduced to 52 after weighting. Would bootstrapping be appropriate in this situation to improve the chances of my model detecting a difference?",
        "created_utc": 1673321328,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to find amount of tries for a probability to almost certainly happen?",
        "author": "DuckySpider885",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107ysml/how_to_find_amount_of_tries_for_a_probability_to/",
        "text": "For example, say you have 100 dimes and every day you randomly pick up one dime then put it back. How could you find out how long it might take to pick up each unique dime?",
        "created_utc": 1673320053,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical test should I use for survey data in my master’s dissertation?",
        "author": "AlternativePlenty983",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107ycvx/which_statistical_test_should_i_use_for_survey/",
        "text": "I have conducted a survey that is exclusively multiple choice answers (strongly agree, agree, neutral, disagree, strongly disagree) except for the first 2, where the respondent disclosed their age and gender.\nFor context, the gist of my dissertation is the psychological effect of exposure to sexualised advertisements in men and women. Using the survey I am essentially trying to prove that sexualised advertising does in fact have adverse effects on men and women (for different reasons) and that more diverse and inclusive advertising does not have these same effects, and can even reverse them.\nI was thinking of using a one-way ANOVA (after using an ordinal scale to organise the responses) but I am not so sure and would really appreciate some guidance.",
        "created_utc": 1673318850,
        "upvote_ratio": 1.0
    },
    {
        "title": "Please help me prove a point",
        "author": "Smooth-Ad5285",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107xvcq/please_help_me_prove_a_point/",
        "text": "If I have to choose 5 numbers out of 100.  Do I have better odds of them being consecutive I.e. 12345 or 34,35,36,37,38 or is there better odds of it being a random set of 5 numbers.  Please let me know.  I want to see work please not just googling the lottery question.",
        "created_utc": 1673317516,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical analysis method would be best to use?",
        "author": "CompoteFluffy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107v1d6/which_statistical_analysis_method_would_be_best/",
        "text": "I have 2 sets of data that I need to analyse, but I’m not sure which method to use (I am using MiniTab). \n\nThe two sets of data are regarding two different cell types that were tested. Within each cell type, different concentrations of 2 different substances were used to see how the different cell types would respond. \n\nI’ve calculated the mean for all the repeats of the experiment, I just need to do a short statistical analysis. \n\nI don’t know if I can use a t test or ANOVA since within the 2 cell types, there are 4 different concentrations. \n\nEssentially, all I need to do is compare the all of the means, but I’m unsure how to do this in a way that represents the fact that there are different categories. \n\nSorry if this is confusing, any help is appreciated. Thank you :)",
        "created_utc": 1673309961,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would anyone be willing to help me make a bell curve?",
        "author": "Azmik8435",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107sryv/would_anyone_be_willing_to_help_me_make_a_bell/",
        "text": "Hello! I hope this kind of question is allowed here, if not I’ll remove it. I try not to go to forums to ask such seemingly simple questions, but I genuinely tried looking for an answer and couldn’t find one. Every result I got was either for how to make a bell curve in MS Excel, or wasn’t helpful for my specific scenario.\n\nI’m trying to make a bell curve of Pokémon. For each individual Pokémon that the game generates, they are given six values that range from 0-31 that correspond to the Pokémon’s six stats, these are called “IVs” (Individual Values). These values give variability to the stats of otherwise identical Pokémon. The sum of Pokémon’s IVs fall on a bell curve that has a mean of 93. My problem is that I can’t for the life of me find out how to get the standard deviation for this bell curve without having a dataset. Whenever I try to look up how to find standard deviation online, all the formulas use datasets. Is it even possible to find the standard deviation with the info I have? It’s so unfortunate because all of these answers are in my college statistics notebook, but I can’t find it and that was four years ago :(\n\nAny response would be greatly appreciated!",
        "created_utc": 1673304503,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to compare two polar dataset? Each group containing around 20 orientations (mean angles) and weights (resultant vector length)",
        "author": "estanislaojoiko",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107rvii/how_to_compare_two_polar_dataset_each_group/",
        "text": "",
        "created_utc": 1673302478,
        "upvote_ratio": 1.0
    },
    {
        "title": "Correlations of between subgroups using SAS",
        "author": "RubiscoActivase",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107pr4z/correlations_of_between_subgroups_using_sas/",
        "text": "Experimental setup. Three types of soybeans (A, B, and C) grown in ambient and elevated CO2. I want to run correlations between dependent variables. For example, we measured Seed K and photosynthesis. I have ran correlations using the entire set of data but now I want to see correlations within sub groups. For example, soybean A in ambient CO2 to soybean A in elevated CO2. Is there code for specifying these subgroups for correlations or do I need to organize a new excel sheet so each subgroup has its own column. If that makes sense. The code below is an example of what I used for comparing entire sets of variables. \n\nproc corr data=SoyData plots=scatter(nvar=all);\n\nvar photo SeedK;\n\nrun;\n\n&amp;#x200B;\n\nThank you!",
        "created_utc": 1673297808,
        "upvote_ratio": 1.0
    },
    {
        "title": "This Will be highly appreciated! This is Adrenaline, a debugger that fixes errors and explains them with GPT-3",
        "author": "Holiday_Snow_2734",
        "url": "https://i.redd.it/8t0k9jkd3vaa1.gif",
        "text": "",
        "created_utc": 1673282769,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is ethical standard the highest predictor?",
        "author": "G-KaiseR",
        "url": "https://www.reddit.com/gallery/107it86",
        "text": "",
        "created_utc": 1673282009,
        "upvote_ratio": 1.0
    },
    {
        "title": "Linear mixed model (SPSS)",
        "author": "Alicia-Emily",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107gpn1/linear_mixed_model_spss/",
        "text": "Hi,\n\nWe're doing a study among couples (we asked both partners to participate, but not every partner wished to do so, so we have a lot of individual cases) to predict an interval DV from 2 interval predictors and 2 categorial predictors (religion and education). The simplest approach would be to do multiple linear regression (with categorial predictors converted into dummy variables), but since (some) cases are nested within couples, nonindependence might be a problem.\n\nThe solution I came up with, is using a linear mixed model. I am using SPSS 27, BTW. I assume I have to enter my 4 predictors as fixed effects and couple number as random effect. But I have little experience with mixed effect models. Does this sound right? Do I have to enter my categorial predictors as covariates (converted into dummy variables like I would do with regression), or can I add them as factors? And how do I handle the “missing partners”?",
        "created_utc": 1673276972,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question: under which circumstances will variable X significantly affect variable Y?",
        "author": "Acrobatic-Ice1411",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107fom9/question_under_which_circumstances_will_variable/",
        "text": "Hi, I'm trying to answer the research question: under which circumstances will variable X significantly affect variable Y?\n\n  \nMy supervisor told me to try using moderation analysis with the PROCESS macro. I think I have understood how to use the software and do the analyses, but am concerned that the result is not really going to answer the research question. E.g. it will tell me which variables significantly moderate the association between X and Y, but it may not be able to help with answering under which circumstances will variable X significantly affect variable Y?\n\nFor example, I am hoping to find a result like, X will significantly affect Y only in females, or X will only significantly affect Y in people with diabetes, or X will only significantly affect Y in people with BMI higher than M etc.\n\nVariables X and Y are continuous and for moderators I have a range of continuous and categorical variables I want to try. \n\nCould someone please point me in the right direction, should I still be trying to conduct moderation analyses with PROCESS, or should I be looking at some other type of analysis?\n\nThanks so much!! Any suggestions would be super helpful, I'm so confused :/",
        "created_utc": 1673274375,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need to prove that someone is gambling.",
        "author": "calling_out_bullsht",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107dtme/i_need_to_prove_that_someone_is_gambling/",
        "text": "So I have a list of winnings (only the big ones) and I was wondering if given enough “big winners tickets” one could deduce a range of how much someone is gambling based on the frequency of their big wins?",
        "created_utc": 1673269211,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you run Central Limit Theorem as pre-processing to apply then a T-Test?",
        "author": "Purple-Character-986",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107dkau/can_you_run_central_limit_theorem_as/",
        "text": "I was reading lately a Kaggle notebook and I am not sure if I understand correctly the thinking behind of it. \n\nThe link is this one: [https://www.kaggle.com/code/ymayank97/clt-lf-rf-90-accuracy?scriptVersionId=114218982&amp;cellId=26](https://www.kaggle.com/code/ymayank97/clt-lf-rf-90-accuracy?scriptVersionId=114218982&amp;cellId=26)\n\nSo the author wants to study if the age affects another variable. When he plots the distributions of age divided by that variable, there is a big overlap. So he mentions this\n\n\"To reduce the variablity around the means of two groups, I will use central limit theorem which states that given a sufficiently large sample size from a population with a finite level of variance, the mean of all sample from the same population will be approximately equal to the mean of the population.\".\n\n&amp;#x200B;\n\nThen, he iterates a lot of times through the dataset, taking small samples, calculating the mean of those and plot it again, getting a different distribution plot. \n\nI apologize if this is trivial, but I am not so experienced in statistics and I try to learn by myself.",
        "created_utc": 1673268403,
        "upvote_ratio": 1.0
    },
    {
        "title": "What can I deduce from the information that expected value of six-sided fair die is 3.5?",
        "author": "maybenexttime82",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107dfo4/what_can_i_deduce_from_the_information_that/",
        "text": "I was reading Wikipedia article on expected value, and they've introduced the idea by two examples, one with the 6-sided fair die and the second one with the fair roulette game. In the example of a roulette game they've obtained the next result:\n\n&gt;The expected value to be won from a $1 bet is −$1/19. **Thus, in 190 bets, the net loss will probably be about $10.**\n\nSo far my understanding of expected value is that either you have a known distribution and there is a formula for calculating that value, or you can also get the expected value if you calculate the arithmetic mean of experiment done n times. The bigger the n, the more arithmetic mean converges to the expected value.\n\nSay I have unlimited money in my pocket and I try to obtain expected value of some game:\n\n1) In terms of roulette game it says that if you play long enough your net loss will be -$1/19 so either you can calculate expected value to know your gains and losses in advance, or you could play n bets and calculate the arithmetic mean to get value that is close to -$1/19? Does 190 bets mean that say you are doing an experiment and after first 190 bets you expect to lose about $10? Or you know that expected value is -$1/19 and if you start playing that game 190 times you will lose about 10$?\n\n2) I don't know how to translate that idea of gaining/loosing the money for the result of E\\[x\\] = 3.5. So, either I know that in the long run I should expect the result of 3.5, or I should try to bet n times find the arithmetic mean which will be close to 3.5. What would mean to play 100 bets on this fair dice? Is it the first 100 tosses of a dice or?\n\nTLDR:\n\nThe thing that is confusing me is that intuitevly I see expected value as some \"future value\", and in order to get that future value you could either calculate it immediately or start at present, do the experiment n times, calculate the arithmetic mean and realise that it will converge to the value. The other thing that is confusing me is when I try to understand the meaning of expected value in real life when someone introduce it with some game.",
        "created_utc": 1673268011,
        "upvote_ratio": 1.0
    },
    {
        "title": "Could anyone help me with this exercise? i'm preparing for an exam and I don't understand this",
        "author": "PurposeFeeling3050",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107c9ta/could_anyone_help_me_with_this_exercise_im/",
        "text": "[removed]",
        "created_utc": 1673264166,
        "upvote_ratio": 1.0
    },
    {
        "title": "One independent variable and Seven Dependent Variable which statistical test to use?",
        "author": "heyguenevere",
        "url": "https://www.reddit.com/r/AskStatistics/comments/107blp7/one_independent_variable_and_seven_dependent/",
        "text": "Hi! Need help with my causal-comparative design study. \n\nOne independent variable and Seven Dependent Variable which statistical test to use?",
        "created_utc": 1673261841,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trying to figure out probability of possible scenarios given set probability for each item",
        "author": "guanbar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1078uvz/trying_to_figure_out_probability_of_possible/",
        "text": " So essentially I would have 10 items/figures, each with the probability of being either A or B (for simplicity).  \nEach of the 10 figures has a set probability that A or B will happen, for example:  \n1 - 80% probability of A  \n2 - 30% probability of A                                                                                                                                              3 - 60% probability of A, etc...                                                                                                                              How could I get the overall probability for each possible scenario, such as the chance that 0/10 are A, 1/10 are A, ..... or all 10/10 are A",
        "created_utc": 1673252070,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confused about the hypergeometric distribution",
        "author": "zogbin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1072je9/confused_about_the_hypergeometric_distribution/",
        "text": "X \\~ Hypergeometric(N, K, n) can be written as a sum of dependent Bernoulli r.v.s such that X = X\\_1 + ... + X\\_n, where X\\_i \\~ Bernoulli(K/N). \n\nY \\~ Binomial(n, K/N) can be written as a sum of independent Bernoulli r.v.s such that Y = X\\_1 + ... + X\\_n, where X\\_i \\~ Bernoulli(K/N). \n\n I understand the difference is that we're using dependent vs independent Bernoullis. But since we're summing them unconditionally, it seems mathematically X = Y? Clearly this can't be right, but I'm not sure where my thinking has gone wrong.",
        "created_utc": 1673232687,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is a 50 sided die and a coin flip x1/x2 same as 100 sided die?",
        "author": "NameReUnused",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1071xcj/is_a_50_sided_die_and_a_coin_flip_x1x2_same_as/",
        "text": "Is a 50 sided die and a coin flip with one side x1 and one side x2 the same as a rolling a 100 sided die?",
        "created_utc": 1673230980,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about stats",
        "author": "TrovadorAngolano",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1071eow/question_about_stats/",
        "text": "I'm learning the basics and I stumbled upon this problem and I've tried but I can't get to the answer (I think it's a poisson distribution)\n\nThe question is: In some hospital, the probability of one or more calls in some hour of the day is 0.5. What is the probability  that in an 8-hour shift there are two hours without calls?",
        "created_utc": 1673229609,
        "upvote_ratio": 1.0
    },
    {
        "title": "Biology Cell Viability Assay question",
        "author": "Medical_Insect_3796",
        "url": "https://www.reddit.com/r/AskStatistics/comments/106zt2h/biology_cell_viability_assay_question/",
        "text": "Hi all,\n\nI am currently doing an experiment to test if a drug can keep neurons with Alzheimer's alive, and I am having trouble understanding the stats behind it.\n\nThe test (alamar blue) gives data in the form of fluorescent units.  To determine if the drug works, you compare the amount of fluorescent units in the control of healthy neurons to the diseased treated neurons, and get a percentage of how many neurons died relative to the control. \n\nWhat I have seen many people do is run this experiment 3 times, get 3 percentages, and simply take the mean and standard deviation of those percentages by using the STDEV.s in excel and publish those results. Can you do that? I didn't think you could take the mean and standard deviation of percentage data like that, although I can't really articulate why that would be bad (its not normally distributed I think?).\n\nIf anyone has any insight on how to handle this kind of data or even where to look I would greatly appreciate it. \n\nThanks so much!!",
        "created_utc": 1673225333,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question: which statistical test would be optimal for this data?",
        "author": "tomcvb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/106zezu/question_which_statistical_test_would_be_optimal/",
        "text": "I am doing research project and I come here for advice - which statistical test should I use?\n\nMy cohort of patients were given drug A for 1 year and then the same cohort of patients were given drug B for 1 year. I have measured their condition using two numerical outcome variables, let's say the heart rate and blood pressure. I have measured the outcome variables at three points (start, 3months, 1 year) for drug A and at three points (start, 3months, 1 year) for drug B. \n\nI want to answer the question whether one drug is leading to statistically significant improvement over the other. Ideally I would also look at whether there are some confounding factors ie patients' age or sex that are influencing the results. What statistical test would you advise for this data? I am leaning towards factorial repeated measures ANOVA, but I am not sure if this is optimal, hence I would appreciate any advice, or direction.",
        "created_utc": 1673224296,
        "upvote_ratio": 1.0
    },
    {
        "title": "Intuition behind definition of expected value?",
        "author": "maybenexttime82",
        "url": "https://www.reddit.com/r/AskStatistics/comments/106vv8x/intuition_behind_definition_of_expected_value/",
        "text": "I understand concept behind expected value, but I'm still trying to figure out how someone came up with an idea of calculating it the way it is. Does anyone have any interpretation of that kind?",
        "created_utc": 1673215630,
        "upvote_ratio": 1.0
    },
    {
        "title": "Random number game - what is the probability you win?",
        "author": "DefinitelyAmNotOP",
        "url": "https://www.reddit.com/r/AskStatistics/comments/106teig/random_number_game_what_is_the_probability_you_win/",
        "text": "Hello. I’m trying to figure out what the probability of winning this made up game is. Here is the scenario:\n\nImagine you are given a random number between 1 and 9. You have an equal probability of being given each number. Then, I am given a random number between 1 and 9. Again, each number has an equal probability of being given to me, except I will not be given the same number as you. You have to guess whether your number is higher or lower than my number. What is the probability you guess correctly? What is the equation to solve this if instead there were n numbers.",
        "created_utc": 1673209869,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics Scholars Struggle: Uncovering The Reasons Behind High Failure Rates In Stats Courses",
        "author": "Proper_Attention1464",
        "url": "https://medium.com/@hiraedu/statistics-scholars-struggle-uncovering-the-reasons-behind-high-failure-rates-in-stats-courses-9dac40cccb34",
        "text": "",
        "created_utc": 1673209627,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to overcome extreme outlier variable when computing mean in small sample size",
        "author": "imreadytolearn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/106mvyt/how_to_overcome_extreme_outlier_variable_when/",
        "text": "I have been doing some disinfection experiments and testing how much residual contamination is found after cleaning medical tools. I am comparing two cleaning methods and allocated 100 instruments to each arm.\n\nMy issue is that in 1 arm, I have 8 instruments contaminated. There is small variance between the colony counts (10,10,10, 20, 10, 10, 40, 10, etc). In the other arm, I also have 8 instruments contaminated BUT have one very extreme outlier (10, 10, 10, 40, 60, **600**, 10 etc.), which is now making the average seem so much higher than the other arm.\n\nI know that one device had 3 different organism colonies so that's probably why the contamination count is high but now unsure what to do with this data point. I know I am not supposed to remove it but genuinely lost on how to overcome this now from a statistical and reporting standpoint. Because the sample sizes of contaminated instruments (n=8) is so small, this 1 outlier is skewing my means.\n\nAny advice?",
        "created_utc": 1673194266,
        "upvote_ratio": 1.0
    },
    {
        "title": "How does removing PILFs from dataset affect estimating 2-year recurrence interval flows?",
        "author": "sulodhun",
        "url": "https://pubs.er.usgs.gov/publication/tm4B5",
        "text": "",
        "created_utc": 1673194003,
        "upvote_ratio": 1.0
    },
    {
        "title": "Critical values for Jarque-Bera test",
        "author": "techcarrot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/106jo2d/critical_values_for_jarquebera_test/",
        "text": "Hi all! I am performing Jarque-Bera test in R using tseries package. In the results, I get the test statistics, DF and the p-value. I am wondering if it is possible to obtain the critical values somehow? Any suggestions?",
        "created_utc": 1673185692,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to show boxplots with interquartile ranges that are very different in size",
        "author": "guileus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1062vdh/how_to_show_boxplots_with_interquartile_ranges/",
        "text": "I'm analyzing the duration of labour contracts for fixed term contracts and permanent ones and want to show their median, p25 and p75 durations in a boxplot for different years. The problem is that fixed term contracts are, logically, of different durations. So, side to side, the boxplots for fixed term contracts look really small, to the point that it's a bit pointless to look at them (you can barely see their duration).  \nWould you say that the usual way to approach this is to separate them in different graphs so that the Y axes are of different scale?",
        "created_utc": 1673133508,
        "upvote_ratio": 1.0
    },
    {
        "title": "college group, what are they talking about? Is my response wrong?",
        "author": "YOU_TUBE_PERSON",
        "url": "https://www.reddit.com/gallery/105xr80",
        "text": "",
        "created_utc": 1673120731,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to interpret R contrasts when given continuous and categorical explanatory variables?",
        "author": "Little-Tutor-6213",
        "url": "https://www.reddit.com/r/AskStatistics/comments/105vhij/how_to_interpret_r_contrasts_when_given/",
        "text": "[removed]",
        "created_utc": 1673115046,
        "upvote_ratio": 1.0
    },
    {
        "title": "Predicting order from individual chaos",
        "author": "Swimming-Penalty4140",
        "url": "https://www.reddit.com/r/AskStatistics/comments/105s3bm/predicting_order_from_individual_chaos/",
        "text": "I saw something recently that claimed, even tho individual actions can be chaotic, on a larger scale they can be predicted. Is this true? If so, how? What must you account for in order to apply this?",
        "created_utc": 1673106506,
        "upvote_ratio": 1.0
    },
    {
        "title": "Tests with and without data normality",
        "author": "Samuuuh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/105qxez/tests_with_and_without_data_normality/",
        "text": "I conducted an experiment, where I collected 3 dependent variables (let's say variable A, B and C) from a group at a certain time. After some time I collected the variables again. I saw that some variables are normally distributed (like A) and other are have p &lt; 0.05 thus are not normally distributed. Should I use a paired T-Test for A and Wilcoxon signed-rank test for the variables that are non-normalized? Or should i use Wilcoxon signed-rank test for everything?",
        "created_utc": 1673103474,
        "upvote_ratio": 1.0
    },
    {
        "title": "Characteristics of Statistics",
        "author": "Imaginary_learner",
        "url": "https://i.redd.it/jpr2medswkaa1.png",
        "text": "",
        "created_utc": 1673079582,
        "upvote_ratio": 1.0
    },
    {
        "title": "Checking/extending my understanding of PCA--specifically the orthogonal relationship between principal components.",
        "author": "Ok-Needleworker-6595",
        "url": "https://www.reddit.com/r/AskStatistics/comments/105ggh2/checkingextending_my_understanding_of/",
        "text": "If I understand correctly:\n\nThe first principal component is essentially the same vector that would be found as the via OLS regression line/plane (which ends up being the eigenvector of the covariance matrix with the largest eigenvalue)\n\nThe second is orthoganol to this.\n\nIs the third orthoganol to that second vector? Would it not then be the same as PC1...?\n\nIs there a way to think of the kth and k+1 th principal components in terms of orthoganality similar to the first two?",
        "created_utc": 1673067986,
        "upvote_ratio": 1.0
    },
    {
        "title": "Recommended journals/papers focusing on DOE?",
        "author": "GhostGlacier",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1059qzr/recommended_journalspapers_focusing_on_doe/",
        "text": "I'm a non-thesis Masters student in applied statistics &amp; I'm interested in keeping up w/ the latest trends/research in DOE, but don't know where to start.",
        "created_utc": 1673049255,
        "upvote_ratio": 1.0
    },
    {
        "title": "Significance Testing Reporting Increase",
        "author": "ifinanceigloos",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1058tkk/significance_testing_reporting_increase/",
        "text": "Hello, \n\nLikely a dumb question, but I’d like to test if a slight increase in numbers of reports is significant. I’m Googling my heart out, but am struggling to get anywhere.\n\nLet’s say in November 50000 people reported the color of the car they bought that month. Then in December another 60000 people reported the color car they bought that month. 100 people said red in November, and 200 people said red in December.\n\nCan that increase in reports of buying a red car be tested for significance or am I totally messed up?\n\nThanks!",
        "created_utc": 1673046960,
        "upvote_ratio": 1.0
    },
    {
        "title": "what's robust method ? i heard it's to correct heteroscedasticity ,any more explanation ?",
        "author": "data_scientist_futur",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1058l0x/whats_robust_method_i_heard_its_to_correct/",
        "text": "",
        "created_utc": 1673046387,
        "upvote_ratio": 1.0
    },
    {
        "title": "can i keep working with the model that i estimate even if the residuals aren't normally distributed? or i shall change it ?",
        "author": "data_scientist_futur",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1058hpj/can_i_keep_working_with_the_model_that_i_estimate/",
        "text": "",
        "created_utc": 1673046165,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for the appropriate model/test",
        "author": "ToTransistorize",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1054rfa/looking_for_the_appropriate_modeltest/",
        "text": "I am wondering if anyone could point me in the right direction on if/how one could draw reasonable conclusions from a dataset that I have from work.\n\n* I am looking 5 customer service metrics (all expressed as a percentage yes versus no).  \n* Every day I get about 20 responses.\n* Every day there are about 3 different managers working (from a pool of 9).\n\nI am interested in whether or not I can associate different metrics with different managers.  In other words, is there a statistical model or test that would be appropriate to measure the correlation between manager X and metric Y?\n\nIn terms of ethics, I am not using this data for performance evaluation.  It is more of a general curiosity that I've been thinking about while bored.  The reality is that these metrics are influenced by many more variables than just the managers on duty, but I would still find the results interesting.\n\nThank you!",
        "created_utc": 1673037372,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] Need some help choosing an appropriate statistical analysis",
        "author": "L-Hagura",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1053ka9/question_need_some_help_choosing_an_appropriate/",
        "text": " \n\nHi all,\n\nCurrently I am facing a problem as follows:\n\nI have a time-elapsed protein interaction profile, in which if two residues are making contacts, the pair is given a score according the contact type and strength.\n\nA simplified data structure would look like this:\n\nSay I have 4 residues (A, B, C, and D) and a total of 3 frames:\n\n&amp;#x200B;\n\n|frame|pair|score|\n|:-|:-|:-|\n|0|A\\_B|1.2|\n|0|A\\_C|3.0|\n|0|B\\_D|0.2|\n|1|A\\_C|7.1|\n|1|B\\_C|4.6|\n|1|C\\_D|5.3|\n|2|A\\_D|3.9|\n|2|B\\_D|2.7|\n|2|C\\_D|0.9|\n\nThe scores are based on the same criteria so they are comparable across frames and pairs . Scores are all positive floats. Pairs that are not present in a certain frame will automatically have a score of 0\n\nMy goal is to find a proper way to compare residues (**not residue pairs**) based on all contacts it makes with other residues through time. In other words, I'd like to derive a metric from said data sheet to compare residues (**not residue pairs**).\n\nThis problem is difficult for me because each residue is associated with multiple scores in one frame and multiple frames. My initial thought is to first average within a frame and then average the frame averages. However, since the distribution and range of scores vary greatly, averaging is definitely not a good approach.\n\nThen I thought about using Z scores, but I was immediately stuck by which dimension I should apply Z score on: should I do Z-score on all the pairs within a frame, or should I do Z score on the same pair across frames?\n\nThat's all I have thought about till now. Could anyone provide me with some insights?\n\nThank you very much!",
        "created_utc": 1673034560,
        "upvote_ratio": 1.0
    },
    {
        "title": "Log-Log Regression on Bimodal data w/ the upper modal being very noisy - confidence in curve itself and extrapolated point on curve?",
        "author": "jzini",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10537t9/loglog_regression_on_bimodal_data_w_the_upper/",
        "text": "Hey everyone - hope this is inline with questions in the community and would greatly appreciate some keywords or a point in the right direction to research.\n\nAs the title suggests, I have a log-log regression model with x:log(spend), log(revenue). x tends to be bimodal with the lower mode being a larger proportion of the sample. When x is higher and in the upper mode, there could be externalities as to why. In a perfect world, I could remove these but I do not have access to that information. I am stumbling my way through Cook's Distance on trying to understand how removing these upper values might show reliable is the curve overall (those higher x points are doing a lot of work in determining slope).\n\nUsing secondary information, we determine an optimal point on the curve, which typically is an extrapolation. I would feel pretty confident if the recommendation was in the center of the lower x bimodal data but frequently the recommendation is outside of the sample. \n\n**Question:** Which techniques should I research to create an overall confidence interval (or value) for the recommendation (x), given the confidence in the curve itself, as well as how far out of sample x is?\n\nNote:  I am a later-in-life practitioner slowly learning academic rigor and my terminology is frequently off. Please be gentle :)",
        "created_utc": 1673033734,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best Coursera Course to Start Statistics",
        "author": "pabll9824",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10522fn/best_coursera_course_to_start_statistics/",
        "text": "Hello,\n\nI am a postdoc in history/IR and I am looking for a way to gain some knowledge in statistics. My goal is to use it in my research, but I have also started considering a career change and I am exploring potential job opportunities in the field of data analysis.\n\nCoursera offers plenty of courses, but I am not sure which one would be more useful to somebody coming from Humanities. I would appreciate your help. \n\nThese are the links:\n\n[https://www.coursera.org/specializations/statistics](https://www.coursera.org/specializations/statistics)\n\n[https://www.coursera.org/specializations/social-science](https://www.coursera.org/specializations/social-science)\n\n[https://www.coursera.org/professional-certificates/google-data-analytics](https://www.coursera.org/professional-certificates/google-data-analytics)\n\n&amp;#x200B;\n\nThank you!",
        "created_utc": 1673030989,
        "upvote_ratio": 1.0
    },
    {
        "title": "Demand Elasticity",
        "author": "saikjuan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104yaeg/demand_elasticity/",
        "text": "Hey there! \nI'm working on a project that involves estimating price elasticity of demand. I got information on daily historic sales and prices, and binary variables on which promotions applied for the day. \n\nI uses the transaction information for each day as an observation and included some seasonality variable for the day.  The thing is, for some products I get positive price elasticities (sometime significant at 5%) no matter what I do, which is not helpful. \n\nI was wondering what things could I try to fix this estimates? \n\nI was thinking on computing the average quantity sold at each price (which leaves me with a tiny fraction of points), and makes me lose things like the seasonality. Some of these are fixed by this, but idk how correct this approach would be. \n\nDo you have any suggestions?",
        "created_utc": 1673022091,
        "upvote_ratio": 1.0
    },
    {
        "title": "How is the log likelihood of a regression model obtained?",
        "author": "Washbowl9845",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104uras/how_is_the_log_likelihood_of_a_regression_model/",
        "text": "Could someone explain to me how likelihoods (or log-likelihoods) are calculated for a regression model? I was under the impression that the predicted values are the maximum likelihood estimates. But these in themselves don't give any more information that can be used to assess model fits, so are the likelihoods calculated with the residuals or something? I'm a bit confused by it all. And would their calculation differ for different models? The only explanations I have seen are examples using the  binomial distribution or the normal distribution but none are applied to actual fitted models.",
        "created_utc": 1673013244,
        "upvote_ratio": 1.0
    },
    {
        "title": "Someone can help me with Rstudio? From hypothesis testing to linear regression.",
        "author": "Business_Guidance",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104un0g/someone_can_help_me_with_rstudio_from_hypothesis/",
        "text": "",
        "created_utc": 1673012922,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confused about Spearman correlation ?",
        "author": "al3arabcoreleone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104uk3r/confused_about_spearman_correlation/",
        "text": "I can't understand well the Spearman coefficient, it is based on the ranked values for each variable rather than the raw data, but why would we use ranks ?",
        "created_utc": 1673012701,
        "upvote_ratio": 1.0
    },
    {
        "title": "Having trouble understanding these results on price moves on a basic financial index.",
        "author": "thinkofanamefast",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104s0ni/having_trouble_understanding_these_results_on/",
        "text": "The \"VIX\" index is a measure of volatility on stocks, and the options on their stocks. The more extreme the stock market is in terms ove price moves, the higher the VIX is. BUT over time it always reverts to its long term mean of about 19...and that goes back 40 years.\n\nSo I am testing the size of avg moves over 5 days, 10 day, 50 days, etc, in percentage terms, using data from Yahoo finance. I simply take say day 5 final price, deduct day 1 price, multiple by 100, and I have my 5 day move percent.  BUT when I do that over 10,000 5 day periods I get positive 1 as a column average. Shouldn't it be near 0 since reverts to 19?  \n\nA positive 1 implies to me it grows 1% every 5 days, to the Vix should now be in the millions, and not it's current 21.\n\nThis happens with every length I pick, 5, 10, 20 days etc. I am thinking I perhaps should be doing some sort of weighted average instead? But how, or rather isn't this inherently a weighted average? Or maybe when higher (like if vix is 30) it's influence on final average is bigger, BUT when vix is higher, down moves are bigger on average than upward average up moves because the stock market avg drop is bigger than avg gain over any period, but way more average gains, and VIX moves opposite that.\n\nI'm stumped...thanks for any help.",
        "created_utc": 1673004975,
        "upvote_ratio": 1.0
    },
    {
        "title": "Likert scale - composite indicator",
        "author": "yingib26",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104nfrx/likert_scale_composite_indicator/",
        "text": "Hi! I have 3 Likert type questions (0-4) and would like to measure 1 latent variable with these 3 questions. \nIf I calculate average for each respondent (this is how official toolkit for measuring this variable suggests), instead of 5 different answers (0,1,2,3,4) i would get 13 (0,0.333,0.6667,1,1.333 etc.). \n\nI decided to round this results to the closest integers (instead of 0.333 I would round it to 0, 0.6667 to 1 etc.). I am aware that be doing this I over/underestimate results, but was wondering is there some theoretical or empirical ground that justifies that?\n\nThank you in advance.",
        "created_utc": 1672988183,
        "upvote_ratio": 1.0
    },
    {
        "title": "Chi Square Test for Independence on Population Data?",
        "author": "wyvern91",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104hvuf/chi_square_test_for_independence_on_population/",
        "text": " \n\nHi All!\n\nThis  feels like a silly and basic question, but I'm not sure I know how to  explain why it seems silly to me: Is it reasonable to do Chi-Square test  for independence on *a* *population*, if we have the data?\n\nHere's  a goofy example: I'm interested in whether there is a meaningful  relationship, among my friends, between their college degree status and  their political affiliation. I go to all of my Facebook friends (because  we are all such great friends) and I ask each of the 645 friends. I get  the following data (degree/nodegree):\n\nDemocrat (118/154)\n\nRepublican (107/148)\n\nThird Party (combining independent, libertarians, and green to not violate 5&lt;) (33/85)\n\nWhen I run a proportions test in R, I get the following output:\n\n*data:  Friends2*\n\n*X-squared = 8.8251, df = 2, p-value = 0.01212*\n\n*alternative hypothesis: two.sided sample estimates:*\n\n*prop 1    prop 2    prop 3*\n\n*0.4338235 0.4196078 0.2796610*\n\nThe  low p-value tells me that having a degree and political affiliation are  statistically significant (at .05), but it also feels odd to do this on  population data, vs a sample (maybe because we only talk about samples  in all of my stats classes?). I'd appreciate any thoughts.",
        "created_utc": 1672971480,
        "upvote_ratio": 1.0
    },
    {
        "title": "Books for Stats",
        "author": "IvanKzov",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104hrjq/books_for_stats/",
        "text": "Been using basic statistics for data analysis. By basic I mean just averages on Excel and cross tabs once in a while to see correlations.\n\nAny suggestions/advice which books to start with to understand data better?",
        "created_utc": 1672971168,
        "upvote_ratio": 1.0
    },
    {
        "title": "I am using a statsmodel LMM to analyse random effects in my data but I am told I need to use GLMM instead. Could someone help me understand why and how to do that?",
        "author": "lifelifebalance",
        "url": "https://www.reddit.com/r/AskStatistics/comments/104e7m5/i_am_using_a_statsmodel_lmm_to_analyse_random/",
        "text": "Currently I have used a linear mixed model with random intercepts and random slopes for my data. The dependent variable is a score value, the independent variables (fixed effects?) are people and the random effect is day of the week. Here is a plot I made that shows all the scores for each person (there are 175 people) and I have overlayed the regression lines for the intercepts and slopes for each day of the week: [https://imgur.com/MafDvjZ](https://imgur.com/MafDvjZ)\n\nThis is what I am using to get the intercepts and slopes for the random effect:\n\n    lmm = smf.mixedlm(\n             \"delta ~ subject\", \n             attendeeEnergy, \n             groups=attendeeEnergy[\"day_of_week\"], \n             re_formula=\"~subject\"\n          ).fit()\n\nBut I am told that I need to be using a generlized linear mixed model instead because that is necessary for categorical predictors. \n\nFirst of all I do not understand why that is the case and I would really like some clarity on that if possible. The person I am getting direction from has used GLIMMIX in sas but doesn't know about the details about what is going on behind the scenes and for me that is critical for my understanding of why I will need to use the GLMM instead of the LMM. \n\nSecondly I am not too sure how to implement it. Statsmodel has a \"BinomialBayesMixedGLM\" method but the documentation is not very detailed. This is their example:\n\n    random = {\"a\": '0 + C(Village)', \"b\": '0 + C(Village)*year_cen'}\n    model = BinomialBayesMixedGLM.from_formula(\n                   'y ~ year_cen', \n                   random, \n                   data\n            ).fit_vb()\n\nDoes anyone know how I could use my data with the BinomialBayesMixedGLM? The dataframe that contains my data is like this:\n\n&amp;#x200B;\n\n|person|score|day\\_of\\_week|\n|:-|:-|:-|\n|1|30|0|\n|1|\\-20|3|\n|2|\\-2|0|\n|3|10|4|\n|1|\\-5|1|",
        "created_utc": 1672962096,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical significance",
        "author": "CyborgG2005",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1049q3o/statistical_significance/",
        "text": "Hello, \n\nI am a highschool student writing a psychology research paper on the correlation between physical activity, anxiety and self-image. During data analysis I've got myself stuck with the question if statistical significance (T test/p-value) is the right method of determining whenether my findings are significant or not. This is my current progress:\n\nI have a list of how many times weekly a certain individual performs physical activity (graded: once is 1 point, twice or three times is 2 points...) and their result of a self-image test. Here is an example:\n\n&amp;#x200B;\n\n|Frequency \\[in points\\]|Self-image \\[test performance\\]||\n|:-|:-|:-|\n|0|0,48||\n|0|0,72||\n|1|0,64||\n|4|0,5||\n|3|0,6||\n\n\\[I've made these examples up for the sake of demonstration\\]\n\nI have calculated the correlation between these two data: cca. -0,237, which agrees with my hypothesis that there is a positive correlation between a good self-image and frequency of physical activity (a higher number in self-image means worse performance on the test).\n\nBut how do I test the significance of my findings? Is the T-test the right test for this - if so, which one; all the tail and other variatons have made me very confused. What I've found so far is that it is used only with two sets of data of the same category (for example two columns of mass if we are researching weight loss).\n\nThank you very much kind redditors\n\nP.S. English is not my first language so be wary of possible spelling errors / word usage :)",
        "created_utc": 1672951746,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help interpreting effect size of odds ratios in an interaction model",
        "author": "copernicanrevolution",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10490jc/help_interpreting_effect_size_of_odds_ratios_in/",
        "text": "Hello, I'm hoping someone can help. I have a logistic regression interaction model and understand how to interpret the coefficients individually, but am struggling to understand how to interpret the *effect size of the interaction*.\n\nI am using [this](https://easystats.github.io/effectsize/articles/interpret.html#chen2010big) article to interpret effect sizes of Odds Ratios. It's fine so long as I'm interpreting effects individually, but I do not understand how to interpret odds ratios of interaction effects.\n\nThis is my regression table (simplified to only include parameters and their estimates), Note: CONDControl at TimeT1 is baseline.\n\nhttps://preview.redd.it/dfn7704s6aaa1.png?width=267&amp;format=png&amp;auto=webp&amp;s=4c4a504219050764b4b3ebe74cf577ff99636a00\n\nI have then calculated the ORs: \n\n\\# odds ratio comparing T1 and T2 in the control condition: exp(-0.24) = 0.7866279\n\n\\# odds ratio comparing T1 and T2 in the sticker condition: exp(-0.24 + 0.77) = 1.698932\n\nFollowing J. Cohen (1988) in the article I linked above on interpreting EFFECT SIZES (very small, small, medium, large) of Odds Ratios, the effect of the change from T1-T2 in the control condition is very small (**0.78** &lt; 1.44). The effect of the change from T1-T2 in the sticker condition, compared to the T1-T2 change for the control condition is small (1.44 &lt;= **1.7** &lt; 2.48). \n\nDo I then need to compare the difference in the differences between T1-T2 for the two groups? i.e. calculate an odds ratio of an odds ratio? If so, how do I interpret effect size of the difference in the differences (following the Cohen 1988 guidelines)? \n\nAlternatively, when reporting results, is it sufficient to say that the effect size of the change between T1 and T2 in the sticker condition when compared to the T1-T2 change in the control condition, was small?",
        "created_utc": 1672950067,
        "upvote_ratio": 1.0
    },
    {
        "title": "my first stat analysis - assessing baseline differences",
        "author": "hello1397",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1048ocs/my_first_stat_analysis_assessing_baseline/",
        "text": "I apologize if this is a simple question compared to those of you with stat expertise but I’m not getting any guidance on my analysis from mentors. Is a chi square the appropriate test for assessing baseline differences for categorical variables between 2 groups? I used independent sample t-tests for continuous. Thank you!! Sorry I’m new to stats and have anxiety about every decision I’m making.",
        "created_utc": 1672949259,
        "upvote_ratio": 1.0
    },
    {
        "title": "good books/web sources on data visualization best practises",
        "author": "mathestnoobest",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1048cw0/good_booksweb_sources_on_data_visualization_best/",
        "text": "looking for good books/web sources that can help me graph better. thanks!",
        "created_utc": 1672948513,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sampling leaves from a tree",
        "author": "dzieciou",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10479ls/sampling_leaves_from_a_tree/",
        "text": "If I have a tree of product categories, where leaves are product names (tomato, soap, etc.) and inner nodes are categories (fresh products, cosmetics) and subcategories (fresh products&gt;vegetables), how do I select a representative sample of N products from the tree?\n\nWhat research area study that?\n\nThis is related to stratified sampling, but in my case, there are many levels (subpopulation can contain multiple subpopulations, which in turn...)..\n\nAlso, the tree can be unbalanced, and some subtrees might be higher, containing many very specialized categories...",
        "created_utc": 1672945914,
        "upvote_ratio": 1.0
    },
    {
        "title": "Data still skewed after log transformation",
        "author": "econgirl210",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10472lz/data_still_skewed_after_log_transformation/",
        "text": "https://ibb.co/G5Z17HQ\n\nI did a Shapiro test in R, which indicated a log transformation was ideal for my data. \n\nAs you can see from the photo, mass shootings is still right skewed. And a new Shapiro test indicated both variables were still not normally distributed. \n\nI tried a variety of other pre processing methods (box-cox, center, scale etc) which were better, but still not normally distributed. Although these transformations were better than the log transformation alone, I couldn’t use them in my analysis because it made the shootings variable negative. Negative values cannot be used in a poisson regression. \n\nI decided to proceed with the log transformation since it gave positive values, but it gave some very unrealistic beta coefficients (in the thousands) when performing an OLS regression. \n\nAny thoughts on what to do from here?",
        "created_utc": 1672945447,
        "upvote_ratio": 1.0
    },
    {
        "title": "Predicting year end survey score",
        "author": "Working_Universe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1046zfl/predicting_year_end_survey_score/",
        "text": "I have a survey dataset that spans three years and about 90,000 respondents. At the end of each year, we report a 12-month satisfaction score (percent of respondents that gave a positive score to a single question). I would like to use that dataset to predict what the 12-month satisfaction score will be at the end of 2023 and generate error bands around this score. Can you provide some guidance on how to do this? (I am using SPSS.)",
        "created_utc": 1672945242,
        "upvote_ratio": 1.0
    },
    {
        "title": "E(X) = λ is used for which distribution?",
        "author": "Nazma2015",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1046q8r/ex_λ_is_used_for_which_distribution/",
        "text": "\n\n[View Poll](https://www.reddit.com/poll/1046q8r)",
        "created_utc": 1672944639,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to measure impact of a player on winning games?",
        "author": "brynbn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1046ojy/how_to_measure_impact_of_a_player_on_winning_games/",
        "text": " The context I'm looking at here is basketball, but I think can be generalized to any team game. And apologies if any of this is obviously answerable on the internet, I haven't found anything myself but I'm admittedly a newbie at statistics.\n\nThe information that I have at my disposal are a bunch of box scores (minutes played, points scored, rebounds, assists, FG, etc...) but no advanced stats. And of course, I can also infer from these which team won based on point totals. I want to get a general sense of how much each player impacts winning.\n\nThe issue is that I want to basically control for how good or bad each player's teammates are. You may have a player who is extremely good, but the team around them is very bad. Or vice-versa. So to just say \"this player is bad because he plays a lot of minutes and doesn't win a lot\" is not nuanced enough. I can look at the stats themselves and see how many points per minute the player scores, but that doesn't take into account the percentage of games won, or if the player impacts the game in other ways other than scoring points.\n\nHow would a statistician approach this problem? I'm sure there are people who have tried, which is why I wonder if I just haven't been looking in the right places. I see similar statistics but they use information I don't have, like possession-by-possession data rather than player-game level data. (I am limited in the amount of data I have because this is for an NBA fantasy GM league, and we only record box scores as historical stats.)\n\nThank you in advance for helping a novice!",
        "created_utc": 1672944528,
        "upvote_ratio": 1.0
    },
    {
        "title": "Questions regarding Cronbach´s Alpha and Questionnarie",
        "author": "BilingualEdResearh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1042fcs/questions_regarding_cronbachs_alpha_and/",
        "text": "Hello! I have a couple of questions for you. Thank you in advance!!\n\n1. I have created a questionnaire of roughly 30 items, but they are divided into 5 sections (It´s a questionnaire for teachers, one section is about class materials, another is about class preparation, etc). Do I run an alpha test for each section or can I do just 1 for the entire questionnaire?\n2. I have a mixture of likert-type questions as well as open-ended and ¨Check all that apply¨questions. I can only include the likert-type items in the cronbach test, is that correct?\n\nThank you!",
        "created_utc": 1672934233,
        "upvote_ratio": 1.0
    },
    {
        "title": "Opening .dta file in STATA help",
        "author": "tothemoon360",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1041afi/opening_dta_file_in_stata_help/",
        "text": "I am trying to open data from the survey of consumer finances 2019 in STATA for my university project. However, when I download the .dta file to my computer it says it is an Adobe acrobat document but it cannot be opened in Adobe acrobat reader and I am unable to open it using STATA use commands as it just says the file is not found.\n\nHas anyone had a similar problem or am I missing something? I am a complete STATA beginner but assumed it should be easy enough to open the data.",
        "created_utc": 1672931483,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you write Lasso in such a way?",
        "author": "Bigg_UN",
        "url": "https://i.redd.it/naz24l2j99aa1.jpg",
        "text": "",
        "created_utc": 1672919819,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are the odds of a blackjack dealer running out a 13 card 21?",
        "author": "StarTrekDaddy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103vnbl/what_are_the_odds_of_a_blackjack_dealer_running/",
        "text": "Rules : 8 decks. Dealer hits on soft 17, stands on hard 17+\n\nThe hand would have to run out as follows for the dealer to actually hit it:\n \nA,A,A,A,A,A,A,5,A,A,A,A,5\n\nIt’s not possible any other way.\n\n\nI also wonder how long it would take the world to see a hand like this given the average amount of blackjack being dealt around the world.",
        "created_utc": 1672914244,
        "upvote_ratio": 1.0
    },
    {
        "title": "Describe why you love statistics in two lines.",
        "author": "Indian-badad",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103v10k/describe_why_you_love_statistics_in_two_lines/",
        "text": "",
        "created_utc": 1672911893,
        "upvote_ratio": 1.0
    },
    {
        "title": "any course or youtube playlist for undergraduate probability and stats?",
        "author": "Willing_Accountant18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103s5z4/any_course_or_youtube_playlist_for_undergraduate/",
        "text": "I am going to take the entrance test for the master's in stat.  any course or youtube playlist which is math-heavy undergraduate-level probability and statistics.\n\nsyllabus     [https://jam.iitg.ac.in/uploads/syllabi/MS\\_Syllabi.pdf](https://jam.iitg.ac.in/uploads/syllabi/MS_Syllabi.pdf)",
        "created_utc": 1672901369,
        "upvote_ratio": 1.0
    },
    {
        "title": "Levene's test not stated in independent sample t-test (SPSS)",
        "author": "hamsterlion2020",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103s3dm/levenes_test_not_stated_in_independent_sample/",
        "text": "Hello, I run an independent sample t-test in SPSS and the result come out as the picture. Usually, I read it from the levene's test result, if the significance &lt;0.05, I see the sig (2-tailed) in  the \"equal variances not assumed\" row. if the significance &gt;0.05, I see the sig (2-tailed) in  the \"equal variances assumed\" row. but in this case, the levene's test not stated. why is it? how should I interpret it? thank you\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ib1cku2586aa1.png?width=1537&amp;format=png&amp;auto=webp&amp;s=4e19fc49420b735eeb3ed5983d1abbf5e3407a32",
        "created_utc": 1672901119,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hey everyone I’m Data Scientist and I play Brawl Stars in my free time. I’ve created a Brawl Stars report using python using Brawl Stars API. The plots in the report help users bring balance among their brawlers and community as well. This requires patience. Please find below the link to the repo.",
        "author": "Andrez2000",
        "url": "https://www.reddit.com/gallery/1030a6d",
        "text": "",
        "created_utc": 1672893752,
        "upvote_ratio": 1.0
    },
    {
        "title": "What analysis do I use to show that a constant (different for each sample, doesn't change through time) variable affects the development of a dependent variable through time?",
        "author": "snouuuflake",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103pn0v/what_analysis_do_i_use_to_show_that_a_constant/",
        "text": "The title is likely worded wrong, but here is my situation. I'm trying to study how exposure to UV-B light affects plant growth. I had pots with multiple plants each. Each pot was exposed to a different intensity of UV-B light over a few days. Each day, the height of all plants was measured. How do I show that the growth of plants with more UV light over time was far less than the ones with less light? (It is very obvious that this happened but I do not know which analysis to use.)",
        "created_utc": 1672893390,
        "upvote_ratio": 1.0
    },
    {
        "title": "Normal distribution conditions when sampling.",
        "author": "itouchedmybread",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103nsr9/normal_distribution_conditions_when_sampling/",
        "text": "Why can a normal distribution not be distributed if only taking in whole number values?",
        "created_utc": 1672888100,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bi-variate correlations with one bimodally and one normally distributed variable",
        "author": "GuybrushManwood",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103m2s0/bivariate_correlations_with_one_bimodally_and_one/",
        "text": "I have two variables: `x1` is normally distributed, while `x2` has a bimodal distribution (see screenshot).\n\nI assume these two variables to be correlated. However, `x2` cannot be interpreted linearly, since the range `-.2 &lt; x &lt; .2` (roughly) contains noise, while everything outside that range is assumed to be signal.\n\nHow do I approach this? Do I need to transform `x2`? I was thinking about omitting the data points in the noise range `-.2 &lt; x &lt; .2` from the data set, but that would create an \"artificial gap\", which I suppose is a problem?\n\nThanks\n\nhttps://preview.redd.it/uh14nfyqr4aa1.png?width=566&amp;format=png&amp;auto=webp&amp;s=777759b78d8448b720539b9d6904dce909805f1b",
        "created_utc": 1672883415,
        "upvote_ratio": 1.0
    },
    {
        "title": "What would be the best statistical test to determine the relationship between deprived areas and higher covid-19 cases?",
        "author": "throwawayq93646",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103fvcx/what_would_be_the_best_statistical_test_to/",
        "text": "",
        "created_utc": 1672868282,
        "upvote_ratio": 1.0
    },
    {
        "title": "Adding Additional Units to a Sample",
        "author": "schfourteen-teen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103fjxe/adding_additional_units_to_a_sample/",
        "text": "I'm in a debate with a coworker that I need some more knowledgeable statistics person to weigh in on.\n\nIn our industry (medical devices), it's pretty common to use statistical tolerance intervals to demonstrate conformance to the engineering specifications, usually with pretty small sample sizes. The tolerance intervals can be done on variables data with very few samples, or on an attribute basis using binomial at slightly higher sample sizes.\n\nA common occurrence is that if you run your test on variables data and there are no outright failures, but the tolerance interval limit does not meet the specification, you just test more samples. And if you continue to not pass statistically and reach the attribute sample size, you \"convert\" to attribute and call it a pass.\n\nI've argued that in a frequentist framework, it's not ok to add units to an existing sample unlike in a Bayesian framework. But I'm struggling with why this is the case (if it even is, cause I could be wrong).\n\nWhat's the explanation?",
        "created_utc": 1672867559,
        "upvote_ratio": 1.0
    },
    {
        "title": "Between subject, independen &amp; dependent variables and best course of action?",
        "author": "juuhe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/103f2np/between_subject_independen_dependent_variables/",
        "text": "Hello, I have an experiment with one Control and one Test group with equal amount of males and females (I will not test sex differences). Within each of these two groups I have 3 days where I have 2 samples each, one before and one after treatment. I understand that studies with similar task have used repeated mesures Anova. All of my groups (Control vs Test, Day 1 vs 2 vs 3) would be independent variables while the repeated measure (pre and post treatment) is a dependent variable.\n\nMy question is, would this be considered a purely between-subject study? To my knowledge a repeated measures anova is a within-subject test so what test should I do? Are there better options to explain if Control or Test had an effect on treatment as a whole when looking at Day 1, 2 and 3 or if Control or Test had an effect on pre or post treatment depending on Day 1, 2 ,3.\n\nI hope I could make myself understood and I'm doing this in R too so if there are any tips on how to do it there I'd be more than happy!",
        "created_utc": 1672866436,
        "upvote_ratio": 1.0
    },
    {
        "title": "https://www.reddit.com/r/statistics/comments/103afs1/spss_pairwise_comparisons/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf",
        "author": "[deleted]",
        "url": "",
        "text": "[deleted]",
        "created_utc": 1672855828,
        "upvote_ratio": 1.0
    },
    {
        "title": "Extremely basic question, but I can't figure out how to Google it, please help",
        "author": "DepressiveHedonist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1037bw1/extremely_basic_question_but_i_cant_figure_out/",
        "text": "I'm going to graduate school for public policy analysis, and I've completed a single semester so far. But my work place knows about this, and asked me to gather a bit of data and do a hypothesis test of a single mean on it to find out whether a particular program of ours has an impact. I cannot emphasize enough the fact that I have taken precisely 2 statistics classes and done precisely zero paid statistics work at this point in my life. This is the reason I have such an embarrassingly basic question. \n\nSo I gathered the data and did the test. But here's my question. The data we're counting is dollars. Dollars are never going to be negative. So does it make any sense for me to set my null hypothesis at $0? Half the sampling distribution is impossible. But if not at $0, where do I set the null? \n\nAnd how is it possible that I got an A in two statistics classes and don't know this yet? Just kidding, no need to answer this last question!",
        "created_utc": 1672848519,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are hazards in Cox regression even meaningful? Why has Cox regression become the norm for time-to-event analysis.",
        "author": "Over_Datum_Melk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10374r5/are_hazards_in_cox_regression_even_meaningful_why/",
        "text": "I've used Cox regression quite a bit but every time I have to explain what the output actually means to a non-statistician I find it very difficult. I feel like the hazard being the \"instantaneous risk of the event occurring\" doesn't really mean much when in reality events only occur within a measurable span of time. Additionally, hazard ratio's can be significant while the confidence intervals for the estimated survival curves of two groups for example still completely overlap. A question I have gotten on several occasions is then: \"How can it be that the instantaneous risk of death is significantly greater in one group, but the predicted probability of survival is the same throughout the entire timespan?\".\n\nI haven't worked much with accelerated failure time models, but from what I have seen their outputs are much more meaningful, being directly interpretable in terms of expected survival time. Why is the Cox framework so dominant when it comes to modelling of time-to-event data?",
        "created_utc": 1672848045,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help on binary variables test of significance",
        "author": "Accurate_Lynx_8464",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1033s3t/help_on_binary_variables_test_of_significance/",
        "text": "  \n\nHi, \n\nI am looking for some one-off support to help me check a piece of work for my job where I am testing the significance of funding amounts across two binary variables. I am using open source data so I can send you a work book if it helps. \n\nI have two variables: variable 1-yes/no and variable 2- yes/no\n\nThe value I am using is USD amounts\n\nSo far, I have used the Chai squared test but I am not sure now if this is the correct one or if I have done this correctly (I am just using excel)\n\nI need some help to:\n\n\\- Check my results (should I have used the Fishers test?)\n\n\\- Explain the results\n\n\\- Check if I should have used a paired test\n\n\\- Anything else I may have missed\n\nI have tried to find a tutor on tutorful and JustAnswer but no luck, I am lookinkg for someone to have a chat with if possible.",
        "created_utc": 1672839107,
        "upvote_ratio": 1.0
    },
    {
        "title": "The chi-square Goodness of fit is to fit one categorical variable to a distribution.",
        "author": "luchins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1033m4m/the_chisquare_goodness_of_fit_is_to_fit_one/",
        "text": "what does it mean to fit a categorical variable to a distribution? Any example please?",
        "created_utc": 1672838622,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is AI tools going to replace Statisticians and Data analysts",
        "author": "Educational_Day8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1030h55/is_ai_tools_going_to_replace_statisticians_and/",
        "text": "I have seen ai tools can do Complex  statistics works\nAnd can creat full dashboard with data interpretation and reports. \n\nWhat is your opinion?",
        "created_utc": 1672828938,
        "upvote_ratio": 1.0
    },
    {
        "title": "Montecarlo simulation Python library - run Montecarlo simulations with two lines",
        "author": "Fragrant-Steak-8754",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102y0sy/montecarlo_simulation_python_library_run/",
        "text": "Hello redditors,\n\nI spent some time refactoring an old code of mine and transforming it into a Python library.  \nI would love to listen what do you guys think about it (this is my first time creating a library).\n\nCode is still working progress, but I released a 0.1.0 version that can be installed easily:  \n\\- it works with Python 3.10.0+;  \n\\- run \\`pip install montecarlosim\\` and enjoy.\n\nYou can find the repository here: [montecarlosim](https://github.com/vallops99/montecarlosim).\n\nThe \\`readme.md\\` explain both how to use it and how to install it.\n\nFurthermore, I'd like to be blasted if I made some errors writing the Montecarlo methods (I'm not a mathematician neither an engineer, so my statistics and mathematics grounds are questionable).\n\nHappy coding y'all!",
        "created_utc": 1672820121,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are conformal predictions? How different are they from Confidence Interval or Bayesian Credible Interval? [Question]",
        "author": "venkarafa",
        "url": "/r/statistics/comments/102fk8n/what_are_conformal_predictions_how_different_are/",
        "text": "",
        "created_utc": 1672806175,
        "upvote_ratio": 1.0
    },
    {
        "title": "do you guys check the residuals on the test dataset or on the entire dataset?",
        "author": "ilrazziatore",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102tdj2/do_you_guys_check_the_residuals_on_the_test/",
        "text": "Hi guys, sorry if it's a stupid question but  i don't know who to ask to. \n\nas the title says, i was wondering if i should check the residuals  only on the test dataset or i can use the entire dataset which obv includes  the training set too?\n\nI tried on the entire dataset and i  saw that my neural network  struggles to accurately fit the initial interval ( x&lt;0.2) but after that it works well.\n\nIf i use only the test dataset it may happen that it doesn't contain any value x&lt;0.2 and i lose that information",
        "created_utc": 1672805048,
        "upvote_ratio": 1.0
    },
    {
        "title": "How could I figure out the odds of this happening? (3 dice player rolls all ones and 2 dice player roles two sixes",
        "author": "ForgotTheBogusName",
        "url": "https://www.reddit.com/r/mildlyinteresting/comments/102rxr2/playing_risk_tonight_and_i_had_the_best_roll_vs/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf",
        "text": "",
        "created_utc": 1672801556,
        "upvote_ratio": 1.0
    },
    {
        "title": "Identifying inaccurate meter within a water network",
        "author": "tallowlab",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102s52x/identifying_inaccurate_meter_within_a_water/",
        "text": "Hi, I'm trying to identify where error is being introduced into a water network system. The system has 8 meters. 1 is an input, 6 are outputs and 1 meters can function as an input or an output. The mass balance more often than not is 'making water' i.e. the input volume is higher than the export volume. Part of this could be explained due to leakage however it is more so the variability that I'm interested in. The difference between the in and out ranges from 0-7% regularly with some larger spikes occurring.   \n\n\nI've completed a basic correlation assessment and found \\~0.8 correlation between the mass balance volume and the input meter and \\~0.9 correlation between the mass balance volume and the 2 way meter.   \n\n\nIs this enough to demonstrate inaccuracy in these two meters? I'd love to hear ideas for improving the assessment. I'd be happy to share some data is anyone is interested.",
        "created_utc": 1672801526,
        "upvote_ratio": 1.0
    },
    {
        "title": "I want to see which d20 is the fairest. Any opinions or advice?",
        "author": "iamngs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102pehr/i_want_to_see_which_d20_is_the_fairest_any/",
        "text": "Hello!\n\nSo, I want to preface this post by asserting that this isn't for any uni assignment or anything like that -- I want to do this experiment just for fun.\n\nBasically, what I want to do is to roll each \"type\" of d20 (20 sided dice) an obscene number of times, and record the data. By type, I mean material; swirled and glittery, solid plastic, glass, metal, liquid-filled, acrylic, etc. from a variety of manufacturers. \n\n&amp;#x200B;\n\nRight now, I am thinking of the following parameters:\n\n\\- Use a dice-tower and tray on a perfectly level surface in order to ensure the dice are all rolled the same way\n\n\\- Roll or \"bounce\" each die in my two cupped hands five times before each roll to sufficiently randomize\n\n\\- Roll each dice 20,000 times to account for variance, record data in spreadsheet\n\n&amp;#x200B;\n\nI am not a math or statistics major; I am a computer science major and I have yet to take statistics. I just want to undertake this project because it sounds like a fun way to spend a weekend and I really really love making spreadsheets.\n\nSo, I guess my question is this -- is there anything I should know before I start this? Do you think my parameters are okay? Should I do anything differently?\n\nSuper sorry for the bother, I hope this isn't a bad place to ask! I can't think of anywhere else to ask this question. Hope you are all having a good new year, thank you!! :)",
        "created_utc": 1672794150,
        "upvote_ratio": 1.0
    },
    {
        "title": "What would be the correct way to use Generalized Linear Mixed Models here?",
        "author": "lifelifebalance",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102p1pn/what_would_be_the_correct_way_to_use_generalized/",
        "text": "Originally I was planning on using just Linear Mixed Models. I have a score value as the dependent variable, different subjects as the independent variable and day of the week as the random effect. Each subject can have multiple score values associated to them and I want to determine the differences between the score values for each day of the week. So when plotted I would have vertical lines of data points for each subject. I didn't think this could work though since it seems like the slope of any regression line would change based on the order that the subjects were in along the x-axis. I was told that I could account for this using a GLMM.\n\nI am not sure how the GLMM would help with this though. I have been reading about them for a while now and it's not clicking for me. I read this [intro to GLMMs](https://stats.oarc.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/) today and it says that the differences between GLMMs and LMMs is that the response variables can come from different distributions besides gaussian and that rather than modeling the responses directly, some link function is often applied, such as a log link. Would one of these differences be the solution to my problem? \n\nI know this is not a lot of information to go off of, if I can add any additional info to make this more answerable please let me know.",
        "created_utc": 1672793264,
        "upvote_ratio": 1.0
    }
]