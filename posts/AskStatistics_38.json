[
    {
        "title": "Stats question about power of a test. Open below",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e7c1a/stats_question_about_power_of_a_test_open_below/",
        "text": "[deleted]",
        "created_utc": 1524441941,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone explain Fisher's method to me?",
        "author": "CielClaire",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e78o4/can_someone_explain_fishers_method_to_me/",
        "text": "I've googled around and can't find any good explanations that make sense to me. I get that in general, you just kind of plug in the right numbers to get the test statistic, but I've never been taught how to use this one.\n\nFor specificity, this is the equation I'm talking about: https://en.wikipedia.org/wiki/Fisher%27s_method\n\nI don't know what \"k\" and \"P sub i\" mean. For context, I'm dealing with a meta analysis of studies, and I have a bunch of p-values, one from each study. They are all about change in fasting blood sugar in response to either cinnamon or a placebo.\n\nIf anyone could explain what the variables in the equation mean to a guy who's not super well-versed on the subject, I'd really appreciate it.",
        "created_utc": 1524440952,
        "upvote_ratio": ""
    },
    {
        "title": "When doing a multiple variable linear regression to predict y given x1 and x2 , if x1 and x2 are perfectly correlated with one another (r = 1.0), what will the correlations be between x1 and y and x2 and y?",
        "author": "FireBoop",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e6hmm/when_doing_a_multiple_variable_linear_regression/",
        "text": "Hi, \n\nI'm struggling to word this properly I think. I know little about multiple variable linear regression. A clarifying example showing what I mean:\n\nI want to find a linear equation to predict Y given x1 and x2. point 1: (x1 = 1, x2 = 2, Y = -1), point 2 (x1 = 2, x2 = 4, y = -2). Both the equation -1/2 * x2 = Y and -x1 = Y equally well explain Y's behavior given x1 and x2. How would a multiple linear regression choose one equation or the infinite alternatives that perfectly predict Y.\n\nI would greatly appreciate any explanation or link to something I can read clarifying this.\n\nAlso, the exact problem I am trying to tackle is  \nfinding the relationship between X1 and Y while controlling for X2. When doing single variable regression, X1 is correlated with X2 (r = 0.5), X2 is correlated with Y (r = 0.5), and X1 is correlated with Y (r = 0.35). Given that 0.35 &gt; 0.5^2, I figure that X1 affects Y in a way that is not totally explained by X2. \n\nThanks",
        "created_utc": 1524433766,
        "upvote_ratio": ""
    },
    {
        "title": "What is the proper statistical test to determine if there is a relationship between a binomial variable and an ordinal variable?",
        "author": "haroldjim",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e5boc/what_is_the_proper_statistical_test_to_determine/",
        "text": "Hey guys. I am completing a project for my local health department. I handed out a survey with a response of yes/no to if seniors in the area have fallen. Then, there were many ordinal likert-style questions (1-unlikely; 5-very likely) along with them. I was wondering what statistical test to use to find a relationship between answering yes/no to falling and their response to certain ordinal questions. Chi-square? Somer’s D? Linear by linear? Thanks so much!",
        "created_utc": 1524423345,
        "upvote_ratio": ""
    },
    {
        "title": "What's the power of white and brownian noise?",
        "author": "IAmAUserOnReddit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e58dd/whats_the_power_of_white_and_brownian_noise/",
        "text": "Edited: Rewritten to expand my question.\n\nSuppose N is a positive integer.\n\nSuppose W, P, B, and X are discrete time signals:\n\n* W is Gaussian [white-noise](https://en.wikipedia.org/wiki/White_noise); in other words, each of its samples has a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).\n* P is pink noise.\n* B is Brownian noise.\n\nSuppose further that:\n\n* B[0]=W[0]\n* B[1]=W[0]+W[1]\n* B[2]=W[0]+W[1]+W[2]\n* etc.\n\nand\n\n* X[0]=P[0]\n* X[1]=P[0]+P[1]\n* X[2]=P[0]+P[1]+P[2]\n* etc.\n\nMy questions: As N goes to infinity,\n\n* what is the average (over i) value of the square of W[i]?\n* what is the average (over i) value of the square of P[i]?\n* what is the average (over i) value of the square of B[i]?\n* what is the average (over i) value of the square of X[i]?\n\nThanks.\n===============================================================\n\nOriginal post:\n\nSuppose N is a positive integer and suppose W is a discrete-time, N-length, Gaussian [white-noise](https://en.wikipedia.org/wiki/White_noise) signal; in other words, W is a discretely indexed time-signal, it's white noise, and each of its samples has a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).\nSuppose B is a discrete-time signal such that:\n\n* B[0]=W[0]\n* B[1]=W[0]+W[1]\n* B[2]=W[0]+W[1]+W[2]\n* etc.\n\n(In other words, B is [Brownian noise](https://en.wikipedia.org/wiki/Brownian_noise).) My questions: As N goes to infinity,\n\n* what is the average value of the square of W[i]?\n* what is the average value of the square of B[i]?\n\nThanks.",
        "created_utc": 1524422554,
        "upvote_ratio": ""
    },
    {
        "title": "Is this an error or am I misunderstanding!?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e5254/is_this_an_error_or_am_i_misunderstanding/",
        "text": "[deleted]",
        "created_utc": 1524421008,
        "upvote_ratio": ""
    },
    {
        "title": "In multiple restriction alternative hypothesis, should this be AND/OR or just OR?",
        "author": "nonighter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e51kf/in_multiple_restriction_alternative_hypothesis/",
        "text": "[removed]",
        "created_utc": 1524420866,
        "upvote_ratio": ""
    },
    {
        "title": "How do you interpret conflicting ANOVA and HSD results?",
        "author": "powderdd",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e43qb/how_do_you_interpret_conflicting_anova_and_hsd/",
        "text": "Why are post-hoc tests, like Tukey's HSD, only run *after* a significant ANOVA result? My understanding is that HSD adjusts to maintain the global Type I error rate. So why do ANOVA  &amp; HSD sometimes disagree (either with sig. ANOVA and non-sig. HSD or vice-versa)?\n\nThanks for your help!",
        "created_utc": 1524412574,
        "upvote_ratio": ""
    },
    {
        "title": "Area of NUTS1 Regions?",
        "author": "billybumpkins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e3r6y/area_of_nuts1_regions/",
        "text": "Hey guys! So I'm currently working on a little project to find the \"center of mass\" of per capita GDP in Continental Europe. Fun stuff! Because this is only small, informal project, I'm using the NUTS1 statistical units that essentially divide EU States+ into 98 different regions based on economic factors, etc.\nWhat I've already done is find the per capita GDP of each of these regions. What I have yet to do is devise some coordinate system so that I can find a double Riemann Sum of the \"density\" (aka per capita GDP) of these regions.\nWhat I'm having trouble with is finding the area of each of these regions. Does anyone know where I can find this?",
        "created_utc": 1524409298,
        "upvote_ratio": ""
    },
    {
        "title": "Movie scenes database searchable by content (not dialogue)",
        "author": "lousyfloozy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e3onq/movie_scenes_database_searchable_by_content_not/",
        "text": "Hi guys, Im doing school project and I would like to know if exists something like database of movie scenes tagged based on what's happening in the scenes, or what objects are in the shot.\n\nFor example when I would need to know in which year were the most movies with people using blenders or just blenders appearing in the scenes and how many times per movie? Does anything like that exists? All I have found were sites where you can search in movie scenes based on dialogue. :-/ \n\nI will be thankful for any advice. ",
        "created_utc": 1524408595,
        "upvote_ratio": ""
    },
    {
        "title": "How can i compute these predictive regressions",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e3mvy/how_can_i_compute_these_predictive_regressions/",
        "text": "[deleted]",
        "created_utc": 1524408130,
        "upvote_ratio": ""
    },
    {
        "title": "[python] time series rolling mean plot",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e2rc7/python_time_series_rolling_mean_plot/",
        "text": "[deleted]",
        "created_utc": 1524397568,
        "upvote_ratio": ""
    },
    {
        "title": "Help choosing the correct test(s) for survey analysis",
        "author": "ThatsAllFolks42",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e20yy/help_choosing_the_correct_tests_for_survey/",
        "text": "This is the first time I've done a study that is survey-based rather than experimental. I am unsure what test(s) I should be using since I haven't manipulated an independent variable. I'm just looking for a correlation between different groups.\n\nI have ordinal data in two categories with one having five levels and one having two levels (I'm not sure if \"levels\" is the appropriate term here since this is not an experiment). \n\nThe first \"category\" is personality traits (five traits) where a respondent receives a score of 0 to 4 for each trait. The other category is study skills which are measured in two ways with a 0 to 2 score for each. \nSo each respondent has five personality trait scores of 0 to 4, and two study skill scores of 0 to 2. \n\nExample: \nPersonality Traits: N - 3, E - 0, O - 2, A - 4, C - 2; Study Skills: A - 1, P - 2\n\nI want to see if there is correlation between the scores in particular personality trait with either study skill measure. What would be the best way to test this? My best guess to use Spearman. Or maybe a Chi-Square test?\n\nThanks!\n\nEdit: Changed example to make it easier to read. Also, I have used the link in sidebar and a few other resources to help me figure out what to do but I'm still not certain which test is the best to use.",
        "created_utc": 1524385559,
        "upvote_ratio": ""
    },
    {
        "title": "Meta-meta-analysis",
        "author": "IsThisWorkingYet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e1t29/metametaanalysis/",
        "text": "Hi all,\n\nI'm looking at conducting a meta-analysis on a series of meta-analyses. However, some individual studies are included in more than one meta-analysis, which (in my mind, at least) violates assumptions of independence. Is this true, and if so, how do I deal with non-independence? Thanks. ",
        "created_utc": 1524381797,
        "upvote_ratio": ""
    },
    {
        "title": "What type of distribution would this be?",
        "author": "lookytooky",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e1pto/what_type_of_distribution_would_this_be/",
        "text": "Hey guys,\n\nI've been stuck trying to figure out the type of distribution this would be. I need to run a Monte Carlo simulation. We're told that every driver has a independent risk of 0.01. I wanted to know if there is 1000 drivers what type of distribution would this be and how would I find the number of accidents in a year. I thought this was a uniform distribution as the probability is constant. Also the mean cost of an individual accident is 50000 what would be the total claims mumber for that year.\n\nThanks for any help ",
        "created_utc": 1524380350,
        "upvote_ratio": ""
    },
    {
        "title": "[HELP] Running THIS specific MSA on Minitab",
        "author": "javiwankenobi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8e012y/help_running_this_specific_msa_on_minitab/",
        "text": "So I work at an automotive paintshop. We have a few dozen types of defects and we have a specially trained quality analyst that calls on what exactly the defect is. I already conducted the R&amp;R on an Excel spreadsheet and I know for a fact that it passes, but I dont know how to run this type of MSA on Minitab (need the variability percentage and graphs screenshots). \n\nTo explain myself better look [here](https://imgur.com/O0Ot99t) and [here](https://imgur.com/sef4i9J).\n\nI believe this type of MSA is called a Round Robin Test. Thank you very much.",
        "created_utc": 1524359020,
        "upvote_ratio": ""
    },
    {
        "title": "Addressing an independent variable with a large amount of 0 values",
        "author": "tetsuoandkaneda",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dztfd/addressing_an_independent_variable_with_a_large/",
        "text": "Here is the background of my project:\n\nI ran an OLS robust regression on National Basketball Association Player data. The dependent variable is a statistic that's called offensive RPM. The independent variables were various shooting zone field goals made and attempted totals as well as minutes total. The regression as an entirety is statistically significant and each independent variable is statistically significant with the results in a broad interpretation coming out as known: making more baskets on fewer shots increases a player's offensive RPM and taking more shots with fewer makes decreases a player's offensive RPM. \n\nHowever, the coefficient for corner three-point shot attempts is the same as midrange shot attempts. This goes against the accepted norms and beliefs of the NBA statistics community because midrange shots have less value than three-point shots, especially in the corner. There are two conclusions to this:\n\n1. Attempting corner three-point shots are not as beneficial as the norm suggests.\n\n2. The fact that so many NBA players do not even attempt corner three-point shots, generating a lot of 0s in the sample, is forcing/pulling the two variables outward and creating a much larger risk-reward ratio than it really is. \n\nI use Stata for my regression analyses on NBA data, and when it comes to adding weights or trying to control for these issues, then I get a bit over my head. I'm going to be running a similar test on some different data to see if this issue continues. With that said, I was wondering if anyone knows how I can attempt to address this potential issue of too many 0s in the sample in Stata to re-run the regression. Thanks.",
        "created_utc": 1524356719,
        "upvote_ratio": ""
    },
    {
        "title": "What math courses can a statistics undergraduate student take to complement its curriculum?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dxt82/what_math_courses_can_a_statistics_undergraduate/",
        "text": "[deleted]",
        "created_utc": 1524337269,
        "upvote_ratio": ""
    },
    {
        "title": "Real life stats scenario, can't figure it out myself. Can I get some help?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dxlqn/real_life_stats_scenario_cant_figure_it_out/",
        "text": "[deleted]",
        "created_utc": 1524335351,
        "upvote_ratio": ""
    },
    {
        "title": "Is linear regression an appropriate approach for identifying if specific units in my data were influenced over time?",
        "author": "iarcfsil",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dx1bj/is_linear_regression_an_appropriate_approach_for/",
        "text": "Sorry for such a bad title - I'm still trying to get used to the proper semantics. I'm teaching myself some statistics and trying to apply it to some data sets I have, so bear with me here\n\nI have data is for schools and their performance for each grade per year. There's a record per grade at each school for every year. So it looks like this:\n\nSchool | Year | Grade | #students | Math supporting context % approached| Math supporting context % met or exceeded\n---|---|----|----|----|----\n610534| 2016\t| Mathematics Grade 3| 57| 5.3%| 94.7%\n610534| 2016\t| Mathematics Grade 4| 60| 8.3%| 91.7%\n610534| 2016\t| Mathematics Grade 5| 59| 6.8%| 93.2%\n610534| 2015\t| Mathematics Grade 3| 57| 5.3%| 94.7%\n610534| 2015\t| Mathematics Grade 4| 60| 8.3%| 91.7%\n610534| 2015\t| Mathematics Grade 5| 59| 6.8%| 93.2%\n\n\n\n\n\nWhat I'm trying to see is if there was any statistical significance of test score improvement after 2015 for 10 specific schools in the 5th grade. The kids in the 5th grade had something in 2016 and 2017 to help them, so I'm trying to compare math test results.\n\nIf I understand correctly, I don't have any predictors, but outcomes in my dataset right? Is linear regression a good approach to this problem? If not, what would be better for this scenario? ",
        "created_utc": 1524330240,
        "upvote_ratio": ""
    },
    {
        "title": "How can I know I'll like this job?",
        "author": "mountainbull",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dwvg4/how_can_i_know_ill_like_this_job/",
        "text": "Hi everybody. I have a general interest in statistics but I'm not really sure if to go for it. Is there any book/resource I could use to have a real understanding of what statistics-related jobs look like? Is math a huge part of it (I'd say no since nowadays most things are automatic but who knows)? I just want to know what it is like on a day to day basis, what's stimulating and what are the most important skills you must have to be a good worker in this field. Thanks in advance! ",
        "created_utc": 1524328715,
        "upvote_ratio": ""
    },
    {
        "title": "How can I create groups in R?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dwsws/how_can_i_create_groups_in_r/",
        "text": "[deleted]",
        "created_utc": 1524328047,
        "upvote_ratio": ""
    },
    {
        "title": "ELI5: Why can the F value decrease despite an increase in R squared?",
        "author": "beckettinga",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dvrar/eli5_why_can_the_f_value_decrease_despite_an/",
        "text": "",
        "created_utc": 1524317539,
        "upvote_ratio": ""
    },
    {
        "title": "Combination of two random variables and covariance",
        "author": "f801fe8957",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dvpbs/combination_of_two_random_variables_and_covariance/",
        "text": "Here is an excerpt from [a paper](http://www.mdpi.com/1099-4300/20/1/55) ([screenshot](https://i.imgur.com/n2RZeaN.png)):\n\n&gt; Suppose we are given a discretely sampled signal y ∈ R^N corrupted with noise. Given a sampling frequency rate f_s, this signal corresponds to a total duration of T = N · f_s seconds. We assume that the signal is stationary with 0 mean amplitude, E(y_t) = 0, and finite power Var(y_t) = σ_0^2 &lt; ∞. We adopt the notation y_t to indicate the t component of the discrete-time signal.\n\n&gt; Now let us imagine that, at sample time t' ∈ {0, . . . , T}, some event has started. Our only assumption is that, whatever the particular nature of this event, it causes a change in the total signal power; this is a rather reasonable assumption, since **for the combination of two random variables to have the same variance as one of its components, it is necessary that the variables have an exact covariance of σ_1^2 / 2**, where σ_1^2 is the variance of either one of the variables.\n\nWhy is it covariance has to be the half of the variance?",
        "created_utc": 1524316884,
        "upvote_ratio": ""
    },
    {
        "title": "A problem with insignificant variables.",
        "author": "Flipsssss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dvi0u/a_problem_with_insignificant_variables/",
        "text": "Hello all!\n I hope this doesn't go under HW help, if so - feel free to remove and sorry for the inconvenience. \n\nI am currently doing a binary logistic regression and am having an issue with variables becoming insignificant. I'd be very happy if someone has some help to offer.\n\nI recently removed a smaller part of my sample (40/790) as these individuals were significantly different from the rest of the sample and basically made up their own population. As I removed them, two of my variables go insignificant. I have three independent variables that I wish to predict my dependent variable with. These independent variables are not present in the removed data, so there really should be no reason for them to change now. I have checked for multicollinearity and these values are not worrisome (1.5 ish for all of them). Any ideas as to how I should proceed? I really can't seem to figure out what has changed just from removing 40 individuals...\n\nIf you run the regression without the variable that remains significant the other two are significant, if that's of any importance.",
        "created_utc": 1524314380,
        "upvote_ratio": ""
    },
    {
        "title": "How do i go about plotting my workout progression",
        "author": "FutureMeBetterMe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dty0e/how_do_i_go_about_plotting_my_workout_progression/",
        "text": "Not sure if this is the appropriate place or if this question/task even makes sense.\n\n\nI go to the gym 3 times a week. I have 10 Exercisers, 3 Sets each and 12 Reps per set.\n\n\nI increase the weight every session and I don't always get in 12 reps consistently.\n\n\nEdit: I try to increase the weight every session but only if I get 8 or more reps. If at anytime I get less than 8 then I go back and re do the same weight the next session.\n\n\nExample: \n\n\nMonday Bench Press 12, 11, 8 Reps at 70 Pounds\n\n\nWednesday Bench Press 12, 12, 10 Reps at 75 Pounds\n\n\nFriday Bench Press 12, 10, 7 Reps at 80 Pounds\n\n\nMonday Bench Press  12, 12, 12, Reps at 80 Pounds\n\n\nIf I am not mistaken, Weight is 1 variable, Reps is another Variable and Days is the last variable, correct ? \n\n\nHow do I go about plotting this ? I am trying to see my weekly progression per exercise.\n\n\nEdit: Trying to plot 3 Dimensions of Data.\n\n\nThanks.\n\n",
        "created_utc": 1524290050,
        "upvote_ratio": ""
    },
    {
        "title": "I don't know if you guys tell me the chances of scenarios, but I have a question.",
        "author": "KyletheAngryAncap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dtl3l/i_dont_know_if_you_guys_tell_me_the_chances_of/",
        "text": "A man who never gets hurt or dies is trapped outside of his home dimension (I know alternate dimensions might not exist. Bear with me) with an interdimensional rocket (bear with me). If there are infinite dimensions and they are constantly shuffling, what are the chances that the man gets to his home dimension?",
        "created_utc": 1524285109,
        "upvote_ratio": ""
    },
    {
        "title": "Controlling for a variable with residuals",
        "author": "AhTerae",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dth56/controlling_for_a_variable_with_residuals/",
        "text": "Hi,\n\nIn a previous class, I did an ANCOVA - which as far as I could tell involved predicting the depent variable (s) from the covariate(s) and then testing whether there residuals of this model varied by group.\n\nAs such, when I encountered partial correlations recently, I expected that they would work roughly the same: predict the DV with the covariate, then see how well the IV predicted the residuals. But actually (unless I'm misinterpreting) you predict the IV from the covariate too, then correlate the IV residuals with the DV residuals.\n\nMy question here is why, and whether you ever do is the simpler way and just correlate the IV with the DV residuals. That makes a bit more sense to me intuitively, and I feel like it might answer a different question that's still important, but I could well be wrong. I'm also a bit foggy why you would try to predict your IV with the covariate (something about a third variable problem?).",
        "created_utc": 1524283713,
        "upvote_ratio": ""
    },
    {
        "title": "Sensitive question, working out the probability through tree diagram",
        "author": "sonia_77",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dslug/sensitive_question_working_out_the_probability/",
        "text": "https://i.redd.it/ye200j5fy5t01.jpg",
        "created_utc": 1524273612,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone ELI5 Proportionate Reduction Error",
        "author": "weguccison",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8drltt/can_someone_eli5_proportionate_reduction_error/",
        "text": "So if I have the value of r2 how would I explain it as the Proportionate reduction of Error? Lets say r2 is .75/75%\n\nHaving a hard time understanding this concept! Thank you reddit",
        "created_utc": 1524263528,
        "upvote_ratio": ""
    },
    {
        "title": "Conducting a Mann-Whitney U test on SPSS, have a question.",
        "author": "k_m_k",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dra9s/conducting_a_mannwhitney_u_test_on_spss_have_a/",
        "text": "I am using SPSS to perform a few stats for a research project and running into a problem.  The data I have (from a survey) is in proportions, not in unique excel cells.  For example, if I was comparing men and women's opinion on a question.  I have the data that 200/300 men agree and 100/200 women agree, but I don't have an excel file with each of the agree/disagree data points.  Is it still possible to use the data this way? Or do I somehow have to transform it into individual data and is there any easy way to do that?\n\nThanks! ",
        "created_utc": 1524260701,
        "upvote_ratio": ""
    },
    {
        "title": "Help with Differences in Statistical Tests",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dqyzs/help_with_differences_in_statistical_tests/",
        "text": "[deleted]",
        "created_utc": 1524258042,
        "upvote_ratio": ""
    },
    {
        "title": "Can I create an average model from a set of models via simulation?",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dq180/can_i_create_an_average_model_from_a_set_of/",
        "text": "Say I read three studies on the effect of age on reaction time. How do I proceed to derive a believe of the \"true\" effect of age on reaction time?\n\nSince I have no idea about meta-analysis, I collect the three intercepts and slopes and error terms from each study. I simulate new data via R with each regression equation and run a new regression analysis with the simulated observations.\n\nI would like to know:\n\n * whether I am wrong in doing so? At least homogeneity of variance is not given.\n * are there better ways to do this?\n * what do I do if the models do not match? Say, we have a paper where a researcher includes gender or any other term into the model, how do I incoorperate this?\n * what do I do if information is missing? Say a researcher has not included the standard deviation for one of the models, do I have to drop the entire model in the averaging process or can I still make use of theinformation provided by the model?\n\nHere is some example R-code from my example:\n\t# The three Models\n\tintercept &lt;- c(1.3,1.1,2)\n\tslope &lt;- c(2,2.2,0.9)\n\tage &lt;- c(18:90)\n\n\t# simulate errors for each model\n\tt=1\t\n\tz=t*NROW(age)\n\terror1 &lt;- rnorm(100*z,0,1)\n\terror2 &lt;- rnorm(50*z,0,1.4)\n\terror3 &lt;- rnorm(10*z,0,2)\n\n\tm1 &lt;- intercept[1]+slope[1]*age\n\tm2 &lt;- intercept[2]+slope[2]*age\n\tm3 &lt;- intercept[3]+slope[3]*age\n\n\tm1 &lt;- data.frame(time=rep(m1,t)+error1,age)\n\tm2 &lt;- data.frame(time=rep(m2,t)+error2,age)\n\tm3 &lt;- data.frame(time=rep(m3,t)+error3,age)\n\n\tAgereg &lt;- rbind(m1,m2,m3)\n\n\tnew.age &lt;- lm(time~age,data=Agereg)\n\tsummary(new.age)\n\n\t# Plot\n\tplot(m1$age,m1$time,type=\"p\")\n\tpoints(m2$age,m2$time,col=\"blue\")\n\tpoints(m3$age,m3$time,col=\"red\")\n\tabline(new.age,col=\"green\")\n\nEDIT: a pic of the Rplot:\nhttps://imgur.com/a/QLmV42W\nThe black points denote simulated data by model 1, blue, is model 2, red is model 3. The green line indicates the new regression line based on the simulated data.\n",
        "created_utc": 1524250384,
        "upvote_ratio": ""
    },
    {
        "title": "How do I determine whether a best of 5 or best of 7 a team with higher winning probability should choose?",
        "author": "1st_lt_Hawkeye",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dpa09/how_do_i_determine_whether_a_best_of_5_or_best_of/",
        "text": "I'm more interested in knowing how to approach these kinds of questions  mathematically.  This came from casual conversation with a friend but I am unfamiliar where to begin or where to read up on.",
        "created_utc": 1524244539,
        "upvote_ratio": ""
    },
    {
        "title": "Poisson occurrence \"on\" N and not \"within\" N",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8doizl/poisson_occurrence_on_n_and_not_within_n/",
        "text": "[deleted]",
        "created_utc": 1524238699,
        "upvote_ratio": ""
    },
    {
        "title": "How can one calculate a critical value for the mean in a hypothesis test when the data are not normally distributed?",
        "author": "jbp12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8doeyc/how_can_one_calculate_a_critical_value_for_the/",
        "text": "In undergrad stats, I learned that one can use a hypothesis test to determine whether a population's mean is not equal to a specific number by stating a null hypothesis (H0) and rejecting it. If the test statistic from the hypothesis test is greater than z_(alpha) (one-sided test) or another test statistic (such as t_(alpha)), then you reject our null hypothesis at confidence level 1-alpha. This formulation requires that the population data are normal, or very close to normal.\n\nFor non-normal data, how does one compute a critical value? What test statistic would one use?",
        "created_utc": 1524237831,
        "upvote_ratio": ""
    },
    {
        "title": "Which of these Stat classes should I take before I graduate?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dn4uw/which_of_these_stat_classes_should_i_take_before/",
        "text": "[deleted]",
        "created_utc": 1524226856,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a typo in part (c) of this question? I don’t see why you would calculate profile log-Likelihood in terms of the MLE’s for (alpha,beta)?",
        "author": "[deleted]",
        "url": "https://i.redd.it/8y5i6t0332t01.jpg",
        "text": "[deleted]",
        "created_utc": 1524226718,
        "upvote_ratio": ""
    },
    {
        "title": "Compare two groups with survival analysis",
        "author": "zealen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dmvm8/compare_two_groups_with_survival_analysis/",
        "text": "I want to see if becoming a widow/widower (w/w) makes the surviving probability lower. \n\nBut then I need a control group and that would be people that don't become a w/w or just married people. How can I compare those two groups?\n\nI have a starting point and ending point for the w/w (start when their partner dies and ends when/if they die). But I don't have a starting point for the control group. \n\nI have made some Kaplan-Meier curves for the w/w and compared the results I got from that to the official surviving probability\n\nhttps://i.imgur.com/ADqPEGO.png\n\nwhere the x-axis are age and y-axis survival prob. and blue line is w/w and red line is the official survival numbers for my country.\nBut I know this isn't ideal, my numbers made with Kaplan-Meier and are the probability of survive one year if you become a w/w at that age.\nAnd the official numbers are summarized numbers of all born and dead in the country and are presented as out of 100k persons born the probability of them get to the specific age is this. \n\nSo not exactly the same, but I don't really know how to compare my data so that I can see if there is a higher risk of dying when becoming a w/w. \n\nDo you have any ideas how I should do? \n",
        "created_utc": 1524224011,
        "upvote_ratio": ""
    },
    {
        "title": "[Reddit Gold] Should I use multiple T tests or an ANOVA:",
        "author": "magicpizza2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dmfky/reddit_gold_should_i_use_multiple_t_tests_or_an/",
        "text": "My test is to determine the difference between A and B, A and C, A and D, A and E, A and F, A and .... (A, B, C, D... are all different countries). \nI dont care about what relationships B C and D have with each other. I just need to know whether A and B are significantly different, whether A and C are and whether A and D are. \n\nReddit gold to the person who can give an easy to understand but in depth response. \nPS: im a stats noob",
        "created_utc": 1524218417,
        "upvote_ratio": ""
    },
    {
        "title": "Two questions about statistical power",
        "author": "DonMatteo13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dm77d/two_questions_about_statistical_power/",
        "text": "Hi guys just wondering if people have a good understanding about statistical power and level of significance and can help with a few questions. The first is in regards to reducing the probability of failing to detect a true risk ratio. To me, the answer would be to just increase power (to reduce type II error risk) and reduce the level of significance (to reduce type I error risk) - but it as simple as that? What factors need to be considered if you want to both increase power and decrease significance level.\n\nThe second question is just in regards to sample size how does power and level of significance effect the sample size needed? My understanding is that if you increase power and decrease significance then you must increase your sample size, but mathematically, why is this the case? \n\nThanks guys ",
        "created_utc": 1524215076,
        "upvote_ratio": ""
    },
    {
        "title": "Randomly sampling by index in R",
        "author": "yqzr09",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dja9g/randomly_sampling_by_index_in_r/",
        "text": "I have two random vectors which are just binary 1's and 0's. I'm trying to randomly sample a third vector whose indices pertain to either the first vector's index or second vector's index.\n\ne.g., vector1 = 1 0 1 0 0, vector2 = 0 0 1 1 0. The new vector, vector3 will sample by index. So, vector3's first index is either 1 or 0, second index is 0 or 0, etc..\n\nI tried the following code\n\n    sample(c(vector1,vector2), length(vector1), replace = TRUE)\n\nHowever, it seems to randomly sample from all of the data rather than by index.",
        "created_utc": 1524181689,
        "upvote_ratio": ""
    },
    {
        "title": "Should I still major in statistics even though I’m getting a C in Calculus?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dim3l/should_i_still_major_in_statistics_even_though_im/",
        "text": "[deleted]",
        "created_utc": 1524175736,
        "upvote_ratio": ""
    },
    {
        "title": "Distribution of a 30 day running tally",
        "author": "13ass13ass",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dik9m/distribution_of_a_30_day_running_tally/",
        "text": "Hello, \n\nI am trying to predict the number of earthquakes above a 5.0 worldwide in the month of May. More specifically, I am trying to give the probability that the number of earthquakes will be in a given range; eg 100 to 140 total earthquakes in May. \n\nOne method a colleague proposed is to begin by taking the earthquakes &gt;5.0 recorded from beginning of 2013 until the end of 2017 and find their 30-day running tally. In other words, the first tally would be from January 1st, 2013 to January 30th, 2013, and so on until the end of 2017 so that there is enough data such that there aren't any edge case issues. Then this dataset can be used to report the proportion of 30-day tallies that fall within the specified range; eg 40% of tallies fall within 100 to 140. \n\nThe idea being that the collection of 30 day tallies reflects the distribution of possible earthquake totals in May. This seems reasonable approach to me, but I'm wondering if I'm missing any obvious flaws with this approach. Also I'm wondering if there's some way to approximate these 30 day tallies as a well-studied distribution? It looks approximately log normal to me.\n\nThank you!\n\nYou can pull some of the data yourself if you'd like: https://earthquake.usgs.gov/earthquakes/search/",
        "created_utc": 1524175306,
        "upvote_ratio": ""
    },
    {
        "title": "How appropriate/inappropriate is it to perform a t-test on categorical ordinal data?",
        "author": "blockhead123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8die5z/how_appropriateinappropriate_is_it_to_perform_a/",
        "text": "Situation:\n\nMy employer uses two different scales for various purposes. One is on a 7 point scale, the other is on a 9 point scale. They're typical self-report measures where differences in responses are not proportional, just ordered. \n\nI've been taught in the past that t-tests are usually *not* appropriate for these kinds of scales, but my employer is extremely distrustful of anything other than a t-test or mean + confidence intervals. I've used the Mann Whitney U test, but I haven't been able to convince my employer that it's appropriate. Truthfully, I don't have an argument are than, \"that's what I was taught\". I vaguely remember it being related to violating the assumption of normality in a t-test. \n\nMy question is this:\n\nIs it irresponsible to use a t-test for these kinds of data? Are there conditions under which it doesn't matter that much? Is there an accessible way of illustrating how conclusions based on t-tests will differ from those based on non-parametric tests? ",
        "created_utc": 1524173897,
        "upvote_ratio": ""
    },
    {
        "title": "Linear Regression coefficient interpretation",
        "author": "Nelectronics",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8didph/linear_regression_coefficient_interpretation/",
        "text": "Hi all,\n\nI'm an econometrics student and I have a seemingly simple question concerning the interpretation of coefficients. Imagine following simple OLS model:\n\nY = B0 \\+ B1X \\+ Error Term\n\nB1 = 1\n\nSo we all know that B1 is the coefficient, or the slope of the fitted line. \n\nIf we increase x by 1 we predict an increase of Y by one. Now here's the question:\n\nCan we also say: if we decrease X by one, we predict a decrease of X by one? For me that was always the obvious interpretation as b1 is dy/dx, or the change in Y per change in X \\(which means depends on the change in X obviously\\), but some person claims that this is not true. Any explanations on this?\n\nthanks for your help",
        "created_utc": 1524173781,
        "upvote_ratio": ""
    },
    {
        "title": "Why type of variable is this?",
        "author": "PublicHealthPunk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dh9hi/why_type_of_variable_is_this/",
        "text": "Hi, I am looking at a survey and a question asks how many hours a day do you watch tv with the choices being never, less than 1 hour, 1 hour, 2 hours, 3 hours, 4 hours, and 5+ hours. So I am a little confused on what type of variable would this be, I was thinking it is interval. Any feedback would be appreciated, thank you.",
        "created_utc": 1524165164,
        "upvote_ratio": ""
    },
    {
        "title": "What do you call this method for combining metrics into a composite number?",
        "author": "babbocom",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dh1n1/what_do_you_call_this_method_for_combining/",
        "text": "I heard about a process to combine metrics into a composite number/score. In the use case I heard, it's used to score each customer. I'm trying to understand how this process works, and if it is a good idea. Here's a 30,000 foot view.\n\n1. Convert all metrics into a similar base. \\(Ex. Convert all customer sales, orders, and count of SKUs purchased into a &amp;#37; change of a current period versus a previous period.\\)\n2. Percent rank each metric. Now each metric ranges from 0 to 1.\n3. Take the square root of each of the scaled metrics from Step 2. Not sure why, something to do with showing outliers more clearly?\n4. Multiply the results from Step 3 by a weight.\n   1. The weight is somewhat arbitrary and represents how important a variable is. For example, sales is multiplied by 0.5, orders is multiplied by 0.3, and SKU count is multiplied by 0.2. Could be broken up a lot of ways as long as it adds up to 1, or 100&amp;#37;.\n5. Sum the results from Step 4. Now I have a composite score per customer.\n   1. Presumably if the score is closer to 1 the customer is good \\(some combination of sales, orders, and/or SKU count increasing over time\\) versus a customer that is closer to 0 \\(some combination of sales, orders, and/or SKU count decreasing over time\\).\n\nI have a few questions:\n\n* Is there a name for this process? I'd love to read more about it.\n* Does this process make sense/seem worthwhile? I don't really know how to evaluate if it is producing good output, other than asking for a gut check from more experienced people.\n* What are some other somewhat simple, straightforward ways to combine data/score/create a composite index like this?\n\nReally, any input/feedback would be super helpful.",
        "created_utc": 1524163539,
        "upvote_ratio": ""
    },
    {
        "title": "Comparison of empirical probability of listening to a familiar artist",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dgu1g/comparison_of_empirical_probability_of_listening/",
        "text": "[deleted]",
        "created_utc": 1524161942,
        "upvote_ratio": ""
    },
    {
        "title": "Help me with this problem please!",
        "author": "Sagardas12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dgk22/help_me_with_this_problem_please/",
        "text": "Find the Moment Generating Function of the random variable whose moments are u'r = (r+1)!2^r .",
        "created_utc": 1524159849,
        "upvote_ratio": ""
    },
    {
        "title": "Is multicollinearity problematic for a moderation hypothesis?",
        "author": "TerryBidet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dgi6u/is_multicollinearity_problematic_for_a_moderation/",
        "text": "I am researching the effect of the number of parliamentary parties on voter turnout. The theory is that the number of parliamentary parties has a negative effect on voter turnout in majoritarian electoral systems, and has a positive effect in proportional electoral systems. However we can expect that the type of electoral system also influences the number of political parties i.e. there is multicollinearity. Is this problematic?\n\nEdit: Maybe I phrased it incorrectly, but I mean to say that there may be correlation between the moderator variable and the independent variable, is this a problem?",
        "created_utc": 1524159453,
        "upvote_ratio": ""
    },
    {
        "title": "[University Intro Statistics] - How to calculate 95% confidence intervals, explain the tests answer please",
        "author": "[deleted]",
        "url": "https://imgur.com/a/8srcwKL",
        "text": "[deleted]",
        "created_utc": 1524154673,
        "upvote_ratio": ""
    },
    {
        "title": "What to take into consideration when trying to find the difference between groups",
        "author": "mooreessential",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ddq2g/what_to_take_into_consideration_when_trying_to/",
        "text": "Hi guys, was wondering if anyone could tell me what factors are needed to be considered when wanting to compare between group differences to find the statistically significant difference that an ANOVA or kruskal wallis picked up? Furthermore, besides doing post tests, is there a manual way to finding this difference between 2 of say 4 groups?",
        "created_utc": 1524134764,
        "upvote_ratio": ""
    },
    {
        "title": "How to write this equation? Average from the last 3 years",
        "author": "irishrapist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dctjr/how_to_write_this_equation_average_from_the_last/",
        "text": "I'm using logit regression for my dissertation, I already ran the regressions and analysed them, but I'm having a little trouble writing the equations in the methodology section.\n\nMy dep variable is CEO turnover. My iv are roa, tobins'q, eps and longtermdebt. I'm using the average performance of my iv in the 3 years before the event of turnover.\n\nThis is the equation I wrote: https://imgur.com/a/1N0ny\n\nIs it right? What should be different?",
        "created_utc": 1524121882,
        "upvote_ratio": ""
    },
    {
        "title": "Help with setting up hypothesis testing?",
        "author": "Luker5555",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dcdsf/help_with_setting_up_hypothesis_testing/",
        "text": "Hi, I have this problem for an exam review:\n\n\"A check-cashing service found that approximately 30% of all checks submitted to the service\nwere bad. After instituting a check-verification system to reduce its losses, the service found\nthat only 100 checks were bad in a random sample of 400 that were cashed.\n\n(a)(5points) Set up hypotheses for testing whether the check-verification system reduced the proportion\nof bad checks.\"\n\nwhere the answer given by the instructor is: H0: p= .5 vs Ha: p &lt; .5\n\nI don't really see why p = .5 would be the null hypothesis. Could anyone explain why that's the case? Thanks for the help",
        "created_utc": 1524116047,
        "upvote_ratio": ""
    },
    {
        "title": "Is this a convention for representing data? Rows vs columns?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8dansr/is_this_a_convention_for_representing_data_rows/",
        "text": "[deleted]",
        "created_utc": 1524098314,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics question related to bingo - what are my odds of winning this jackpot?",
        "author": "Sterbin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8daeut/statistics_question_related_to_bingo_what_are_my/",
        "text": "This jackpot requires a full board to be covered after 58 numbers called. Since there are 75 total numbers and you have 24 on your board, what are the odds of actually winning?",
        "created_utc": 1524096067,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating difference between 5 exponential equations?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d9v3w/calculating_difference_between_5_exponential/",
        "text": "[deleted]",
        "created_utc": 1524091184,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing boxplots",
        "author": "DonMatteo13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d9rgi/comparing_boxplots/",
        "text": "Hi just have a homework question that asks to compare 4 boxplots and without doing any analysis, give an interpretation as to which is different from the others, how is this done? one of the boxplots has no q1,q2 or q3, overlapping with any of the others only it's whiskers so i'm leaning towards saying this is the different boxplot but how do I explain that correctly? What does it mean when q1,q2,q3 don't overlap with other boxes.",
        "created_utc": 1524090329,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a name for this?",
        "author": "doomgasp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d9pd6/is_there_a_name_for_this/",
        "text": "I want to estimate the average amount of time a firm spends working on a new client's case, from beginning to end of case. The work for each case breaks down into a discrete number of tasks, and each task has an average time to completion. Not every type of task will occur in every case, and some very time intensive tasks only happen in a small handful of cases.\n\nThe formula I want to use to represent the average work-time generated by a case is: \n\n(average time spent working on a case) = (average time to complete Task #1)×(likelihood of Task #1 occurring in a given case) + (average time to complete Task #2)×(likelihood of Task #2 occurring in a given case) + ...\n\nMy question: is what I'm describing here a specific statistical concept that I might be able to read more on? Thanks in advance for any responses.",
        "created_utc": 1524089859,
        "upvote_ratio": ""
    },
    {
        "title": "Multivariate dataset recommendations",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d97pb/multivariate_dataset_recommendations/",
        "text": "[deleted]",
        "created_utc": 1524085837,
        "upvote_ratio": ""
    },
    {
        "title": "hypothesis testing (LRT)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d900z/hypothesis_testing_lrt/",
        "text": "[deleted]",
        "created_utc": 1524084218,
        "upvote_ratio": ""
    },
    {
        "title": "CI on two values, how can I rate a % chance on which one is higher?",
        "author": "ajskelt",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d8b6o/ci_on_two_values_how_can_i_rate_a_chance_on_which/",
        "text": "Let's say I have 2 values, x and y that are completely independent.  x is very slightly greater than y, however they both have a confidence interval of +-2%.  Is there a way I can calculate the likelihood that y i actually greater than x in reality? \n\nMy goal is to have something like:  There is a 30% chance that y is actually greater than x.",
        "created_utc": 1524078900,
        "upvote_ratio": ""
    },
    {
        "title": "Significant figures when the uncertainty is larger than the value itself?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d87rh/significant_figures_when_the_uncertainty_is/",
        "text": "[deleted]",
        "created_utc": 1524078185,
        "upvote_ratio": ""
    },
    {
        "title": "Statistic tests!",
        "author": "gaatjesprikker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d7xee/statistic_tests/",
        "text": "Dear fellow redditors, I’m in need of your help, since I’ve got this MAJOR deadline and I have no guidance at all. I’m working on this paper that I need to submit, and for this paper I’ve done my proper experiment(pilot study). It is about movement breaks in an university setting, which are actually just some interuptions of sitting time each X time(sedentary interuptions). I have one small sample group (n=24) and \r\nI’m measuring pre-and post intervention: exercise enjoyment (PACES) and a concentration questionnaire from california state uni. \r\n\r\nSince instructions of the movement breaks differ every week, we were also asking what is the most pleasurefull(choose between weeks or no difference between weeks + why through open ending question) and practical(opinion through open ending question) way to provide these breaks in an university population. This is offcourse only done post test. \r\nFurthermore health guidelines knowledge is being tested post test, since they are provided throughout the experiment.\r\n\r\r\nNow I hope you have a pretty clear view of my study, but the thing is I need to do some statistical analyses on this all, so that’s where my questions are popping up. \r\nI’ve called this experimental design a one group, pre- posttest design?, and I’m planning to do a paired sample T-test from the following variables: \r\nPACES(enjoyment) and concentration. \r\nWould this be a good idea? \r\nDo I need to run some separate test(s) to see a difference between sex? And which one(s) would that be. I was thinking about two sample paired/unpaired t test??\r\n\nOh and the concentration questionnaire talks about some cut off points, would it be a good idea to use a one sample t test on this as an extra?\n\r\nHow can I analyse the open end questions, since they differ a lot? \r\nFurther I guess I’m obliged to use SPSS and I collected the data through Google forms. \r\n\r\nHope to get some good info regarding this all, also I would be more than happy to give some compensation for helping me! (I hope this is not against the rules from this group, or I will edit this). \r\n\nOne last thing: do I need to 'pair' the data like testperson x = number x on the spreadsheet and z=z?\nAlready thanks a lot ! \r\n\r\n\rMe.\r\n\r\n",
        "created_utc": 1524076031,
        "upvote_ratio": ""
    },
    {
        "title": "A few basic questions on weighting as a way to even out a survey's sample size",
        "author": "ComfortableSound",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d70tv/a_few_basic_questions_on_weighting_as_a_way_to/",
        "text": "Hi,\n\nas a part of my sociolinguistics thesis I am conducting a survey. My research interests focus on American people and I found Reddit was the best way to reach them/you guys. As you may know, the [Reddit demographics](https://imgur.com/gallery/cPzlB) aren't really representative of the real population. I wanted to even out my results by generating identical answers for underrepresentated groups, [as explained here.](https://www.researchgate.net/post/How_can_I_deal_with_uneven_sample_sizes_in_my_study) \n\nFor example, if I only have a 8/2 male ratio and the actual repartition is 5/5, can I just multiply the female answers by 2,5 (5/2) and call it a day? Can I multiply various factors this way? Is there a limit to this technique? And lastly, how to pick the single answer I am going to multiply to fill the breach? Wouldn't this skew things too much?\n\nThanks for reading, I'm a total statistics noob that want to do things right. Thanks in advance.",
        "created_utc": 1524069315,
        "upvote_ratio": ""
    },
    {
        "title": "Need assistance with Cox Mixed Effects model",
        "author": "necrobiosis1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d6zim/need_assistance_with_cox_mixed_effects_model/",
        "text": "Hi there! I am currently using Cox Mixed Effects model under the 'coxme 'package in R, and there are two things which I have been trying to find out but to no avail.\n\n\nWhat are the assumptions of the Cox Mixed Effects model? Are they the same as the Cox PH model?\n\nHow do I test for multicollinearity of my variables? The vif function does not appear to work here.\n\n\nAny help would be greatly appreciated!!",
        "created_utc": 1524069044,
        "upvote_ratio": ""
    },
    {
        "title": "How to define my sensor efficacy?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d6p25/how_to_define_my_sensor_efficacy/",
        "text": "[deleted]",
        "created_utc": 1524066843,
        "upvote_ratio": ""
    },
    {
        "title": "I'm having trouble in classifying our variables (Independent, Dependent, etc. and what type).",
        "author": "DarkPotatoKing7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d69zk/im_having_trouble_in_classifying_our_variables/",
        "text": "* Background: We have a program that does Phylogenetic Tree Reconstruction. What it does is given an input string sequence (like for example DNA sequences of different species) it will try to make an \"evolutionary tree\" of how the actual evolution of these species went. The process is complicated and there is no exact solution for it, so if I run the program twice on the same input I might get a different output. Each output tree is associated with a cost which corresponds to the number of mutations in it, the lower the better. We have 7 datasets that we can use for testing, ranging from 125 sequences to 8000 sequences. Of course, if we test using the 125 sequences it would run faster and have a lower cost than if we test using the 8000 sequences.\n\n\n* Task: We want to modify the program to make it faster without significantly affecting the cost. So what we have are two versions of the program (Original and Modified), the datasets we can use for testing, and the running time and cost of the output.\n\n\nMy inititial thought is that these our our variables\n\nIndependent Variables\n\n* Program Version (Original or Modified) = Categorical(?)\n* Dataset size (from 125 to 8000) = Categorical(?)\n\nDependent variables:\n\n* Running Time\n* Cost\n\nSo if I follow this https://stats.idre.ucla.edu/other/mult-pkg/whatstat/ the recommendation is multivariate multiple linear regression.\n\nBut then after thinking about it for a while I'm not sure if what I did is correct and I have several questions.\n\n1. Is it ok if I perform two separate statistical test for each (one for running time and one for cost) instead of doing it at the same time?\n\n2. Is the dataset size actually a variable? For example, if I want to test the effectiveness of a pill that reduces blood pressure I can take 7 people and measure their blood pressure before and after taking the pill. In that case, the only variable would only be the pill, while the 7 people are the samples. Does it also apply that the only variable in our setup is the program version and the dataset size are only the samples?",
        "created_utc": 1524063700,
        "upvote_ratio": ""
    },
    {
        "title": "How do I use my beer data to find my \"favorite\" brewery?",
        "author": "zeekaran",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d65tp/how_do_i_use_my_beer_data_to_find_my_favorite/",
        "text": "Originally I thought I'd take my dataset and remove duplicate beers (number of times I've had a beer has to do with availability more so than my rating), keeping the most recently rated one. Then I'd take every brewery and average all my ratings together, filtering out based on a minimum number of beers.\n\nThe issue with this is that my favorite breweries are ones I'll frequent and try out as many beers as possible, which will undoubtedly lower my average rating across all beers by that brewery.\n\nMy new idea is that I should take any brewery I've had 3+ beers at, and average only the top 5 beers (or 3-4 if that's all I've had). That way, exploring a brewery's selection and trying out beers I know I won't like doesn't take away from how much I enjoyed their best. This seems kind of clunky though. Is there a more elegant way to look for a favorite?",
        "created_utc": 1524062797,
        "upvote_ratio": ""
    },
    {
        "title": "I need help understanding how 2100 people can be representative of 32 million in a poll about politics?",
        "author": "yaxyakalagalis",
        "url": "http://angusreid.org/kinder-morgan-transmountain/",
        "text": "",
        "created_utc": 1524061401,
        "upvote_ratio": ""
    },
    {
        "title": "Best practice for time series graphs of stochastic model output",
        "author": "DennisChrDk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d4a4n/best_practice_for_time_series_graphs_of/",
        "text": "For my master thesis I created an Asset-Liabilities Management model and I’ve done a lot of stochastic simulations where I model the interest rate, asset price, etc. over a 40-year period.  The model then calculates the liabilities and uses 4 different portfolio methods to calculate the assets, capital requirements, cash flow for each year and then the total bonus paid out for the members. I ran the simulation 1000 times and for each simulation the liabilities remain the same, but for the rest I now have 4000 different scenarios.  \n\nThen problem is then, how am I supposed to show all of this output? Is there some best practice to follow? Maybe someone have an article or book where I could read up on this?\nThe total bonus can simply be showed as a frequency table, but what about the assets, liabilities, cash flow, etc. that evolve how time. Could it be shown with a time series plot with the average and some upper and lower percentiles? \n",
        "created_utc": 1524042915,
        "upvote_ratio": ""
    },
    {
        "title": "How many times would this have happend during the 10 hour hearings?",
        "author": "the_collin",
        "url": "https://i.redd.it/pgen3vyxvms01.jpg",
        "text": "",
        "created_utc": 1524042713,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics tools for simulating true population based on existing samples collection",
        "author": "kjetjor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d42ua/statistics_tools_for_simulating_true_population/",
        "text": "Hi. I currently work with biological data from mice treated with very small doses of radiation. Working with animals is rather restricted, which means i have a rather small sample size(10 for each treatment group). Observable effect is there, however the effect is minor. Because my sample size low, and the effect is minor its hard to validate the actuall findings through basic st.dev and SEM. \n\nI am currently looking into statistical approaches that can validate these minor changes better than T-Test. I am also interested in tools that can simulate a larger population from my observable data. My question for you guys is if you have any suggestions as to what methods i can use? ",
        "created_utc": 1524039983,
        "upvote_ratio": ""
    },
    {
        "title": "Is it appropriate to use a repeated measures One-Way ANOVA on this data?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d3wsk/is_it_appropriate_to_use_a_repeated_measures/",
        "text": "[deleted]",
        "created_utc": 1524037643,
        "upvote_ratio": ""
    },
    {
        "title": "Help with correlation between 3 variables.",
        "author": "GengLai",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d3s1v/help_with_correlation_between_3_variables/",
        "text": "Hi, i have a question regarding correlation.\nI am tasked to test for the presence of statistical significance between the relationship of 3 variables(fear of stats, satisfaction with life and flourishing) i am required to report in APA, can anyone let me know if my answer is correct?\n\nA two-tailed bivariate Pearson Correlation was performed to test for the presence of statistically significant relationship between fear of statistics, satisfaction with life as well as flourishing. \nThe Pearson Correlation between fear of statistics (M= 2.96, SD = 1.22), satisfaction with life (M = 22.87, SD = 5.64) and flourishing (M = 42.89, SD = 6.87) revealed no significant correlation, r(93) = .07, p = .527 and r(93) = .04, p = .681. Pearson’s r data analysis revealed a moderate positive correlation, r = .04 and r = .07. \nThere is no significant relationship between fear of statistics, satisfaction with life as well as flourishing. \n\n\n",
        "created_utc": 1524035787,
        "upvote_ratio": ""
    },
    {
        "title": "Null hypothesis of non inferiority study?",
        "author": "mathwimp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d2oro/null_hypothesis_of_non_inferiority_study/",
        "text": "If I have a non inferiority study. I.e drug x is not inferior to drug y. Then what is my null hypothesis. To me it would be:\n\nTreatment with drug x results in fewer healed patients compared with treatment with drug y.\n\nHowever I have been told this is wrong and it should be \"drug x results in the same amount healed as drug y\"\n\nI'm curious as to why this is? Because having the same effectiveness of treatment would validate the original hypothesis.",
        "created_utc": 1524022737,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating effect of an intervention when only elevating group level data",
        "author": "Weyden235",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d2far/estimating_effect_of_an_intervention_when_only/",
        "text": "We are using a pre-post treatment-control group design for a planned leadership intervention in which we are training communication skills. \n\nSupervisors who will be getting the training mostly work in groups of 2 or 3 people and each group shares supervision over teams of around 20 employees. Some supervisors also work alone. To measure the effects of the training we want to rely on the ratings of supervisor communication performance by the employees working under them. \nHowever, in the setting we are working in, supervisors leading a work team can only be rated together so we can not get any information about intra-group variance in cases of supervisor groups with n&gt;1. \nCan we still make valid statistical assumptions about the efficacy of the training intervention given that we will mostly work without individual level data? \n\nIn total we will assign around 30 people to the treatment and control groups and we will also make sure to only include whole teams of supervisors and not split them up when making the assignment.\n\nThanks in advance for any answers!",
        "created_utc": 1524020000,
        "upvote_ratio": ""
    },
    {
        "title": "How can plane crashes be considered an independent event, but also continue to have an increasing chance of occurring statistically?",
        "author": "pepperpotts13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d26em/how_can_plane_crashes_be_considered_an/",
        "text": "If I flip a coin twenty times and get tails each time, my chances of getting heads on the twenty-first flip is still fifty percent, regardless of the prior pattern. Though it inherently feels unlikely, it is just as probable as any other pattern. Flipping the coin is an independent event and one flip's outcome is not affected by the outcome to the flip prior to it.\n\nBack to planes: according to statistics, the probability of a plane crash are (loosely) one in 1.2 million, made up from the average of plane crashes to overall flights. Presumably meaning the more often someone flies on a plane, the higher the probability they would have of experiencing a crash.\n\nBut, since every flight is an independent event where the plane either lands safely or does not land safely, isn't it just as likely for someone to experience a crash if it is their first flight as someone who is on their one thousandth flight? Much like the coin flip, a flight's likelihood of crashing is completely separate from the event of the flight crashing or not prior. I guess my point being, my mind sees the plane crashing or not crashing as a binary event that is independent of any flight prior. So how can the chances of experiencing a plane crash increase with taking more flights or decrease with taking less flights? Maybe, I am wondering, are the statistic of the plane crashing relevant to the passenger or to the flight itself?\n\nI'm sure my logic is flawed somewhere here, I just can't sort out how.",
        "created_utc": 1524017609,
        "upvote_ratio": ""
    },
    {
        "title": "Where are you investing your money?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d1zqm/where_are_you_investing_your_money/",
        "text": "[deleted]",
        "created_utc": 1524015859,
        "upvote_ratio": ""
    },
    {
        "title": "I have run an experiment but I need help deciding what statistical analyses to run.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d1y8q/i_have_run_an_experiment_but_i_need_help_deciding/",
        "text": "[deleted]",
        "created_utc": 1524015479,
        "upvote_ratio": ""
    },
    {
        "title": "In the following scenario, would a statistical method be used to answer the question? My gut says no because we have the population and no sampling was needed.",
        "author": "the_mood_abides",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d1rgs/in_the_following_scenario_would_a_statistical/",
        "text": "I'm a farmer and I plant 4 rows of 100 sunflower seeds, for a total of 400 sunflowers if they all grow perfectly. Unfortunately they don't because my soil is bad. In the first three rows, I get 60 good sunflowers out of the 300 seeds. In the fourth row only get 10 sunflowers. I remember after the fact that I forgot to water that row. My question is then: if i didn't forget to water that fourth row, can I conclude that I would have gotten the same rate of good sunflowers as the first three rows?  ",
        "created_utc": 1524013719,
        "upvote_ratio": ""
    },
    {
        "title": "Can you run a t-test on averages from two groups that each have different sample sizes for each average? (not homework)",
        "author": "Aleuna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d0j34/can_you_run_a_ttest_on_averages_from_two_groups/",
        "text": "Hi, sorry if that doesn't make sense! I'm trying to run some stats on data from a study I did, but I can't figure out which tests to run.\n\nBasically - could you run a t-test on data like this? If so, which kind (two-tailed, unpaired, unequal variance??)? If not, which test could you use?\n\nGroup 1 Averages: 4 (n=10), 5 (n=10), 4.5 (n=8), 7 (n=3)\n\nGroup 2 Averages: 3 (n=10), 2 (n=10), 3 (n=10), 5 (n=5)\n\nThis is basically what my data looks like, except way simplified. Basically, the sample sizes for the two groups start off at the same value, but decline at unequal rates. Each group has different subjects (unpaired). \n\nHUGE thanks to anyone who helps me out!!!",
        "created_utc": 1524002989,
        "upvote_ratio": ""
    },
    {
        "title": "Need help showing that something is exponentially distributed",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d0fj7/need_help_showing_that_something_is_exponentially/",
        "text": "[deleted]",
        "created_utc": 1524002173,
        "upvote_ratio": ""
    },
    {
        "title": "Can anyone help me with some online work? Willing to compensate",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8d07lc/can_anyone_help_me_with_some_online_work_willing/",
        "text": "[deleted]",
        "created_utc": 1524000361,
        "upvote_ratio": ""
    },
    {
        "title": "Z Scores",
        "author": "comeandgetit10",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8czyx6/z_scores/",
        "text": "Just something I need to correct. \n\nThe life time of a certain brand of battery x is normally distributed with a mean value of 6 hours and a standard deviation of 0.5 hours.\n\na) What is the probability that the cassette player works between 4.5 and 6.8 hours?\n\nb) Find a number “c” such that only 5% of all cassette players will function without battery replacement for more than “c” hours. That is, find a number “c” such that P(x &gt; c) = 0.05\n\nc) Find a number “c” such that P(x ≤ c) = 0.9133",
        "created_utc": 1523998479,
        "upvote_ratio": ""
    },
    {
        "title": "Quick question about forest plots",
        "author": "Wilson404",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cz3pl/quick_question_about_forest_plots/",
        "text": "Hey guys,\nVery quick question if possible re: forest plots. We're just working on a homework assignment and used the open meta anlaysis software from http://www.cebm.brown.edu/openmeta/ \n\nI'm just worndering for these graphs - which is the line of null effect? I'm not sure if it is the dotted line in the middle or the y axis at 0. \n\nhttps://imgur.com/a/LFEO0\n\nThanks so much!!!! ",
        "created_utc": 1523991827,
        "upvote_ratio": ""
    },
    {
        "title": "Factor analysis with 200+ variables, where do I begin?",
        "author": "GENsurvey",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cyjfb/factor_analysis_with_200_variables_where_do_i/",
        "text": "Hi! I'm working on analyzing survey data for a my graduate degree project, and my advisor has pointed me down to route of factor analysis. I'm reading up on it right now, and (for the most part) understand the steps I need to take to get there, with the exception of my starting point. The issue is, I'm  having a huge mental block on where to start because our survey breaks down in to 200 variables that are likely highly correlated w/ each other (note, I didn't design the survey, or else I wouldn't have made it so long!). I want to begin by looking at a correlation matrix, but do I first need to narrow my scope? Do I run multiple factor analyses on each distinct section of the survey (although I worry about missing any overlap)? The survey is on perceptions and experience w/ gender equity in the workplace. Distinct sections include questions about hiring practices, programs/benefits offered by employer, perceptions of org leadership, perceptions of personal career growth, etc. We have ~640 responses to date and I'm using Stata fwiw",
        "created_utc": 1523987615,
        "upvote_ratio": ""
    },
    {
        "title": "Tidy data and replicate values",
        "author": "mrg58",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cxw6w/tidy_data_and_replicate_values/",
        "text": "I work in a research lab and have been generating a lot of data. As of now, it's kind of all spread out in a very unordered way. I recently read about the principle of tidy data from [Hadley Whickham's paper](https://vita.had.co.nz/papers/tidy-data.pdf). One thing I wasn't sure about was how to organize replicate data. \n\nLet's say I measure people's height. For each person, I measure 5 times. How would these 5 measurements be arranged in the data table? Should I have 5 columns (height1, height2, height3, etc.)? \n\nThank you!\n",
        "created_utc": 1523982726,
        "upvote_ratio": ""
    },
    {
        "title": "OLS - significant relationship between independent variables",
        "author": "mrndcn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cxrx4/ols_significant_relationship_between_independent/",
        "text": "Hi everyone! Here's my question. I am not well educated in statistics and I wanted to understand some mechanisms when launching ols regressions.\n\nI have 1 dependent variable and 5 independent variables. By looking at the correlation matrix there is nothing over +/-0.3. I launched the regression and got my results.\n\nThen my thesis supervisor made me use two of the 5 independent variables as dependent variables, so I tested the other three independent variables against the other two. Significant relationships came out. The adjusted R squared are 0.211 and 0.045 in each model.\n\nDoes this mean that I couldn't use them altogether in one model? Does this go against the OLS assumptions? I simply assumed that given the non-high correlation, I could do it, but now I am not too sure anymore.",
        "created_utc": 1523981796,
        "upvote_ratio": ""
    },
    {
        "title": "Which statistical test would be most appropriate for my randomised control trial?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cvzq5/which_statistical_test_would_be_most_appropriate/",
        "text": "[deleted]",
        "created_utc": 1523966651,
        "upvote_ratio": ""
    },
    {
        "title": "Overestimation of OR by logistic regression when testing for a highly prevalent condition.",
        "author": "abcbrakka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cv2qa/overestimation_of_or_by_logistic_regression_when/",
        "text": "Hi there, I am testing for a a highly prevalent condition (45% in my study population). I remember someone saying that if a condition is highly prevalent, standard logistic regression (in SPSS) will overestimate the odds ratio, and that this can be circumenvented by choosing another function. \n\n\nIs this correct? \n\nI cannot find this anywhere on the internet...",
        "created_utc": 1523954807,
        "upvote_ratio": ""
    },
    {
        "title": "Semi-partial R-squared in robust regression",
        "author": "Henfsag",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cuyw9/semipartial_rsquared_in_robust_regression/",
        "text": "Hi\n\nIm currently working on my masterthesis, in which Im doing a hierarchical regression. Since I have a problem with heteroskedasticity, I have chosen to use the \"rebust regression\". \n\nRight now im working on the tabel in which Im gonne present my findings. And when I google around, its seems like the norm is to either present \n\n- Unstandardized coeff., standard error and standardized Beta- coeff.\n\n-  Unstandardized coeff., standard error and semi-partial R-squared.\n\nIt seems like I cant get Standardized Beta coeff. when Im doing robust regression. Is it still legit to use semi-partial R-squared?\n\n",
        "created_utc": 1523953193,
        "upvote_ratio": ""
    },
    {
        "title": "Please help: Combined regression producing weird results!",
        "author": "KittyKatB99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cuub1/please_help_combined_regression_producing_weird/",
        "text": "Hello! So let's say I'm looking at how bread prices were influenced by the value of gold and silver currency in the seventeenth century. When I regress the gold and silver values (independent variables) against bread prices (dependent variable) individually, I see that there is a negative coefficient. This is what I expect, given that if the quality of currency was lowered, price inflation followed. When I do a combined regression, however, the coefficient of silver stays negative, but for gold it goes positive. When I did the same thing again but with cheese prices, both were negative when regressed individually but this time silver was positive and gold was negative when I did a combined regression. Does anyone have any idea why this might be the case please? Any help is greatly appreciated!! Thank you!",
        "created_utc": 1523951274,
        "upvote_ratio": ""
    },
    {
        "title": "Find departure time with known arrive time",
        "author": "Lethrix",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cu7ok/find_departure_time_with_known_arrive_time/",
        "text": "Hello Statistics People, :)\n\nI have a long term plan/goal with a new position at work.  It has been... a very long time since I've taken a stats class and I could use a little help to try to narrow down what I'm looking for.  My apologies if this was in the read me, I read but didn't really understand.  I work in the transportation industry and I am trying to use some statistics to come up with some numbers we can base future analysis on.  I have data galore but no one seems likely to pull it all together, so I've taken it on.\n\nThe Problem:\nLet A be the origin and B the destination.  I want to find a departure time that, with a 95% confidence would arrive at B by a certain time,say 0900.  To make matters more interesting, there are sometimes legs between A and B, so it can go A to B or A to Z to B.  I have data going back years for departures and arrivals at every point.\n\nThe Question:\nHow on earth can I go about finding this?  I could look at huge spreadsheets for every trip we run but that seems like a bad way to go about it.  There are about 45 vertexes if this was a graph.  I'm not sure how many edges.  My apologies if this is quite easy or quite challenging, I'm just hoping for a little direction.\n\nThanks all :)\n\n\n\n",
        "created_utc": 1523942920,
        "upvote_ratio": ""
    },
    {
        "title": "What test do I use?",
        "author": "StatsHelp3663",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ctbds/what_test_do_i_use/",
        "text": "I have two groups each with pre and post testing. What statistical test should I use?\n\n",
        "created_utc": 1523933135,
        "upvote_ratio": ""
    },
    {
        "title": "I need some help with a non-trivial (Bayesian) decision making process",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cta23/i_need_some_help_with_a_nontrivial_bayesian/",
        "text": "[deleted]",
        "created_utc": 1523932746,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a formula or way for calculating a confidence interval for a chi-squared test of independence with three samples? I can't find a formula or tutorial anywhere.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8csy9j/is_there_a_formula_or_way_for_calculating_a/",
        "text": "[deleted]",
        "created_utc": 1523929556,
        "upvote_ratio": ""
    },
    {
        "title": "How are sampling distributions related to t-distributions?",
        "author": "aguyfromucdavis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8csv17/how_are_sampling_distributions_related_to/",
        "text": "I'm learning about sampling distributions and t-distributions. I learned that the t-distribution is an example of a sampling distribution. What I'm confused about is this:\n\n1. A sampling distribution is derived by taking repeated samples of size n from a given population and taking, say, the mean of each sample and plotting it. According to the CLT, this would resemble the shape of a normal distribution (if n is at least 30).\n\n2. The t-distribution is also a sampling distribution. The t-distribution is derived by taking the t-score of each sample and plotting a distribution of each of the t-scores [(source)](http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-t-tests-t-values-and-t-distributions). \n\nHowever, on a separate occasion, I also learned that the t-distribution is defined by degrees of freedom. There is a t-distribution for any sample size. All one needs to provide is the sample size, subtract by 1, then you have your t-distribution. \n\nMaybe I'm missing something here, but I'm trying to reconcile the idea of deriving the t-distribution from the sampling distribution (by taking the t-score of each sample in the sampling distribution), and the idea of simply providing a sample size to get a t-distribution. These appear to be two different approaches. Thanks for any clarification!\n",
        "created_utc": 1523928713,
        "upvote_ratio": ""
    },
    {
        "title": "High School Advice",
        "author": "halfback43",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8csg62/high_school_advice/",
        "text": "I'm not sure if this is the right place but I am currently a junior who is planning on majoring in Statistics. I am currently taking AP Stats and I am taking the AP test in May. Any advice on college, work, or anything else in the field would be greatly appreciated! Thanks!",
        "created_utc": 1523924892,
        "upvote_ratio": ""
    }
]