[
    {
        "title": "Stats Help!!! Which test to use for before and after data?",
        "author": "jsjs0991",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tkn3z/stats_help_which_test_to_use_for_before_and_after/",
        "text": "I'm looking to see if there is a significant change between before and after treatment to see if stigma was still reported by the subjects. ",
        "created_utc": 1517151251,
        "upvote_ratio": ""
    },
    {
        "title": "Any free or trial software I can use for some simple statistical tests?",
        "author": "SerenityNow312",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tkmpb/any_free_or_trial_software_i_can_use_for_some/",
        "text": "Need to do some relatively simple data analysis like a paired t test and a few other things. Is there any free or trial software I could use for a few days to get the job done? ",
        "created_utc": 1517151122,
        "upvote_ratio": ""
    },
    {
        "title": "When developing new chemical quatification methods, do non-significant differences with previous methods stack resulting in a significant difference with the original method?",
        "author": "PotentiallyNernst",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tjrnk/when_developing_new_chemical_quatification/",
        "text": "I know, r/titlegore. Not a native English speaker so I found it pretty hard to cram this into a short title. My appologies ;)\n\nSo, a thing chemists do is make new protocols and methods for quantification of elements and compounds in sollutions.\n\nLet's say it's 1990, and some guy develops a method of measuring the amount of dissolved oxygen in surface water.\n\nIn 2000, a student develops a cheaper, faster method. It detects slightly less oxygen consistently, but the difference is insignificant and so the method is adopted worldwide.\n\nIn 2010 another student develops an even cheaper, faster method, but it also detects consistently less. The difference is not significant so the method is adopted worldwide.\n\nThis happens every 10 years, and in 2050 the method used detects oxygen at virtually no cost in only 1 second, but every 10 years they detected a little bit less with every new method used.\n\nDo they compare the amount detected to the original method (1990) as well? Or do they just detect a little less every time and, unknowingly, detect significantly less over the years without knowing it because it happens in such small steps?\n\n[I hope this isn't a horror to read and I hope I got the point of my question across]\n\nTL;DR:\n\nDo statistically insignificant changes over time and between different methods amount to a statistical significant difference that goes unnoticed?",
        "created_utc": 1517138897,
        "upvote_ratio": ""
    },
    {
        "title": "Difference between assumptions and what to do when violated",
        "author": "sassafrasfly76",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tj8cm/difference_between_assumptions_and_what_to_do/",
        "text": "Fave Reddit page- newest question\nConsidering the two-way mixed ANOVA.\n\nIf we have non-normality and a violation in  homogeneity of variance. \n\nThere is evidence to suggest that F ratio is robust to non-normality when groups are equal and df is over 40.\n\nDoes this include being robust to homogeneity of variance violation? Or is it dealt with differently?",
        "created_utc": 1517129475,
        "upvote_ratio": ""
    },
    {
        "title": "How much do school rankings matter?",
        "author": "DistantMirror",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ththh/how_much_do_school_rankings_matter/",
        "text": "So, I'm currently in undergrad and considering changing my major to statistics after falling in love with it while taking Statistical Methods as an elective. I had a very rocky start to my academic career but have turned it around this year. I had been planning to try for law school, where killing the LSAT will make a lot of good schools overlook a poor GPA. \n\nDoes it work the same way when going to grad school for statistics? I know I'd prefer to get at least my master's, but I also know it can be very tough to get into graduate school. Looking over some of the people posting on mathematicsgre.com, some students with records much better than mine have some trouble getting accepted to programs.\n\nIf I finish my bachelor's with a 3.2 GPA, and ace the GRE, do I have a shot at a very good school? If not, will my later career be severely impacted if I'm going to a lesser known school for my master's or PhD?",
        "created_utc": 1517109976,
        "upvote_ratio": ""
    },
    {
        "title": "Struggling to understand when to use Bayes Rule vs Basic Conditional Probability",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tgbvv/struggling_to_understand_when_to_use_bayes_rule/",
        "text": "[deleted]",
        "created_utc": 1517094227,
        "upvote_ratio": ""
    },
    {
        "title": "What statistic test should I be using?",
        "author": "goblueeeeeee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tfiub/what_statistic_test_should_i_be_using/",
        "text": "This is the question for a prelab assignment that I THINK I get but now I'm doubting myself after taking stats 2 years ago.\n\nLast week, everyone took pulse rate measurements before exercise, 2-minutes after exercise, and 5-minutes after exercise. \n\nIf you want to test whether the heart rate for all Bio 226 students changed at these 3 times points, what test would you use and why? \n\nI said it was paired since you're using before and after but now I'm thinking it might be ANOVA since there's 3 time points?? The three options are paired, independent t test, or ANOVA. I guess the question seems more vague than I intended because there will be a pulse rate change from pre and post 2 minutes, but then post 5 minutes should return back to normal now that I think about it.",
        "created_utc": 1517086557,
        "upvote_ratio": ""
    },
    {
        "title": "How do I use the Σ function on my Casio fx-991ES Plus' STAT mode?",
        "author": "Schinki",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tesy0/how_do_i_use_the_σ_function_on_my_casio_fx991es/",
        "text": "Hey guys!\n\nI'm not sure if this is the right place to ask, but I guess my question is pretty basic, although specific. What I'm trying to do is automatically calculating something like this:\n\nhttps://imgur.com/N9yVd2j\n; y obviously being the different values of my sample.\n\non [this](https://ph-live-02.slatic.net/p/3/casio-fx-991es-plus-scientific-calculator-1483412204-2805553-29ad834b48749ef089e683e46e28b271.jpg) calculator.\n\nI am able to create a table for all my sample values and the calculator is able to automatically calculate the standard deviation and mean for example. But how do I get the different instances of my variable x into the input screen to calculate? \n\n**If this isn't possible to do with the Σ function: Is there a better way to calculate instead of just doing (y1 - ymean)^4 + (y2 - ymean)^4 and so on?**\n\nThank you very much!\n\nPlease excuse if I used weird terms for this problem, it's a bit harder in English.",
        "created_utc": 1517080098,
        "upvote_ratio": ""
    },
    {
        "title": "A question about panle data",
        "author": "Payne77",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tem0a/a_question_about_panle_data/",
        "text": "Hi everyone, Just like the title I have a problem about panel data. Now I have two panel data which one include (2008-2011) 4 years and the other one include 4 years (2012-2015). But these are two seperate panel data. I want to pool (add each other) these two panel data and I want to get a panel data between 2008-2015. Btw these two panel data have 100.000 observation(id - N). Now If I do that this panel structure will be acceptable? I also write here  r/econometrics",
        "created_utc": 1517078392,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical test selection with continuous data",
        "author": "tylerdurden46",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7te0u3/statistical_test_selection_with_continuous_data/",
        "text": "I am struggling to select the correct statistical test to identify any relationship between 2 data sets. I believe the data is continuous, normal distribution and should just use Pearson correlation. \n\n1st data set is Injury Rate for 2017 per 100 employees - which is a calculation that includes the number of injuries in 2017 *200,000 / the number of employees in 2017 * 2,000. This rate controls for # of employees across all locations. \n\n2nd data set is a weighted risk score based on audit performance averaged throughout the year. Each location receives 4 audits in a year. The auditor assigns a value based on the conditions they observe. We average these 4 together to get the 2017 risk score. \n\nI started to doubt this data was continuous since I'm averaging 4 scores and it is based on qualitative observed conditions, but maybe I'm wrong. \n\nFeedback? ",
        "created_utc": 1517073625,
        "upvote_ratio": ""
    },
    {
        "title": "Two-way mixed ANOVA design question",
        "author": "sassafrasfly76",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tcmb7/twoway_mixed_anova_design_question/",
        "text": "We are testing whether anxiety leads to increased accuracy in shooting game.\nTargets are-unarmed or armed. If armed participant has to shoot, if unarmed participant should not shoot.\n\nSo- I think\n IV - 1. Anxiety (at least two levels).(Between groups effect)\n        2. Shooting target.(armed/unarmed)\nDV- accuracy of shots fired.\n\nRepeated measure =2. Shooting target which are shot at multiple times by single participant. Two levels for this variable are  armed/unarmed which would yield an overall accuracy score. (% would be best for this?)\n\nQuestion: does this look like a reasonable mixed design? \nIf so, what is the best way to describe this model in terms of repeated measure variable.\n",
        "created_utc": 1517058697,
        "upvote_ratio": ""
    },
    {
        "title": "Standardising data and the covariance",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tcla8/standardising_data_and_the_covariance/",
        "text": "[deleted]",
        "created_utc": 1517058312,
        "upvote_ratio": ""
    },
    {
        "title": "Question about unique triad pairing [Crosspost from askmath]",
        "author": "Monkeys_all_around",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tbuim/question_about_unique_triad_pairing_crosspost/",
        "text": "[Crosspost from askmath]\n\nI am trying to find all the possible unique arrangements of 18 items into sets of triads.\n\nI can do this for dyads: For all possible unique arrangements of dyads of 18 items, there are n-1 unique sets of dyad pairings. Each possible dyad is present only once in the resultant 17 sets of dyad pairings:\n\nhttps://imgur.com/8k9wblp\n\nFor example, the dyad formed by 1 and 2 only appears in one of the 17 possible sets of nine dyads. In other words, all items are included in each set of the 17 sets of dyads, but each dyad is unique across all sets of dyads such that the combination of the two items forming a dyad is never repeated.\n\nNow I need to do the same except I need to construct all possible triads of the 18 numbers. Two items can repeat between dyads, but three items cannot repeat between dyads (e.g., given 123, 241 can be used but 213 cannot be used). I’m searching for a formulaic way of doing this (both to write as a nice equation and to implement in MATLAB). Any help is greatly appreciated!",
        "created_utc": 1517046549,
        "upvote_ratio": ""
    },
    {
        "title": "Did I do this Extra credit Problem correctly? [AP Statistics]",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tbnxu/did_i_do_this_extra_credit_problem_correctly_ap/",
        "text": "Here's the problem: (Let me know if it is hard to read and I can type out the question)\n\nhttps://docs.google.com/drawings/d/e/2PACX-1vTpCGe647DE6_Uy0UnNm9JyDQ1xEn4Ev8Ty_DrvCm5sOU0jOjfiUHe7QVfBYmy97Ocwdn8Kh4IYa9SX/pub?w=1440&amp;h=1080\n\n\nHere's what I did:\n\nhttps://s13.postimg.org/7ur51abiv/ink.png\n\nEDIT: The confidence interval I got was (.1744, .3055)\n\nSince .174 &lt;.2, I said he can't start production.\n\nThis is not stated in the image above.\n\nJust wondering if I did this correctly.\n\nThanks\n\n\n\n",
        "created_utc": 1517043515,
        "upvote_ratio": ""
    },
    {
        "title": "Random vs fixed quantities in simple linear regression model",
        "author": "tcush89",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7tb8lt/random_vs_fixed_quantities_in_simple_linear/",
        "text": "In a simple linear regression model (y = beta_0 + beta_1*x + u), are all quantities random? Or are any fixed? What about their estimates (e.g. beta_1 hat, u hat, etc.)?\n\nI'm pretty sure they're all random quantities, but would just like some verification.",
        "created_utc": 1517036608,
        "upvote_ratio": ""
    },
    {
        "title": "Need help finding probability,only frequency and x are given",
        "author": "evilho",
        "url": "https://i.redd.it/ix3155jp3kc01.jpg",
        "text": "",
        "created_utc": 1517035437,
        "upvote_ratio": ""
    },
    {
        "title": "Can the Mahalanobis distance be used to compare the anomalies in two separate data sets?",
        "author": "uiuc_throwaway_23",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t7gn7/can_the_mahalanobis_distance_be_used_to_compare/",
        "text": "I am working on a project that does anomaly detection using the Mahalanobis distance. I have managed to use the metric to identify anomalous events from a given year (2011). I have also managed to identify these events from another year (2016). My question is can I use the distance to quantify the impacts even thought they are two separate data sets? Say, the max distance for year 2011 is 100 and the max distance for year 2016 is 200. Does that mean the year 2016 has had more of an anomaly/impact?\n\nI can clarify further, if there is a need to, about what my concerns are.",
        "created_utc": 1516999001,
        "upvote_ratio": ""
    },
    {
        "title": "How would you describe the relationship in these scatter diagrams?",
        "author": "xcxxfspxdxs",
        "url": "https://i.redd.it/h1ulna19hgc01.jpg",
        "text": "",
        "created_utc": 1516991566,
        "upvote_ratio": ""
    },
    {
        "title": "Crosstabs – rows and columns",
        "author": "NogitsuneX",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t5zcv/crosstabs_rows_and_columns/",
        "text": "Hello. I am battling a decision which percentage to report in crosstabs analysis. My DV is parental substance abuse, my IV is suicidal ideation. So I am a bit confused what do I need to report, considering that I am focusing on suicidal ideation. If I report rows, it means that x% of those whose parents abused substances experience suicidal ideation. If I report columns, it means that x% of those who have suicidal ideation had parents who abused substances. They both seem useful, but I don't fully understand which one is the right one to choose for reporting on suicidal ideation (in the sense of \"people who experience suicidal ideation are more likely to have had parents who abused substances...\"). \n\nI would really appreciate any constructive input, as I have been going in circles.",
        "created_utc": 1516986844,
        "upvote_ratio": ""
    },
    {
        "title": "Can I test for outliers without normal distribution?",
        "author": "anna_id",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t5v83/can_i_test_for_outliers_without_normal/",
        "text": "",
        "created_utc": 1516985914,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to perform a meta-analysis with standard error??",
        "author": "friendsintidepools",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t5jg0/trying_to_perform_a_metaanalysis_with_standard/",
        "text": "Hi, \n\nI'm trying to run a meta-analysis for an outcome (pain, on a scale of 1-100) comparing two groups, SG1 (surgical group) and NS1 (nonsurgical group) at 6 months. \nI'm including five studies, but some studies report the outcome as mean SD while some report it as mean SE. \n\nMy question is two-fold: \n\n1) Is there a way to convert SE to SD? I've searched around online and can see some people using SD = SE * sqrt(N),  but that would produce SDs that are slightly ridiculous for this outcome (ie if SG1 mean= 40, SG1 SE = 10, SG1 N=100, a calculated SD is 100..... that seems wrong for an outcome scale of 1 to 100) \n\n2) Is there a way to do a meta-analysis using SE instead of SD? Every formula I've seen for calculating effect size only uses SD.  \n\nAny help is appreciated - my searching has been largely unhelpful. \nThank you in advance!\n",
        "created_utc": 1516983234,
        "upvote_ratio": ""
    },
    {
        "title": "Finding Agreement between continuous measurements.",
        "author": "yoganium",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t4xqs/finding_agreement_between_continuous_measurements/",
        "text": "Hello,\n\nI have a project where three reviewers measured an image of a leaf in various ways (height, width, etc) and a computer algorithm did the same. This was done for 700 images.\n\nMy goal is to compare the agreement between the reviewers and also the agreement between the reviewers and the algorithm.\n\nMy first thought was to use some form of interclass correlation coefficient but I ran into some issues into how to handle ranges of acceptability between individual measurements.\n\nFor example stem length could be measured as follows:\n\nReviewer 1: 0.41cm\nReviewer 2: 0.38cm\nReviewer 3: 0.43cm\nAlgorithm: 0.39825cm\n\nIn the identification of stem length there is a 0.05cm tolerance in the variation, thus all these measurements would be considered identacle.\n\nThis would approximately give an ICC of 0.65 which is substantial but it should technically be near perfect. I am wondering if anyone has ideas on how to handle this scenario?\n\nA second thought of mine was to used a linear mixed model with an equivalence test and image_id as the random effect. \n\nMost of my work is model building and diagnostic testing so inter-rarer agreement is new to me.\n\nBest,\nJ\n\n",
        "created_utc": 1516977921,
        "upvote_ratio": ""
    },
    {
        "title": "Sampling technique name help",
        "author": "witteduppity",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t416q/sampling_technique_name_help/",
        "text": "Hi, \n\nThere are 2 levels to my sampling for my PhD study\n\nFirst, healthcare professionals in my field will approach schools within our Local Authority to tell them about the study, then I will follow up with head teachers of the schools to see if they want to take part. \n\nSecond, teachers will identify children likely to benefit from my intervention, and I will then meet with their parents to see if they want their children to take part. \n\nSo far I have described this as convenience sampling in my thesis; I'm sure there are more specific terms, however I can't find any which seem right thus far. \nBy the way, the study is quasi-experiemtnal (schools will be matched on oral language environment and other factors), with 3 intervention arms.  \n\nAny help would be much appreciated! Thanks\n",
        "created_utc": 1516967582,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence bands in linear regression (looking for a reference)",
        "author": "Batavis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t3qbk/confidence_bands_in_linear_regression_looking_for/",
        "text": "Hey guys. I was curious how to manually calculate (in, say, excel) confidence *bands* for linear models. Don't worry, I understand the difference between the confidence region of a model and of a predicted value, not confusing those two.    \nI found this post on stackexchange: https://stats.stackexchange.com/questions/101318/understanding-shape-and-calculation-of-confidence-bands-in-linear-regression\n\nThe comment by Alexis shows a formula which I could easily use in excel. Still, it leaves me with two questions:\n\n1. Does that formula work for all polynomial (b_0 + b_1x^1 + b_2x^2 + ...) models?\n\n2. Does anyone have a reference to a statistics book or paper that explains the formula? Partially asking out of my own interest, but also to have a reference for it. \n\nThanks!\n",
        "created_utc": 1516963000,
        "upvote_ratio": ""
    },
    {
        "title": "Correct statistical analysis for comparison of two groups with 5 subgroups (in R)",
        "author": "LilleOel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t3pye/correct_statistical_analysis_for_comparison_of/",
        "text": "I need to find the correct way to compare the following:\n\nI have 500 patients split into two groups (Seazure and No-Seazure)\n\nThe groups characteristics match (age, gender etc. is non significant)\n\nThe comparison variable is WFNS-grading:\n\n* grade 1: GCS 15, no motor deficit. \n* grade 2: GCS 13-14 without deficit \n* grade 3: GCS 13-14 with focal neurological deficit \n* grade 4: GCS 7-12, with or without deficit. \n* grade 5: GCS &lt;7 , with or without deficit.\n\nI just can't able to find the median or the Mean because the grades Are more like A-E.\n\nSo what statistical analysis do I need to use to calculate if the severity (grading) is significant different between Seazure and No-Seazure.\n\nI am using R, to make My calculations",
        "created_utc": 1516962824,
        "upvote_ratio": ""
    },
    {
        "title": "Advice on comprehensive statistics &amp; R textbook",
        "author": "AllanRipley",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t3le1/advice_on_comprehensive_statistics_r_textbook/",
        "text": "Hello world\n\nI'm currently looking for a textbook that covers all major concepts in statistics, and also that teaches applications in R. I have no advanced math background. I have narrowed it to the 2 following books :\n\n1. Discovering statistics using R, by Andy Field\n2. The R book, by Michael Crawley\n\nI have heard mixed advice about both, especially the fact that they are not written by statisticians so to speak, and contain some conceptual mistakes.\n\nWhich one would you recommend ?\n\nThanks",
        "created_utc": 1516960807,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating the 25th and 75th percentile",
        "author": "tintuna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t3ivh/calculating_the_25th_and_75th_percentile/",
        "text": "Hello\n\nPlease could you help - how can I calculate the 25th and 75th percentile if I only have the following information? I am using Excel\n\nConcentration of X (mg/mL) in dogs (N=16)\n\nMin:2\nMedian: 11\nMax: 80\nMean: 18\nSD: 20\n\nThank you in advance.\n\n",
        "created_utc": 1516959709,
        "upvote_ratio": ""
    },
    {
        "title": "Using z-distributions vs. t-distributions in constructing confidence intervals",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t3cue/using_zdistributions_vs_tdistributions_in/",
        "text": "Hey guys I have a question, please respond within about 9 hours from when i am commenting this (that's when I'll have my test). So IK that there are t-distributions and z-distributions. How do you know whether to use t- or z- distributions when constructing confidence intervals? like using x-bar +- t* (sx/sqrt(n)) vs x-bar +- (z* (sx/sqrt(n))?\n\nThx",
        "created_utc": 1516956924,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing cell means in Factorial ANOVA",
        "author": "playcdagain",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t23x8/comparing_cell_means_in_factorial_anova/",
        "text": "I am running a study and need to figure out how to compare the cell means of a Factorial ANOVA. I have two factors, which makes four cell means. My hypotheses are that the mean in Cell A is going to be greater than the mean in Cell C AND the mean in Cell B is going to be greater than the mean in Cell D. What test do you run after the Two-Way ANOVA where you find out the interaction is significant to compare A with C and B with D?\n\nIf specifics help, my two factors are gender and tests and my DV is score. I hypothesize the two main effects. More importantly, I hypothesize an interaction such that males will score higher on Test 1 than Test 2 and females will score higher on Test 2 than Test 1. How do I do these comparisons? ",
        "created_utc": 1516939905,
        "upvote_ratio": ""
    },
    {
        "title": "Standard deviation and variance of {35, 35, 38, 41, 41}",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t1o9q/standard_deviation_and_variance_of_35_35_38_41_41/",
        "text": "[deleted]",
        "created_utc": 1516935363,
        "upvote_ratio": ""
    },
    {
        "title": "Need help with a work scenario",
        "author": "EyeEsTeePee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t17r2/need_help_with_a_work_scenario/",
        "text": "Scenario: 1,000 calls came in during an hour. 200 weren't answered because the callers hung up before someone got to their call, so 20% weren't answered. A few extra people helped out during this hour, and they took a combined 100 calls. If those extra people never took any calls, could we possibly know how many calls wouldn't have been answered?\n\nWould you assume that all 100 wouldn't have been answered, therefore 30% wouldn't have been answered?",
        "created_utc": 1516930834,
        "upvote_ratio": ""
    },
    {
        "title": "Building a Data Set for Linear Regression in Excel",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t0ymz/building_a_data_set_for_linear_regression_in_excel/",
        "text": "Does anyone have any recommendations for sources on how to construct a data set in Excel for a linear regression analysis?",
        "created_utc": 1516928353,
        "upvote_ratio": ""
    },
    {
        "title": "Why are logs used in regression analysis?",
        "author": "xcxxfspxdxs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7t022g/why_are_logs_used_in_regression_analysis/",
        "text": "For example in the following OLS Model, why are logs used?\nGreenhouse Gas Emissions = 𝛽0 + 𝛽1log GDP per Capita + 𝛽2Population+ 𝛽3 Investment",
        "created_utc": 1516920133,
        "upvote_ratio": ""
    },
    {
        "title": "Binomial distribution question",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7szk1w/binomial_distribution_question/",
        "text": "p = 0.03\n\nIf n policyholders are selected at random, how large would n need to be so that there is at least a 60% chance that at least one of the policyholders makes a claim?\n\n\n******\n\nI'm not really sure how to go about this type of question. \n\nI mean, I want to have some n such that \n\nBin(n, 0.03) \n\nWill have a 60 percent chance of having *at least* one person who \" makes a claim \". \n\n*******\n\nIf I want P(x = k) then I want to have k successes. \n\nHere, I want to have P(x &gt;= 1), I want at least one success. \n\nSo that I have \n\n1 - P(X = 0) \n\nThis is what I think I'm interested in. \n\nThen I would use \n\nP(x = 0) = (n choose x) * p^0 * q^n\n\nwhich is \n\n1 * 1 * q^n \n\nSo that I am saying I want to have some \n\n1 - q^n &gt;= 0.6 \n\n0.4 &gt;= q^n\n\nln(0.4) &gt;= n * ln(q) = n * ln(0.97) \n\nln(0.4) / ln(0.97) &gt;= n\n\nthen the left hand side is 30.08\n\nI can't have decimal people so that I would say that I need 31 people in this situation... \n\nDoes this logic seem OK? \n\nThanks for checking",
        "created_utc": 1516916026,
        "upvote_ratio": ""
    },
    {
        "title": "Where can I get decent data files (not kaggle)?",
        "author": "AlmostMichaelCera",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7szd2v/where_can_i_get_decent_data_files_not_kaggle/",
        "text": "I'm looking for free excel files with limited categorical variables for an independent study project on variable selection methods. Kaggle.com didn't have what I was looking for. Any suggestions is greatly appreciated.",
        "created_utc": 1516914459,
        "upvote_ratio": ""
    },
    {
        "title": "I found a formula through regression, but it represents a \"moving average\", I'm trying to \"unaverage\" it",
        "author": "litehacker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sw7tt/i_found_a_formula_through_regression_but_it/",
        "text": "Sorry about the troubling title of this post.\n\nI have a formula that looks like this:\n\n    -0.9*ln(x) + 5\n\nGetting a few values:\n\n    1: 5\n    2: 4.376\n    3: 4.011\n    4: 3.75\n    5: 3.55\n\nRight... So now, this formula actually represents the average of these values:\n\n    1: 5\n    2: 3.752\n    3: 3.281\n    4: 2.967\n    5: 2.75\n\nSo, the bottom: (5 + 3.752) / 2 = The top 4.376\n\nThe bottom: (5 + 3.752 + 3.281) / 3 = The top 4.011\n\nThe bottom: (5 + 3.752 + 3.281 + 2.967) / 4 = The top 3.75\n\netc. That is what I mean.. The top set of numbers is an average of the bottom set of numbers.\n\nNow, I can calculate the bottom set of numbers from the top, manually, by reversing this operation.\n\nFor example, with the Top numbers 4.376 * 2 - (5) = The bottom 3.752\n\nThe next Top numbers 4.011 * 3 - (5 + the previous 3.752) = The bottom 3.281\n\netc. I currently do this, one by one, in excel, to \"uncover\" the original numbers without the average.\n\nIs there a way for me to get a formula that spits out these numbers instead?\n\n    -0.9*ln(x) + 5\n\nRepresents the top row. Is there a way for me to get the formula for a bottom row?\n\nPlease let me know if there's a better subreddit for this question.",
        "created_utc": 1516888703,
        "upvote_ratio": ""
    },
    {
        "title": "Help with finding odds ratios?",
        "author": "averylawson3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7svq73/help_with_finding_odds_ratios/",
        "text": "I'm really stuck on this concept.\n\nI have an I x J table.\n\nI have all the odds ratios of each every i row and j column compared to the last row and column ( (i,j) with (I, J) ). Given this information, how could i find the odds ratio between any row and column, ( (i,j) and (i',j') )?\n\nI'm struggling to see how this can be done. Any ideas? ",
        "created_utc": 1516883057,
        "upvote_ratio": ""
    },
    {
        "title": "Just want to compare drug use between two populations...",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7stay4/just_want_to_compare_drug_use_between_two/",
        "text": "[deleted]",
        "created_utc": 1516851641,
        "upvote_ratio": ""
    },
    {
        "title": "Cointegration in R",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/statistics/comments/7ssenf/cointegration_in_r/",
        "text": "[deleted]",
        "created_utc": 1516845762,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting Effect Size",
        "author": "keh12003",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ssat4/interpreting_effect_size/",
        "text": "I'm used to looking at hazard ratios, relative risks, etc in drug studies. However, a meta-analysis has me confused by an effect size per g/d (gram per day).\n\nHow do you interpret that benefit in relation to all-cause mortality?\n\nContext:\n\"overall no significant association between marine oil intake and all-cause death across a median dose range of 0.066 and 1.58 g/d (effect size per g/d = 0.62; 95% CI 0.31 to 1.25)\"",
        "created_utc": 1516843990,
        "upvote_ratio": ""
    },
    {
        "title": "Finding difference in three means of binomial data",
        "author": "KingVarun",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sr5ew/finding_difference_in_three_means_of_binomial_data/",
        "text": "Hello!\n\nI'm trying check whether there is any statistical advantage, in a multiple choice exam, in guessing consistently vs. guessing randomly. I generated 10,000 answers (A-D), and three different trials: 1) Always guessing \"C\"; 2) Guessing truly randomly; and 3) Guessing \"humanly randomly\" with differing weights to each letter. \n\n\nThe proportions I got were as such:\n\nExpected: 2500 correct : 7500 incorrect\n\nAlways C: 2499 correct : 7501 incorrect\n\nRandom: 2425 correct : 7575 incorrect\n\nWeighted: 2539 correct : 7461 incorrect\n\n\nI want to find a p-value. Chi-square makes sense to test these, because I have categorical data--or at least I think so, but when I make my table it's confusing...am I comparing those expected values with each tested values to find my chi test statistic?\n\nThanks. Note: I am using excel. ",
        "created_utc": 1516833768,
        "upvote_ratio": ""
    },
    {
        "title": "Dumb question about Standard Deviation",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sqmff/dumb_question_about_standard_deviation/",
        "text": "[deleted]",
        "created_utc": 1516829715,
        "upvote_ratio": ""
    },
    {
        "title": "Proper notation for taking the mean of a rate of change",
        "author": "lololom",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7spnfd/proper_notation_for_taking_the_mean_of_a_rate_of/",
        "text": "I have tree volume data, where for every tree, I have multiple volume records through time (ex, for tree 1, I have a volume record at 1 m3, 4 m3, 7 m3.. recorded at different years). \n\nI want to know mean rate of growth per year for all the trees. In practice, that is super easy: sum of (change in volume / change in year) between each record / (n - 1).. and then do that for all the trees and average it again...\n\nWhat I want to know is: what is the proper statistical notation for this! Is there a way to have this represented all in one formula? or does it need to be separated out?\n\n",
        "created_utc": 1516822170,
        "upvote_ratio": ""
    },
    {
        "title": "Determining proper statistical analysis",
        "author": "unreliablepirate",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sph2z/determining_proper_statistical_analysis/",
        "text": "The data I’m working with a city’s street tree inventory. I want to assess the relative statistical significance of factors contributing to sidewalk damage. Im tasked with determining if certain species are unfit for the municipally approved street tree list. My inclination is the size of the planting site is much more significant of an indicator than the tree species for the presence of sidewalk damage. A breakdown of what I’m analyzing is if the tree species(categorical independent) or the size of the planting strip (categorical independent)is more responsible for the presence or absence of sidewalk damage. I’ve been attempting to use a fishers exact test, is this an appropriate analysis? I really don’t know how to go about addressing my question so any guidance would be much appreciated ",
        "created_utc": 1516820774,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to download Statistics book by David Rupert(Publication::- Springer)",
        "author": "BaT_M4N",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sl7az/trying_to_download_statistics_book_by_david/",
        "text": "I am looking forward to read a book, implementation of Statistical concepts into Finance/ Stock Market by David Rupert (Publication::- Springer)?\nBook seems to be costly on Amazon... \n\nDoes anyone have access to this book's pdf or any similar PDFs; and willing to share???\n\nThanks!!!",
        "created_utc": 1516775324,
        "upvote_ratio": ""
    },
    {
        "title": "Kurtosis",
        "author": "Roderick1998",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sksbn/kurtosis/",
        "text": "Why is kurtosis for a normal distribution 3? I know that 3 is often subtracted off and excess kurtosis is used but why mathematically does kurtosis of a normal cure turn out to be 3?",
        "created_utc": 1516769373,
        "upvote_ratio": ""
    },
    {
        "title": "Figuring out P Values",
        "author": "HorseWizard",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7shxjq/figuring_out_p_values/",
        "text": "So I've figured out the calculations on SPSS and the SIG was .000 for all three levels which has completely thrown me. I would therefore assume that I can reject the null hypothesis as there is a statiscally significant difference in the means not due to sampling error?\n\nCould someone explain why (if correct) this would be (very basically if possible, analogies welcome!)",
        "created_utc": 1516743207,
        "upvote_ratio": ""
    },
    {
        "title": "Random Effect in a 3D Model",
        "author": "bowman9",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7shaxx/random_effect_in_a_3d_model/",
        "text": "Hi all,\n\nI am constructing a model using two independent variables for a dependent variable. There are roughly thirty subjects, each of which has multiple events, making the data points non-independent, which is why I want to incorporate a random effect into the model. Using a mixed effect multiple linear regression, the two independent variables are correlated with my dependent, as is the two independent variables' interaction. If I want to plot all of my subjects' data points, which again are not independent, on a 3D graph, how do I incorporate a random effect? ",
        "created_utc": 1516738292,
        "upvote_ratio": ""
    },
    {
        "title": "[ Confidence interval ] Two samples",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sh076/confidence_interval_two_samples/",
        "text": "\nQuestion :\nhttps://i.imgur.com/Dl9ToEN.png\n\nThe answer is :\n\n    (-3.6, 7.9)\n\nFor this I've tried the following : \n\n    # mice samples, standard and test\n\n    std = c(40, 30, 41, 41, 41, 42, 31)\n    tst = c(34, 28, 41, 38, 34, 41, 35)\n\n\n    # i want to find a confidence interval for the difference between the two means\n\n    diff &lt;- mean(std) - mean(tst)\n\n    # Testing at a 95% confidence level, so that's 97.5 % of each side\n\n    zcrit &lt;- qnorm(0.975) \n\n    # Calculate the standard error for use in the confidence interval \n\n    std_error = sqrt( \n      (sd(std)^2/7 ) # sigma^2 / n_1 \n      + \n      (sd(tst)^2/7)  # sigma_2^2 / n_2\n      )\n\n    # now use the above to find the upper and lower levels\n\n    diff + zcrit * std_error\n    diff - zcrit * std_error\n\noutput : \n\n    &gt; diff + zcrit * std_error\n    [1] 7.265054\n\n    &gt; diff - zcrit * std_error\n    [1] -2.97934\n\n\nI've used `qnorm` (normal) rather than `t` because I know the populations for\neach of the samples. So that I can find the standard deviation for each of them.\n\nI tried using t instead (which is `qt()` in `R`) just to see if that made a\ndifference, but it didn't help.\n\n\nIts really close though, which makes me I think I must just be doing something\nsmall that's wrong?\n\nIn the calculation for the standard error : \n\n    std_error = sqrt( \n      (sd(std)^2/7 ) # sigma^2 / n_1 \n      + \n      (sd(tst)^2/7)  # sigma_2^2 / n_2\n    )\n\nIf I change that from `7` to `6` it returns something that's a bit closer to the\nanswer :\n\n    &gt; diff + zcrit * std_error\n    [1] 7.675462\n\n    &gt; diff - zcrit * std_error\n    [1] -3.389748\n\nBut this still isn't right, and at this point I'm kind of hacking about a bit.\n\nHopefully there are some reasonably obvious errors here. \n",
        "created_utc": 1516736008,
        "upvote_ratio": ""
    },
    {
        "title": "OR &amp; Effects Size",
        "author": "JustJumpIt17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sgtfx/or_effects_size/",
        "text": "I was reading a paper about a case/control study, where the study had an 80% power at a 2-sided alpha = 0.05, with the ability to detect an OR of 2.33. I believe this means that 80% of the time, a true effect will be detected. A true effect being an OR of at least 2.33. Is this correct?",
        "created_utc": 1516734577,
        "upvote_ratio": ""
    },
    {
        "title": "[ Chi Squared definition ] : a sum of squares of independent standard Normals",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sgoys/chi_squared_definition_a_sum_of_squares_of/",
        "text": "\n\nI don't really understand what this is saying - a sum of squares of independent\nstandard normals...? \n\nI mean, if I have a standard normal then the mean is zero and the area is 1.\n\nBut I don't know what it means to square that, and I don't know what it means to\nhave two ( or more... ) standard normals independent of each other\n",
        "created_utc": 1516733666,
        "upvote_ratio": ""
    },
    {
        "title": "Which statistical test for standardised mean differences?",
        "author": "amyxol",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sg8da/which_statistical_test_for_standardised_mean/",
        "text": "I'm doing two comparisons that require a statistical test:\n\n- the same experimental group before and after a drug\n- the experimental group after the trial versus a control group after the trial\n\nHowever the outcome has had to be normalized as a standardised mean difference because the measurements are all different. Therefore I'm not sure how to do a statistical test? I was using RevMan which gives an output similar to https://ars.els-cdn.com/content/image/1-s2.0-S1063458414012953-gr1.jpg (note that this is not my data I just found it on Google Images)\n\nAlso when doing subgroup analyses, when it says on the image I linked \"test for subgroup differences\" what stat test would this be?\n\nThank you so much!",
        "created_utc": 1516730120,
        "upvote_ratio": ""
    },
    {
        "title": "What is better, 80% effort for 100% of the time or 100% effort for 80% of the time? The time is 200 minutes",
        "author": "ACPrime2000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sf4kg/what_is_better_80_effort_for_100_of_the_time_or/",
        "text": "*Effort should instead be productivity. This is closer to what I meant.",
        "created_utc": 1516721246,
        "upvote_ratio": ""
    },
    {
        "title": "Literature with Inverse CDF Equations",
        "author": "milano13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7seluk/literature_with_inverse_cdf_equations/",
        "text": "Hello AskStatistics. I'm working on my master's thesis right now, and am trying to find some references to equations that I have used. I used the CDF and inverse CDF equations for normal distribution and lognormal distributions. I also used one of the estimation functions for the standard normal CDF's in the equations of the other inverse CDF's. \n\nDoes anyone know a textbook that might have these equations? I've tried searching the library for statistics textbooks, high school statistics books, and I can't find any inverse CDF equations. Any advice or help is appreciated, thanks!",
        "created_utc": 1516716549,
        "upvote_ratio": ""
    },
    {
        "title": "Linear regression: association largely caused by lower quartile of group. How to interpret?",
        "author": "abcbrakka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sdlj4/linear_regression_association_largely_caused_by/",
        "text": "Hi guys, I am doing linear regression analysis in a group of cardiovascular patients (n=130) and am studying association between measurements of atherosclerosis in different arterial vessel beds (ankle-brachial index, kidney function, intracranial stenosis, carotid artery stenosis, common carotid artery IMT).\n\n\n\nAnalysis of kidney function (independent variable, continuous) and intracranial stenosis (dependent, continuous) yields a significant association. When I dichotomize kidney function into quartiles I notice that the lowest (severe kidney dysfunction) is associated with worse intracranial stenosis when compared to upper quartile (reference), 2nd and 3rd quartile are not significantly associated.\n\n\n\nHow could I interpret this result? Is this a treshold effect, meaning the association between the two variables is not linear? So low-medium atherosclerosis shows no correlation between different arterial beds, only severe atherosclerosis shows correlation between different beds?\n\n\n\nThanks in advance!",
        "created_utc": 1516704410,
        "upvote_ratio": ""
    },
    {
        "title": "How shuffled is shuffled?",
        "author": "dcsprings",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sd3wc/how_shuffled_is_shuffled/",
        "text": "I'm playing with card shuffles to create a probability project for students. I know the best shuffle is to spread the cards out on a table stir them around and push them back together. \n\nI'm thinking that you would quantify the effectiveness of a shuffle by counting the number of cards that are in a different position in the deck before and after the shuffle. Would you also measure the position of the cards with respect to each other (before: 2club, 3club, 4club after: 2club, 8hart, 4club regardless of position within deck)? Is there a good shuffle threshold or range? Is there an ideal shuffle? \n\nAs I write this I realize that there must by some dynamic that happens in the deal and that shuffled (cards that are organized random with some degree of randomness) can be organized by the process of dealing, producing \"good hands.\"\n\nI am happy with as much or as little information as is available in the community. References are also welcome.",
        "created_utc": 1516697116,
        "upvote_ratio": ""
    },
    {
        "title": "Two groups with the second group containing the same measurements from the first group",
        "author": "quincti1lius",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7scuuk/two_groups_with_the_second_group_containing_the/",
        "text": "I have two groups I will be comparing the group means.  The measurement is the age of diagnosis of a particular disease.  I have the mean of a group of patients from 2005.  We have now more patients and have a new mean age of diagnosis (this is including the data from the 2005 cohort).  What would be the best statistical test for comparing the two sample means considering the second sample continues to contain all the original data from the first sample in 2005?",
        "created_utc": 1516693282,
        "upvote_ratio": ""
    },
    {
        "title": "Need help combining probabilities?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sbee1/need_help_combining_probabilities/",
        "text": "[deleted]",
        "created_utc": 1516676510,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating data in small increments when given data for a larger range.",
        "author": "shammy8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7salf4/estimating_data_in_small_increments_when_given/",
        "text": "So I have some data with the range 0 to 3, 3 to 7, 7 to 12 mph etc etc... Each range corresponds to an amount of hour for those speeds. Is it possible to estimate a time for each 1m/s increments without additional data???\n\nEdit: a word",
        "created_utc": 1516668730,
        "upvote_ratio": ""
    },
    {
        "title": "A lot of different stats problems i really need help with!",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7saf13/a_lot_of_different_stats_problems_i_really_need/",
        "text": "[deleted]",
        "created_utc": 1516667093,
        "upvote_ratio": ""
    },
    {
        "title": "Struggling to interpret results of t-test",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7sa75u/struggling_to_interpret_results_of_ttest/",
        "text": "[deleted]",
        "created_utc": 1516665109,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence interval with a frequency",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7s9h38/confidence_interval_with_a_frequency/",
        "text": "\n\n\nI have this :\n\nhttps://i.imgur.com/XYUEz4a.png\n\nAnd the answer is : `(40.1 , 45.3)`\n\nThis is what I thought the approach was :\n\n\n    # sample size\n    n = 1000\n    # number of positive observations\n    X = 427\n    # proportion of positive observations\n    f = X/n\n\n    # upper and lower levels of the confidence interval\n    upper = f + qnorm(0.9) * sqrt( f*(1-f)/n )\n    lower = f - qnorm(0.9) * sqrt( f*(1-f)/n )\n\n    # print out\n    upper\n    lower\n\nWhich outputs\n\n    &gt; upper\n    [1] 0.447046\n\n    &gt; lower\n    [1] 0.406954\n\nI'm not sure why this is off? The lower level seems like it could just be a\nrounding error, whereas the upper level doesn't so much.\n\nIs the method that I've used correct? \n\nI think I've followed the steps correctly\n",
        "created_utc": 1516658963,
        "upvote_ratio": ""
    },
    {
        "title": "help with forecasting a quarter",
        "author": "zendakin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7s98mv/help_with_forecasting_a_quarter/",
        "text": "Looking for help to forecast the last quarter of 2017.\nI don't really know where to start. \n\nThe data that I have is \n\n\nQuarter | Revenue\n---|---\nQ1-2016|$500.00\nQ2-2016|\t$400.00\nQ3-2016|\t$350.00\nQ4-2016|\t$600.00\nQ1-2017|\t$1,000.00\nQ2-2017|\t$650.00\nQ3-2017|\t$890.00\n\nAny tips on where to start and what to look for is appreciated.",
        "created_utc": 1516657111,
        "upvote_ratio": ""
    },
    {
        "title": "Basic question about calculating sample sizes",
        "author": "lostinemail",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7s9426/basic_question_about_calculating_sample_sizes/",
        "text": "Hi all,\n\nI have a basic question about this formula to calculate sample sizes:\n**n=(z^2p(1-p))/d.** I've seen this formula in various places, but [here's where I first found it.] (https://marketingexperiments.com/a-b-testing/testing-sample-size)\n\n*n=sample size\n*z = z score\n*p = expected conversation rate\n*d = margin of error\n\nI am struggling to understand how to calculate p. Different sites give slightly different definitions. This page defines p as \"p is the conversion rate we expect to see (estimate of the true conversion rate in the population).\" \n\nI understand z, n, and d. Can someone put p in layman's terms for an email marketer? Thank you kindly. \n\n\n\n",
        "created_utc": 1516656058,
        "upvote_ratio": ""
    },
    {
        "title": "Help required with assumption of sphericity being met/not met?",
        "author": "HorseWizard",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7s7tu1/help_required_with_assumption_of_sphericity_being/",
        "text": "So I am trying to conduct a study where the aim is to see whether change blindness varies according to the type of change that is made to a scene. To test this, three different change types have been made to the stimuli.\n\nThe results returned from SPSS are as follows:\n\nAverage RT for Congruent Changes Mean 4.46 - Standard Deviation 3.00 and N is 250\n\nAverage RT for Incongruent Changes Mean 5.31 - Standard Deviation 1.98 and N is 250\n\nAverage RT for Within Category Changes Mean 5.13 - Standard Deviation 2.41 and N is 250\n\n• Within-category change: an object is replaced with another that belongs to the same category (for example, in a kitchen scene, a wooden chopping board might change to a plastic chopping board). • Congruent change: an object is replaced with another that is in keeping with the scene (that is, you would expect to see it in that context), but does not belong to the same category as the original object (for example, a hole punch in an office scene might change to a stapler). • Incongruent change: an object is replaced with an unexpected item (for example, in a dining room scene, a plate might change to a watering can).\n\nBased on those results I am trying to figure out if I can assume sphericity? Homogenecity of variances etc? Really struggling to interpret the data as stats are pretty new to me so any advise/guidance would be most appreciated.",
        "created_utc": 1516645993,
        "upvote_ratio": ""
    },
    {
        "title": "Profile likelihood",
        "author": "WamblingDisc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7s4q3q/profile_likelihood/",
        "text": "Hi there!\n\nCan anybody shed some light on the profile likelihood for me? I'm following an MSc in Epidemiology in The Netherlands, and have a statistics exam coming up.\n\n\nIn my lectures, the professor told that it is used in determining confidence intervals based on the likelihood ratio test, and it is the maximum value of the log-likelihood is the alternative hypothesis is true.\n\nI'm a bit confused on how to interpret this value. Thanks in advance!",
        "created_utc": 1516614264,
        "upvote_ratio": ""
    },
    {
        "title": "Sample means follow normal distribution",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7s29rf/sample_means_follow_normal_distribution/",
        "text": "I'm a little confused about this : \n\nhttps://i.imgur.com/QMlTDPu.png\n\nIf I have some \n\n    X = x1, x2, x3, ... , xn\n\nThen what is this saying? \n\nHow I'm interpreting it is : \n\nIf I have n &gt; 30 , and take sample sizes of 30 each time, then the means of\nthese samples will follow a normal distribution.\n\nBut this feels a bit stupid, I've basically just used words that sound similar,\nI'm not sure if that's actually correct because I don't know what that's saying.\n\nMy sample is X\n\nMy sample **mean** is (1/n)(x1 + x2 + ... + xn)\n\nMy sample **means** are...? \n\nthat last part is what I don't follow here. \n\nThanks\n\n# edit \n\nhere is some code that (i think) displays how wrong I am \n    \n    \n    # create some sample... I guess this is my population\n    n = 1000\n    X = runif(n, min = 0, max = 10)\n    \n    # this will store the \" means \" \n    values = c()\n    # I'm choosing the magic thirty\n    n = 30\n    \n    # I'm going to find the mean of 30 samples from the original 'population'\n    # this many times\n    iterations = 400\n    # fill up the values vector\n    for (i in 1:iterations){\n      values = c(values,mean(sample(X, n)))\n    }\n    \n    # witness something that doesn't look normal\n    plot(values)\n",
        "created_utc": 1516584786,
        "upvote_ratio": ""
    },
    {
        "title": "Please help me understand how sample size, population size, mean, mean distance from the mean, confidence, and error rate interact.",
        "author": "Freeloading_Sponger",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7s0gtu/please_help_me_understand_how_sample_size/",
        "text": "Here's a made up problem to demonstrate what I want to know:\n\nI play poker. I have played 10,000 hands. I have won on average 1 unit money per hand. I have a record of every hand, and am able to calculate the average distance from 0 or the mean win rate, in this case 1. (For simplicity's sake, let's assume the poker room doesn't charge any fees.)\n\nI want to know if I'm on a hot streak, or have a genuine ability to beat the field.\n\nHow many hands do I have to maintain M mean win rate for to be C% sure that my true win rate is no further than +/- E unit money from M?\n\nHere is what I'm unsure about or feel like I've gotten conflicting answers to online:\n\n1. What's the actual formula and process for computing what I want to know?\n2. What's the name for the concept I describe? What to google for, etc.\n3. The \"average distance from the mean\" I talk about, what's that called, and is that what I want to use? Or do I want to calculate the average distance from 0? Or do I want to calculate the average distance from n, where n is the win rate if the null hypothesis is true?\n4. How does population size interact with all of this? Especially given that there is an infinite amount of possible poker hands, and I can therefore only ever play an infinitely small proportion of them?\n5. Some of the formulas I've seen online make me think if I plugged in a sample of just 1 hand where I won 10 money, I could be 100% confident that my true win rate was exactly 10, since there'd be a variance of 0. Obviously this can't be true.\n\nMainly though, I'm interested in the answer to question 1.\n\nThanks.",
        "created_utc": 1516568204,
        "upvote_ratio": ""
    },
    {
        "title": "Which of these examples are IID sequences?",
        "author": "JoeTheShome",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rz4gd/which_of_these_examples_are_iid_sequences/",
        "text": "Not homework, I'm coming up with the examples, I'm just confused with the concept of what constitutes IID sequences.\n\nFor example, is [; bar{x}_n ;] an iid sequence? I would suppose not because the distribution is nth dimensional with each iteration, but I read somewhere it is an iid sequence.\n\nis [; sqrt{n} X;] an i.i.d. sequence if X is distributed [; N(0,1) ;]?  Why would it be an identical distribution when the distribution changes (i.e. the standard deviation is increasing each iteration).  \n\nThanks for your help in advance!\n\nPS not sure if latex works properly on this subreddit. ",
        "created_utc": 1516556703,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate the importance of each variable in a model?",
        "author": "loveleis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rvedg/how_to_calculate_the_importance_of_each_variable/",
        "text": "I have a deterministic model (which boils down to a complex equation) that has 5 variables and a dataset that feeds this model. I want to calculate how relevant each variable is to the result in terms of percentage (as in, if it is 0, changing that variable would not change the result, and being 1, that is the only important variable).\n\nI'm not a statistician, so my first though was just doing a stardard correlation, and the results kind of make sense (in terms of which variable yields the higher correlation), but the sum of the results is quite low, its like, in terms of R² - x1 = 0,01 ; x2 = 0,01 ; x3 = 0,1; x4 = 0,3 ; x5 = 0,15.\n\nCan I just use these values in a 0 - 100 range to find the importance percentage or I'm saying non-sense here?",
        "created_utc": 1516505783,
        "upvote_ratio": ""
    },
    {
        "title": "Is it just me or is the wording in this question misleading?",
        "author": "rigthesystem",
        "url": "http://www.mathxl.com/info/exercise.aspx?fromask=yes&amp;dataid=c8fbcc18-fe6e-4fe3-989c-61eda21d9da9",
        "text": "",
        "created_utc": 1516499253,
        "upvote_ratio": ""
    },
    {
        "title": "What are good sites to learn more about the different types of sampling?",
        "author": "purple-2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rsh0k/what_are_good_sites_to_learn_more_about_the/",
        "text": "Also, what is a good site to learn about unbiased estimators vs ratio estimation?\n\nEdit: In regards to cluster sampling",
        "created_utc": 1516476480,
        "upvote_ratio": ""
    },
    {
        "title": "Best way to go about normalizing 'spike-like curves' in data",
        "author": "1cedrake",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rrq6c/best_way_to_go_about_normalizing_spikelike_curves/",
        "text": "Hello all. For a project I'm working on, I'm collecting battery temperature data from smartphones as well as ambient temperature during the same time period in order to create some sort of model that'll be able to estimate the ambient temperature based on the battery temperature. \n\nHowever, given it's a smartphone battery that's prone to having temperature fluctuations when CPU usage goes up and heats up the interior of the phone, I'm wondering what sort of methods might be best to normalize these 'unwanted' fluctuations. \n\nHere's an example of a chart from a set of data: https://imgur.com/r4VLTnl\n\nYou can see in that picture how the battery temperatures on the phones start rising as the CPU usage goes up (this is when I started stressing the CPU), and then the temperatures go down (when I stopped the stress test). \n\nIs there a way to normalize that curve and essentially make it a line like before and after the CPU increase? Some sort of algorithm, or machine learning maybe?\n\nI wasn't 100% sure where to ask this question, if anyone has any recommendations for somewhere else to ask, please let me know. ",
        "created_utc": 1516469480,
        "upvote_ratio": ""
    },
    {
        "title": "Standardised Mean Difference / Statistics Help",
        "author": "amyxol",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rpn4s/standardised_mean_difference_statistics_help/",
        "text": "So for my project I am studying the effect of a drug intervention on a continuous outcome. I am doing two different \"sets\" of stats - one is comparing the experimental group before and after the drug (so paired) and the other is comparing the experimental group after the drug and the control group after the drug (unpaired).\n\nBecause the outcome is on different scales I am using a standardised mean difference. The statistics software I am using (Review Manager by Cochrane) gives:\n\n- an overall standardised mean difference (e.g. SMD=1.50)\n- confidence intervals for the SMD\n- Chi squared value\n- I squared value\n- A 'Z' score with a p value associated\n\nHere is a picture of what my results look like. These aren't my results as I can't share them as that would be cheating so instead they're just taken from Google Images. http://www.jmir.org/article/viewFile/2777/1/37247 however I don't have a Tau-squared but a Chi-squared value.\n\nI am wondering if someone can clarify what statistics from these are relevant as I am a little confused? For example my general question is does DRUG reduce OUTCOME. My sample size is N&gt;30 but not sure how to tell if the data is normally distributed? Then would I be comparing the SMD before to SMD in group B?",
        "created_utc": 1516443413,
        "upvote_ratio": ""
    },
    {
        "title": "The right statistical method to use for retrospective cohort studies? ANOVA?",
        "author": "WhereDoIPutMyMoose",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rn5t4/the_right_statistical_method_to_use_for/",
        "text": "So I am trying to look at if there is a difference between time to a particular event based on 2 cohorts that received different exposures. The problem is, the timing of exposures varies among subjects within each of the two cohorts. My thought was to match up the subjects by year/month of the event and then look if a difference in the time to event is different between the two cohorts. The question is, what is the best model/method to use if am trying to match up the cohorts like this? one tailed ANOVA? And follow up question: if it is ANOVA or some other method, is there a way I can combine all of the results of each cohort that I matched by time into one result? Thanks for your help!",
        "created_utc": 1516411028,
        "upvote_ratio": ""
    },
    {
        "title": "Question About Gaussian Distribution (sports related)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rn33x/question_about_gaussian_distribution_sports/",
        "text": "[deleted]",
        "created_utc": 1516410278,
        "upvote_ratio": ""
    },
    {
        "title": "Compare model performance?",
        "author": "dm287",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rmekk/compare_model_performance/",
        "text": "I have two models estimating 100 quantities each. So I have 100 prediction errors, coming from Model 1 and Model 2. How do I test that Model 1 tends to produce lower errors than Model 2? I have seen the Signed Rank test, but my issue with this is that the null hypothesis is that the PDF is symmetric about 0. It's possible for an assymmetrical PDF to still have Model 1 higher than Model 2 though (imagine a bimodal asymmetric distribution). \n\nI like the look of the Kruskal Wallis test, and empirically it's looking fine, but one of the assumptions is that the data is independent. My data is \"paired\" in the sense that if Model 1 has a high error, Model 2 probably will as well. Can I still use it? If not, is there an equivalent test with the same null as KW that does not assume independence? ",
        "created_utc": 1516403663,
        "upvote_ratio": ""
    },
    {
        "title": "How can I correlate my data?",
        "author": "rhxxn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rl3e7/how_can_i_correlate_my_data/",
        "text": "So I am currently gathering data for noise levels in classrooms and would like to find out if there is a relationship between the actual noise level and the perceived noise level of students. But the thing is the type of data from the perceived noise level is ordinal (in form of High, Medium, or Low), while the actual noise levels in decibels are numerical, is it possible to find the correlation? and how?\nThank you!",
        "created_utc": 1516392357,
        "upvote_ratio": ""
    },
    {
        "title": "Stats concerning sibling age differences of 10 years or more?",
        "author": "mr-crk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ritzz/stats_concerning_sibling_age_differences_of_10/",
        "text": "I have been searching for about a week now, and I seem to be unable to find any statistics concerning sibling age differences of 10 years or more. More specifically, what percent of the population has a sibling with whom there is at least a 10 year age difference regardless of birth order. (including half-siblings but not step-siblings).\nAny help would be greatly appreciated, thanks!\n\n(Also if there is a more appropriate subreddit for this question that would be useful as well)",
        "created_utc": 1516373907,
        "upvote_ratio": ""
    },
    {
        "title": "Need a sanity check on reporting Wilcoxon Signed Rank test scores, especially considering z scores and p values",
        "author": "apolotary",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ricxn/need_a_sanity_check_on_reporting_wilcoxon_signed/",
        "text": "I have conducted an experiment where I tested two versions of one program (say version A and B), my hypothesis is that version B improves user experience in qualities X, Y, Z when compared with version A. I recruited 20 participants, asked them to complete a certain task, and fill out a questionnaire afterwards.\n\nSince the questionnaire is composed of ordinal Likert items, using a non-parametric test such as Wilcoxon Signed Rank test for paired samples was recommended by observed relevant literature.\n\nAccording to the test results, I observed p &lt; 0.05 in several cases, thus claiming the statistical significance of said results (preceded by a sensitivity analysis). \n\nNow one of the reviewers is requesting to report z-scores for each measurement and p values that are larger than 0.001. So my questions are:\n\n* What is the point of reporting z-scores for such a small sample? As far as I understand, z-score is applied for large sample size (e.g., &gt; 30 participants) studies.\n\n* Even if I should report it, for what cutoff values am I supposed to be looking for? As far as I understand the statistical significance can be reported in cases when | z | &gt; 1.96 if alpha = 0.05 for two-tailed tests (and around 1.65 for one-tailed)\n\n* What is the point in reporting of exact p values if they are &gt; 0.001? Again the literature suggests that grading p values by level of significance for p &lt; 0.05 (e.g., p &lt; 0.01, p &lt; 0.001) is meaningless, and reporting the effect size and confidence intervals is recommended instead.\n",
        "created_utc": 1516369220,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for an example of a non-normal DV with IID normal errors when performing OLS linear regression",
        "author": "medfunk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rhyku/looking_for_an_example_of_a_nonnormal_dv_with_iid/",
        "text": "My goal is to clarify the common misinterpretation of the \"normality assumption\" in linear regression, by providing a heuristic. \n\nMany attribute the \"normality assumption\" for OLS regression to mean that the dependent variable must be normally distributed. However, it is the error terms to which this applies. To illustrate this, I am looking for an example where the error terms are homoscedastic (and uncorrelated) when performing OLS regression, but the dependent variable is not. \n\nI am familiar with the generalised linear model framework, and how a link function can be applied to account for binary and count data, but I am specifically trying to avoid dependent variables that might follow a particular distribution for this purpose.\n\nIf there is a dataset or some other way of demonstrating this that would be great. \n\n",
        "created_utc": 1516364537,
        "upvote_ratio": ""
    },
    {
        "title": "ANOVA, ANCOVA?",
        "author": "nifsr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rgwwv/anova_ancova/",
        "text": "Hello,\nI'm looking for help in this subreddit to figure out an XLS with RAW data containing 22 subjects (14 male, 8 female). I have no background in statistics but a colleague of mine asked me for help, so here I am turning to a higher power.\n\nI'm looking to see if there's any correlation between the subjects, especially those who have the % higher than 100% (marked with blue) and furthermore between those with O.U rating (marked with red)\n\nhttps://www.dropbox.com/s/jbg3gkzfilmc7ua/date%20brute.xlsx?dl=1\n\nThank you!",
        "created_utc": 1516349519,
        "upvote_ratio": ""
    },
    {
        "title": "What approach should I start with for b?",
        "author": "AndreasKralj",
        "url": "https://imgur.com/T661ktu",
        "text": "",
        "created_utc": 1516331718,
        "upvote_ratio": ""
    },
    {
        "title": "How to explain Wilcoxon and Mannwhitney test?",
        "author": "o-rka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rbyu1/how_to_explain_wilcoxon_and_mannwhitney_test/",
        "text": "If you have a vector u and a vector v both with 1000 values each and you use a Wilcoxon test to get a p &lt; 0.001 . Would you say that the distributions of u and v are significantly different ? ",
        "created_utc": 1516301690,
        "upvote_ratio": ""
    },
    {
        "title": "How do you create a z-value for a ranking system?",
        "author": "2400hoops",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rb9vp/how_do_you_create_a_zvalue_for_a_ranking_system/",
        "text": "I think my question is fairly simple, but I am having some trouble finding straight forward answers online. I am looking to give a value to a variable that is between 0-1 where 1 is the highest value this variable can take and 257 is the lowest value the variable can take. For added context, I am creating a ranking based system for College Football 1 is the highest rank team and 257 is the lowest ranked team I want to create a spectrum from 0-1 where 257 would be 0 and 1 would be 1.\n\nHow would I go about creating this normalized variable? ",
        "created_utc": 1516296192,
        "upvote_ratio": ""
    },
    {
        "title": "Question about study design and sampling (scenario included)",
        "author": "shinracorp_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7raxe8/question_about_study_design_and_sampling_scenario/",
        "text": "Hi guys, I have a question about study designs and sampling. I have written out the scenario I have questions about below:\n\n**Scenario:** Organization \"ABC\" is a healthcare provider interested in opening a department dedicated to cardiac rehabilitation for a specific type of heart condition, using a new treatment approach called X. \"ABC\" recently received funding to evaluate whether X is better than the standard treatment. The funding will provide new staffing resources, allowing for approximately 30 patients to be enrolled into X.\n\n\"ABC\" sees patients as they are referred, and at any given time the volume of patients presenting with the eligibility criteria for their heart disease program will vary. They do however have a diagnostic process that can determine whether a patient would be eligible for X. \"ABC\" has been given 1 year to determine the effectiveness of X as the funding provided will only cover 1 year's worth of resourcing.\n\n**Question:** \"ABC\" would like to conduct a randomized controlled trial to determine whether X is more effective at treating this specific type of heart condition, in comparison to the standard treatment. There are concerns however that the sample won't be representative of the population, given the study's time constraints, funding, and the variability surrounding patient enrollment and their presenting issues. How would you approach sampling and study design for this program? Is an RCT out the window?\n\n**My thoughts:** Since the number of patients they see varies, \"ABC\" could essentially have a sampling window of approx. 1-2 months to gather the necessary patients. As for the approach, \"ABC\" will recruit 60 (or as close to 60 as possible) patients that all qualify for X, but will be divided into control and intervention groups; 30 for standard treatment and 30 for X.\n\nI'm curious to hear your thoughts on my thoughts regarding the general approach towards setting up the evaluation. Are there design alternatives that could demonstrate differences in effects between the two groups?\n\nThanks guys any thoughts are appreciated",
        "created_utc": 1516293509,
        "upvote_ratio": ""
    },
    {
        "title": "Advice for using Likert Scale to assess purchase intention",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rarwm/advice_for_using_likert_scale_to_assess_purchase/",
        "text": "[deleted]",
        "created_utc": 1516292285,
        "upvote_ratio": ""
    },
    {
        "title": "How should I average these ratios?",
        "author": "RifRifRif",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7rartg/how_should_i_average_these_ratios/",
        "text": "For my New Year resolution, I decided that every week, my [time spent usefully/time wasted] ratio (referred to as TSU/TW) would be larger than the weekly average ratio, accumulated over the year. \n\nThis is the second week that I'm doing this (because I wasn't home for most of the first), and I've encountered a problem with calculating the averages. More specifically, I've considered two methods: \n\n- Taking an average of TSU, an average of TW, and using the ratio of these values. \n- Calculating the TSU/TW ratio of each day, and taking the average of these ratios. \n\nFor instance, if Sunday's ratio was a/b and Monday's ratio was c/d, how should I calculate the average: \n\n- ((a+c)/2)/((b+d)/2) = (a+c)/(b+d)\n- (a/b + c/d)/2 = (ad + bc)/2bd\n\nThanks a lot in advance! ",
        "created_utc": 1516292258,
        "upvote_ratio": ""
    },
    {
        "title": "Assumption for one-way ANOVA test",
        "author": "devilwearstoms",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7ralkk/assumption_for_oneway_anova_test/",
        "text": "Should my data be normally distributed or the error? ",
        "created_utc": 1516290809,
        "upvote_ratio": ""
    },
    {
        "title": "Showing which variables move more in sync with each other over time.",
        "author": "ninjadiplomat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r9rpr/showing_which_variables_move_more_in_sync_with/",
        "text": "[removed]",
        "created_utc": 1516283110,
        "upvote_ratio": ""
    },
    {
        "title": "Confused because of statement saying a prob dist with lower entropy is a briefer and therefore simpler explanation",
        "author": "Nimitz14",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r8x6k/confused_because_of_statement_saying_a_prob_dist/",
        "text": "From http://andrewgelman.com/2011/12/04/david-mackay-and-occams-razor/\n\n&gt; When you think about it from an information theory / data compression point of view, the Occam story becomes very compelling. P &lt;-&gt; l = log 1/P … Replace “Simpler explanation” by “Briefer explanation”, and it becomes effectively a tautology. “Of course the briefest explanation is the most probable explanation! Because brevity is just log P.” You might enjoy reading about the “bits back” coding method of Geoff Hinton, which accurately embodies this data-compression / Bayesian phenomenon for quite complicated models including latent variables. \n\nBut a simpler model is the model that is smoother. So the simpler model will actually have a higher entropy!\n\nedit: To elaborate, the discussion is how to justify picking a simpler model. Mackay here is saying that from an IT perspective it makes sense to pick a simpler model.",
        "created_utc": 1516272597,
        "upvote_ratio": ""
    },
    {
        "title": "Using Stata for this",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r8t2n/using_stata_for_this/",
        "text": "[deleted]",
        "created_utc": 1516270976,
        "upvote_ratio": ""
    },
    {
        "title": "Chance within chance in a card game",
        "author": "kronikcLubby",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r8dct/chance_within_chance_in_a_card_game/",
        "text": "Taking a new angle of attack at one of my favorite card games, *Magic the Gathering*.\n\n I want to know some specific numbers when it comes to the probability of certain draws and I'm starting with a card called Ponder.\n\n[For the unaware](http://gatherer.wizards.com/Handlers/Image.ashx?multiverseid=244313&amp;type=card)\n\n**can someone follow this train of thought and tell me if I'm right and answer my question at the end?**\n\nHere's the situation:  you have a deck with 99 cards in it.  35 of the cards in your deck are called *Lands*.  Lands are a resource that you can play once per turn.  In your opening hand there was only 1 land and the card *Ponder*\n\n**i want to know the % chance that i will be able to draw a second land by the end of turn 1 using Ponder, assuming I'm going first and not drawing for the turn.**\n\nSo, here's the merry little logic trail i galavanted down:\n\n99 total cards in the deck\n\n35/99 are lands.\n\nOpening hand of 7 cards with 1 land and a Ponder\n\n92 cards to draw from\n\n34 lands to hit with a minimum needed of 1.\n\nPonder has two stages.  First look at the top 3 cards, if we find a land we put the top 3 back in an order with the land on top and draw it.  Stage two is if we *fail* to find a land in the top three we may shuffle and draw 1.\n\nA little research into hypergeometric probability and i have \n\n(N)=92\n\n(n)=3\n\n(M)=34\n\n(X)=1\n\nQuick crunch with a calculator says in stage one i have ~45% chance to find at least 1 land.\n\nIf i fail i replace those 3 cards, shuffle and draw 1.  So...\n\n(N)=92\n\n(n)=1\n\n(M)=34\n\n(X)=1\n\nAnd i find i have ~37% chance to find a land during stage two.\n\nHere's where I'm close but can't make the last jump.  \n\nI have 45% chance to succeed.  If i fail then i get to try again and have a 37% chance to succeed.  What is the % chance i will succeed at all?\n\nI understand how to calculate the likelihood of multiple independent probabilities but I'm a little hung up on how to calculate the odds of success of a test that gets a retest if it fails.",
        "created_utc": 1516264560,
        "upvote_ratio": ""
    },
    {
        "title": "How to find hypergeometric probability of a range of samples within n sample size?",
        "author": "Realinternetpoints",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r74h3/how_to_find_hypergeometric_probability_of_a_range/",
        "text": "For example: \nIf I have a collection of 14 marbles, 9 are green and 5 are red. \nPerson A draws 2 marbles but keeps them secret. \nPerson B then draws 2 marbles too. \nWhat is the probability of Person B drawing 2 green marbles?",
        "created_utc": 1516248837,
        "upvote_ratio": ""
    },
    {
        "title": "Senior Undergrad Project- Passenger throughput",
        "author": "juice88",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r71nn/senior_undergrad_project_passenger_throughput/",
        "text": "Just looking for some ideas and/or advice guys. So I'm currently working at the airport and for my senior project I wanted to use some data that I could relate to. Luckily that data is available and was able to find passenger throughput numbers for 2015 at all US airports.\n\nHere is an example:\nhttps://www.dhs.gov/sites/default/files/publications/tsa-throughput-december-20-2015-to-december-26-2015.pdf\n\nThe data contains many variables such as city, airport, time, day, and even the checkpoint of that airport.\n\nIn what different ways could I use this data? I'm gonna talk to my professor on Monday but wanted to have a couple ideas that I can bring to him. Thanks!\n",
        "created_utc": 1516247998,
        "upvote_ratio": ""
    },
    {
        "title": "Help analysing mini project data",
        "author": "alecmobreezy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r6djv/help_analysing_mini_project_data/",
        "text": "Anyone with a background in statistics and SPSS available to help me review a mini project with SPSS data? You do not need access to SPSS for this I already ran the data etc, just need someone to help me answer some questions and to report the data in a standardised format. I need to submit by midnight Jan 18th",
        "created_utc": 1516241356,
        "upvote_ratio": ""
    },
    {
        "title": "Need Chinese-language educational videos or podcasts on statistics",
        "author": "helpfulcin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r69ti/need_chineselanguage_educational_videos_or/",
        "text": "Hello!\n\nMy Mom wants to learn about statistics, but she's not computer savvy enough to look for them. I want her to have access to quality sources. Any suggestions would be appreciated. Thank you!",
        "created_utc": 1516240335,
        "upvote_ratio": ""
    },
    {
        "title": "What type of regression should I use?",
        "author": "re186282",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r610l/what_type_of_regression_should_i_use/",
        "text": "I'm currently gathering data for a study of the impact of World Bank loans on the democratization process in developing countries. I will have two independent variables: 1) receipt of an investment loan and 2) receipt of a development loan. These are dichotomous variables. I would like the dependent variable to be based on the following question: \"Was there a democratic transition?\" where -1 = democratic breakdown, 0 = no change, and 1 = democratic transition. This might be a stupid question, but what type of regression analysis would I need to conduct for this? ",
        "created_utc": 1516238048,
        "upvote_ratio": ""
    },
    {
        "title": "Correlation does not ensure causation. But does causation ensure correlation?",
        "author": "PickleMobster",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r5v6t/correlation_does_not_ensure_causation_but_does/",
        "text": "EDIT: I was thinking in terms of data collected in a study.\n\nA tightly controlled experiment showed that patients with their stomachs removed were unable to have feelings of hunger be induced by images of food, compared to controls. We can say that the stomach has some causal factors that induce hunger in the presence of images of food. \n\nHOWEVER, if an alternate study that claimed stomach removal does not correlate with induced hunger, would the presence of evidence against correlation necessarily be evidence against causation?",
        "created_utc": 1516236578,
        "upvote_ratio": ""
    },
    {
        "title": "[BSc Psychology] Study Design Help",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/7r5r5t/bsc_psychology_study_design_help/",
        "text": "[deleted]",
        "created_utc": 1516235545,
        "upvote_ratio": ""
    }
]