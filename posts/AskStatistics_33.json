[
    {
        "title": "Setting odds for pool",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n9j9g/setting_odds_for_pool/",
        "text": "[deleted]",
        "created_utc": 1527694507,
        "upvote_ratio": ""
    },
    {
        "title": "Need help interpreting the correlation coefficient",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n8eo5/need_help_interpreting_the_correlation_coefficient/",
        "text": "[deleted]",
        "created_utc": 1527685474,
        "upvote_ratio": ""
    },
    {
        "title": "a/b testing with only overall results of measures and no raw data",
        "author": "Taiikvei",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n7qgm/ab_testing_with_only_overall_results_of_measures/",
        "text": "hi,\n\nI have a task to do for a job, and they gave me a small 4x5 table with some descriptive statistics. it looks something like this:\n\n\n    measure       ctrl        policychange1    policychange2    policychange3\n    measure1        %               %               %                %\n    measure2   number            number          number             number\n    measure3        %               %                %                %\n\n\nI've done statistics (ANOVAS, t-tests) in Uni using IBM SPSS, but I've always had a bunch of raw data. All I know about the groups is that they each had an even amount of participants (2500)\n\nHow do I go about examining these groups? I need to decide which policy/combination of is best, or what can be expected if 2 policies are combined?\n\nI would appreciate if anyone could point me in the right direction for some reading material on how to go about doing this. Will I just use ANOVA/MANOVA? Is it okay to use them without the raw data and with just this summarized data?\n\n\n",
        "created_utc": 1527678937,
        "upvote_ratio": ""
    },
    {
        "title": "How to put observed counts on table for chi-squared homogeneity test when working with proportions?",
        "author": "deeplit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n6tcg/how_to_put_observed_counts_on_table_for/",
        "text": "Quick question guys: So I am collecting data to do a chi-squared test of homogeneity to see if the proportion of students who commit to a 4 yr college right out of high school is different for schools with proportion of Asians &gt; .5 vs.&lt; .5.  I am able to get the data of the proportion of Asians and students who commit to a 4 yr out of HS for my random sample of 50 Bay Area high schools. How would I present the data on the chi-square homogeneity table though?\n\n\nHere is the table i will be filling (one for observed and one for expected counts). what is so confusing is that we are working with proportions. do i take the average proportion for all schools for each category (community college, 4-year, etc) and use that as my observed counts?\n\n\nhttps://cdn.discordapp.com/attachments/337690985419243522/451288409286115348/unknown.png ",
        "created_utc": 1527667366,
        "upvote_ratio": ""
    },
    {
        "title": "Need assistance with Chi-Square Test for Independence with large sample size",
        "author": "delegatedcreations",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n6rq8/need_assistance_with_chisquare_test_for/",
        "text": "I'm gonna try to summarize everything as easily as possible: I'm doing a class project in which I have to use a chi-square test for independence, and my question for said project is asking whether average sleep time has an effect on GPA. I sampled 50 students for GPA and average sleep hours, and I split them 23 to 27 depending on whether they had less than or equal to 6 hours or more than 6 hours. I try entering this into a chi-square test calculator and it's repeatedly sending me through errors and I'm not the most savvy with chi-square tests so I was just coming here for help. The GPAs range from 1.0 to 4.5, if anyone would like the actual data just ask and I will gladly provide. Anything helps, thanks\n\nEdit: I have tried splitting the samples between the two evenly in 25 to 25 versus the earlier 23 to 27 based on sleeping hours and put it in my calculator but there I still get an error there too.",
        "created_utc": 1527666781,
        "upvote_ratio": ""
    },
    {
        "title": "Yu-Gi-Oh combo probability question",
        "author": "Indescribled",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n4j22/yugioh_combo_probability_question/",
        "text": "I'm currently playing a deck that has a lot of strong combos that require a wide variety of two different cards. I'm curious on what I would do to calculate the chance that I start the game with the possibility to do the combo with my opening hand.\n\nSome relevant information is:\n\nDeck size is 40 cards\n\nOpening hand size is 5 cards\n\nI need 1 of each, drawing multiple is not an issue. \n\nThe maximum limit of playing any certain card is 3, so when I say that I play 6 or 10 copies of the card it is just multiple different cards that serve the same function. \n\nFor example, if I open with any of a card I play 6 copies of and any card I play 10 copies of, my combo will start. \n\nI know that trying to calculate the chance of opening X copies of a single card would be a hypergeometric distribution, what would this be categorized as and how would I calculate it?",
        "created_utc": 1527643149,
        "upvote_ratio": ""
    },
    {
        "title": "Newb trying to wrap his head around an application of a Binomial Distribution.",
        "author": "runningsneaker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n3yis/newb_trying_to_wrap_his_head_around_an/",
        "text": "Hey gang - so sorry if this is below the threshold for this sub. I went back to school last year and just last month I finished my Statistics final and THE NEXT DAY hopped on a plane to Vegas for work. \n\nWhile flying I got to thinking about this gambling strategy - the Labouchere system (sometimes called the Cancellation method). Effectively the gambler has a list of values (usually between 7 and 12 numbers) whose sum is the total amount they aim to win. Each attempt they gamble the sum of the first and last number on the list. If they win, they cross off the numbers for the values they won, and if they loose, they dont cross off anything and instead add the amount that they lost as a new number to the end of the list. This does two things - it effectively amortizes the amount that they lost over more turn (unlike the \"doubling method\"' where the gambler doubles their loosing bet), and it also makes it so over time, the gambler only needs to win [(quantity of numbers on the list)+(amount of times the lost)]/2 times. In theory, provided the gambler has unlimited bankroll, they would only have to win 34% on the time to win their desired amount. \n\nIn the real world, of course, people dont have an unlimited bankroll. As I was bored on the flight, I wanted to see what I could learn about this betting scheme and see if I could come up with the probability it would be successful with a fixed max amount in losses and a goal quantity to win. \n\nI was able to come up with a surprising amount of info, but I am not sure the conclusions I am drawing are accurate and I was wondering if someone can point me in the right direction. \n\nI abatriaily picked numebers that felt like something one might use in the real world: Goal Winnings of $100 (10 number of 10 each so the starting and min bet was $20), and a max loss of $300. After playing with these numbers a bit on the flight I was able to figure out the amount of wins require to reach our goal, and the amount of losses that would bankrupt us (seems to be 7 more than our wins regardless of the quantity of total bets). \n\nWith these in mind, I was able to determine the probability that a set of a given number of tries resulted in one of the three possible states (Won / Loss / Continuing to play because neither has occurred yet) using binomial distributions. I even graphed the likelihood a win or loss would occur with the X axis as number of plays. \n\nI think my question is, are my claims valid? Anecdotally - I can imagine a situation where a string of 20 play starts off with 5 or 6 wins (enough to cause the gambler to reach their goal and then stop playing) however the remaining turn result in losses such that at the end of 20 they would be in  the loss column. Is there a way to set up a random walk diagram/analysis with thresholds that cause the walk to Stop? \n\nAgain - clearly I am just a new statistics user, but I am really fascinated by these concepts and I paged through the text book for that course and didnt find what I am looking for so any help would be great! ",
        "created_utc": 1527638092,
        "upvote_ratio": ""
    },
    {
        "title": "probability of k consecutive successes in n trials?",
        "author": "needtoquithelp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n3cp2/probability_of_k_consecutive_successes_in_n_trials/",
        "text": "so its been awhile since my college stats class, but came across this https://math.stackexchange.com/questions/59738/probability-for-the-length-of-the-longest-run-in-n-bernoulli-trials/59749#59749. \n\nUsing the above, what's the probability of 11 runs given p=.5 and n = 40? \n\nAlso saw the term Markov chain thrown around. Would love some clarification. Thanks ",
        "created_utc": 1527632943,
        "upvote_ratio": ""
    },
    {
        "title": "Finding Meaningful breakpoints in a variable",
        "author": "Master_File",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n1tpo/finding_meaningful_breakpoints_in_a_variable/",
        "text": "Lets say you have a few variables that relate to each customer:  Purchases, Income, Household Size\n\nFor each variable,  I want to see if there is meaningful breakpoint where a certain segment of customers really stick out.\n\nFor example,  looking at household size.  hypothetically, most customers are between 2\\-4 people.  But, then there is a meaningful number of customers that fall in subset  with household sizes over 5.\n\nHow would you find this?  would you just do this looking at distribution curves?",
        "created_utc": 1527621511,
        "upvote_ratio": ""
    },
    {
        "title": "How is standard error calculated for use in funnel plots?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n1tnn/how_is_standard_error_calculated_for_use_in/",
        "text": "[deleted]",
        "created_utc": 1527621500,
        "upvote_ratio": ""
    },
    {
        "title": "Homework question? I've done all the work but its wrong. Tell me where I went wrong?",
        "author": "dumbatmath4ever",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8n0b2h/homework_question_ive_done_all_the_work_but_its/",
        "text": "x 0 P 0.659\n\nx 1 P 0.287\n\nx 2 P 0.050\n\nx 3 P 0.004\n\nx 4  P 0.001\n\nx 5 P 0\\+\n\nTrying to find the mean and SD\n\nHere's what I've got so far:\n\nE\\(x\\)= 0\\(.659\\)\\+1\\(.287\\)\\+2\\(.050\\)\\+3\\(.004\\)\\+4\\(.001\\)\\+5\\(0\\)= 0.511 which I assume is my mean\n\nV\\(x\\)= \\(0\\-.511\\) Squared  \\(.659\\) \\+ \\(1\\-.511\\) Squared \\(.287\\) \\+ \\(2\\-.511\\) Squared \\(.050\\) \\+ \\(3\\-.511\\) Squared \\(.004\\) \\+ \\(4\\-.511\\) Squared \\(.001\\) \\+ \\(5\\-.511\\) Squared \\(0\\) = .3884 which is the variance.\n\nSD = V Squared which equals .623\n\nThe answer is provided in the text but I'm wrong and stumped. Can anyone tell me where I went wrong? I hope the formatting doesn't make the question confusing for you. Thanks in advance for your help. I really want to understand but continue to struggle with stats. Ugh.",
        "created_utc": 1527610209,
        "upvote_ratio": ""
    },
    {
        "title": "Basketball",
        "author": "Drilldoc45",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mzz6l/basketball/",
        "text": "Hello all,  I have a question about basketball.  IMO, the only part of basketball that matters is the last 2 minutes.  Weather it's a blowout, then the game is over, or it's close and every point/second matters.  So, my question is; if basketball games were only 5 minutes long would that be statically the same outcome as the last two minutes of a regular game? ",
        "created_utc": 1527607710,
        "upvote_ratio": ""
    },
    {
        "title": "Intraclass Correlation Coefficient is .57 without data?",
        "author": "martyparty020",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mzup1/intraclass_correlation_coefficient_is_57_without/",
        "text": "Hi, I calculated the ICC over the data from 6 different coders with a N=20. This specific variable wasn't coded at all, so the data looked like this:\n\nCoder 1: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nCoder 2: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nCoder 3: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\netc.\n\nI still get a ICC of .57 which is fair (Cichetti, 1994).. how is this possible? How can I interpret this? I mean I can't say: \"This variable wasn't coded but still got a ICC of .57 which is fair..\"",
        "created_utc": 1527606734,
        "upvote_ratio": ""
    },
    {
        "title": "What sample size would one require in order to get a meaningful result when trying to establish normal values in live animal studies?",
        "author": "Appersonation",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8myalu/what_sample_size_would_one_require_in_order_to/",
        "text": "",
        "created_utc": 1527592593,
        "upvote_ratio": ""
    },
    {
        "title": "What am I reading wrong in this study?",
        "author": "MRD85",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8my6il/what_am_i_reading_wrong_in_this_study/",
        "text": "I was reading a fitness article when it linked to this study: [Prediction of the oxygen cost of the deadlift exercise.](https://www.ncbi.nlm.nih.gov/pubmed/7932947)\n\nThey've made two separate models, one for each of the two independent variables. The two independent variables are highly correlated. I'll look at bar load since it's the easiest one to think of real world examples.\n\n&gt;The calculated regression equation was TVO2 (litres O2) = 2.88 + 0.005 bar load (kg) with a S.E.E. of 1.5 litres O2.\n\n&gt;An R of 0.909 for bar load and TVO2 \n\nWith an R of 0.909 we have an R^2 of 0.826.\n\nI compare that with their regression equation for a bar load of 200kg, a typical value. TV02 = 3.88, S.E = 1.5\n\nThe part that is confusing me is how do we have such a large r^2 value when the standard error is so large? It seems the data would not be very close to the fitted line.\n\nA second question: The study missed an easy blocking factor (male v female), and it appears the samples aren't independent (42 observations but only 24 participants). Why would they ignore easy factors plus violate one of the assumptions of SLR?",
        "created_utc": 1527591337,
        "upvote_ratio": ""
    },
    {
        "title": "Hey guys! I want to know if I approached this correctly. Sorry for the redaction, English is not my language so I had to translate the problem",
        "author": "fisicleta",
        "url": "https://i.redd.it/ayy9y5rq5r011.jpg",
        "text": "",
        "created_utc": 1527581228,
        "upvote_ratio": ""
    },
    {
        "title": "Interpretation of standard deviation",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mwgke/interpretation_of_standard_deviation/",
        "text": "[deleted]",
        "created_utc": 1527569958,
        "upvote_ratio": ""
    },
    {
        "title": "Urgent! Need help with choosing stats test for project",
        "author": "ChiaoMein",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mw3zl/urgent_need_help_with_choosing_stats_test_for/",
        "text": "So i'm doing a project on animal behavior and im researching the effect gender has on dog play fighting. i have three groups \\(M/M, M/F, and F/F\\) and i want to test weather the frequency of which a dog expresses dominant behavior is related to its gender. I'm mainly using the M/M and F/F groups as a control to see if females in the M/F group are less likely to play the dominant role. In other words, because females of various species are considered the submissive, female dogs will show dominant behavior less often than male dogs. I totally forgot which tests are best for what so some help would be greatly appreciated.\n\nEdit: wording",
        "created_utc": 1527566214,
        "upvote_ratio": ""
    },
    {
        "title": "Recommenced section: self study (good notes). Academic Help for students.",
        "author": "studywalk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8msvew/recommenced_section_self_study_good_notes/",
        "text": "[Studywalk](https://www.studywalk.com)\n\n[http://studywalk.com/index.php?route=product/category&amp;path=59](http://studywalk.com/index.php?route=product/category&amp;path=59)\n\nhttps://i.redd.it/803o0vc0jn011.png",
        "created_utc": 1527537326,
        "upvote_ratio": ""
    },
    {
        "title": "What are the chances?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mshz3/what_are_the_chances/",
        "text": "[deleted]",
        "created_utc": 1527534254,
        "upvote_ratio": ""
    },
    {
        "title": "Why would it be better to report confidence intervals than p-values?",
        "author": "axlrosen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mrgwb/why_would_it_be_better_to_report_confidence/",
        "text": "I have heard that it would be better if everyone reported confidence intervals than p-values. If this is true, I don‚Äôt quite understand why. In layman‚Äôs terms, why might confidence intervals be better?",
        "created_utc": 1527526025,
        "upvote_ratio": ""
    },
    {
        "title": "How to analyze size-frequency distributions over a large spatial scale (~70 sites)",
        "author": "Takeurvitamins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mqrj2/how_to_analyze_sizefrequency_distributions_over_a/",
        "text": "Hi all,\n\n[EDITED TO BE MORE SPECIFIC]\n\n[EDIT 2: basically trying to give statistical support for the [trend I'm seeing here](https://mokacytle.tumblr.com/image/174346138667) ie blue species decreasing down the river, and red species increasing, along with shifts in sizes]\n\nWhen I started my PhD, I had virtually no prep-time. My advisor needed me in the field and told me to go grab samples. As such, my planning wasn't the best, so sorry if this is a completely terrible question:\n\nI have individuals of two species from 70 sites along a river. I measured individuals sizes and total site weight and density. So in effect, I have size-frequency distributions, as well as weight/site and density/site. I also have a measure of juveniles v. adults by classifying anything under 10mm as juvenile.\n\nBasically, I want to know how the size frequency changes over the course of the river. \n\nI'd like to know if higher density of one species correlates to decreased density of the other; \n\nDoes the number of adults correlate with the number of juveniles? \n\nDoes density correlate to size? \n\nWhich species dominant at different sites? \n\nIs that dominance signal driven by a lot of adults or a recent settlement of small individuals?\n\nSo does anyone have any advice for the analysis?",
        "created_utc": 1527520345,
        "upvote_ratio": ""
    },
    {
        "title": "Representative sample size vs actual size",
        "author": "Wabbadabbo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mqe0t/representative_sample_size_vs_actual_size/",
        "text": "Maybe a stupid questions, but how is a sample size of 1000 considered representative of the population of the US",
        "created_utc": 1527517207,
        "upvote_ratio": ""
    },
    {
        "title": "For research what is a method for minimum and maximum sample size and what are the considerations?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mpb0w/for_research_what_is_a_method_for_minimum_and/",
        "text": "[deleted]",
        "created_utc": 1527505985,
        "upvote_ratio": ""
    },
    {
        "title": "Geometric Mean vs. Pareto Scaling",
        "author": "DrSucculentOrchid",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ml81b/geometric_mean_vs_pareto_scaling/",
        "text": "So I'm running large data sets through different software and they each have a different way to normalize the data.... My end goal is to run either a PCA or PLS analysis on the data. \n\n\nThe first software promotes using what I think they are describing as the geometric mean. \"All compound ion abundances yi will be multiplied by the factor Œ±k to give a normalized abundance y‚Ä≤i. There are several ways to calculate the scalar factor Œ±k. we use a median and mean absolute deviation approach based on all the detected abundances to calculate the scalar factor. The scaling factor is the anti-log of the mean of the log(ratios).\" \n\n\nThe second software has an option to first transform your data using generalized logarithm transformation and then scale it using Pareto scaling (mean-centered and divided by the square root of standard deviation of each variable).\n\n\nSo my question is, which is a better method and why? If the geometric mean is better, do I need to perform any additional scaling or transformation since it is incorporated into this method?",
        "created_utc": 1527459506,
        "upvote_ratio": ""
    },
    {
        "title": "t test - one tailed or two tailed?",
        "author": "mikeemice",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mk3v0/t_test_one_tailed_or_two_tailed/",
        "text": "given this information \nhttps://imgur.com/a/VdVoyv7\n\nhow am i supposed to know if i should use one tailed or two tailed paired t test?",
        "created_utc": 1527449467,
        "upvote_ratio": ""
    },
    {
        "title": "Skew distribution - choosing which way it's skewed",
        "author": "freedamanan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mjjtf/skew_distribution_choosing_which_way_its_skewed/",
        "text": "For this question https://i.imgur.com/EwCQiZE.png \n\nI'm not sure how to reason about this. \n\nI think that it's going to be skewed. Most students will have taken fewer maths courses, so I think that there will be more students less than average than over average. \n\nMeaning that the answer is (i), and that it's positively skewed. \n\nIs this reasoning alright? \n\nThank you",
        "created_utc": 1527444571,
        "upvote_ratio": ""
    },
    {
        "title": "How is the covariance/precision matrix and normal distribution connected?",
        "author": "mattematik",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mj8xe/how_is_the_covarianceprecision_matrix_and_normal/",
        "text": "I often hear the covariance matrix when talking about normal distribution. But I can take the sample covariance matrix of any distribution. But since the covariance matrix is a parameter in the multivariate normal distribution I guess there is some special connection, but what is it?\n\nI have seen that to estimate a covariance matrix a common way is to use the log likelihood of mulitvariate normal data. Is this only applicable to normally distributed data?\n\nI have also heard that the precision matrix has a connection with a graph showing the partial correlations. Is this also just for the normal distribution?",
        "created_utc": 1527441982,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical analysis of difference between two groups with multiple-choice pre- and post-tests.",
        "author": "AminamHD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mhoyi/statistical_analysis_of_difference_between_two/",
        "text": "Hello, for my thesis I'm trying to find if there is a difference in learning between two different teaching methods. Subjects are randomly assigned to 1 of 2 groups of which one group uses one teaching method and the other group uses another teaching method. \nThere are 3 phases: \n\n1. In the pre-test the subjects first have to answer a multiple-choice test (12 questions with 4 possible answers of which only 1 is correct), to test their initial knowledge.\n2. Then they are taught about the subject using the assigned method.\n3. In the post-test they make a multiple-choice test again, to see how much they improved.  \n\nI am trying to find if one teaching method results in a better improvement of the test scores compared to the other teaching method. With what statistical test can I best analyze the data? Does a t-test that compares the pre- and post-test difference between the two group make any sense here? ",
        "created_utc": 1527427211,
        "upvote_ratio": ""
    },
    {
        "title": "The Significance of Poisson Distribution in Statistics | Hashtag Statistics",
        "author": "LearningFromData",
        "url": "http://www.hashtagstatistics.com/2018/05/the-significance-of-poisson.html",
        "text": "",
        "created_utc": 1527425939,
        "upvote_ratio": ""
    },
    {
        "title": "How to make a prediction about baseline categorical variable ‚Äì Logistic Regression - SPSS",
        "author": "Skygor_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mhknn/how_to_make_a_prediction_about_baseline/",
        "text": "For example:\n\nLet‚Äòs say I have a categorical variable Hand with two values 0=‚ÄùLeft‚Äù and 1=‚ÄùRight‚Äù. DV is ReactionTestResult -  0=‚ÄùFail‚Äù, 1=‚ÄùPass‚Äù. \nOne of the categories from variable Hand will be chosen as baseline and won‚Äôt be included in the model, thus I will end up with only one B coefficient after regression is complete. Let‚Äôs assume that Left was chosen as baseline.\nThen I end up with B coefficient for Right hand and B constant.\nThe model for predicting would be \nlog(ùëùÃÇ/( 1 ‚àí ùëùÃÇ )) = Bconstant+HandBcoefficient*Hand.\n\nHowever, because left hand is baseline and I only have coefficient for right hand, how can I then proceed to make predictions about how likely left handed participants would be to pass the supposed reaction test? Do I somehow need to derive coefficient for baseline, perhaps using Exp(B) values or something? Or is there nothing I can do and I simply can‚Äôt make predictions about left handed participants?\n\nAny help is appreciated. Started learning about logistic regression fairly recently and this is confusing.\n",
        "created_utc": 1527425800,
        "upvote_ratio": ""
    },
    {
        "title": "Mixed Effects Linear Model for repeated measures (Statsmodels)",
        "author": "HuskyKeith",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mduh0/mixed_effects_linear_model_for_repeated_measures/",
        "text": "Currently performing an analysis on repeated measures data, but want to confirm that I'm setting up my tests correctly. The data corresponds to a within-subjects experiment with the following design:\n\n* 6 Trials spread across 2 blocks\n* 3 speeds (80/100/120% of a subject-specific frequency), ordered randomly within each block\n\nAll of my dependent variables are continuous. I have the following independent variables:\n\n* Age (continuous/ordinal)\n* Gender (binary)\n* Musician (binary)\n* Athlete (binary)\n* Preferred Period (continuous) -- the subject-specific frequency measured before the first block\n\nI've specified my model as \n    \"DV ~ age + C(gender) + trial + C(block) + C(speed) + preferred_period+ C(musician) +C(athlete)\".\n\nI get different levels of significance depending on how I specify my random effects (i.e. \"~ trial + C(block)\" vs. \"~ trial\"). \n\nCurrently using the MixedLM package in python's Statsmodels and unfortunately can't find much documentation that aligns with my experiment.\n",
        "created_utc": 1527375983,
        "upvote_ratio": ""
    },
    {
        "title": "Old brain + stats = SOS. [First Year University Stats Class] Needing some help! Q: P(|X-8|&lt;3) mean=9 sd=3",
        "author": "jfg902",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mdn74/old_brain_stats_sos_first_year_university_stats/",
        "text": "So, I'll start with saying that I haven't been in a math class in at least 5 years. So one can imagine how out of world\\-ly a statistics class is. Feeling pretty old right about now ! Anyway, I digress \\- if you can help me with this question you will be my hero for at least the next 24 hours. \n\nDirections: \n\nP\\(|X\\-8|\\&lt;3\\) with a mean=9 sd=3 \n\nfind P using empirical rule instead of table \\(so it should be easier than z\\-scoring everything \\- but I'm still lost on figuring out where to start.\\)\n\nSomeone once told me to solve I add 8 to X and just cancel it out? Is that a thing? What steps do I go through to solve this? ",
        "created_utc": 1527374059,
        "upvote_ratio": ""
    },
    {
        "title": "Interpretation of the log-likelihood in logistic regression: Are higher values or lower values better?",
        "author": "firebolt22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mc5pf/interpretation_of_the_loglikelihood_in_logistic/",
        "text": "Hi there,\nI frequently read on the internet that the higher the log-likelihood the better. (This seems intuitively true because a model with a higher likelihood should fit the data better than a model with a lower one. And since this log function (with the base e) is a monotonically increasing function this should also be true for the log-likelihood). \nI also read that the log-likelihood often has negative values, therefore I assume that a log-likelihood of -150 is better than a log-likelihood of, say, -200.\n\nHowever, one textbook about statistics that I have here says:\n\"The log-likelihood statistic is analogous to the residual sum of squares in multiple regression in the sense that it is an indicator of how much unexplained information there is after the model has been fitted. It, therefore, follows that large values of the log-likelihood statistic indicate poorly fitting statistical models, because the larger the value of the log-likelihood, the more unexplained observations there are.\"\n\nSo, which is actually true? Is this an error in this textbook?",
        "created_utc": 1527360230,
        "upvote_ratio": ""
    },
    {
        "title": "Beginner question: Can I determine \"the most significant reason\"? (I'm using Bonferroni correction and Chi-square test)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mbgw7/beginner_question_can_i_determine_the_most/",
        "text": "[deleted]",
        "created_utc": 1527354124,
        "upvote_ratio": ""
    },
    {
        "title": "all statistics resource recommendation",
        "author": "Brimirvaar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8mbc6y/all_statistics_resource_recommendation/",
        "text": "What is your recommendation on resources (ie. books, pdf, not videos) about the following subjects treated together:\n\n - [level of measurements](https://www.wikiwand.com/en/Level_of_measurement)\n - statistics (a function of data)\n - tests\n\nI already know examples of them, I would like to have an exhaustive-ish resource (ie. show all the statistics there are as of today), a reference. The more mathematical, terse and short it is, the better.\n\nMy background is major in maths and I have extended knowledge about probability, applied maths and I‚Äôm a machine learning scientist.\n\nThank you very much\n\nedit: I see that I may have been misleading. I know the theory already. What I'd like is sort of a comprehensive list of the statistics and tests that exist as of today. However, deepening my theory knowledge with additional theory on top of the basic theory (eg. @efrique with robustisity) would be interesting too, but is probably for another question.",
        "created_utc": 1527352938,
        "upvote_ratio": ""
    },
    {
        "title": "Masters dissertation help",
        "author": "bwin2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ma8g3/masters_dissertation_help/",
        "text": "Hey guys!  Doing my masters dissertation(it's a proposal) I'm really clueless to stats and can't seem to find a legit tutor to help me online.  My supervisor wasn't really helpful.  \n\nMy proposed study has 3 groups: \na= exercise intervention 1 \nb= exercise intervention 2\nc = combination of exercise intervention 1+2\n\nI'm using 2 primary outcome measures and 2 secondary outcome measures, and will collect data pre, post, and 2 followups.  \n\nWe don't actually have to conduct the study, but need to know which tests to use.  I looked at a few places and came up with a Krusukal-Wallis anova?  \n\nany help would be appreciated! thanks!\n",
        "created_utc": 1527342285,
        "upvote_ratio": ""
    },
    {
        "title": "How can I pass bootstrap standardised residuals and pass it into a forward simulation using an estimated ARMA-GARCH model?",
        "author": "Pimp_Fada",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m9w7i/how_can_i_pass_bootstrap_standardised_residuals/",
        "text": "I'm following this generic steps for filtered historical simulations (FHS). \n\n1. Fitting the best (ARMA-GARCH) model \n2. Standardization of residuals \n3. Bootstrapping of standardised residuals \n4. Passing of bootstrapped residuals in a forward simulation using the estimated ARMA-GARCH model \n5. Estimation of returns\n\n\nI'm currently stuck trying to use the standardized residuals of my estimated GARCH(1,1) model using the \"rugarch\" package in R. \n\nI have successfully computed the standardized residuals which are now i.i.d. \n\nHow can I bootstrap these residuals and forward them via simulation so that I can calculate the hypothetical daily returns? I'm looking at something like 1000 simulations over 5 trading days. \n\nAny ideas?\n\nHere's the code with the sp500 returns data from the same package: https://dpaste.de/DAF1\n\nThanks in advance. \n\nUPDATE: I found this code that seems to do the same thing but it's too messy for me to understand fully: https://github.com/8xiom/myBH-R-Financial/blob/master/FilteredHS%20V4c.R",
        "created_utc": 1527338359,
        "upvote_ratio": ""
    },
    {
        "title": "Is this the correct way to compare 3 sample proportions using Chi-square test?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m9dd0/is_this_the_correct_way_to_compare_3_sample/",
        "text": "[deleted]",
        "created_utc": 1527331361,
        "upvote_ratio": ""
    },
    {
        "title": "Distributing elements over multiple buckets of a varying size",
        "author": "stanniemanni",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m8rjd/distributing_elements_over_multiple_buckets_of_a/",
        "text": "If one has b buckets that each have a size of b_i and one has to place n elements in the buckets. How can I mathematically describe the number of different permutations the elements can be placed in the buckets when the order of the elements in the buckets matters?\n\nP.S. One has to add the elements to the buckets in a chronological order. This means that in a bucket one can have element 1 and then element 2 but not vice versa.\n\nExamples for clarification:\n\nIf I have 3 buckets of size: b_1=2, b_2=1 and b_3=1.\n\n- Placing 1 element = 3 permutations\n- Placing 2 elements = 7 permutations\n- Placing 3 elements = 12 permutations\n- Placing 4 elements = 12 permutations",
        "created_utc": 1527322406,
        "upvote_ratio": ""
    },
    {
        "title": "DoE on an existing set of data?",
        "author": "Loneliest-Intern",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m4uds/doe_on_an_existing_set_of_data/",
        "text": "How do I run an experiment on a somewhat incomplete set of data? I've got a bunch of data that I'm hunting for trends in and my experiments class didn't prepare me for it (I could design all sorts of experiments on custom compositions and send stuff out for tensile testing all day, but I think my boss might not have communicated the goal very clearly so I've been off in the weeds all week).",
        "created_utc": 1527279880,
        "upvote_ratio": ""
    },
    {
        "title": "exploring the association between criminal defendant filing charges",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m4h5q/exploring_the_association_between_criminal/",
        "text": "[deleted]",
        "created_utc": 1527276786,
        "upvote_ratio": ""
    },
    {
        "title": "AP Stats final project - help appreciated",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m39ms/ap_stats_final_project_help_appreciated/",
        "text": "So our final project in Stats is to make our own experimental design or observational study.\n\nMy observational study is to see if the proportion of asians in a bay area high school affects the proportion of students who go to a 4 yr college right after graduating. \\(Does an increase in proportion of asians in a Bay Area high school increase the proportion of students who go to a 4 yr high school right after graduation? This is the question I will be answering.\\)\n\nI will be making a chi square test of homogeneity as my inference test.\n\nHere is a pic of the chart I will be filling out for the chi square homogeneity test: [https://media.discordapp.net/attachments/182221637108760577/448913248734150666/image.jpg?width=400&amp;height=300](https://media.discordapp.net/attachments/182221637108760577/448913248734150666/image.jpg?width=400&amp;height=300)\n\nHowever, as per the project requirements, I must also conduct confidence interval.\n\nI feel a 2 sample t interval would be good, but I'm not exactly sure how to execute it. Which samples should I be using for conducting the interval? What would the interval portray?",
        "created_utc": 1527267118,
        "upvote_ratio": ""
    },
    {
        "title": "What to do when cases in a random sample aren't available?",
        "author": "BioStatStudent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m318y/what_to_do_when_cases_in_a_random_sample_arent/",
        "text": "Hi AskStatistics,\n\nI have a large file of case attributes (N=4437) that includes all of the cases in a given year.  Each of these cases should correspond to an audio file that I intend to take a small random sample of (n=61) and then abstract into some behavioral scales, merge back onto the case attributes, and model something like \"case outcome = case attributes + audio abstraction + e\".  \n\nBUT in the random sample of 61 cases, 28 audio cases weren't present.  I'm looking at the case attributes for this missing audio to see if there's a pattern, but it doesn't appear so (at least not with the outcome):\n\nOutcome 0/1 on the top, Missing audio 0/1 on the side\n\n         0  1\n      0 23  5\n      1 25  8\n\n\nSo I wanted to ask for your guidance concerning the sampling integrity:\n\nIf the audio files are missing at random (tbd, looking at other variables now), can I just run another random sample to fill in the 28 cases that were missing?  Or is there something additional that I should do to account for the missingness in the original random selection? \n\nI used sample() in R to get the original sample list and set a seed to reproduce it so I can manipulate it easily enough and document the steps and excluded cases.  \n   \nThanks in advance for any advice, suggestions, or resources.  I'm realizing that this is kind of a blind spot for me that I'd like to improve.\n\n\n**EDIT:** \nAs I'm thinking through this more, could I treat the first sample of n=61 as a sample to discover the non-response rate (for lack of a better word-- what would you call this?) and then follow-up with a random sample of 110 (NR = .459) or so to get back to the target of 61 audios?  \n\n\n**EDIT 2**\nI over-sampled based on the initial missing rate, looked at 113 cases and found 60 audios.\n\nNo SES variables were significant on missingness in the first sample pull; BUT it turned out after creating some timing variables to cross on missing audio cases, most occurred at a point in patient care where the audio would not have been collected (phone call would not have been made).  I subset out this portion of the data (the focus is on the phone calls) and still have a similar missing rate of .42.  Not sure what this means.  The time variable describes the missing audio, but subsetting doesn't improve the rate?\n\n\n",
        "created_utc": 1527265327,
        "upvote_ratio": ""
    },
    {
        "title": "How to strengthen my credentials for PhD programs?",
        "author": "hinomarucurrydisc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m2jfy/how_to_strengthen_my_credentials_for_phd_programs/",
        "text": "I'm a clinical trials data manager currently working at a school of public health. I applied to a bunch of Biostatistics PhD programs last cycle, but didn't get into a single one. One of the programs sent me some feedback saying \"Academic work is not as competitive as that of other applicants.\"\n\nOne of the schools I applied to admitted me to their full-time MS program, but I couldn't afford to quit my job.\n\nWhat should my next course of action be? A part-time master's degree? An online graduate certificate? I also have the option of taking some courses from the school I work at, although it won't count towards a degree.\n\nI have an MS and BS in chemical engineering. I would like to not spend time and money getting another MS (since I'm also not young anymore), but would also like to maximize the chances of me getting into a program next time I apply. My interests are in adaptive trials, and the use of EMRs and data management tools in clinical trials.",
        "created_utc": 1527261456,
        "upvote_ratio": ""
    },
    {
        "title": "Not homework, but for fun, I forgot which locker was mine, picked a locker at the gym that I thought was mine, this locker had the same combination, but it wasn't my locker. What are the odds?",
        "author": "l33tb3rt",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m25zd/not_homework_but_for_fun_i_forgot_which_locker/",
        "text": "Hi,\n\nThis isn't a homework question or anything, just a fun happening that made me wonder the odds.  At the gym I go to, there are 160 lockers that have a locking device that uses a 4 digit combination.  These digits range from 0-9.  Because all the lockers look the same, I went down the wrong row, and tried what I thought was my locker.  It wasn't.  However, the combination I used opened the locker.  What made this even more interesting is that this was the first locker I tried.\n\nAnyhoo, I just found this really interesting, and wanted to know what the odds would be.  I think that the odds of getting the combination correct are 1 in 10,000 (10^4)?  Any help is appreciated.",
        "created_utc": 1527258476,
        "upvote_ratio": ""
    },
    {
        "title": "How do i interpret my transformed coefficients?",
        "author": "kayanh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m1vzc/how_do_i_interpret_my_transformed_coefficients/",
        "text": "I have preformed a multiple regression where both the dependant and the independant variables are transformed with log10(x+1), and i do now have a significant coefficient of -0.1. How do i convert this coefficient back ‚Äùto normal‚Äù?",
        "created_utc": 1527256171,
        "upvote_ratio": ""
    },
    {
        "title": "Is there average overlap percentage between two related groups?",
        "author": "Siguardius",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m1qmx/is_there_average_overlap_percentage_between_two/",
        "text": "I was wondering if anybody encountered a statistical analysis of overlapping groups of people or communities. I'm interested in percentage that usually comes up when comparing two groups of people. For example: if 10 millions of people love Star Wars and 10 millions of people love Star Trek, what's the average overlap (people who love both)?\n\nI'll understand if the is not researched (because it's nonsense to me in the first place) or there is no significance or co-relation there.",
        "created_utc": 1527254860,
        "upvote_ratio": ""
    },
    {
        "title": "How argument which statistical test to use on identical samples.",
        "author": "InSpeScientist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m1emd/how_argument_which_statistical_test_to_use_on/",
        "text": "Hi All,\n\nGoing through this forum i saw a lot of clear explanations in other posts and therefore would like to ask if someone could help me with the reasoning on how to perform the correct statistics on my sample.\n\nI am struggling with which statistical test to use on the data I collected during my  research project.  I have the following Experimental design:\n\nThe experiment was repeated 3 times, each time with cells (same cell type) from a different donor .\nAt the start of the experiment, cells would be split into 2 identical cultures, where one culture would get treatment A (which simulates a physiological condition) and the other would get treatment B. Everything else is the same. \nAfter analysis, the protein expression of cells receiving treatment A  would be set to 1 and the protein expression of the cells that received treatment B would be the relative fold change compared to cells that obtained treatment A after normalizing to a protein whose expression would not be altered by the treatment. \n\nSay I would get the following hypothetical values:\n\nTreatment A: 1, 1, 1\nTreatment B: 2.1, 1.9, 2.6\n\nNow my question is which statistical test to use:\n\nI was thinking of a paired T test since the cells of each individual experiment are the same at the start of the experiment. However it is not a before ‚Äì after measurement. And the cells that underwent treatment A are set at 1\nThat would lead to the unpaired T-Test however I would think that the samples are not two independent, random samples.  \n\nIs there anyone that could clarify this for me and how to reason properly which test to use in this case?\n\nThanks in advance\n",
        "created_utc": 1527251827,
        "upvote_ratio": ""
    },
    {
        "title": "Picking the winner of 100 boxing matches, statistical significance.",
        "author": "antusheng",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8m0pwl/picking_the_winner_of_100_boxing_matches/",
        "text": "Someone has picked who they think will win before 100 boxing matches and got it right 60% of the time.  Is there any rule of thumb to judge whether a 60% success rate across 100 attempts is statistically significant?  You could easily get 6 out of 10 right if you just relied on a coin toss, but 60 out of 100?\n\nI followed the UCLA link in the sidebar, this seems to be a [binomial test](https://stats.idre.ucla.edu/sas/whatstat/what-statistical-analysis-should-i-usestatistical-analyses-using-sas/#bitest), but I couldn't follow how the conclusion ('The results indicate that there is no statistically significant difference (p = .2292)') was arrived at, or why p= .2292 is defined as not statistically significant.",
        "created_utc": 1527244275,
        "upvote_ratio": ""
    },
    {
        "title": "Help needed with OLS Regressions!",
        "author": "xEmperorNero",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lvfc2/help_needed_with_ols_regressions/",
        "text": "Hello r/AskStatistics,\n\nI am working on my Bachelor Thesis and I stumbled upon a problem that I don't know how to solve. To give some insights, the goal of my thesis is to test whether Google Trends is a reliable predictor of presidential elections, specifically in the 2016 presidential election. I was told to compare the data provided by Google Trends with the actual outcome of that election and, by using an OLS regression, see if it is reliable or not. But the problem is that I have no idea how to create such a regression. Could you guys possibly have some tips for me?\nI'd greatly appreciate it!",
        "created_utc": 1527190190,
        "upvote_ratio": ""
    },
    {
        "title": "How to handle data in the form of range?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lufd4/how_to_handle_data_in_the_form_of_range/",
        "text": "One of the feature in the data set is in the form of range, for example 45-50 or 50-55. It is of string type obviously, but how to make use of this kind of data for analysis?",
        "created_utc": 1527182591,
        "upvote_ratio": ""
    },
    {
        "title": "Finding the probability of a two(or more samples) being within some value of each other?",
        "author": "petemate",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lufcc/finding_the_probability_of_a_twoor_more_samples/",
        "text": "X = N(mu, sigma^2). I know how to find the probability of e.g. P(X&lt;n) and so on. But I don't know how to find out what the probability is that two random variables are within e.g. n of each other. P(X1-X2&lt;n), I guess.. And what if I want to check for three(or more) samples being within e.g. n of each other? I would imagine something along the lines of P(X1-X2&lt;n)*P(X2-X3&lt;n)*P(X3-X1&lt;n) or something, but I am not sure..\n\nIm sure this is a fairly easy problem, but I have a hard time finding the right words to look up, so I would be grateful if someone could point me in the right direction or give me a link to a tutorial. Thank you very much.",
        "created_utc": 1527182587,
        "upvote_ratio": ""
    },
    {
        "title": "How to make sense of the lognormal distribution",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ltv8q/how_to_make_sense_of_the_lognormal_distribution/",
        "text": "The normal distribution is defined by mu and its variance. I can reasonably make sense of the results if mu or the variance change. I could draw a normal distribution if I where given some random values for mu and the variance. How do I get this intuition for other distributions? For example. I simulated different lognormal distributions  for both parameters and I have to say that I cannot make sense of the behavior of the distribution if one parameter changes. is there some way to pump my intuition about the effect of each parameter in different distributions?\n\n    x11()\n    # install.packages(\"tidyverse\")\n    #library(tidyverse)\n    n=1000\n    rlnorm(n,0,1)%&gt;%density()%&gt;%plot()\n    rlnorm(n,1,1)%&gt;%density()%&gt;%lines(col=\"red\")\n    rlnorm(n,2,1)%&gt;%density()%&gt;%lines(col=\"red\")\n    rlnorm(n,3,1)%&gt;%density()%&gt;%lines(col=\"red\")\n    rlnorm(n,0,1)%&gt;%density()%&gt;%plot()\n    rlnorm(n,0,1.5)%&gt;%density()%&gt;%lines(col=\"blue\")\n    rlnorm(n,0,2)%&gt;%density()%&gt;%lines(col=\"red\")\n    rlnorm(n,0,2.1)%&gt;%density()%&gt;%lines(col=\"green\")\n    rlnorm(n,0,1.5)%&gt;%density()%&gt;%lines(col=\"yellow\")",
        "created_utc": 1527178369,
        "upvote_ratio": ""
    },
    {
        "title": "Help understanding statistical analysis",
        "author": "Noob_eternally",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lttcv/help_understanding_statistical_analysis/",
        "text": "I am sorry, if this is not the right place to post this. I am currently working on my bachelor's thesis about type 1 diabetes and islet transplantation. I have read the results of a clinical trial, but I do not understand the statistical analysis. English is my second language and statistics was never my strongest subject in math\n\nIf anyone could help me make any sense of the following, I would much appreciate it\n\nhttps://i.redd.it/3krfjkgfttz01.png",
        "created_utc": 1527177977,
        "upvote_ratio": ""
    },
    {
        "title": "Have you ever seen nonmonotonic heteroscedasticity",
        "author": "hwerste",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lt372/have_you_ever_seen_nonmonotonic_heteroscedasticity/",
        "text": "Usually variance of random component increases as variable increase (for example variance of spending of wealthy person is much larger than variance of poor). Have you ever come across situation when variance is low at beginning, peaks in middle and then fades out? ",
        "created_utc": 1527172303,
        "upvote_ratio": ""
    },
    {
        "title": "Problem finding null hypothesis in multinomial model",
        "author": "wout189",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lslkz/problem_finding_null_hypothesis_in_multinomial/",
        "text": "For a project we're researching scanner data to gain insight in purchase decisions of Eggs \\(E\\) and Bacon \\(B\\), using a multinomial logit model with the following dependent variables:\n\nY\\*\\_1 =  E  ‚ãÇ  B  \nY\\*\\_2 = !E  ‚ãÇ  B\n\nY\\*\\_3 = E  ‚ãÇ  !B\n\nY\\*\\_4 = !E  ‚ãÇ  !B\n\nAnd we want to answer the following research question: Does the purchase decision \\(yes/no\\) in one category influence the purchase decision in the other category?\n\nIdeally we want  a null hypothesis with a log\\-odds ratio derived from: P\\(E|B\\) = P\\(E\\) \\-\\-\\&gt; P\\(E|B\\) = P\\(E|!B\\)  \nHowever, I have difficulties getting there, as we do not have expressions for P\\(E\\) and P\\(B\\), making rewriting using bayes difficult.\n\nIf someone has any idea's as to how to formulate a test for this research question or has a hint/solution for my problem, I'd be happy to hear.\n\nExcuse me if I broke any rules, this is one of my first posts on reddit.",
        "created_utc": 1527168223,
        "upvote_ratio": ""
    },
    {
        "title": "Machine learning help",
        "author": "goingtobegreat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ls4tt/machine_learning_help/",
        "text": "I'm looking for a reference or point in the right direction since I'm not too familiar with machine learning or algorithms. I'd prefer to work in R, but I could also do Python. Any reference to a specific package I could use would be great.\n\nSo I have a list of Senate bills and their descriptions that are one or two sentences long. I have a subset of bills that also have accompanied bill types (ie \"budget\", \"immigration\", etc). I want to create an algorithm that will assign a bill type(s) to the other subset of bills that don't have bill types. \n\nI would want to be able to do this by using the underlying relationship between descriptions and bill types for the first subset and apply it to the second so that I can predict bill type from bill description. Does this make sense? Any advice would be welcome!",
        "created_utc": 1527163824,
        "upvote_ratio": ""
    },
    {
        "title": "What is the most interesting statistic?",
        "author": "IsshunGi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lr8qo/what_is_the_most_interesting_statistic/",
        "text": "",
        "created_utc": 1527153163,
        "upvote_ratio": ""
    },
    {
        "title": "EWMA converging problem",
        "author": "Stormregion0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lr3uu/ewma_converging_problem/",
        "text": "Hello, I hope this is the correct subreddit.\nMy question is about the exponentially weighted moving average.\n\nS_t=alpha*S_{t-1}+(1-alpha)*dp\n\nI define Delta = |S_init-dp|\nwhere **S_init** (or S_0) is the intialization value of the EWMA\nand **dp** is a new Datapoint\n\nIn my problem dp is constant. So S_t should converge for any alpha to dp.\n\n\nAlso:\n\n    Delta&lt;=epsilon\n\nI try to find an relation ship which give me the number of how many steps t it takes to be within this boundary. But at the moment I fail. Maybe you can help me or at least give me a tip. From experimental calculating I know it is depend on epsilon and the delta.\n\nThank you\n\nStorm\n",
        "created_utc": 1527151203,
        "upvote_ratio": ""
    },
    {
        "title": "How do eigenvalues and eigenvectors of a covariance matrix change when you exchange two entries (e.g. elephant and mouse) in the underlying data set?",
        "author": "Danjox",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lqwyw/how_do_eigenvalues_and_eigenvectors_of_a/",
        "text": "Hi everyone! I am currently familiarizing myself with PCA and I am wondering what effects it will have on the eigenvalues and eigenvectors, if you change somethin in the underlying data set. Does it have any effect?\n\nThanks for any help",
        "created_utc": 1527148381,
        "upvote_ratio": ""
    },
    {
        "title": "Is it reasonable to combine data sets in my case? I don't know much about statistics.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lpcbp/is_it_reasonable_to_combine_data_sets_in_my_case/",
        "text": "[deleted]",
        "created_utc": 1527130275,
        "upvote_ratio": ""
    },
    {
        "title": "Can you use the difference between the median and the mean to estimate a standard deviation?",
        "author": "wordboyhere",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8looki/can_you_use_the_difference_between_the_median_and/",
        "text": "We got a test back and were told the mean is 80 and the median is 88. I was wondering if there was anyway to estimate the standard deviation? ",
        "created_utc": 1527124494,
        "upvote_ratio": ""
    },
    {
        "title": "Best way to manually stat test percentages from survey data",
        "author": "cam417",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lolff/best_way_to_manually_stat_test_percentages_from/",
        "text": "Hi all,\n\nWhats your preferred way for stat testing multiple choice survey day by hand??\n\nIs chi\\-square the only way?Is there a way to do t\\-tests? Like if I want to test for significance between two percentages from a multiple choice question.\n\nI have heard multiple arguments about parametric vs. non\\-parametric testing on nominal data. Whats your preferred method? Especially if your survey software doesn't provide auto testing and you dont have SPSS.\n\nthanks for the help!",
        "created_utc": 1527123713,
        "upvote_ratio": ""
    },
    {
        "title": "Given two distributions, probability a number randomly selected from the first is larger than one from the second?",
        "author": "dsgasdgdsagasdhg",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lo7s8/given_two_distributions_probability_a_number/",
        "text": "I am brainfarting on this, but say you have two distributions with known pdf's, say X and Y, and take an element x of X and y of Y, what is Prob(x &lt; y)??\n\nThis seems like it should be easy but for some reason (the heat maybe) I am not getting it.\n\n(Obviously in the case where both are finite distributions it's easy to solve directly but I need to know when they are continuous)\n\nCheers",
        "created_utc": 1527120352,
        "upvote_ratio": ""
    },
    {
        "title": "Question about multiple linear regression",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ln5km/question_about_multiple_linear_regression/",
        "text": "[deleted]",
        "created_utc": 1527111157,
        "upvote_ratio": ""
    },
    {
        "title": "Need some advice to analyse some data",
        "author": "savemeimdying",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lm9ed/need_some_advice_to_analyse_some_data/",
        "text": "My data involves treating people with 3 levels of a drug \\(and a control\\) and then performing a test on them that gives a score out of 100. Each treatment group has approximately 25 people. Here is an image of the data:\n\nhttps://i.redd.it/17or4rxwonz01.png\n\nAt a glance there doesn't seem to be much affect of the drug on the test score here but when I do an analysis of variance using \"aov\" R I get a p value of 0.056 which suggests weak evidence for the affect. Would you consider that unusual given the distribution above? Is an analysis of variance the correct form of statistical test to run here? When I do a linear model using \"lm\" I get an R\\^2 of 0.06 which again seems better than the data would suggest? Am I doing something wrong?\n\nI also tried testing the normality using a qqplot of the residuals and this was my result:\n\nhttps://i.redd.it/uvt8h0elrnz01.png\n\nWhich seems a bit curvy to me so maybe that could explain the issue? But then I am not really sure what to do to fix this?\n\nAs a little side question that has been bugging me. Could you consider the test score to be a continuous variable despite the fact that there are 100 distinct possibilities?",
        "created_utc": 1527104372,
        "upvote_ratio": ""
    },
    {
        "title": "Question about data reduction in a likert scale using factor analysis (PCA)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ljusb/question_about_data_reduction_in_a_likert_scale/",
        "text": "[deleted]",
        "created_utc": 1527086201,
        "upvote_ratio": ""
    },
    {
        "title": "Instructors edition of a text (freedman - statistics)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lhqdr/instructors_edition_of_a_text_freedman_statistics/",
        "text": "[deleted]",
        "created_utc": 1527063828,
        "upvote_ratio": ""
    },
    {
        "title": "Boundary of nonlinear models - statistics on the performance of a model outside (or closer to the limit) of observed data",
        "author": "Hydroceros",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lhile/boundary_of_nonlinear_models_statistics_on_the/",
        "text": "Hello statisticians,\n\nI am not very strong in statistics, but I really need to perform some proper statistics now. I have made an empirical ecological model that is supposed to predict phytoplankton growth in a lake based on environmental conditions, such as nutrient concentrations, temperature and grazing by primary consumers.\n\nI would like to use the model to test the response of phytoplankton to new environmental conditions, such as higher nutrient concentrations, or higher temperatures. These conditions will be different from the conditions that we measured and used to make the model. Is there a way to quantify the models predictive power for these conditions?\n\nI was thinking about making density distributions for the observed (input) data. When a temperature or concentration did not occur very often, it may have had less of an effect on the calibration of the model, so that must also say something about the models predictive power for such conditions.\n\nI hope you can provide me with some tips. Just to put me in the right direction.\n\nThanks,\nHydroceros",
        "created_utc": 1527060880,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing these two linear models",
        "author": "mattematik",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lgp9a/comparing_these_two_linear_models/",
        "text": "This is from an exercise (its in a different language so I wont post the initial question)\n\nSo I was given three observations: (1, ‚àí0.18), (3, 1.32), (6, 3.55). \n\nThese are used to estimate two linear models, first one is y = -0.923 + 0.746x where s^2 = 0.0000421, and the second one we only use the mean as the intercept and get the following model y = 1.563 with s^2 = 3.523. Given three new observations , (2, 1.47), (4, 2.04), (5, 2.93), we are asked to find which of the two models gives the \"highest joint probability\" and here I get a bit lost from the solution from the teacher.\n\n\"According to the linear model the new Y observations are normally distributed with variance 0.0000421 and the expected value 0.569, 2.061 and 2.806.\"\n\nWhere these values come from isn't really explained. If I use the new observed x-values in my linear model I get these as the y values. But what do they have to do with mean value? And why is the mean value different for each observation? When we use the constant model we get the variance 3.523 and mean 1.563 for all three observations.\n\nMy next question is about the next step. Three normal pdfs (one for each observation) with the given mean and variance are multiplied with each other for each model. In the end we get two probabilities e^-9815 for the linear model and 0.0071 for the constant model.\n\nThe conclusion is that the first model is a really bad fit because the low probability. Is this some sort of likelihood? Also it doesn't say much about the second model but it also seem like a bad fit since 0.0071 is a low probability too?",
        "created_utc": 1527050992,
        "upvote_ratio": ""
    },
    {
        "title": "Know of any good YouTube channels which cover statistics in a causal format?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8leuyg/know_of_any_good_youtube_channels_which_cover/",
        "text": "[deleted]",
        "created_utc": 1527033541,
        "upvote_ratio": ""
    },
    {
        "title": "What are some key words in statistics word questions to know which equation to use?",
        "author": "MoonBoots69",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lem98/what_are_some_key_words_in_statistics_word/",
        "text": "For example, if the question says something like \"theory predicts\" or \"model\" or \"expected\", I immediately think chi square goodness of fit. Or if I see \"is the sample representative of the population\" I think one sample z test.\n\nWhat are some other key words to look for in a word question to know which one to run? I'm looking for basics, such as as chi square for independence or binomial probability function.\n\nI have a stats test coming up and knowing which test to run is by far the biggest obstacle!",
        "created_utc": 1527031347,
        "upvote_ratio": ""
    },
    {
        "title": "Paired vs. independent t-test for two populations",
        "author": "Todasa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lee0q/paired_vs_independent_ttest_for_two_populations/",
        "text": "Hello,\nI seem to be confusing myself on the difference between the independent two-sample t-test (R uses the Welch method) and the paired two-sample t-test.\n\nThis analogy represents my situation:\n\nI have ten shovels: 5 small ones and 5 big ones.  I distribute the shovels and ask all the people to approach the same big pile of dirt, and dump 100 shovelfuls onto his or her own scale.\n\nMy two groups are the 'big shovel' group and the 'small shovel' group.  My data is the total weight of dirt after the 100 scoops.  Intuitively, I know the group with the large shovel will have a larger mean.  \n\nWhich t-test approach do I use to get the appropriate statistic (and if you know why, why)?\n\nThank you for any help.\n\n\n",
        "created_utc": 1527029383,
        "upvote_ratio": ""
    },
    {
        "title": "A test to establish significance to the differences between correlations?",
        "author": "And12rew",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lcqbl/a_test_to_establish_significance_to_the/",
        "text": "\nI would like to determine if the correlations of the last four data-sets are **significantly different** than the initial three, PIPA, PIEC and QUST.  What test should I use?\n\nTree species | May | June | July | August | September | October | Analysis | n=\n---------|----------|---------|----------|----------|----------|----------|---------|---------\nPIPA |0.168 | 0.027| 0.209 | 0.270* |0.245* | 0.220*|Pearson Correlation|n=30\nPIEC | 0.174 | 0.071 | 0.197 | 0.273* | 0.261* | 0.260* | Pearson Correlation | n=30\nQUST| 0.369**|0.539**|0.512**|0.436**|0.275*|0.118|Pearson Correlation|n=30\nPIPA_PIEC|0.18|0.051|0.214|0.286**|0.267*|0.253*|Pearson Correlation|n=60\nPIPA_QUST|.277*|0.199| .362**| .396**|.320**|.246*|Pearson Correlation|n=60\nPIEC_QUST|.284**|.244*|.352**|.398**|.334**|.281*|Pearson Correlation|n=60\nCOMBO| .243*|0.147|.302**|.359**|.311**|.269*| Pearson Correlation| n=90\n ",
        "created_utc": 1527016627,
        "upvote_ratio": ""
    },
    {
        "title": "Am I going in the right direction? I'm Developing an equation for supplier quality ratings to plot on a ternary triangle.",
        "author": "rex9802",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lcpcr/am_i_going_in_the_right_direction_im_developing/",
        "text": "I am working on developing a rating system for buyers of wholesale products to determine which of their foreign suppliers should be used for various purchases. \n\nThe basics are this:\n\n1. There are essentially 3 parties involved to start my rating system. 2 of the parties are the buyers, the 3rd is the supplier. \n\n2. When they first begin, the buyers input their preferences for 3 questions (time, quality, cost) on a 10 point scale. 5 is average and is the expected amount. The buyer can bump a preference score above 5, but to do so, they have to reduce one of the other numbers (total can only be 15). \n\n3. When the buyer uses a specific supplier and the transaction has been completed, they rate the supplier on the same, 3 categories. \n\n4. Based on their previously given preferences, I want my system to use this supplier rating and compare it to their originally given preferences to determine a true rating of the supplier. \n\nI've tried using the deviation from \"5\" as a multiplier, I've also tried using the raw number. (eg. if I do not care about price and give is a \"1\" on the 10 point scale- it multiples the rating by 1 but if I do care, if would multiply it by 10.)\n\nI'm stuck on this,,, any ideas or suggestions?",
        "created_utc": 1527016421,
        "upvote_ratio": ""
    },
    {
        "title": "What is the use of MACROS data?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lc1sx/what_is_the_use_of_macros_data/",
        "text": "I picked a random project in kaggle to practice and I noticed that along with train and test data, they also provided MACROS data.  I'm sure it has a use  in a prediction model. So can someone be kind enough and help a lad out?",
        "created_utc": 1527011737,
        "upvote_ratio": ""
    },
    {
        "title": "Best way to analyze improvement of scores using a Likert scale data?",
        "author": "headupyoungperson",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lc10c/best_way_to_analyze_improvement_of_scores_using_a/",
        "text": "I'm working to develop an evaluation plan which uses Likert scale competency scores to rate supervisory satisfation &amp; effectiveness \\- this scale is administered to clinicians three times over the course of their placement.\n\nOverall, I want to be able to identify how many scores improve from the first to last interval \\- this will tell me whether the interventions are working to improve supervisory alliance.  It's a really small sample size \\(only 8 clinicians in the program\\).\n\nBasically it's akin to checking whether satisfaction scores increased for an individual respondent.  Any idea how I might go about measuring this?  ",
        "created_utc": 1527011586,
        "upvote_ratio": ""
    },
    {
        "title": "Need help with derivation of Standard Error",
        "author": "sky_dragoon",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lborg/need_help_with_derivation_of_standard_error/",
        "text": "The first line in the derivation explanation in  [SEM wiki page](https://en.wikipedia.org/wiki/Standard_error) is\n\n\"variance of the total T\" is n\\*s\\^2\n\nI don't understand that line \"variance of the total\". The variance of any set of data is Sum of Difference from that point and mean divided by n. \n\nI see the Sum here since it's summing up all the data points \\(kind of\\). But where is the subtraction of the mean?\n\nThank you. ",
        "created_utc": 1527009210,
        "upvote_ratio": ""
    },
    {
        "title": "Difference between CV and RSE?",
        "author": "sonotrainbowrhythms",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lbeeb/difference_between_cv_and_rse/",
        "text": "I want to identify survey data points that have a relative standard error of 30 percent or more (trying to evaluate my sample size for another round of the survey). I'm using SAS for data management and see that there is a \"CV\" command to output the coefficient of variation, which I've always understood to be another term for RSE . . . but the CV value the code spits out is different from the value I calculate when I divide the standard deviation of my weighted frequency over my total weighted frequency, which I was told is the way to calculate RSE. Which part(s) am I misunderstanding here?\n\nSorry, is a total ELI5 stats question . . . but can you help a newbie out?",
        "created_utc": 1527007147,
        "upvote_ratio": ""
    },
    {
        "title": "How do I find a \"meaningful\" maximum in a data-set with many outliers?",
        "author": "exhuma",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lazrd/how_do_i_find_a_meaningful_maximum_in_a_dataset/",
        "text": "First off, I majored in CS, not statistics. So I have only a _very_ basic understanding of stats, and most of my knowledge has already eroded away from non-use. So here goes:\n\nI'm currently playing around with a data-set containing the pollen-count in the air. I have a date column and and pollen-count column for that date. Each column represents the pollen count for each plant.\n\nThe pollen count is represented as positive integers. It contains however a lot of \"zero\" values, where there was nothing in the air as pollen are usually only active during a short period.\n\nAdditionally, the range of the values is dependent on plant.\n\nSo I would like to normalise the values to have something that ranges from `0.0` to `1.0`, where `1.0` represents 100% of the typical pollen-concentration. Everything that's over that typical maximum is considered to be `1.0`.\n\nThe issue here is the word _typical_.\n\nI have uploaded a simple data-set [to pastebin](https://pastebin.com/Fjw0A5mY).\n\nWhen I look at the distribution, it is _strongly_ skewed toward `0`, even if I remove the `0` values. And you can also see many outliers to the right.\n\nAs I mentioned before, I would like to find a \"typical\" maximum for those values. Not the \"absolute\" maximum.\n\nI could additionally remove rows of dates where the plant is usually inactive to get rid of a lot of `0` values. I have not done that yet, assuming that simply eliminating the `0` values would be a good-enough estimation for now.",
        "created_utc": 1527004081,
        "upvote_ratio": ""
    },
    {
        "title": "How to deal with Count data?",
        "author": "Genots-knots",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lauvx/how_to_deal_with_count_data/",
        "text": "I have a dataset as follows:\n\nThere are several patients, for each patient is measured how much sleep he/she has had the previous night, in seconds, and how many seizures he/she has had this night.\n\n| patient | sleep previous day | seizures |\n|:--------|:-----------------:|:--------:|\n| 0132 | 37979 | 0\n| 0132 | 48204 | 2\n| 0132 | 35626 | 1\n| 0132 | 56612 | 4\n| 0133 | 66799 | 2\n| 0133 | 32899 | 0\n| 0133 | 34295 | 1\n\nI'd like to test if there is a correlation between the time slept in the previous night and the amount of seizures in the current night. I planned on using a \"Linear Mixed-Effects Models\" to do this. Linear regression because both my dependent and independent var are interval data, and Mixed-Effects because every patient occurs multiple times. \n\nNow someone informed me my dependent var (seizures) isn't just interval data, but actually count data, which has to be dealt with differently and that I'd probably want a \"nonlinear multilevel model\". \n\nDo I indeed need to change my statistical method? After some searching I've come across \"Generalized Linear Mixed-Effects Models\", which uses logistic regression instead linear regression, is this the \"nonlinear multilevel model\" that I might need? ",
        "created_utc": 1527002998,
        "upvote_ratio": ""
    },
    {
        "title": "Feeling like I have weak statistics skills despite majoring in it and I would like a data oriented job. What should I do?",
        "author": "Zippityzinga",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8lanls/feeling_like_i_have_weak_statistics_skills/",
        "text": "So I am a recent grad with a BA in stats. My lowest grade in a Stats class is a B- (Applied Regression) and I am not strong when it comes to logic so my coding skills are very very limited. I realize that over the past four years I've been just trying to get this major over with and not really honing it. Hence, I didn't learn much.\n\nWhat should I do? The stat program at my school didn't require its students to use much R (maybe 3 courses?) and I didn't bother to take any CS courses.\n\n Currently unemployed.",
        "created_utc": 1527001416,
        "upvote_ratio": ""
    },
    {
        "title": "When comparing a subpopulation with the general population with a Chi Squared Goodness of Fit, what is the explanatory variable?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l9yxr/when_comparing_a_subpopulation_with_the_general/",
        "text": "[deleted]",
        "created_utc": 1526995652,
        "upvote_ratio": ""
    },
    {
        "title": "How do I visualize my life \"achievements\"?",
        "author": "Throwawaydatastats",
        "url": "https://www.scribd.com/document/379856654/10000-Days-Stats",
        "text": "",
        "created_utc": 1526979559,
        "upvote_ratio": ""
    },
    {
        "title": "How to decide when to exclude outliers?",
        "author": "-ToTornoTToT-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l82pk/how_to_decide_when_to_exclude_outliers/",
        "text": "Hello everyone,\n\none item on my latest survey study has some outliers. I asked people to report how much they think one ought to give to charity. The majority responded in the range of 0 \\- 25, though one person each entered 35, 50, and 95. The mean is 5.65 and the SD is 7.1I get that one might exclude those outliers due to their statistical anomaly, but would I not need an argument to do so? Like it being error, it having been recorded under exceptional circumstances, or it just being part of a different population.\n\nBut I have no reason to assume either of this as all values are valid possibilities present in the literature of charitable giving.\n\nMy question: Do I have to exclude these values simply due to their statistical anomaly or can I keep them given I see no reason to exclude them?",
        "created_utc": 1526972735,
        "upvote_ratio": ""
    },
    {
        "title": "Mathematical text/resource as companion to Intuitive Biostatistics?",
        "author": "rouxgaroux00",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l7nhn/mathematical_textresource_as_companion_to/",
        "text": "I am just starting to go through *Intuitive Biostatistics* (4e, Motulsky) in prep for my PhD qual exam (biochemistry &amp; genomics). The book is specifically written in a \"non-mathematical\" approach, but I may want to delve into the maths/why/how on some or many topics. \n\nIs there a consensus for a great beginner‚Äìintermediate *mathematical* statistics text or other resource? Any recommendations? All help is appreciated.",
        "created_utc": 1526967685,
        "upvote_ratio": ""
    },
    {
        "title": "Good books for reading up on Sampling techniques?",
        "author": "yinyang_zen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l5kdq/good_books_for_reading_up_on_sampling_techniques/",
        "text": "This is pertaining to survey research.",
        "created_utc": 1526947866,
        "upvote_ratio": ""
    },
    {
        "title": "Combinations question: number of pairs drawn from a set of pairs",
        "author": "quasar-3c273",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l4qno/combinations_question_number_of_pairs_drawn_from/",
        "text": "My experience with statistics is meager, so forgive me in advance.\n\nI would like to know what equation would solve the following problem:\n\nImagine a set of arbitrary size that only includes pairs. E.g., a set containing 100 elements would have in it 1.a (1 sub a), 1.b, 2.a, 2.b, ... 50.a, 50.b. Now, from that set, I'm going to randomly draw an arbitrary number of elements. In that subset of randomly drawn elements, what are the odds that I draw at least 1 pair (e.g. 3.a &amp; 3.b)? 2 pairs (e.g. 4.a, 4.b, 7.a, 7.b)? And so on.\n\nIn other words, there are 3 variables: set size, draw size, and number of pairs. \n\nAny help would be appreciated. ",
        "created_utc": 1526940538,
        "upvote_ratio": ""
    },
    {
        "title": "Highest Possible Average Score for First Place Win",
        "author": "Cassandralsc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l4gb1/highest_possible_average_score_for_first_place_win/",
        "text": "I'm involved in dance competitions called \"Jack and Jills\" where you are scored by 5 judges with a number out of the total number of final round couples, and the lowest average score gets first place. I was wondering: since you are ranked \\(once a judge ranks someone 1st place, they can't reuse that number on a different couple\\) but judges' scores can be quite different for the same couple they judge, what is the highest possible average score in a final round of 15 couples that can still be the first place winner?\n\nThanks for your input :\\)",
        "created_utc": 1526938143,
        "upvote_ratio": ""
    },
    {
        "title": "Lowest possible average score for a first place win",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l4do3/lowest_possible_average_score_for_a_first_place/",
        "text": "[deleted]",
        "created_utc": 1526937543,
        "upvote_ratio": ""
    },
    {
        "title": "Minimizing type 1 error of a sample mean, with probability value unknown.",
        "author": "Towlss",
        "url": "https://i.imgur.com/Cw3UiZx.png",
        "text": "",
        "created_utc": 1526936067,
        "upvote_ratio": ""
    },
    {
        "title": "Question about normalizing a dataset using medians",
        "author": "doublezero23",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l3uon/question_about_normalizing_a_dataset_using_medians/",
        "text": "Hello all,\n\nI am working on a sample dataset for a company. I cannot disclose the dataset due to an NDA, but maybe I can jimmy up a sample later \\(currently at a different job, and it does not pertain to this project, just making a quick post before lunch is over\\). Anyways, I need to normalize a dataset about two different medians, and the final values are supposed to be between \\-100 and 0. I have only normalized datasets to be between 0 and 1, and so I have no understanding about trying to go about this. I've done some google searches, but I am not really sure how to ask my question, and I keep getting bad results. Any ideas on what I should do? What methods do I need to keep in my mind for this? I am currently working with the dataset in R, a statistical programming language.",
        "created_utc": 1526933320,
        "upvote_ratio": ""
    },
    {
        "title": "ELI5: Can I stat test MaxDiff Data?",
        "author": "LeTonyDanza",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l3lvr/eli5_can_i_stat_test_maxdiff_data/",
        "text": "Hi All,\n\nI'm admittedly out of my depth when it comes to stat testing. I've relied so long on the software I've used to automatically highlight statistical differences that most of my schooling has been forgotten.\n\nI now have MaxDiff data, and I'm unsure what significant difference is in the scores (using a basic count difference). Can I just be lazy and say anything outside of 1 margin of error is significant?\n\nThank you. ",
        "created_utc": 1526931380,
        "upvote_ratio": ""
    },
    {
        "title": "What are some good jobs someone with a BS in Statistics could get that are related to finance?",
        "author": "13reputations",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l31pk/what_are_some_good_jobs_someone_with_a_bs_in/",
        "text": "I just graduated with my BS in Statistics and I‚Äôm ideally looking for a job related to finance. I took a few business classes during my degree such as financial accounting and corporate finance. I‚Äôve been searching for jobs like ‚Äòfinancial analyst‚Äô but those are usually directed for people with business degrees. What should I search for?",
        "created_utc": 1526927000,
        "upvote_ratio": ""
    },
    {
        "title": "checking if exponentially distributed",
        "author": "mikeemice",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l1wwl/checking_if_exponentially_distributed/",
        "text": "I have about 60 data points and I need to know it its exponentially distributed. how could i do this?",
        "created_utc": 1526918533,
        "upvote_ratio": ""
    },
    {
        "title": "Use of Wald, Score, and LR tests in Mixed Model Inference?",
        "author": "CuddlesTheFilthyBear",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l1v6f/use_of_wald_score_and_lr_tests_in_mixed_model/",
        "text": "When testing the null hypothesis of a parameter in a mixed model being equal to 0 against the alternative hypothesis that it is not equal to 0, why must a test like the Wald, Score, or LR be used?\n\nIn simple linear regression, I understand that one can use something simple like a t-test to test whether a regression coefficient for a fixed effect is significant in the model (ie. whether it is not equal to 0 and therefore not contributing to the model).\n\nHowever, testing the same hypotheses (as mentioned above) in mixed models, I only see Wald, Score, and LR tests being used. Why? (in the case of fixed effects)\n\nFrom what I have been able to gather, it seems to have something to do with the fact that maximum likelihood is often used to estimate parameters in the mixed model...I also don't understand why in mixed model inference, model fits are tested...I'm not interested in a model fit, but in a single parameter of the model.\n\nI'm not a statistician; I have minimal knowledge of the math behind these things. I ask here in hope of someone being able to provide a clear (relatively simple) answer, so that I can intuitively understand what is going on, even if I do not understand the math behind it.\n\nWhile there exists a lot of literature about mixed models and inference, I have not been able to find anything that specifically addresses (in terms I can understand) why we can't use a t-test to test significance of a model parameter in a mixed model, but can in a simple linear model.",
        "created_utc": 1526918168,
        "upvote_ratio": ""
    },
    {
        "title": "Question about factor analysis (PCA) and reverse coding of likert scale.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8l0a4e/question_about_factor_analysis_pca_and_reverse/",
        "text": "[deleted]",
        "created_utc": 1526904166,
        "upvote_ratio": ""
    },
    {
        "title": "The odds of meeting someone I could marry",
        "author": "Vlavlaha",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kyf19/the_odds_of_meeting_someone_i_could_marry/",
        "text": "I'm trying to figure out how likely it is that I'll ever get married. I'm a gay, Eastern Orthodox Christian woman (great combination, right?). As this is a hypothetical question, don't come at me for not being celibate. \n\nWhat are the odds of finding a woman who ticks all my boxes, if I'm not looking for her? I'm looking for a very rough estimate. Also, it would be nice if you could compare it to another group of people (i.e. \"It's just as likely that you find a professional soccer player\"). \n\nI have some basic needs in a woman: \n- Eastern Orthodox, maybe Catholic. I've read studies that marriages are best when the two agree on basic issues, and it would be nice to have someone to share a liturgical life with. \n- Pious. As in, I want to be able to share my spiritual life with her, and it's hard to do that if she's only nominally Christian, and doesn't really have a spiritual life in the technical Orthodox sense of the term. \n- Preferrably introverted. \n\nI did some math and collected some statistics but idk how good my math is. \n\nTotal US population is 325,700,000. \n\nLGB population is 5% of entire population. 325,700,000 x .05=16,285,000. Source: http://www.pewresearch.org/fact-tank/2015/05/26/lesbian-gay-and-bisexual-americans-differ-from-general-public-in-their-religious-affiliations/?amp=1 \n\nFirst, we'll look at how many LGB women are reasonably within my age range. Of LGB women, 27% are 18-24 and 27% are 25-39. 16,285,000 x .27 x 2=8,793,900. Source: https://williamsinstitute.law.ucla.edu/visualization/lgbt-stats/?topic=LGBT\n\n17% of the LGB population is Catholic. 8,793,900 x .17=1,494,963. \"Other Christian\" is 2%, so we'll estimate that .5% of the population is Orthodox. 8,793,900 x .005=43,970. So 1,494,963 Catholics + 43,970 Orthodox=1,538,933. Source: http://www.pewresearch.org/fact-tank/2015/05/26/lesbian-gay-and-bisexual-americans-differ-from-general-public-in-their-religious-affiliations/?amp=1\n\nTo show piety, we'll look at weekly church attendance. Among all Americans (including non-LGB), 39% of Catholics go to church every Sunday. We'll assume LGB rates are similar. 1,494,963 Catholics x .39=583,035. 26% of Orthodox go to church regularly, but we'll up that to 40% because I'd guess not many LGB folks are nominally Orthodox; it's just too hard to be gay and Orthodox. 43,970 Orthodox x .4=17,588. So 583,035 + 17,588=600,623. However, church attendance is not everything. She needs to be willing to wait until marriage. Although 20% of people in highly religious groups wait, 60% of women wait until marriage. Since religious reasons are the main reason to wait, we'll use the 60% statistic. 600,623 x .6=360,374. Sources: https://m.huffpost.com/us/entry/us_5acd082ae4b06a6aac8c7506/amp and http://hirr.hartsem.edu/research/EightFactsAboutChurchAttendance.pdf and https://www.statisticbrain.com/abstinence-statistics/ \n\nWe'll look at what personalities would go well with mine. I'm not really picky, I just need to be able to connect. I'm INFP and so I have always connected best with iNtuitives. The number of iNtuitive people is 26.7% of the population. .27 x 360,374=97,301. Source: http://www.myersbriggs.org/my-mbti-personality-type/my-mbti-results/how-frequent-is-my-type.htm?bhcp=1 \n\nNow let's look at reciprocity. We'll estimate 50% of Orthodox/Catholic LGB girls to represent who is not celibate. 97,301 x .5=48,651. Then we'll use 10% of that to represent people I am attracted to and who are attracted to me. N=48,651 x .1=4,865. \n",
        "created_utc": 1526879369,
        "upvote_ratio": ""
    },
    {
        "title": "What can I do statistically to get the most precise averages with several sets data with large heterogeneity?",
        "author": "elletequila",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kxdk5/what_can_i_do_statistically_to_get_the_most/",
        "text": "In this case I have the result of successful treatment (from 3 separate studies of the same drug) in form of percentage. For example, 70%, 90% and 100%.\nThe problem is the research designs are really different from each other. Some are clinical trials while some are cohort studies or even real-world studies. Not to mention the treatment duration are slightly different and so as general characteristic of the patient.\nAnd I have 5 different drugs to compare with each other and I obtained 3 studies of each drug. It would be nice to be able to provide the conclusion, like, Drug A vs drug B with pooled mean of 88% vs 93% respectively. Or things like that.\nI'm not very strict about getting extreme high quality averages as long as it's not wrong statistically. I just want to obtain some solid figures, to be specific, numbers.\nCan I do the pooled mean? Like, just bluntly calculated the mean percentage of each drug? Alternate ideas are also greatly appreciated.",
        "created_utc": 1526868035,
        "upvote_ratio": ""
    }
]