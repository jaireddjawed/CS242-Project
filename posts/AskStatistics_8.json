[
    {
        "title": "What are some good sources to learn Mathematics for Machine learning?",
        "author": "HunkyRanger",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10p624k/what_are_some_good_sources_to_learn_mathematics/",
        "text": "",
        "created_utc": 1675093639,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are there tests of significance of the difference between two means/medians for SEQUENCES of random variables of different sizes?, for example, X_1 against Y_i (with i = 36). If exist, should the random variables inside Y_i be independent? Thanks in advance.",
        "author": "isadorenabi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10p3iul/are_there_tests_of_significance_of_the_difference/",
        "text": "",
        "created_utc": 1675088024,
        "upvote_ratio": 1.0
    },
    {
        "title": "(SNP) Observed vs Expected values. How to know if significant difference?",
        "author": "la_valse_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10p1sbh/snp_observed_vs_expected_values_how_to_know_if/",
        "text": "I have SNP data (single nucleotide polymorphisms), 20 individual samples and around 10 000 SNPs. I've calculated the observed heterozygosity and expected heterozygosity values (means) of the SNPs, but how do I test if there's a significant difference?\n\nObserved value (mean): 0.159 Str err 0.00125\n\nExpected value (mean): 0.219 Str err 0.00117",
        "created_utc": 1675084716,
        "upvote_ratio": 1.0
    },
    {
        "title": "Entropy and correlation",
        "author": "Stack3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10p1lp5/entropy_and_correlation/",
        "text": "If I have two datasets, A and B, and if I union them into a dataset C, and if the entropy of C is lower than A and B does that mean A and B are correlated?",
        "created_utc": 1675084370,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical method to divide into groups ?",
        "author": "Pat_Kup",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10p0wt8/which_statistical_method_to_divide_into_groups/",
        "text": "Hi everyone\n\nI  need to analyze data from a some biological experiment. 15 strains were  tested on 3 carbon sources and their growth was determined at 3  different pH values. The results do not show a normal distribution. I  would like to classify individual strains into groups within each carbon  source. Can you recommend me a statistical method suitable for this  purpose ? Thanks in advance for any answers",
        "created_utc": 1675082301,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate the sample size for the development of a new clinical test?",
        "author": "Agitated_Control_156",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10p0h1z/how_to_calculate_the_sample_size_for_the/",
        "text": "How to calculate the appropriate sample size for the development of a new clinical test? The test would either be a quantitative test if the accuracy is good enough (i.e., test would predict a quantitative value similar to blood concentration) or be a semi-quantitative test if not accurate enough for quantitative testing (i.e., indicating statuses such as \"healthy\", \"at risk\", \"disease\"). The accuracy of the test would be evaluated by comparison with a quantitative gold standard.",
        "created_utc": 1675080898,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the minimum and Q1 value included in the bottom 25% of a box plot graph?",
        "author": "jimcontos182",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10oz5m4/is_the_minimum_and_q1_value_included_in_the/",
        "text": "",
        "created_utc": 1675076150,
        "upvote_ratio": 1.0
    },
    {
        "title": "Advice needed",
        "author": "Primary-Ad6447",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10oz4qb/advice_needed/",
        "text": "Hello! I studied statistics and econometrics many years ago but now I  have unfortunately forgotten most of the good stuff :) Perhaps you can  help me out. So I am helping a company to do a customer survey and now I  have obtained the results which are answers to survey questions.  Questions are like age, income bracket, monthly spending habits, what  products they bought, what colors they like/dislike, what  characteristics of the product they value the most and the least, etc.  Now I have to make some conclusions from the survey results - the  obvious ones are what I see right away - like average income of the  customer, most liked products, colors etc. But is there something deeper  I can pull from this data? Like, for example, customers who praise  product characteristic X have such and such features (age, income group  etc.)? If yes, how to do it? Is there anything else that comes in mind?",
        "created_utc": 1675076062,
        "upvote_ratio": 1.0
    },
    {
        "title": "what does it mean when the covariance matrix is split in statistical covariance and statistical + systemic covariance matrix?",
        "author": "ilrazziatore",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10oz37s/what_does_it_mean_when_the_covariance_matrix_is/",
        "text": "i was reading this dataset [https://github.com/PantheonPlusSH0ES/DataRelease/tree/main/Pantheon%2B\\_Data/4\\_DISTANCES\\_AND\\_COVAR](https://github.com/PantheonPlusSH0ES/DataRelease/tree/main/Pantheon%2B_Data/4_DISTANCES_AND_COVAR)\n\n&amp;#x200B;\n\nand there are 2 covariance matrices, one only for statistical errors and one that sums the systemic errors too .\n\nmy question is, if the systemic errors affects equally all measures how can they  have an influence the covariance matrix? and if they are systemic, how can someone determine  their values? \n\n&amp;#x200B;\n\nthank you to anyone who helps me understand",
        "created_utc": 1675075902,
        "upvote_ratio": 1.0
    },
    {
        "title": "Linear regression",
        "author": "Independent-Carry-80",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10owysy/linear_regression/",
        "text": "",
        "created_utc": 1675067853,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need Help Avoiding MANCOVAs",
        "author": "elducky434",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10owls4/need_help_avoiding_mancovas/",
        "text": "Hello! I am trying to create an analysis plan for a study without having to use MANCOVAs. Here are the study details:\n\nIV: binary (experimental or control group)\n\nDVs: 6 outcomes, all measured with Likert scales from 1-5 (unsure if any outcomes are related, would prefer to analyze each one separately unless that would be considered bad practice)\n\nCovariates: 3 (1 binary, 2 continuous)\n\nAny suggestions for the best way to analyze the data? I'm considering multiple ANCOVAS, a MANCOVA, or maybe even some form of linear regression. However, I don't know if I'll be able to reach enough participants to obtain sufficient power for more extensive tests like MANCOVAs. Any help at all is appreciated!",
        "created_utc": 1675066429,
        "upvote_ratio": 1.0
    },
    {
        "title": "PPV with two tests",
        "author": "Antique_Coconut_8896",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ong5n/ppv_with_two_tests/",
        "text": "There are two tests for a disease. Its prevalence is equal to 0.5. Test A has a sensitivity of 0.7 and a specificity of 0.7. Test B has a sensitivity of 0.8 and a specificity of 0.9. Assume that both tests are independent, and they are also conditionally independent given the presence of the disease as well as the absence of the disease. What is the positive predictive value (PPV) and negative predictive value (NPV) of the two test taken together?  \n\n\nI found out that this approach doesn't work:  \nPPV = P(C|A&amp;B) = \\[P(A&amp;B|C) \\* P(C)\\] / P(A&amp;B).  \nNext we will calculate P(A&amp;B) where A and B are independent.  \nP(A&amp;B) = \\[P(C) \\* P(A|C) + P(\\~C) \\* P(A|\\~C)\\] \\* \\[P(C) \\* P(B|C) + P(\\~C) \\* P(B|\\~C)\\].  \nWe use conditional independence given C: P(A&amp;B|C) = P(A|C) \\* P(B|C).  \n\n\nMainly because of P(A&amp;B) giving this PPV values above 1 (most of the time).  \n\n\nBut is this approach correct PPV = tpr\\*prevalence / (tpr\\*prevalence + fpr\\*(1-\\*prevalence)), i.e.:  \nPPV = (P(A|C) \\* P(B|C) \\* P(C))  /  \\[ (P(A|C) \\* P(B|C) \\* P(C)) + (P(A|\\~C) \\* P(B|\\~C) \\* P(\\~C))\\]?  \n\n\nIs the assumption that \"P(A|\\~C) \\* P(B|\\~C)\" represents fpr (false positive rate) correct? It is the same as  P(A&amp;B|\\~C) .",
        "created_utc": 1675038197,
        "upvote_ratio": 1.0
    },
    {
        "title": "Applied Statistics and the SAS Programming Language 5th ed. Help?",
        "author": "Such-Status-3802",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10oncmb/applied_statistics_and_the_sas_programming/",
        "text": "Hi!\n\nI know this is a huge shot in the dark but, I realized about halfway through my homework that I’d purchased the 4th edition instead of the 5th edition for my graduate class. I have the 5th edition on the way but it won’t be here in time for this assignment. After scouring the internet I am officially convinced the 5th edition has been scrubbed from any possible online forum that I could reference. Pearson Publishing is one thorough SOB.\n\nDoes anyone have access to the 5th edition who could send me/write out the Chapter One questions 1-1, 1-2, 1-4, 1-6, 1-8, 1-10? I’d be forever in your debt.",
        "created_utc": 1675037950,
        "upvote_ratio": 1.0
    },
    {
        "title": "Resources for checking mixed model assumptions in SPSS",
        "author": "ebear22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10on8qv/resources_for_checking_mixed_model_assumptions_in/",
        "text": "Hello all!\n\nI am running a mixed model (aka multilevel model) analysis and am trying to check the assumptions of my data. I've found guides for R but am not sure how to run this in SPSS. Does anyone have any resources for the steps to do this in SPSS. Thank you!",
        "created_utc": 1675037681,
        "upvote_ratio": 1.0
    },
    {
        "title": "What happens to Standard Deviation when 10% condition is violated?",
        "author": "Ok_Ad7527",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ojcz2/what_happens_to_standard_deviation_when_10/",
        "text": "In this problem the sample size is too small compared to the population. The sample is not independent so the 10% condition is violated. How does violating this condition effect the Standard Deviation of a sampling distribution of p^.",
        "created_utc": 1675028122,
        "upvote_ratio": 1.0
    },
    {
        "title": "How many times would you have to flip a coin before it's as close to 50/50 as possible?",
        "author": "thecg123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ogewx/how_many_times_would_you_have_to_flip_a_coin/",
        "text": "",
        "created_utc": 1675021190,
        "upvote_ratio": 1.0
    },
    {
        "title": "May I asks simple probability or logic problem that I argued wth my dad?",
        "author": "AustinYaoChen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ogd5g/may_i_asks_simple_probability_or_logic_problem/",
        "text": "The question is if event A has a probability of 50 percent leading to event B. If event C has a probability of 60 percent leading to event B. Event C has a probability of 100 percent leading to event A. What is the probability of leading to event B?\n\nI think the answer is 80 percent leading to event B. My Dad thinks it is 60% or the probability of event C leading to event B.\n\nWho is correct?",
        "created_utc": 1675021070,
        "upvote_ratio": 1.0
    },
    {
        "title": "Upper and lower limits for a very specific fake dataset - experimenting with blocking extraneous context",
        "author": "keithreid-sfw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10odlus/upper_and_lower_limits_for_a_very_specific_fake/",
        "text": "I have blocked out some peripheral stuff &gt;!but my question is designed to be coherent without the blanked stuff!&lt;. I hope it’s interesting and find it useful; I hope it’s not irritating &gt;!if I’ve made any mistakes or gone on and on!&lt;. \n\nPreamble &gt;!about being a mainly autodidact applied statistician!&lt;:\n\n&gt;!I am a medic doing applied stats just submitted thesis in quantitative analysis of reducing restraint in hospitals. I always feel I have to say that but I think it might piss people off. I have become intensely preoccupied with expressing various stats ideas in terms of enumerative combinatorics and I love it. Though I love this sub, me and other applied statisticians sometimes get roasted, rightly or wrongly, and I accept my knowledge may be patchy. Our ambition outstrips our fundamental knowledge on occasion. But I am here to learn and that may include making mistakes.!&lt; Please be gentle. \n\nContext of question:\n\nMy relevant published papers include a test of disinformation in safety reports.  I require &gt;! and enjoy!&lt; to be making quantifiable unreliable statements &gt;!and to date have constructed prototypical lies using coin tosses and the prime counting functions.!&lt;.\n\nMotivation:\n\nI love Terry Pratchett and I am using his idea of Zoons &gt;!from Equal Rites at p.127 in the 80s softback edition!&lt; to extend that concept of quantifiable lies. &gt;!Long story short, Zoons can’t usually lie, and they constructed a small lie for training purposes for their future diplomats. It is this: “_actually my grandfather is quite tall_”.! !&lt; I wish to show how a lie about, a grandfather’s height, affects the stochastic impression of an assumed population of grandfathers when that false point is added to the distribution functions. \n\nMy task:\n\nI need to generate heights of a prototypical population of granddads for the context of the lie. &gt;!For CS reasons I have chosen to have comparator populations of grandads with cardinal numbers which are powers of two from 2 to the zero i.e. 1 to big ones like 2 to the twenty i.e. 1048576. Forget bout inbreeding.  They are normally distributed for height with no tail.!&lt;\n\nAttempts:\n\nIf I generate/plot a cdf I need to choose the upper and lower interval for truncating. &gt;!I am thinking I need to choose a number of standard deviations from mu. With a low number of granddads it becomes silly to have high numbers of sigma. At the limit it becomes 0,0,0,0,1,1,1,1 !&lt; \n\nQuestion:\n\nIs there a standard or derivable/provable set of limits to truncate a fake distribution - especially when you know the n of the set is a power of two? &gt;! Intuitively I think the power constraint matters. Feel free to comment on that. !&lt;",
        "created_utc": 1675014475,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about uncertainty in linear fitting",
        "author": "KQ2eZPackers",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ocmzz/question_about_uncertainty_in_linear_fitting/",
        "text": "I am using the Origin analysis software to do linear fitting of some data I have. I have a question regarding uncertainty and error propagation in linear fitting.\n\nThe linear regression output gives a standard error value for the slope and intercept calculated from the fitting. However, my data consists of eight points, and each point has its own uncertainty value. Does the uncertainty of each data point affect the linear fitting at all? Does the standard error value from the fitting account for the uncertainty in each data point that is being fit? If it doesn't, is there anyway I can essentially propagate the error from each data point to calculate a new error value for the slope and intercept of the fitting?",
        "created_utc": 1675012169,
        "upvote_ratio": 1.0
    },
    {
        "title": "Weird ACF plot",
        "author": "Actual_Plant_862",
        "url": "https://i.redd.it/6tvu2xpp22fa1.png",
        "text": "Hi all, self-taught person here. I'm working on some data that has moderate positive correlation and this is an act plot I've constructed. What on earth is this plot saying? I'll be honest I'm still not great at certain things but still. Please. Any advice or thoughts I'd love to hear and thank you.",
        "created_utc": 1675012020,
        "upvote_ratio": 1.0
    },
    {
        "title": "ML in Python: transforming non-linear data to linear data to apply KNN",
        "author": "achsoNchaos",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10obnoz/ml_in_python_transforming_nonlinear_data_to/",
        "text": "I have some data `samples_train` that represents coordinates, each pair of coordinates are assigned to a label. `labels` is a numpy array representing a binary variable of values `0` or `1` . I plotted the data (see result in picture).\n\n    samples_train = np.load('ex2_samples.npy')\n    labels_train = np.load('ex2_labels.npy')\n    \n    boolean_labels = labels_train.astype(dtype=bool)\n    plt.scatter(*samples_train[~boolean_labels].T, label=\"Group 1\",  alpha=0.5)\n    plt.scatter(*samples_train[boolean_labels].T, label=\"Group 2\", alpha=0.5)\n    plt.legend()\n    plt.show()\n\nI'd like to apply KNN to this data set so I'm wondering how I can make the data linear. I've read about the kernel trick and tried to add an additional dimension:\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import sklearn\n    import sklearn.neighbors\n    from sklearn.neighbors import KNeighborsClassifier\n    \n    \n    X1 = samples_train[:, 0].reshape((-1, 1))\n    X2 = samples_train[:, 1].reshape((-1, 1))\n    X3 = (X1 ** 2 + X2 ** 2)\n    X = np.hstack((samples_train, X3))\n\nand visualized it\n\n    fig = plt.figure()\n    axes = fig.add_subplot(111, projection='3d')\n    axes.scatter(X1, X2, X1 ** 2 + X2 ** 2, c=labels_train, depthshade=True)\n    plt.show()\n\nCould I now apply KNN (here with 3 neighbours)?\n\n    nb = KNeighborsClassifier(n_neighbors=3)\n    neigh.fit(X, labels_train) \n\nCould one also use a function to transform the data? Since the data looks quite circular, is it possible to determine a radius `r` s.t. `x**2 + y**2 = r`, `x`, `y` being a pair of coordinates and rearrange the equation to e.g. `y`. I.e. `y = sqrt(r - x**2)`? If so how could one find a fitting `r`?\n\nhttps://preview.redd.it/0eup7ambe0fa1.png?width=403&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9796eac25b0f22e55411ee03cd0444fbb7395d0b",
        "created_utc": 1675009718,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question regarding medical statistics and a Cox proportional hazards model",
        "author": "NecessaryAd1403",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10o72se/question_regarding_medical_statistics_and_a_cox/",
        "text": "I intend to work on a medical paper from a pre-existing patient registry of data spanning 5 years and want to make the application to the ethics committee. As i am not an expert at statistics, i would like to ask you if this section of my statistical analysis logically makes sense:\n\nTo examine the association between the primary endpoint and **overall mortality**, a Cox proportional hazards model will be used to estimate hazard ratios and 95% CIs. Person-time shall be calculated from the date of the replacement to either death or the last available follow-up. To estimate the **long term mortality** of our study population, we intend to made the assumption that the hazard rate remains constant after the end of our data collection period. This assumption allows us to extend the time horizon of our study and estimate the expected number of deaths over a longer time frame. The hazard ratio will be stratified based on significant findings in the ROC analysis and adjusted for baseline characteristics in a stepwise fashion. While this method does not account for potential changes in the hazard rate over time, it provides a rough estimate of the long-term mortality based on the observed data. To make the assumption that the hazard rate remains constant, we will examine the graphical representation of the survival curve and test the proportionality assumption of the Cox proportional hazards model. The proportionality assumption will be checked by adding a time-dependent covariate to the model and testing its significance. If the time-dependent covariate is not significant, it suggests that the hazard rate remains constant over time.\n\nDoes this makes sense to you or should i word it differently? Thank you!",
        "created_utc": 1674997113,
        "upvote_ratio": 1.0
    },
    {
        "title": "p in binominal distribution",
        "author": "RagnarDa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10o4a0d/p_in_binominal_distribution/",
        "text": "In a binominal distribution I would expect a high p-value (p of success) to give many successes early on, but looking on the charts on Wikipedia [https://en.wikipedia.org/wiki/Binomial\\_distribution](https://en.wikipedia.org/wiki/Binomial_distribution) for the PMF and CDF it looks like p=0.5 has a earlier \"rise\" than p=0.7. What am I misunderstanding?",
        "created_utc": 1674986411,
        "upvote_ratio": 1.0
    },
    {
        "title": "Anyone use Python for statistics, particularly DOE or QA/QC? What are your thoughts?",
        "author": "GhostGlacier",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10nv839/anyone_use_python_for_statistics_particularly_doe/",
        "text": "I know of statsmodels &amp;  scikit-learn, but have never used them. I'm wondering if anyone here uses those libraries for stats - particularly interested in python can be used for industrial design of experiments, or basic exploratory data analysis and what everyones thoughts are? It seems python is becoming more commonly taught in stats grad schools.  How does it compare to R or JMP/Minitab etc..?",
        "created_utc": 1674955207,
        "upvote_ratio": 1.0
    },
    {
        "title": "I LOST MY HEART, MY SOUL, MY EVERYTHING",
        "author": "IMPRAYINGFORAMIRACLE",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10nsg2m/i_lost_my_heart_my_soul_my_everything/",
        "text": "[removed]\n\n[View Poll](https://www.reddit.com/poll/10nsg2m)",
        "created_utc": 1674947471,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone please explain this problem to me, I know it’s simple but I just don’t get how to calculate the answer",
        "author": "Ok_Vast_8723",
        "url": "https://i.redd.it/xfpd9am3qwea1.jpg",
        "text": "",
        "created_utc": 1674947283,
        "upvote_ratio": 1.0
    },
    {
        "title": "Spearman Rho",
        "author": "Ordinary-Phrase-1185",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10nrypb/spearman_rho/",
        "text": "Hello!\nI’m a med student. I wonder what are the right cut off to quantify the strength of spearman correlation if there is significance. Is there a citable table u can suggest? I really don’t feel qualified to choose online… to many options.\n\nThank u!",
        "created_utc": 1674946213,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for data science or bootcamps, or bootcamps with a decent ML component.",
        "author": "Lamp0blanket",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10nrtpo/looking_for_data_science_or_bootcamps_or/",
        "text": "So, here's where I am right now.  \nI have a master's degree in stats, and I'm already employed. However, I'm not a fan of the job I have; I'm not using my stats background \\*nearly\\* as much as I would have thought, and I'm not making as much as I'd like. I'm looking to switch to something that will pay better and actually let me use my background, and I think a bootcamp would be a good way to add some projects to my portfolio and give me greater bargaining power when applying to other jobs.  \n\n\nHere's a wishlist of things I'm looking for in a bootcamp. I don't imagine I'd be able to find a bootcamp that offers all of these, but if anybody knows of one that offers a decent chunk of these, that would be helpful.  \n\n\n\\- cost isn't much more than 2,500$. I'm willing to considerably bend on this if the program has income share options.  \n\\- Lasts between 2 and 3 months; I'm willing to go to 4, but I'm trying to get out of my current job sooner than later.\n\n\\- Will take around 10 to 15 hours of study per week. I've seen a few that only seem to be around a cumulative 40 hours of study, and I'm skeptical that that's enough time to learn anything substantial. I could go as high as 20 hours per week, but I'm already working full time, so I think that's really starting to push it for me.\n\n\\- Has a non-trivial coverage of ML (didn't really go into a lot of ML in my masters program). It doesn't have to be exclusively ML, but I'd like a decent amount of exposure.  \n\\- Remote\n\nIf anybody knows of any explicit bootcamps, or can just point me in the direction of where I can look for  bootcamps like this, that would be greatly appreciated. I've tried googling some on my own, but some of the stuff I find is hard to make heads or tails of.  \n\n\nThanks.",
        "created_utc": 1674945840,
        "upvote_ratio": 1.0
    },
    {
        "title": "How many comics do I need to check for market value (looking them up) when I buy a comic collection, for a reasonable degree of accuracy?",
        "author": "1401rivasjakara",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10npllf/how_many_comics_do_i_need_to_check_for_market/",
        "text": "I buy and resell comic collections as a hobby. If a collection has 1000 comics, for example, how many comics do I need to price check at random to put a market value on the whole collection with a reasonable degree of accuracy? Also, do I need to literally pull books at random or can I just pull like every 40th comic or so? Thanks so much. Sorry if this is against sub rules, I did check and it seems ok bc this is not a made up challenge. I really want to know!! Please !!  😀",
        "created_utc": 1674940063,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about Medical statistics and Cox proportional hazards model",
        "author": "NecessaryAd1403",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10nme85/question_about_medical_statistics_and_cox/",
        "text": "I intend to work on a medical paper from a pre-existing patient registry of data spanning 5 years and have made the application to the ethics committee, however they had remarks concerning my statistical analysis. They say: „Retrospective Data from all patients enrolled in x Registry from year x to year y shall be reviewed.\" Furthermore: \"To examine the association between the primary endpoint and overall long-term mortality, a Cox proportional hazards model will be used to estimate hazard ratios and 95% CIs.\" How can you determine the long-term mortality, when you have data only from year x to year y?\"\n\nThis is their feedback. Can i get more more insight/ how could i maybe word it better in my protocoll?",
        "created_utc": 1674931913,
        "upvote_ratio": 1.0
    },
    {
        "title": "Combining instruments with different uncertainty levels",
        "author": "feudalismo_com_wifi",
        "url": "/r/solar/comments/10lt86h/combining_instruments_with_different_uncertainty/",
        "text": "",
        "created_utc": 1674926294,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does anyone have access to Statista and willing to help a poor student?",
        "author": "LisaGamilton",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ni7fg/does_anyone_have_access_to_statista_and_willing/",
        "text": "&amp;#x200B;\n\nDoing a uni project at the moment and I found the data that would really help in my research.\n\n[https://www.statista.com/statistics/1312584/ukrainian-refugees-by-country/](https://www.statista.com/statistics/1312584/ukrainian-refugees-by-country/)\n\nThough in order to cite the author i need an access to the website and unfortunately our college doesn't provide it with us.\n\nLet me know if you can help plz\n\nThanks in advance for your time",
        "created_utc": 1674921172,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does anyone have access to Statista and willing to help a poor student?",
        "author": "LisaGamilton",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ni0cx/does_anyone_have_access_to_statista_and_willing/",
        "text": " Doing a uni project at the moment and I found the data that would really help in my research.\n\n[https://www.statista.com/statistics/1312584/ukrainian-refugees-by-country/](https://www.statista.com/statistics/1312584/ukrainian-refugees-by-country/)\n\nThough in order to cite the author i need an access to the website and unfortunately our college doesn't provide it with us.\n\nLet me know if you can help plz\n\nThanks in advance for your time",
        "created_utc": 1674920660,
        "upvote_ratio": 1.0
    },
    {
        "title": "Regression technique that will accommodate variances for each data point?",
        "author": "RabidMortal",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10nhsun/regression_technique_that_will_accommodate/",
        "text": "Not sure how to phrase this (and hence how to search for this).\n\nTask is assessing how strongly two variables correlate (e.g. weight and longevity). However, the data points are not individuals but species, and the values for each species is some  *average* for that species (that also has +/- s.d.). Moreover, these average values are being drawn together from many studies, many of which only report these summary stats (so getting the raw data is not an option)\n\nSo my questions: are there any established techniques that exist for such data (and what are the \"jargon-y\" terms I should be searching on ?)",
        "created_utc": 1674920109,
        "upvote_ratio": 1.0
    },
    {
        "title": "Independent Samples Test",
        "author": "MrLivingLife",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mvwaw/independent_samples_test/",
        "text": "Hello,\n\nI was wondering if anyone can help me whether based on the following information I can somehow find the .Sig number (marked in ??? on the photo).\n\nThank you! This is much appreciated.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/wla3yyzvgnea1.png?width=925&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3b4edc87ea061836fbcb5d0103a0ea96314bf52f",
        "created_utc": 1674853189,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why didn't you do a post-doc?",
        "author": "ProposalLeast3057",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mtoku/why_didnt_you_do_a_postdoc/",
        "text": "",
        "created_utc": 1674847828,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which analysis method to use?",
        "author": "-i-dont-know--",
        "url": "/r/dataanalysis/comments/10mtc53/which_analysis_method_to_use/",
        "text": "",
        "created_utc": 1674847652,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Fit data that come in stream",
        "author": "sonicking12",
        "url": "/r/statistics/comments/10msf1g/q_fit_data_that_come_in_stream/",
        "text": "",
        "created_utc": 1674847091,
        "upvote_ratio": 1.0
    },
    {
        "title": "Helloo i need help with this exercise of statistics please",
        "author": "Old-Yoghurt-9171",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10msbsn/helloo_i_need_help_with_this_exercise_of/",
        "text": " \n\nHello i need help with this what do i need to learn in order to do this exercise ?  \nExercise 2: (fill in the blanks): a sample of n=50 students is investigated about systolic blood pressure; the sample gives a mean of 115 mmHg and a standard deviation of 12 mmHg. We want to infer about the student population at a standard level of confidence.  \nFixed the confidence level equal to \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ It involves calculating a confidence interval for the \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ (choose between: mean/proportion).\n\nThe punctual estimate is \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nThe suitable standard error is \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nThe confidence interval is (indicate the two extremes): \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_; \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nSo the width of the confidence interval is: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_",
        "created_utc": 1674844702,
        "upvote_ratio": 1.0
    },
    {
        "title": "How could I explain a relationship between the oil prices and the changes in the motor vehicle industry?",
        "author": "TrainAccomplished382",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mr6d1/how_could_i_explain_a_relationship_between_the/",
        "text": "So I finished a course in R in my university and I really liked, so now on holidays I am goofing with it. I created a data frame with the oil prices from the 50's up to last year. For each year I also have data from different classes of vehicles (truck/car). I am trying to find the relationship between the price of oil and the changes in vehicles (miles per gallon, horsepower, weight, co2 emissions). I tried first to do a linear model using both HP and weightas independent variables and MPG as a dependent variable. I then used the error term as a variable for technological development. But then, there was not a clear relationship between the error term and the oil prices. Maybe I could get some advice on how to connect these variables? Thanks",
        "created_utc": 1674841997,
        "upvote_ratio": 1.0
    },
    {
        "title": "From zero to data science: How I taught myself Python and Pandas",
        "author": "Eezikwultoo",
        "url": "/r/DataScienceDigest/comments/10mpd4l/from_zero_to_data_science_how_i_taught_myself/",
        "text": "",
        "created_utc": 1674838460,
        "upvote_ratio": 1.0
    },
    {
        "title": "Logic of statistical inference",
        "author": "DeathByLeprosy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mnvvo/logic_of_statistical_inference/",
        "text": "Hey,\n\nI am currently taking a course in statistical inference and while doing the math isn't all that hard, I do have significant problems understanding why all of this works. In other words, I have no idea how the methods we are supposed to apply (e.g. Hypothesis testing) are justified. What I find most confusing is thinking about when to use data vs when to use theoretical considerations (domain knowledge etc.). For example, it is super weird to me that the quality of an estimator is judged realtive to our theoretical considerations (e.g. bias is the expected square deviation of our estimator from the unknown population parameter). I'm sorry this is pretty confused and all over the place, but I just can't seem to get a firm grip on the principles/core ideas. To the point it is hard to even state exactly what my problem is. Are there ressources which do a good job of explaining the basics?",
        "created_utc": 1674834071,
        "upvote_ratio": 1.0
    },
    {
        "title": "Pairwise comparisons after non-significant ANOVA?",
        "author": "greenteaaalemonade",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mlgf0/pairwise_comparisons_after_nonsignificant_anova/",
        "text": "I ran a 2 (gender) x 3 (subject) ANOVA and the interaction between gender and subject is not significant. For context, the hypotheses I’ve specified beforehand is that girls will do better than boys for xx subject and xx subject, while boys will do better for xx subject. \n\nI’m confused as to whether it is right for me to go ahead look at pairwise comparisons even with the ns interaction effect? Sources I’m reading seem not to really come to a consensus on this, with some saying that post-hoc results after a ns ANOVA is valid but others saying post-hoc comparisons shouldn’t even be looked at if ANOVA is ns. Would appreciate any help/advice on this!",
        "created_utc": 1674827879,
        "upvote_ratio": 1.0
    },
    {
        "title": "The effect of treatment is positive for all observations but statistical significance is not reached due to a large effect variance. Is there anything that can be done to make variance smaller?",
        "author": "NucleiRaphe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mimr4/the_effect_of_treatment_is_positive_for_all/",
        "text": "Hi! I am trying to measure whether treatment A has an effect on the expression of gene X. I have 4 observations and for each one the expression level increases. Problem is, that the some observation have way larger increase than the others. This, combined with small sample size, make stastical tests fail. Below is the data. Gene expression changes are calculated as relative changes so control is set as 100%. Both measurements of a row are from same patient.\n\nctrl ; treatment  \n100% ; 331.4%  \n100% ; 205.3%  \n100% ; 131.0%  \n100% ; 163.7%  \n\nT-test gives p-value of 0.0729. Mean difference 100.4,   SD 73.94, 95% CI -17.31 - 218.0.\n\nI understand that variance is large, but intuitively it makes no sense to conclude that treatment has no effect when we can clearly see that the treatment has positive effect for every chance observation studied. The larger treatment effect for the first row somehow makes the treatment worse? I tried changing 331 to 130 and the treatment was statistically signifcant which just intuitively feels *wrong* although I understand that the tests work with variance which is smaller if the effect size is similar in all observations.\n\nIs there a way I could scale the values or use another test to find the significance? I have bumbed into same sort of problem few times already with slightly larger sample sizes too so i'm curious to learn if there is good answer to this problem.",
        "created_utc": 1674818926,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which distribution to use and am I structuring this correctly?",
        "author": "ESharer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mgpk1/which_distribution_to_use_and_am_i_structuring/",
        "text": "Two Questions.\n\n1. I am attempting to test the hypothesis that stimuli presentation order impacts gaze count proportion. I have been considering Poisson, quasipoisson, and negative binomial regression. Looking at the data it appears as though the mean &gt; var, but residual variance &gt; df when I do the poisson regression models. As I understand negative binomial is ideal for overdispersion. I am a bit at a loss regarding this conflicting information.\n2. Am I thinking about building this model correctly? Notably I currently maintain gaze count for both AOI and not AOI using the dummy variable to model the impact. However since gaze points are either AOI or not AOI only e.g., p and 1-p could think of gaze count as 100 gaze counts out of 200 valid gaze points for video #1. I believe I could do gaze count on AOI out of \n   1. glm.nb(gaze count \\~ 1 + offset(tot valid gaze), data = data)\n   2. glm.nb(gaze count \\~ aoi/not aoi + offset(tot valid gaze, which varies by stim), data = data)\n      1. gaze count is different in aoi versus not aoi - tends to warrant inclusion\n   3. glm.nb(gaze count \\~ aoi/not aoi + stimuli presentation order + offset(tot valid gaze, which varies by stim), data = data)\n   4. glm.nb(gaze count \\~ aoi/not aoi + stimuli presentation order + aoi/not aoi\\*stimuli presentation order + offset(tot valid gaze, which varies by stim), data = data)",
        "created_utc": 1674811123,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you determine the minimum number of tests to be able to run a hypothesis?",
        "author": "Trench2Mount",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mcg0q/how_do_you_determine_the_minimum_number_of_tests/",
        "text": "Let's say we want to know whether a drug is going to improve patients conditions tested by a quantitative metric (let's say m). Let's say we want to do a two sample mean test for treated and untreated patients. How do I determine the minimum number of trials or subjects that are needed for the test to achieve 95% confidence in rejecting the null hypothesis of the two means being from the same population.   \nAlso this was a specific problem, how do we determine sample size in general for hypothesis testing? Most of the problems I have seen n = number of samples is given.",
        "created_utc": 1674794945,
        "upvote_ratio": 1.0
    },
    {
        "title": "Longitudinal analysis of two waves with large variability in time interval",
        "author": "ravencrawr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mc5ri/longitudinal_analysis_of_two_waves_with_large/",
        "text": "I am interested in modelling a neuroimaging variable over time, but I am limited to two timepoints due to the number of observations. I would like to use other variables collected at baseline to predict change in the neuroimaging variable. My baseline N is 181, and follow-up is 95. There was a third wave, but too few observations. I have experience with latent growth modelling in MPlus, but with a much larger dataset with more follow-ups.\n\nI looked into latent change score models, but my data is further complicated by the fact that there is huge variability in the time interval between baseline and follow-up (range: 6 months to 6 years).\n\nAnother idea was to simply regress the follow-up neuroimaging score on the baseline score + covariates of interest (i.e., predict the future scores rather than the change), but I feel I still can't ignore the fact that the variation in time interval could impact their association. Particularly since I am dealing with age-related processes.\n\nI briefly thought of \"reassigning\" the waves based on the time intervals (e.g., visit #1 is 1-2 years from baseline, visit #2 is 2-3 years, etc), but I haven't properly considered that option, nor heard of that being done before. If that's feasible, I would then attempt latent growth modelling in MPlus. \n\nDo you have any suggestions for how to model longitudinal change in this situation, or any literature you can point me to? I'm most confident with MPlus, but R isn't out of the question.",
        "created_utc": 1674793999,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is OLS/multiple regression for time series data where there are replicates with repeated measures?",
        "author": "Ok-Needleworker-6595",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10mc172/is_olsmultiple_regression_for_time_series_data/",
        "text": "I have several sets of data from different experiments tthat has one continuous outcome, a time component and one categorical variable with 2 categories. I want to look at significance both of the effect of time, the categories and (occasionally/for a susbet of the models) their interactions. I have only on the order of ~100 observations.\n\nMy data *appear* to meet the criteria of OLS (normality of residuals/qqplot looks good, doesn't appear heteroskedastic or to have any outkiers/influential points, etc). R^2 is very high for all models (.6-8).\n\nAm I violating any assumptions about independence because of the repeated measures? Technically speaking I know that the previous measurements could certainly affect later measurements, but there is no apparent relationship between my outcome and the residuals, meaning the model doesn't seem to violate the independence assumption as I understand it between them, but not with respect to the residuals \n\nI mostly want to make sure I am not publishing something where the p values themselves are not valid for the assumption I've made. That said, I'm not in a pure statistics field and all of my conclusions are built on multiple levels or evidence and nobody in my field will really question the validity on them. But I would still like the p values to be as meaningful as possible.",
        "created_utc": 1674793592,
        "upvote_ratio": 1.0
    },
    {
        "title": "Medical samples from population of counties question",
        "author": "ScaredComment2321",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10m9rro/medical_samples_from_population_of_counties/",
        "text": "\nIs it a good general rule to convert counts to cases / 100,000, which allows for standard OLS regressions, or maybe leave as counts of cases and consider a Poisson or negative binomial? These data are possibly / probably zero-inflated. Looking for general guidelines.",
        "created_utc": 1674786716,
        "upvote_ratio": 1.0
    },
    {
        "title": "What test should be used to compare data from two different years?",
        "author": "lil_honey_bun",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10m4on6/what_test_should_be_used_to_compare_data_from_two/",
        "text": "This is not my field of study whatsoever so I am completely lost. \n\nI developed a quality improvement project to increase patient data collection the last few months of 2022. I want to compare our 2021 patient numbers with our 2022 numbers. \n\nThere are 2,000 more patients reporting this year than last year but as I said, the project wasn't implemented until Nov. Are there any analysis tools that could tell me if this increase is significant, if so, which one(s)?",
        "created_utc": 1674772505,
        "upvote_ratio": 1.0
    },
    {
        "title": "Normal distribution to recommend staffing levels.",
        "author": "ToweringMaple2190",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10m3uwe/normal_distribution_to_recommend_staffing_levels/",
        "text": "Hello everyone, I am working on an analysis for determining extra hospital staffing during flu season. From my analysis I have found that at risk populations (65+) follow normal distribution according to state population. Meaning that each state has about the same percentage of at risk.\n\nIt’s been a while since I’ve taken a stats class and I’m wondering what type of equation should I use to recommend staffing levels based on population when there is a cap on the number of extra medical staff you have.",
        "created_utc": 1674770471,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can anyone help me solve this and show me how to solve it so that I can learn?",
        "author": "3rr03",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10m1oar/can_anyone_help_me_solve_this_and_show_me_how_to/",
        "text": " \n\nThe weight of an organ in adult males has a​ bell-shaped distribution with a mean of 325   \ngrams and a standard deviation of 40   \ngrams. Use the empirical rule to determine the following.\n\n​(a) About 68​%   \nof organs will be between what​ weights?\n\n​(b) What percentage of organs weighs between 245   \ngrams and 405   \n​grams?\n\n​(c) What percentage of organs weighs less than 245   \ngrams or more than 405   \n​grams?\n\n​(d) What percentage of organs weighs between 245   \ngrams and 445   \n​grams?",
        "created_utc": 1674765082,
        "upvote_ratio": 1.0
    },
    {
        "title": "Independent Sample T-Test or Paired Sample T-Test?",
        "author": "r_derham1166",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10m16r4/independent_sample_ttest_or_paired_sample_ttest/",
        "text": "So I have this question for college and am just stumped on which statistical test I should use. The question is “Is there a significant difference between the Revenue generated from Customers with Headquarters in the US and Customers with Headquarters in Europe?”",
        "created_utc": 1674763833,
        "upvote_ratio": 1.0
    },
    {
        "title": "Career guidelines to become a football(/soccer) data analyst?",
        "author": "Playful_Effect",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10m0d23/career_guidelines_to_become_a_footballsoccer_data/",
        "text": "",
        "created_utc": 1674761775,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical significance, how?",
        "author": "Isatuximab",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lzzd6/statistical_significance_how/",
        "text": "Hello, it's been a while since I had statistics in university so I'm a bit lost.\n\n\nIf I have data that looks like\n\n-:|-:|-:\nGroup| year 1| year 3\n1| 45| 30\n2| 200| 250\n3| 10| 15\n\nWhat kind of test would be best to examine if the change between the years per group is significant or not? As in if the change for group 1 from year 1 to 3 is significant? I never learned spss in university, we just used excel :/",
        "created_utc": 1674760824,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confidence Level %",
        "author": "TemoSahn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lwp4y/confidence_level/",
        "text": "Hey Statfolks, I have a very beginner level of statistics and I'm trying to illustrate a business problem for a client...\n\nCurrently they are testing only 1 out of approx. 1200 samples each month for quality (attribute data, yes/no response, basically did it pass or not). I know the confidence level of this approach would be extremely low, but how do I quantify this?",
        "created_utc": 1674752594,
        "upvote_ratio": 1.0
    },
    {
        "title": "I was just wondering how to solve this.",
        "author": "Fun-Proof3797",
        "url": "https://i.redd.it/2sl7hklvcgea1.jpg",
        "text": "",
        "created_utc": 1674749086,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to eliminate sampling error?",
        "author": "ContestWaste1421",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lulkc/how_to_eliminate_sampling_error/",
        "text": "We sent out a survey for 20,000 total population (companies in EU) and received 3000 replies. There are all types of replies from each country but the distribution doesn’t follow perfectly the country/population.\n\nI calculated 99% confidence level and 2% margin of error with this response rate.\n\nSome team members say it must be random sampling with compulsory replies, otherwise we can’t use the confidence level and margin of error.\n\nCan someone please help me understand if this is correct or if we can use the results?\n\nThank you in advance",
        "created_utc": 1674747409,
        "upvote_ratio": 1.0
    },
    {
        "title": "dumb question about statistics",
        "author": "aNieke4bToSega8cIomu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ltlea/dumb_question_about_statistics/",
        "text": "Hi, I'm a hobby programmer and pretty uneducated. (please keep that in mind when formulating your answers thanks)\n\nI'm interested in people and how they behave and that is why I'm looking into statistics right now. Let's start with arbitrary question that I would like to know the answer to.\n\n\"Are women who like the TV show Game of Thrones more likely to be democrats than women who don't like it?\"\n\n1. Is this even a valid question to research, I don't mean moral but more like logical? Is there something wrong with the setup of the question? Or is it a totally fine question to statistically inquire? (Is every variation of the question perfectly valid? eg. \"Are women who like the TV show Game of Thrones more likely to be democrats than men who like it?\")\n\n2. How do I go about this? \n\nI'm thinking of just asking eg. 300 people 3 simple Yes/No questions. \"Do you like GOT?\", \"Are you female?\", \"Do you vote democrat?\". I've drawn this example as some kind of a tree structure for me to visualize this.\n\n3. How do I calculate the answers?\nSay I have this tree in front of me now. I follow the decisions. \n\n200 Yes to \"like GOT?\" -&gt; 70 of those YES to \"female?\" -&gt; 10 of those YES to \"democrat?\"\n50 NO to \"like GOT?\" -&gt; 25 of those NO to \"female?\" -&gt; 20 of those NO to \"democrat?\"\n\nThank you for your help!",
        "created_utc": 1674744822,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hi, I need help solving this exercise about Multivariate Statistics, in particolar distribution of a multivariate normal",
        "author": "hawkeyeninefive",
        "url": "https://i.redd.it/r9s2zccdtfea1.jpg",
        "text": "",
        "created_utc": 1674742526,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the population of a medical study?",
        "author": "thesubjectsmatter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lsqvp/what_is_the_population_of_a_medical_study/",
        "text": "Note: This is about question from a textbook, but the answer is given in the textbook. I'm asking for help understanding the answer.\n\nIn Openstax Introductory Statistics there is a [question (1)](https://openstax.org/books/introductory-statistics/pages/1-practice) relating to AIDS patients taking a new drug and the length of time in months that patients lived after they started taking the drug.\n\nIt asks what the population is and the answer provided is \"AIDS patients\".\n\nI assume there is a population \"AIDS patients\" and a subpopulation of that is \"AIDS patients that took the drug\". \n\nIt would only be possible to sample the subpopulation \"AIDS patients that took the drug\" if you're interested in the length of time in months that patients lived after they started taking the drug. \n\nSo is \"AIDS patients that took the drug\" not the population in the context of the study?\n\nOr is  \"AIDS patients\" considered the population because you are taking findings from a sample from \"AIDS patients that took the drug\" to draw conclusions about all \"AIDS patients\"?",
        "created_utc": 1674742494,
        "upvote_ratio": 1.0
    },
    {
        "title": "Optimal sample allocation",
        "author": "Curious-Fig-9882",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lrj70/optimal_sample_allocation/",
        "text": "Hi everyone, \n\nI  was taught that in experimental designs, samples should be randomly and equally distributed across conditions (for increased power and to reduce variance inequality). However, in reality, and especially in clinical trials, sample allocation is used (e.g., 1:2). I   am trying to figure out a formula you can use that can predict the optimal sample size allocation. Any help with this?",
        "created_utc": 1674738907,
        "upvote_ratio": 1.0
    },
    {
        "title": "Test between groups and in one group",
        "author": "LiLu1197",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lomx6/test_between_groups_and_in_one_group/",
        "text": "HI everybody,\n\nI did an study with a intervention group (IG, n=17) and a control group (CG, n=16). I performed a lot of assesments for each group before the intervention and afterwards. There was 1 person in the IG who dropped out. Now I need to compare the results in each group (pre vs post test) and in-between the groups.\n\nMy question are now\n\n\\- Should I still include the data of the drop-out patient to e.g. calculate the relative change of the assesments? (That's what my professor told me to do but seems weird to me)\n\n\\- Which test could I use to compare the assessments? I already used levene-test to look for normal distribution and there are some assesments of the IG and some of the CG that are normally distributed and some that are not. So should I use Wilcoxon/Mann-Whitney test each time there is a not normally distributed score? And then use paired/independent t-test each time both CG and IG (or their difference in case of paired t-test) are normally distributed?\n\n\\-&gt; but then I would change up tests quite a bit and that does not seem correct neither.\n\nI am very thankful for every help!",
        "created_utc": 1674728132,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Alternatives to factor analysis with a small sample and too many items",
        "author": "elsextoelemento00",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lo1jx/q_alternatives_to_factor_analysis_with_a_small/",
        "text": "I am participating as a consultant in a validation for a psychometric test that aims to measure six personality traits but I'm having problems. This measure has six factors, and 24 items each, likert scale 1 to 6, and the group I am helping is working with a sample of 92 people.  My client states they need to eliminate 12 items for each factor, so I proposed to perform factor analysis in order to determine what items could be less related to the factors they belong to, and are more adequate to be discarded (at least statistically, because the text of the item should be checked too).\n\nI am performing the statistical validation process, and when I tried to use exploratory factor analysis, I see that the factor matrix is considerably different to the theoretical hierarchisation of items in factors.  Many items are associated to any factors, as well as the  I used Jamovi (an R based point and click API).\n\nI tried to perform confirmatory factor analysis to see if the theoretical distribution of items in dimensions is valid. I tried it in Jamovi, and R Studio using the lavaan library, grouping the theoretical 24 items of each dimension in each one of the 6 factors. However, it takes forever, and doesn't show me any results. I guess that there are too many items to make the calculations, but I know there are bigger psychological tests that were validated with the same technique (like 16PF, or MMPI).\n\nHere is my question:\n\nIs there any other statistical procedure that i can perform to determine if the items behave consistently around the proposed six latent factors? In other words is there an alternative to factor analysis that can work with samples smaller than 200? What other statistical techniques can I use (apart from reliability with Cronbach's alpha) to help to decide what items are better to be discarded from the test?I understand that reliability and factor analysis are based on variances, so the small sample is a disadvantage and that's the reason because bigger samples are recommended. I am prepared to be advised to work with a bigger sample, but that's not possible at the moment.\n\nThanks in advance for your thoughts and suggestions.",
        "created_utc": 1674725435,
        "upvote_ratio": 1.0
    },
    {
        "title": "Guidance re data analysis and representation",
        "author": "Ivar-the-Dark",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lndmt/guidance_re_data_analysis_and_representation/",
        "text": "With regards to spasticity there is a well known dated  evolution model called the Brunnstrom recovery staging.  Basically it maps out post stroke motor recovery patterns, but is is flawed. Not only does it contradict itself, but it doesn't adequately represent the data it obtains. \n\nI'm doing a study that has the following objectives:\n\n1. expose the model's shortcomings\n2. represent the discrepancy using accurate figures vs the model\n3. suggest a more accurate scale per stroke severity  \n\nI looked for what studies to use and cam to Rash and UMNC as the ways to express the discrepancy. The problem is I'm not a statistics person. I hope to approach our Biostatistician with some knowledge of what I'm trying to do do.\n\nSuggestions?",
        "created_utc": 1674722438,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why won't hosmer and lemeshow work in spss?",
        "author": "BusyStudying",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lkq3j/why_wont_hosmer_and_lemeshow_work_in_spss/",
        "text": "I am running a binary logistic regression in SPSS to see if there's a correlation between the independent variable, mood symptoms (1=yes 0=no) and the dependent variable, concussion status 1=definite 2=probable. Hosmer and Lemeshow shows up with a \".\" under significance. There are about 50 points in the dataset.",
        "created_utc": 1674711770,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics Question, Which Type of Test Should I Use?",
        "author": "BusyStudying",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lk633/statistics_question_which_type_of_test_should_i/",
        "text": "Dependent Variables: Definite Concussion, Probable Concussion. 1=Definite 2=Probable\n\nIndependent Variables: Mood Symptoms 1=yes 0=no, Sleep Symptoms 1=yes 0=no, Migraine Symptoms 1=yes 0=no\n\nWould like to see how one affects the other.\n\na.) What test should I run for this? Some form of generalized linear regression?\n\nb.) How do I do this in SPSS? (can probably just figure this out if I know what exact test to use and if I can interpret the results)",
        "created_utc": 1674709811,
        "upvote_ratio": 1.0
    },
    {
        "title": "In absence of theory, how do I use LASSO to do variable selection?",
        "author": "sonicking12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lj5py/in_absence_of_theory_how_do_i_use_lasso_to_do/",
        "text": "Hello.  I have a regression model and I have a lot of independent variables.  Luckily, this is not a p &gt;&gt; n problem.  I want to know what variables are important to affect the outcome.   I have a few questions:\n\n1) Should I just put all the independent variables in a regression and see what comes out to be significant?\n\n2) Should I use LASSO to find variables?  But how do I use this technique to find the final set of variables?  I am looking at the package glmnet in R.  Please help.",
        "created_utc": 1674706501,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with a Statistics Problem related to Forestry",
        "author": "Patient_Science_4224",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lj0aq/help_with_a_statistics_problem_related_to_forestry/",
        "text": "Hello, I am hoping someone can help me with a statistic word problem I've been stuck on for a few days. Here it is; \n\n **To assess the potential to provide seed for regeneration, a manager of a seed orchard with 252 trees wants to know the average number of cones per tree in the orchard. A preliminary sample of a few trees is made and cones counted. Mean and standard deviation of the preliminary sample observations was 32 cones per tree and 11 cones per tree, respectively. How many trees would have to be sampled in a simple random sample without replacement effort in order to estimate the population mean within ± 10 percent at a confidence level of 90 percent? (Use 1.645 as the initial value for t; this value is from appendix table 6 of the textbook with degrees of freedom infinity; when you compute the first value for n, use that value to update the degrees of freedom used looking up t and then recalculate n using the updated t value.)**\n\nI have gotten as far as figuring out n, but I am unsure if that is true because I used the s value of 11, which is based on cones, and not on the whole tree population. Any help again would be appreciated! Thanks!",
        "created_utc": 1674706020,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone translate this into English, please? I think my stats prof is assuming we know more than we do...",
        "author": "AstroINTJ",
        "url": "https://i.redd.it/svhxv4xfecea1.jpg",
        "text": "",
        "created_utc": 1674701184,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you explain Bernoulli trial",
        "author": "juicemilki",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lfhb9/how_do_you_explain_bernoulli_trial/",
        "text": "I am extremely confused with bernoulli trial and how it works. Does anyone have a very simple explanation for it or even a video?",
        "created_utc": 1674695535,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I rate forecasts?",
        "author": "Endleofon",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lf91l/how_can_i_rate_forecasts/",
        "text": "Assume that I am trying to rate forecasts for GDP growth.\n\n* Forecast #1 is 1% while the realized GDP growth is 2%. The deviation can be expressed as 1 percentage point or 100%.\n* Forecast #2 is 10% while the realized GDP growth is 11%. The deviation can be expressed as 1 percentage point or 10%.\n* Forecast #3 is 10% while the realized GDP growth is 20%. The deviation can be expressed as 10 percentage point or 100%.\n\nIntuitively, it seems like Forecast #2 is more accurate than Forecast #1, which in turn is more accurate than Forecast #3. If the deviation is equal in terms of percentage points, choose the lower percentage deviation. If the deviation is equal in terms of percentages, choose the lower percentage point deviation.\n\nBut what if there is no equality?\n\n* Forecast #4 is 4% when the realized GDP growth is 6%. The deviation can be expressed as 2 percentage points or 50%.\n* Forecast #5 is 16% when the realized GDP growth is 20%. The deviation can be expressed as 4 percentage points or 25%.\n\nWhich forecast is more accurate, #4 or #5?",
        "created_utc": 1674694888,
        "upvote_ratio": 1.0
    },
    {
        "title": "Combining means and standard deviations from two groups",
        "author": "BeachLover54",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ldevo/combining_means_and_standard_deviations_from_two/",
        "text": "Hey all,\n\nI was wondering how to calculate a combined mean and std from two groups. like for example;\n\ngroup 1: mean 100.95/120, std=8.49\n\ngroup 2: mean 25.44/30, std=3.21\n\ni figured for the means you could just add 100.95+25.44 = 126.39, but wasn't sure how that works for std? thanks!",
        "created_utc": 1674689926,
        "upvote_ratio": 1.0
    },
    {
        "title": "G Power/Power Analysis Guidance?",
        "author": "aub51zzz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lcpti/g_powerpower_analysis_guidance/",
        "text": "Hi, all! I’m trying to figure out the best way to conduct a power analysis for my study, but I’m at a loss, admittedly. Some more context: \n\n- My study focuses on the impact of IV on DV1, DV2, DV3, and DV4 (with some additional moderating relationships, but I’m not sure this matters for the problem at hand) \n- I’m using a survey with Likert-style questions (45 items, not including demographics)\n- I’ll be using a series of linear regressions as well as MANOVA with DDA in my analysis \n\nTo determine if my sample size is appropriate, I’ve been advised to conduct a power analysis and am so lost. I tried first using GLM on SPSS:\n\n- Power was extremely low when I used Univariate GLM and did it for each DV (.19, .48, .14, .17)\n- Power was low but significantly higher when I used Multivariate GLM (.5)\n\nOn to the main questions:\n1. Was one option better than the other when it comes to univariate vs multivariate? \n2. I also wanna try my hand using G Power, and I have no idea which test to use (as well as how to input predictors and such.) Each time, the sample size is super low (like 15% of my current sample which wasn’t powerful in SPSS).\n\nAny guidance would be super appreciated! I’m kind of navigating statistics on my own as someone with little formal stats education, so thanks in advance for your patience with any dumb questions. :)",
        "created_utc": 1674688170,
        "upvote_ratio": 1.0
    },
    {
        "title": "statistics project — football (soccer)",
        "author": "AlphaDHarris",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lce1i/statistics_project_football_soccer/",
        "text": "Hi Reddit\n\nI’ve got a project coming up for my statistics module at uni, it involves analysing data — can be to do with anything, and we are encouraged to follow our interests, so I want to do something to do with football (soccer)\n\nThe trouble is I’m not really much of an “ideas” person — I’ve had some vague thoughts about maybe looking at something to do with xG, or transfer fees, or how players are valued, but no real concrete ideas. My professor suggested looking at how home advantage basically disappeared during lockdown with there being no fans at the grounds, and whether it has returned since. \n\nBut anyway I know you guys are a lot more creative than me, so can anyone suggest any ideas for this project? What would be a cool/interesting statistical football project, ideally something that hasn’t really been looked at before? \n\nMany thanks for your help and look forward to hearing your cool ideas",
        "created_utc": 1674687370,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you segment this tri-modal curve, and what are the rules to do so?",
        "author": "Motor-Performance-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10lc0l5/how_do_you_segment_this_trimodal_curve_and_what/",
        "text": "[I've regressed the log of the average deal size to the frequency, and I'm seeing this.](https://imgur.com/a/3EvHKOn)\n\nThere's a huge right-skew to it. What are the rules to segment this into the \"harmonics\" or the \"fundamental frequencies?\"",
        "created_utc": 1674686406,
        "upvote_ratio": 1.0
    },
    {
        "title": "VECM model",
        "author": "Rude_Scene_8523",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10la0lz/vecm_model/",
        "text": "Hi! How do I check if VECM model coefficients are significant?\nI have estimated a VECM model. It reports standard error and t-statistic instead of the P-value. So how do I check if the coefficients are significant using the t-statistics?\n*I'm using Eviews for my estimations",
        "created_utc": 1674681494,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should I do master in artificial intelligence or statistic.",
        "author": "DesperateSandwich256",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l88in/should_i_do_master_in_artificial_intelligence_or/",
        "text": "I will finish my bachelor in applied mathematics this summer and I want to work in statistic field in the future.\n\nWould a master in AI also give me the same chance as a master in statistic? Since I read a post online and it is basically that machine learning is also statistic, only with difference terms. \n\nWhat is the cons and pros for each master?",
        "created_utc": 1674677164,
        "upvote_ratio": 1.0
    },
    {
        "title": "Considering a masters in stats after working as a trader for half a year. Any advice?",
        "author": "pointless-bean",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l7y6v/considering_a_masters_in_stats_after_working_as_a/",
        "text": "I graduated in 2022 May and started to work as a trader at a quant trading firm in August. I was placed in a role that was very different than what I originally anticipated and am now considering going back to school for statistics. Although I was a computer science major in college, I’ve usually preferred statistic courses at my university.\n\nI went to a reputable public university in California but did not try very hard in a lot of my classes which resulted in me getting approximately a 3.4 gpa, which makes me very hesitant to apply.\n\nI’ve been thinking about going back to school for a while now and was wondering if anyone had any advice on here.",
        "created_utc": 1674676506,
        "upvote_ratio": 1.0
    },
    {
        "title": "1/8",
        "author": "Elite_Smash44",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l7r3k/18/",
        "text": "what are the chances of winning 1/8 chances 3 times in a row (i'm too lazy to know figure out how to put it in a calculator)",
        "created_utc": 1674676038,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is a meta regression and sensitivity analysis on a meta analysis?",
        "author": "Agile-Escape-8080",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l6tte/what_is_a_meta_regression_and_sensitivity/",
        "text": "Hi, I’m doing a systematic review and I have no prior knowledge of statistics or software. I’ve been watching YouTube vids but I still can’t quite understand. Could someone explain the concepts simply? \nI’d be very grateful",
        "created_utc": 1674673889,
        "upvote_ratio": 1.0
    },
    {
        "title": "Applying predictive analytics to build NBA 4th quarter scoring projections",
        "author": "Leakypodz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l4at2/applying_predictive_analytics_to_build_nba_4th/",
        "text": "This is my first time posting here so if I’m in the wrong place let me know. I by no means have any statistical training other than a couple college courses and books I’ve been reading.\n\nI’m trying to build a model to predict 4Q scoring in NBA games. I would like to build a 4Q power ranking system and be able to plug two teams into it so I can produce an answer of whether these teams against each-other will increase or decrease their scoring pace in the 4th. \n\nI have currently been logging each NBA box score into excel and then finding each teams mean points per quarter. I understand these are descriptive statistics and although on average I have found teams score less in the 4th than the first 3 quarters it doesn’t necessarily give me any predictive power. \n\nI am hoping to be pointed in the right direction of what kind of function I can apply to accomplish this or at least get me started. Thanks.\n\nI’m using Excel BTW.",
        "created_utc": 1674667989,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting chi square results for an analysis comparing 2 groups on an outcome variable with 5 response options…",
        "author": "hello1397",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l47ye/interpreting_chi_square_results_for_an_analysis/",
        "text": "Hi everyone - I had to run a chi square test assessing differences between two groups on an outcome variable with 5 response options. There were significant associations, which I know how to report. I’m just wondering how to really interpret the specific results for each individual response category between groups (e.g., strongly agree, slightly agree, neither agree nor disagree, slightly disagree, strongly disagree). \n\nJust because for one response category (e.g., strongly agree) group A has a higher proportion than group B, doesn’t mean group A is more likely to strongly agree overall…I hope this makes sense. Thank you.",
        "created_utc": 1674667806,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to compare data from two single case studies",
        "author": "No-Context-6966",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l3mi4/how_to_compare_data_from_two_single_case_studies/",
        "text": "Hey guys!\n\nI am currently looking to compare questionnaire data from two single case studies (from a total of two people). How can I do this?\n\nThe questionnaire includes Likert scale questions and multiple-choice questions.\n\nthank you so much… it's my first time doing this and i have no idea what im doing",
        "created_utc": 1674666327,
        "upvote_ratio": 1.0
    },
    {
        "title": "I have a problem",
        "author": "Vegetable_Number3369",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l3dvd/i_have_a_problem/",
        "text": "So basically i am doing a research where i am comparing a variable before and after. The problem is according to my data my variable before is normally distributed whereas the same variable after isn’t. So what descriptive statistic method should i use? And are they even comparable?",
        "created_utc": 1674665762,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mathematical Statistics - Margin of Error Formula: Why is the maximum width of the confidence interval used?",
        "author": "marko_v24",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l0zaq/mathematical_statistics_margin_of_error_formula/",
        "text": "I was confused on the formula for the margin of error. In the derivation, we have a (k/n)(1-k/n) term, which the book then says has a maximum value of 1/4 when k=n/2, and replaces this part of the expression with 1/4.\n\nI am confused on why we do this. If we already know k (number of successes) and n (total samples), why don’t we just use them to arrive at a more accurate margin of error, since we do in fact use them when calculating confidence intervals. I get that the expression is only an estimate for the standard deviation of the binomial distribution, but why did we choose to use the expression when calculating confidence intervals, but not use it for the margin of error?",
        "created_utc": 1674659816,
        "upvote_ratio": 1.0
    },
    {
        "title": "Suggestions for correlating sleeping with performance?",
        "author": "Buzz_Cut",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l0thl/suggestions_for_correlating_sleeping_with/",
        "text": "I've taken a basic statistics course so I have a little background but probably not enough for what I would like to do.\n\nBasically I have two datasets, one of my sleep patterns for the past 2 years and the other is a year's worth of performance data in a video game I play. I would like to see if there's a correlation between the two, but if I did I direct comparison I think there wouldn't be that interesting of a pattern because I gradually have improved over time independent of sleep. \n\nI guess what I'm interested in is there an association between my relative performance on any given day in comparison to how I was doing around the same time. Maybe this isn't possible, but if anyone has some ideas I would appreciate it!",
        "created_utc": 1674659408,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help!! Need Help Learning Path Analysis (Results Interpretation)",
        "author": "santiago-laura",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10l0b1t/help_need_help_learning_path_analysis_results/",
        "text": "Hey everyone. I'm a doctoral candidate finishing up my dissertation and I need to be able to speak intelligently about my analysis. I already carried it all out and it came out mixed, as most dissertation research likely does.\n\nHere's a recap: one exogenous variable, two sets of mediator variables (each set represents a different theoretical framework), and one endogenous variable.\n\n1. I had to check, because of my hypotheses and lit review, that the direct relationship exists, so I ran a model with just the exogenous variable and the endogenous variable. The fit stats, unsurprisingly, are bonkers. However, I wasn't concerned because this was just a base model before I added the other variables. I just need help explaining that this is an acceptable decision (and if it's not, some help with what to do about that). (Base Model 1)\n2. Next I ran 2 separate models, one for each of the theoretical frameworks, checking to see which mediators were actually predicting, based on the data. (Base Model 2 and Base Model 3)\n3. Next I put all the mediators into one model, terrible fit because of the non-significant mediators (Full Model 1)\n4. Removed the mediators and only left the significant ones, fit improves (Full Model 2)\n5. Removed one more mediator that did not remain significant in the overall model, fit improves (Full Model 3)\n6. Allowed covariance of the last standing mediators, final model (Full Model 4)\n\nWith a layman's understanding of path analysis, I get all of this and I've seen other researchers run an analysis similarly. However, I need to be able to speak to the model fit at each stage and explain why it looks weird, why it looks bad, etc.\n\nSpecifically, the problem: for the initial base model, chi squared is .001, but non-significant. df =2. CFI = 1, TLI = 1.039, RMSEA = 0. Both the base models are pretty similar. Once I add everything, the fit statistics are more interpretable for me. Added the model fit statistics particulars at the bottom\n\nI'm digging around in statistics text books, articles on the topic, even message boards where similar questions have been posted, but I can't figure out how to explain this in a way that I understand. My layman-terms assumption is that because the initial model is so basic (just an IV, a DV, and a control variable), SEM is throwing weird fit statistics because there's really not much room for changes in the parameters. I can't say that during my defense though, and I need to be able to cite something in my paper. For the two other base models, I have even less of an explanation--I believe it's throwing weird fit statistics because of the insignificant mediators, but wouldn't it just show poor fit? Why are the stats all 'good fit' yet insignificant chi square, instead of showing poor fit?\n\nAny articles or cites you've got would be great, I can do the reading on my own to save you time. Would really really appreciate some layman terms explanations though. TIA\n\nModel\t                  x2\t       Df\tsig.\t        CFI\t           TLI\tRMSEA\t    95% CI\t\tSRMR\tc2Δ\n\nLL\tUL\n\nBase Model 1a\t0.001\t2\t.999\t1.000\t1.039\t.000\t.000\t.000\t.001\n\nBase Model 2b\t3.075\t8\t.930\t1.000\t1.013\t.000\t.000\t.018\t.013\n\nBase Model 3c\t10.114\t10\t.431\t1.000\t1.000\t.006\t.000\t.057\t.023\n\nFull Model 1d\t409.172\t28\t.000\t0.836\t0.683\t.193\t.176\t.209\t.110\n\nFull Model 2e\t54.378\t10\t.000\t0.952\t0.905\t.110\t.082\t.139\t.051\t354.794\n\nFull Model 3f\t       31.632\t7\t.000\t0.962\t0.924\t.098\t.065\t.134\t.041\t22.746\n\nFull Model 4g\t5.074\t6\t.000\t1.000\t1.003\t.000\t.000\t.062\t.017\t26.558\n\na Base Model: Value Congruence to Engagement\n\nb Base Model with JDR Mediators\n\nc Base Model with SDT Mediators\n\nd Full Model: All Mediators\n\ne Full Model Without Non-Significant Mediators\n\nf Full Model Without Non-Significant Meidators or Autonomy\n\ng Full Model WIthout Non-Significant Mediators or Autonomy, Allowing Competence to Covary with Coaching",
        "created_utc": 1674658083,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about fitting a VGAM model giving a dataset",
        "author": "CryptographerIll9211",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kxtie/question_about_fitting_a_vgam_model_giving_a/",
        "text": "Does anybody know or have examples on VGAM model? I have a dataset with different variables and I am trying to fit a VGAM model to it on R.\n\nDo you have any suggestions? Thank you in advance.",
        "created_utc": 1674650776,
        "upvote_ratio": 1.0
    },
    {
        "title": "FDR = 1 for all tests using BH. Something wrong with my package or is it really so?",
        "author": "c00kieRaptor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kwb48/fdr_1_for_all_tests_using_bh_something_wrong_with/",
        "text": "I am performing RNA sequencing analysis using [edgeR](https://www.rdocumentation.org/packages/edgeR/versions/3.14.0) in R. The package contains a way to perform Benjamini-Hochberg (BH) test to control for false discovery (FDR) for each gene. \n\nDoing this I get that for some versuses, an FDR = 1.00 for every single gene. I would assume that this is extremely unlikely. I worry that there could be something wrong with how [edgeR](https://www.rdocumentation.org/packages/edgeR/versions/3.14.0) performs the BH calculations, but I'm not skilled enough in statistics to calculate this by myself to test. For most versuses it calculates what's to be expected.\n\nCould anyone explain to me how BH works?  \nOr better yet, have used edgeR and found the same results?\n\nThanks in advance!",
        "created_utc": 1674645306,
        "upvote_ratio": 1.0
    },
    {
        "title": "Fisher's? Chi-squared? Something else?",
        "author": "dyep49",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kl0r0/fishers_chisquared_something_else/",
        "text": "I'm interested in determining whether there is a relationship between race and arrest rates in a city.\n\nSay the city has 1,000,000 people, 100,000 of whom are green. \n\n100 people were arrested for a particular charge and 25 of them were green. \n\nIs this sample size sufficient for Chi-squared or is Fisher's a better test in this instance?\n\nIs this the correct way to format a 2x2 table for a [Fisher's exact test calculator](https://www.medcalc.org/calc/fisher.php)?\n\n\n | Arrests | Population\n---|---|----\nGreen | 25 | 100000\nNon-green | 75 | 1000000\n\nThank you!",
        "created_utc": 1674606265,
        "upvote_ratio": 1.0
    },
    {
        "title": "Seeking advice for appropriate model for fitting count data from manufacturing sector",
        "author": "hesperoyucca",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kk7w8/seeking_advice_for_appropriate_model_for_fitting/",
        "text": "Hi All,\n\nI'm mulling how to approach a project I've been assigned at my work relating to a manufacturing process. In this process, over some months, there are three kinds of warnings that are flagged. We'll label them by grades of 1, 2, and 3 in this case in order of severity. \n\nWhat is of interest to stakeholders is if an absolute count threshold is exceeded for any kind of these errors in any given month. The threshold is different for each severity of error, with the the monthly threshold being lower for Grade 3 events rather than Grade 1 events. So, for example, company leadership wants the process to be flagged if in some month there are 100 Grade 1 events, 20 Grade 2 events, and 10 Grade 3 events.\n\nWith this being ordinal data, one way I could model this would be with an ordered logit/probit model. However, as the proportion of events per month is presently less important to the stakeholders, I was thinking that it may be more appropriate to partition the data by grade into univariate time series and then start by running a Poisson time series regression for each grade. If overdispersion seemed present, I was thinking of opting for a generalized Poisson regression, as the negative binomial interpretation of binary failures and successes does not make sense in light of there being multiple failures in the overall process.\n\nIs this approach appropriate or very misguided? I appreciate your time and input, thank you very much.",
        "created_utc": 1674604180,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is a simple linear regression the best solution?",
        "author": "MostEntrepreneur2530",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kj7fy/is_a_simple_linear_regression_the_best_solution/",
        "text": " \n\nI have data on the daily returns from April 2020 - May 2020 for several tech stocks. I also have data on the daily amount of shares bought and sold from February 2020 - March 2020. I am trying to investigate whether or not the shares bought and sold have any impact on the daily returns.\n\nMy approach is to do a simple linear regression of daily returns on net shares bought (i.e., bough - sold). However, I am not really sure and wanted to see if anyone in the community had a smarter or better approach?",
        "created_utc": 1674601566,
        "upvote_ratio": 1.0
    },
    {
        "title": "Q: if 15 out of 100 products failed quality control (15% failure rate) how many products out of the next 100 need to be randomly selected to be fixed to be 95% sure that the failure rate has been brought down to 2% post-quality control?",
        "author": "yousmelllikebeets",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kinhd/q_if_15_out_of_100_products_failed_quality/",
        "text": "Thanks for your help, my quant guy at work is blowing me off an I haven’t done math since high school.",
        "created_utc": 1674600200,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to compare three dependent correlation coefficients?",
        "author": "OlSmokeyZap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kimmf/how_to_compare_three_dependent_correlation/",
        "text": "\nI have done some data collection on Hermit crabs. I have recorded the shell aperture, the shell slope length, and then calculated the volume of the shell using formula for a a cone. I have collected the distance each crab travelled in a given period of time. From this, I have obtained 3 correlation coefficients. Obviously, they are dependent, as they all use the same crabs and distance. I am thinking, and have asked here about using Fisher’s Z-test. Is there anything better or more suitable?",
        "created_utc": 1674600142,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to use Fisher’s Z-transformation to compare Correlation Coefficients?",
        "author": "OlSmokeyZap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kij0k/how_to_use_fishers_ztransformation_to_compare/",
        "text": "I have done some data collection on Hermit crabs. I have recorded the shell aperture, the shell slope length, and then calculated the volume using a cone. I have collected the distance each crab travelled in a given period of time. From this, I have obtained 3 correlation coefficients. Obviously, they are dependent, as they all use the same crabs and distance. I want to use Fisher’s Z-transformation. I know it is just for two correlations, but I can compare each one to the other two. How do I do it, and how do I  plot and interpret this data?",
        "created_utc": 1674599893,
        "upvote_ratio": 1.0
    },
    {
        "title": "Homework help: can anyone please help me learn how to do this?",
        "author": "benjimychild",
        "url": "https://i.redd.it/bx999024m3ea1.jpg",
        "text": "",
        "created_utc": 1674594803,
        "upvote_ratio": 1.0
    },
    {
        "title": "ELI5: Independent t-tests on change scores between groups",
        "author": "hello1397",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kfq1w/eli5_independent_ttests_on_change_scores_between/",
        "text": "Hi all,\n\nI ran a pre-post test study. My advisor asked me to run an independent samples t-test on the change scores between groups.\n\nI’m a little confused on what an independent samples t-test on change scores between group 1 and 2 would tell me, other than the two groups are different (or not) from each other? For example, if one group’s score increases and the other group’s score decreases and it’s significant at p&lt;.001 … what is this really saying? I would understand this better with a paired t-test in one group on pre to post, but confused at what this is telling me. Thanks !",
        "created_utc": 1674593110,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which other instruments could be feasible?",
        "author": "willlael",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kfkvx/which_other_instruments_could_be_feasible/",
        "text": "Hey guys, I'm reading this paper (The Effectiveness of Health Screenings by Hackl et al.) and was wondering what other possible instruments could have been used here to implement such study in R. Do you guys have any ideas?",
        "created_utc": 1674592765,
        "upvote_ratio": 1.0
    }
]