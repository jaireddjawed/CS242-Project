[
    {
        "title": "Question about Wilcox Signed Rank",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kvfnu/question_about_wilcox_signed_rank/",
        "text": "[deleted]",
        "created_utc": 1526849306,
        "upvote_ratio": ""
    },
    {
        "title": "What statistical test should I use",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kuvz0/what_statistical_test_should_i_use/",
        "text": "[deleted]",
        "created_utc": 1526844520,
        "upvote_ratio": ""
    },
    {
        "title": "regarding the validity of a trend line and R2",
        "author": "Sunowo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kunhk/regarding_the_validity_of_a_trend_line_and_r2/",
        "text": "I have to say that I'm very new to statistics so maybe what I'm asking is stupid...:\n\nLet's imagine that I want to know if my tree gives more and more apples as time goes by. I have two variables: X (unit of time, say week), and Y (number of apples). I have 100 weeks on record. When I use the excell to create a scatter plot and add the straight trend line to it, I get a growing line, but I've heard that the R2 value is important to determine if the line is valid or not, and my R2 value is 0.1 which seems very low... My question is: does this mean that my trend line is misleading? or can I say without fear that the number of apples per week is growing and even say how much? I'm really lost in this...\n\nThank u so much in advance!",
        "created_utc": 1526842424,
        "upvote_ratio": ""
    },
    {
        "title": "Thinking of starting over...",
        "author": "RocketTwitch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kumg5/thinking_of_starting_over/",
        "text": "\\(For a friend\\)\n\nSo  I'm thinking of starting over with a career using math. I got my  undergraduate degree in English and have been working in HR for the last  4 years. I really don't like my current job and want to do something  more fulfilling with my time. Ideally, I'd like to work for a non\\-profit  with some issues that are important to me.\n\nThis  year I decided to do something about it an I took an introductory  statistics course. I've always been good at math but was never  encouraged to pursue it when I was in high school. So far, I have  absolutely loved it. What I'd like to do is get a job for a non\\-profit  dealing with an issue that is important to me. But at this point I don't  see a clear path on how to get there.\n\nWhat  are some things I can do with statistics? What classes would be  valuable to take? Do I need to get a full undergraduate degree? Or is  there a better route?\n\nIn a few months I  plan to go part time at my current job so I can begin working towards my  new career path. Are there any good online schools that you all would  recommend? I've also been told that data science uses a lot of  statistics. Could someone explain the difference between the two? And  would it be advisable to look into a career in data science if I want to  work for a non\\-profit?\n\nAny and all advise is greatly appreciated.",
        "created_utc": 1526842172,
        "upvote_ratio": ""
    },
    {
        "title": "Fucking confused on response and explanatory variables???",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8krdde/fucking_confused_on_response_and_explanatory/",
        "text": "[deleted]",
        "created_utc": 1526803876,
        "upvote_ratio": ""
    },
    {
        "title": "Sequential Monte Carlo (SMC)",
        "author": "Jcm487",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8krb7w/sequential_monte_carlo_smc/",
        "text": "So I recently learned a bit about sequential monte carlo (SMC) and was told by my instructor that the method is particularly useful when you want to keep updating distributions -- you can take a sample from f_1(x) to f_2(x) and then update again to f_3(x). He said that this arises, for example, if you are tracking a noisily observed process like a random walk . \n\nHe said, however, that if we only want to sample/generate from a single distribution, this method may not be a good idea/approach. Did not really go into much detail however.  \n\nWhy would SMC not be such a good idea to use if we only want to sample/generate from a single distribution? \n\n\n\n\n\n",
        "created_utc": 1526802859,
        "upvote_ratio": ""
    },
    {
        "title": "Logarithmic binning technique question",
        "author": "abstractquatsch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kqn8y/logarithmic_binning_technique_question/",
        "text": "Hi everyone!\n\nI’m trying to sort data into 20 logarithmically-spaced bins in order to make a histogram. I’ve been looking up different techniques, and I’m confused. I feel like the answer to my question is simple but it’s going over my head right now.  I’m using MATLAB for this. I’m not sure which “technique” is best practice. \n\nMethod 1: use the `logspace` function to generate a logarithmically-spaced vector and plot my data against this. \n\nMethod 2: define bins linearly-spaced, get counts, and then plot the log(bins) and the counts.\n\nIs any method here correct? Am I missing a better one? Any help is appreciated.  Thanks!",
        "created_utc": 1526792405,
        "upvote_ratio": ""
    },
    {
        "title": "What method(s) of analysis may be appropriate for comparing female heights to male heights, given the sample drawn?",
        "author": "stone_97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kq72y/what_methods_of_analysis_may_be_appropriate_for/",
        "text": "Doing a project right now and am comparing the height of the typical male and female height from my sample to the average height of the height of the population of male and females from the United States. I'm not sure exactly how to go about this question. Am I suppose to do a hypothesis test and do a T-test? Sorry, stats is clearly not my strong point.  ",
        "created_utc": 1526786532,
        "upvote_ratio": ""
    },
    {
        "title": "Probability for a Single Continuous Predictor (Logistic Regression)",
        "author": "WholeSortOfMishMash",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kopd3/probability_for_a_single_continuous_predictor/",
        "text": "Hi all,\n\nIf I have a data set and I am trying to create a logistic regression for an output Y = 0 or 1, where the predictor X is a continuous variable, how do I calculate the value P(Y=1 | X=x)? \n\nI know that if X is categorical, then for each category, I take the number of points where Y = 1, and divide it by the total points of that category, but is there a way to do this for a continuous X? \n\nThank you in advance!",
        "created_utc": 1526769843,
        "upvote_ratio": ""
    },
    {
        "title": "Time series decompositions",
        "author": "ztnq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ko07f/time_series_decompositions/",
        "text": "I was reading about time series decompositions, and different sources claim different  decompositions. All of them include the noise, trend, and seasonal components. But there  is always something different\n\nhttps://en.wikipedia.org/wiki/Seasonal_adjustment\nhttps://en.wikipedia.org/wiki/Decomposition_of_time_series\n\"\n\n    St: The seasonal component\n    Tt: The trend component\n    Ct: The cyclical component\n    Et: The error, or irregular component.\n\"\nhttps://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n\n\"\n\n    Level: The average value in the series.\n    Trend: The increasing or decreasing value in the series.\n    Seasonality: The repeating short-term cycle in the series.\n    Noise: The random variation in the series.\n\"\nThis one seems to include the level component instead of the cyclical component\n\n\n\nAnd then:\nhttp://www.abs.gov.au/websitedbs/D3310114.nsf/home/Time+Series+Analysis:+The+Basics\n\n\"In the additive model, the observed time series (Ot) is considered to be the sum of three independent components: the seasonal St, the trend Tt and the irregular It. \"\n\nSo no 4th component here.\nHow do I reconcile these differences?\n\nThanks\n",
        "created_utc": 1526763004,
        "upvote_ratio": ""
    },
    {
        "title": "Transform \"1 in X\" odds in a lifetime to yearly odds.",
        "author": "zehcoutinho",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8knw3c/transform_1_in_x_odds_in_a_lifetime_to_yearly_odds/",
        "text": "Hi, I saw an interesting article in Forbes about the odds of dying from different stuff, like 1 in 303 for car crashes. The article didn´t explicitly say this was in a lifetime, and their link to National Safety Council source is 404, so I´m just guessing it is. How would I transform that to yearly odds? I guess I´d have to use life expectancy, and intuitively think I should multiply the 303 in the car crash example for the number of years of life expectancy, but I suspect it may not be as simple as that. Can anyone help me? Thanks in advance. ",
        "created_utc": 1526761904,
        "upvote_ratio": ""
    },
    {
        "title": "My samples are not independent of one another, I want to find correlations",
        "author": "FireBoop",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kmbex/my_samples_are_not_independent_of_one_another_i/",
        "text": "1. Hi, I have a dataset. 40 subjects did 80 trials. I am trying to find correlation between features of the same trial. ie is every trial has a feature A and feature B and I want to see if A and B are correlated.\n2. I am worried that the 3200 trials being the result of only 40 subjects is problematic. I'm worried biases related to each subject may mess things up. ie subject 1 has a higher A and B than normal. This may create an illusion of A and B being correlated.\n3. Furthermore, I want to find if any subject features \\(which are  the same across all trials\\) mediate any relationship between A and B. For instance, the subject feature of \"depression score\" \\(feature C\\) would be 4 for all of subject 1's trials, this score would be 9 for all of subject 2's trials, etc. So each sample would have feature A, B, C. I don't think I can just simply find the correlation between AC and B because there is no variance of C within each subject. \n4. Trying to find the correlation between AC and B gives me tests of extremely high significance for many different variables C. \\(Testing 12 different features of C, gives me about 20&amp;#37; where C acts as a mediator variable with p \\&lt; 0.001\\). I figure this abundance of significance is due to the lack of variance of C within each subject...\n5. Should I be controlling for how each subject's biases interact with B? How would I do this?\n\nThank you for reading this far",
        "created_utc": 1526747313,
        "upvote_ratio": ""
    },
    {
        "title": "How to Best compare year over year data for patterns",
        "author": "Dirk-Hardpeck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8km721/how_to_best_compare_year_over_year_data_for/",
        "text": "I’ve read the “choosing the correct stat test…” link, but forgive me, it’s been a while since I’ve had to apply my stats class and I’m a little lost. Is there a best way to analyze data year over year, when dates don’t align due to business days and holidays, when looking for patterns? I’m not necessarily looking for a measure of seasonality/cyclicality, I’d rather be able to highlight a period (or periods) of time, year over year, in the data is highly correlated. The data set is daily sales for the last ten+ years, closed on weekends and holidays. My intuition says I could visually analyze data, layering charts over each other, or I could pick a sample time frame (e.g. five days) and compare the rolling five days against the same time period in a prior year, and look for high correlation. Both methods seem very inefficient, comparing several years with a rolling period would create an overwhelming correlation matrix compounded by the problem of arbitrarily selecting the rolling period, 5 days vs 6 days etc. I appreciate any suggestions.",
        "created_utc": 1526746161,
        "upvote_ratio": ""
    },
    {
        "title": "Why is divorce rates across sexes not equal?",
        "author": "Africa-Unite",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8km4wp/why_is_divorce_rates_across_sexes_not_equal/",
        "text": "While reading a report on early marriage in Ethiopia, I came across this stat:\n\n&gt;\" In Ethiopia, marital dissolution is not  especially stigmatised, with women having higher rates of divorce or separation than men \\(7.5&amp;#37; compared to 2.5&amp;#37; among the 15\\-49 years age cohort\\)\"\n\nFor some reason, I can't workably wrap my head around this. Assuming a closed system of 100&amp;#37; heteronormativity, wouldn't divorce rates be equal across genders? The only thing that counters that is the thought that more women in this closed system are married than men, which if is the case, makes the divorce by gender count a moot stat.\n\nCould someone explain to me why I am dead wrong in this thinking, or provide more logical scenarios?",
        "created_utc": 1526745619,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone help with this problem?",
        "author": "[deleted]",
        "url": "https://i.redd.it/d3xyx7s9zsy01.jpg",
        "text": "[deleted]",
        "created_utc": 1526731585,
        "upvote_ratio": ""
    },
    {
        "title": "Time at risk problem",
        "author": "kofnim",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kkj50/time_at_risk_problem/",
        "text": "What kind of statistical method do you use if you want to calculate time at risk?\n\nfor example: we have a retrospective data\\-base study where we have 300 diabetes patients. They were diagnosed at different point of time, some 1970 and some as early as 2017 \\(range is 47 years\\), and we want to know how many of them will experience an heart attack. Some will not experience an heart attack because they were newly diagnosed and some will more likely experience it due the time at risk. Is there any statistical method to calculate this time at risk?",
        "created_utc": 1526727127,
        "upvote_ratio": ""
    },
    {
        "title": "Model comparison of non-statistical models",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kk4fc/model_comparison_of_nonstatistical_models/",
        "text": "Let's say I have two models that try to predict  touch interface interaction times. I have two libraries wherein duration estimates for serveral movements are stored. \"pressing a button takes 0.5 seconds\", performing a button turn takes 0.85 seconds. The other library has sliglty differen duration estimates and splits some movement in finer parts. tunring a knob might be split in \"turn the knob\" and \"reach back again\". If I try to predict how long it will take someone to do task a, where you normally need to press twice and turn a button twice I simply need to calculate press\\+press\\+turn\\+turn for model one and Press\\+Press\\+turn\\+return\\+turn to predict the times.\n\nBoth models will slightly disagree. How do I assess whether one model is better? Can I calculate measures Like BIC  or AIC for this as it is standard for model comparison in statistics?\n\nI calculated the hit rate, false alarm rate, RMSD and correlation of each model with the empirical data.",
        "created_utc": 1526720485,
        "upvote_ratio": ""
    },
    {
        "title": "Why exactly is it important to present population referencing statistics using the *per 100,000* convention?",
        "author": "DousedSun",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kjpk1/why_exactly_is_it_important_to_present_population/",
        "text": "",
        "created_utc": 1526713487,
        "upvote_ratio": ""
    },
    {
        "title": "Basic Linear Regression / Excel Question",
        "author": "Fijas",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kiarh/basic_linear_regression_excel_question/",
        "text": "Hello!  I have a few basic linear regression questions:\n\nSuppose I have 50 line segments, each utilizing two points.  While the y values for each point may vary, half of the x values are 0, and the other half are 2 \\(so 50 points have \\(0, y1\\) and 50 points have \\(2, y2\\)\\).  I would like to calculate the mean slope for these 50 lines, as well as find the 95&amp;#37; confidence interval for the slope.\n\n1\\)  For my sample size, do I use n=50 \\(the number of line segments\\), or n=100 \\(the number of data points\\)?\n\n2\\)  I read somewhere that the 95&amp;#37; confidence interval for the slope can be expressed as the mean coefficient \\+\\- z \\* sigma \\* n\\^\\-0.5, where z = 1.96, sigma is the standard error of the coefficient, and n=sample size.  Is this correct?\n\n3\\)  I'd like to do this regression in Excel.  I've used the LINEST function, feeding it my 100 data points, 50 of which are \\(0, y1\\) and 50 of which are \\(2, y2\\).  In terms of the arguments, I've fed in as the x\\-series all 100 x\\-values \\(50 zeros, followed by 50 twos\\), and then as the y\\-series all 100 y\\-values \\(the first 50 being y1 and the second 50 being y2\\).  I intend to use the SE\\(Slope\\) output from LINEST as my standard error of the coefficient, which I then use to calculate my 95&amp;#37; confidence interval, according to the equation in 2\\).  Does this sound correct?\n\nThanks for your help!\n\nFijas",
        "created_utc": 1526695714,
        "upvote_ratio": ""
    },
    {
        "title": "Test for measure Predictive Power",
        "author": "WholeEWater",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kfk8o/test_for_measure_predictive_power/",
        "text": "Hi guys,\n\nI have a model that outputs probabilities for three discrete segment assignments for each customer.  These segment assignments are modeled against what the true segment is for that customer.  So my question is this:  What kind of test do I run to evaluate the predictive power of the model's probabilities?  Naturally, I'd like my highest probability to be as close to 1 as possible to the true segment.\n\nThanks in advance.",
        "created_utc": 1526669553,
        "upvote_ratio": ""
    },
    {
        "title": "Business Problem - Boss asking me to deliver stuff I have no knowledge on",
        "author": "AcrobaticTip",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kfg63/business_problem_boss_asking_me_to_deliver_stuff/",
        "text": "Hi,\n\nI work for a small sales company (mainly as an accountant). \n\nThe Sales person has to log the first point of contact with a potential client.\n\nThey also have to log the last point of contact (whether a sale is made or not).\n\nWe track how long it takes from start to finish. \n\nMy boss wants to see if the salespeople spend longer with clients with potential sales that are bigger. \n\nI have taken the averages but he wants more.\n\nHe wants to see if we are spending too much time chasing a lost cause just because the potential sale is big.\n\nAny ideas?\n\nThanks\n\n",
        "created_utc": 1526668615,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics (I think) and Record Predictions",
        "author": "smallchimp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kepgd/statistics_i_think_and_record_predictions/",
        "text": "In r/nfl, someone posted a record prediction for the upcoming season. Each of the 32 teams plays 16 games leading to a total game count of 256 \\(16 two\\-team contests in 16 weeks\\). 6 of the 16 games a team plays are against the other three teams in their own division \\(2 games per division rivals\\);\\(there are 8 divisions of 4 teams each\\). Obviously the most average record across the league would be 8\\-8, and the most extreme would be 16\\-0 for half of the teams and 0\\-16 for the other half of the teams. Whatever happens, a league average of .500, or 50&amp;#37;, will occur. \n\nGiven the need for divisional games to impact the record of division members \\(4 teams of the same division cannot have 16\\-0 or 0\\-16 records because one win for one team is a loss for another team\\) as well as one team's loss is another team's win in the other 10 games, how would I go about finding the plausibility of a record prediction? A NFL fan would probably be the most able to help me solve this, but I'm sure there's math to it that doesn't require football knowledge. \n\n[Here is the thread](https://old.reddit.com/r/nfl/comments/8kbnqw/my_personal_view_of_where_all_32_teams_are_at_and/)\n\n[Here is a spreadsheet comparing records](https://docs.google.com/spreadsheets/d/1gOAdN-Bi8qkgnPR0pSMYUSikDofCcI9LJkSjrlY7_nM/edit#gid=0)\n\nI can't imagine it's necessary to analyze each team's scheduled opponents \\(well at least for you guys\\), I am more in the market to find out how I'd go about tackling this math problem. ",
        "created_utc": 1526662734,
        "upvote_ratio": ""
    },
    {
        "title": "Going against the conventional: Having alpha at 0.3",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kc5l1/going_against_the_conventional_having_alpha_at_03/",
        "text": "[deleted]",
        "created_utc": 1526639508,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics Project (German)",
        "author": "mahasi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kbv8p/statistics_project_german/",
        "text": "Hey there! \nNot sure if this post is allowed under the rules, so I will just try. - If not, I am sorry -  just delete it. \nAny german speaking statistics enthusiasts here that can help me with a statistic project? \nIt is only 2 papes and already done, but the prof. sent it back as not good enough, so I rewrote it and now I would need someone to just read through it, and check it. \nThanks in advance \n",
        "created_utc": 1526635812,
        "upvote_ratio": ""
    },
    {
        "title": "Issues with conducting under-powered trials",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kas6d/issues_with_conducting_underpowered_trials/",
        "text": "[deleted]",
        "created_utc": 1526621031,
        "upvote_ratio": ""
    },
    {
        "title": "Grubb's Test Critical value and Test Statistic the same.",
        "author": "ImaTryMyBest",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8kabyr/grubbs_test_critical_value_and_test_statistic_the/",
        "text": "Hey Statisticians,\n\nI'm just running some stats tests for a chemistry report, and after completely a Grubb's test to figure out if a value is an outlier, but I ended up with my Test statistic (Q-value) and table value being the same. \n\nI have no idea what to do in this situation, any help would be greatly appreciated :) ",
        "created_utc": 1526615760,
        "upvote_ratio": ""
    },
    {
        "title": "Is this an appropriate use of ANOVA?",
        "author": "chaneg",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k9mpm/is_this_an_appropriate_use_of_anova/",
        "text": "I am examining some time series data for commodity prices and I am concerned about whether the assumptions are being adequately met to use a one-way ANOVA test for equality of mean demand between months. Note that I am not that familiar with statistics.\n\nFirst, the data is obviously not normal and not stationary as demand seems to increase in a linear fashion as time *t* increases. To compensate for this, I have taken the time series X(t) (the demand of the commodity at time *t*) and applied the transform Y(t) = X(t) - l(t) where l(t) is the least square line.\n\nFrom Y(t), I have split it into twelve individual time series Y_(t,i) where i is an integer over [1,12] representing the month. I would like to test the null hypothesis that mean(Y(t,1)) = mean(Y(t,2)) = .. = mean(Y(t,12)). \n\nFirst, I am concerned about problems that may arise since there is a discontinuity when t jumps from say May 31 2017 to May 1 2018. Further, I am not sure if the obvious non-normality in the distribution of Y(t) can be safely ignored since the number of samples I have is extremely large (for each fixed i, the set {Y(t,i)} has about 10000 elements).\n\nFrom my reading, I thought that there might be covariance terms causing problems here if I used my entire data set, so I did a random sample 2000 draws from Y(t,i) for each fixed i with replacement.\n\nNext, I tested for quality of variances using Barlette's test, I rejected the null hypothesis that the variances were equal with a p-value of about 0.0011.\n\nGiven that fact, I used Welch's one way ANOVA test and it rather strongly failed to reject the null hypothesis of equal means between months with a p-value of around 0.45. Some additional graphical plotting seems to indicate to me that this is a reasonable conclusion.\n\nHaving said all that, I really don't know all that much about statistics. Is my process statistically sound?\n\nI noticed in many writings, they seem to merely provide an F-statistic without much explanation even when the data does not seem to satisfy the requirements for ANOVA I read in a book. Do people tend to just ignore the requirements and naively use it anyway? Is it typical to just have faith that the correct kind of ANOVA is used, or expect that the author has already tested for equality of variances but omitted those test statistics?",
        "created_utc": 1526608415,
        "upvote_ratio": ""
    },
    {
        "title": "What are the f****** odds?!?",
        "author": "soccercw22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k7xs5/what_are_the_f_odds/",
        "text": "My friend and I were playing the card game war the other day and the craziest thing happened. We got a double war of all fours. I am trying to figure out the odds of that happening. For simplicity, assume we each started with 26 cards. Thanks!",
        "created_utc": 1526592652,
        "upvote_ratio": ""
    },
    {
        "title": "How to determine the number of results to validate in order to be confident in effectiveness of method?",
        "author": "CompanyResearch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k6z66/how_to_determine_the_number_of_results_to/",
        "text": "I wrote an algorithm that compares two objects and decides if they are the same object (match or no match).  There are about 100k comparissons.  Sort of looks like this\n\n    1.  item_1, item_2, match\n    2.  item_1, item_3, no_match\n    3.  item_2, item_4, match\n    ...\n\nIn order to determine if the output match or no match is correct, I need to manually look at each comparison to validate the conclusion.  How do I go about determining how many results to validate in order to be confident in the accuracy of this model? Each comparisson takes about 20s to validate.\n\n\nedit: there are about 10k unique objects",
        "created_utc": 1526585089,
        "upvote_ratio": ""
    },
    {
        "title": "Hierarchical Regression- Adjusted R squared decrease but also significant.",
        "author": "anandoknows",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k5vox/hierarchical_regression_adjusted_r_squared/",
        "text": "hello, Im slightly confused as to how to interpret some findings involving a hierarchical regression.\n\nI am looking at a paper that has conducted a hierarchical regression which involves four steps. With each step (more variables being added) the adjusted R squared decreases indicating to me that the gains from adding another variable are smaller than the costs, per the adjusted R2 method. But im not sure how to interpret it if it looks like these different scenarios (pulled from the paper).\n\nModel 1- Step 1 Ar2 .6(sig), Step 2 Ar2 .03(sig), Step 3 Ar2 .04 (sig), Step 4 Ar2 .02 (sig)\n\nModel 2- Step 1 Ar2 .59(sig), Step 2 Ar2 .002(NONsig), Step 3 Ar2 .01 (NONsig), Step 4 Ar2 .02 (sig)\n\nif you could help me understand how to interpret this that would be amazing. The authors of the paper make no attempt however, I believe it is important to explain and understand.",
        "created_utc": 1526576778,
        "upvote_ratio": ""
    },
    {
        "title": "Given data on many apartments, how to compute the effect of each amenity on apartment value?",
        "author": "kronn8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k5roj/given_data_on_many_apartments_how_to_compute_the/",
        "text": "For example, I'd like to be able to say that having a pool increases the value of an apartment by *at least* 15 $/mo with 90% confidence, by *at least* 10 $/mo with 95% confidence, or by *at least* 8 $/mo with 99% confidence.\n\nIs there a statistical test that outputs a correlation strength in this way?\n\nEdit: I added the \"*at least*\"s in italics.\n\nImagine I have two probability distributions that are roughly Gaussian, plotted on the same axes, with the x-axis representing rent. The distribution on the left (lower rent) would be the without-pool distribution, while the distribution on the right (higher rent) would be the with-pool distribution.\n\nBasically what I want is the *difference or ratio between these distributions*. Something like the [statistical distance](https://en.wikipedia.org/wiki/Statistical_distance). I should end up with another distribution, also roughly Gaussian, centered around ~20 $/mo. The 5th percentile of this distribution would lie at approximately 10 $/mo, meaning that 95% of the apartments with pools have rents at least 10 $/mo higher than apartments without pools.\n\nBonus points if anyone can suggest a multi-variable method that would allow me to control for the presence of other amenities, such as an in-unit washer/dryer.",
        "created_utc": 1526575926,
        "upvote_ratio": ""
    },
    {
        "title": "Finding intra-subject variability (%CV) for sample size calculation in clinical trials?",
        "author": "Fennecat",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k5h1e/finding_intrasubject_variability_cv_for_sample/",
        "text": "Hi, I'm looking for intrasubject variability (aka intra-individual subject variability) figures in the public domain for the calculation of sample size in a 2x2x4 BA/BE study.  The drug in question is divalproex sodium, delayed-release pellets and extended release tablets.\n\nI've found a good source for the ER tablets, page 33 of 40 in https://www.accessdata.fda.gov/drugsatfda_docs/nda/2002/020782_s000_ClinPharmR.pdf\n\nStill looking for DR pellets.  Nothing in the FDA website, or google. Does anybody have any tips in finding this data?  If I can't find, may just use CV% in similar products.",
        "created_utc": 1526573666,
        "upvote_ratio": ""
    },
    {
        "title": "[Textbook] Which textbook should I read next? [x-post /r/statistics]",
        "author": "me-bo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k59jv/textbook_which_textbook_should_i_read_next_xpost/",
        "text": "I have a BSc in math \\(didn't take any stats courses though!\\) but I work in the marketing field now. Our company has a lot of marketing data \\(Adwords primarily\\) and I've been tasked to analyze it, find trends, make bidding recommendations, and possibly build models.\n\nI've read and worked through the OpenIntro textbook, which is great for an initial overview and is very basic.\n\nWhat textbook should I read next?\n\nI'm not interested in grinding through integrals or proving theorems. I'm familiar with R \\- not a pro but I can figure things out as I go along. I'd prefer a book that has solutions to the exercises available \\(either in the back of the book, or somewhere online\\) since I learn best by solving problems. Thank you!",
        "created_utc": 1526572101,
        "upvote_ratio": ""
    },
    {
        "title": "how KL of multivariate Gaussians with diagonal covariance factorizes?",
        "author": "[deleted]",
        "url": "https://stats.stackexchange.com/questions/346758/how-kl-of-multivariate-gaussians-with-diagonal-covariance-factorizes",
        "text": "[deleted]",
        "created_utc": 1526565991,
        "upvote_ratio": ""
    },
    {
        "title": "Minitab Ln Sampling Formula Modified by Failure Percent",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k4hkx/minitab_ln_sampling_formula_modified_by_failure/",
        "text": "[deleted]",
        "created_utc": 1526565825,
        "upvote_ratio": ""
    },
    {
        "title": "What's it all mean (average?)",
        "author": "dcsprings",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k1z9y/whats_it_all_mean_average/",
        "text": "I'm an interested outsider and noticed the recent post about black on white crime. Which makes me wonder about how statistics are used. Are there days when you want to tell people: 90% of the time it's a bad idea to walk in front of a moving bus, for EVERYTHING ESLE use your best judgement.",
        "created_utc": 1526537248,
        "upvote_ratio": ""
    },
    {
        "title": "A question about the exponential distribution",
        "author": "ztnq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k1rz6/a_question_about_the_exponential_distribution/",
        "text": "I'm slightly confused about a situation in the exponential distribution.For the following example:\n\"if time t minutes has passed, whats the probability that I would have to wait t+s or more minutes?\"\nIt is set up as: P(X&gt;=t+s|X&gt;=t). I am not sure why I am using X&gt;=t instead of X=&lt;t?",
        "created_utc": 1526535016,
        "upvote_ratio": ""
    },
    {
        "title": "Dice outcomes question, looking for the proper formula/analysis",
        "author": "Genorb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k1nja/dice_outcomes_question_looking_for_the_proper/",
        "text": "Here is the problem:\n\nI have ten ten-sided dice and twenty twenty-sided dice. That's thirty dice total. I'm looking for the formula that one would use to determine the probability of rolling all the dice and getting:\n\nOne 1\n\nTwo 1's\n\nThree 1's\n\netc\n\nThat's it. Any help is appreciated!",
        "created_utc": 1526533697,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a way to find the Z-Score without the z-score table?",
        "author": "impolitestar",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k1ksy/is_there_a_way_to_find_the_zscore_without_the/",
        "text": "Guys, in my entire semester of class, we've been using the computer for it to input the numbers to us and the finals are coming up and I find out that we're going to have to find z-score without a z-table ?? I think the formula is z=(x-u)/(o) BUT is there more to it afterwards?!",
        "created_utc": 1526532903,
        "upvote_ratio": ""
    },
    {
        "title": "Baseball Statistics Problem",
        "author": "JaysonTatumIsMyDad",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k1bhz/baseball_statistics_problem/",
        "text": "For a history project, I would like to see the percentage of overall players from Virginia in the MLB in any given year. I have the start and end dates of each player from Virginias career and the total number of mlb players from each year, I just need to find the overlap in each year. How would I do this?",
        "created_utc": 1526530316,
        "upvote_ratio": ""
    },
    {
        "title": "if n goes to infinity, how do you prove that a test statistic would converge to a chi squared?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k103a/if_n_goes_to_infinity_how_do_you_prove_that_a/",
        "text": "[deleted]",
        "created_utc": 1526527156,
        "upvote_ratio": ""
    },
    {
        "title": "What does this mean: If we rank 5th in points scored, and 3rd in points against, yet we still win all our games.",
        "author": "TheSkyIsBeautiful",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8k03ml/what_does_this_mean_if_we_rank_5th_in_points/",
        "text": "So this is my intramural team right now. Out of 5 teams, we score the least amount, and we're only 3rd in preventing teams from scoring, yet we've won all our games. In my head this doesn't make too much sense. We score the least, yet we're also not the 1st in preventing people from scoring? shouldn't that mean that at least 1 team should be better\n\nWhat would you be able to say about our team?",
        "created_utc": 1526518547,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate df for two-sample Kolmogorov-Smirnov test?",
        "author": "alnumero",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jz7oe/how_to_calculate_df_for_twosample/",
        "text": "Hi there,\n\nI used R to run this test which doesn’t provide df in its output. Therefore, I’ve been trying to figure out how to calculate the df independently.\n\nI have two groups with 54 samples each. \n\nI’m hoping I’m not being a total dope on this!",
        "created_utc": 1526510553,
        "upvote_ratio": ""
    },
    {
        "title": "NORMAL DISTRIBUTION IN EXCEL FOR FORECASTING?",
        "author": "CaptainWazzleFluff",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jz0cq/normal_distribution_in_excel_for_forecasting/",
        "text": "I'm trying to split a total sales forecast for (e.g.) 5 shops for April based on March results. \n\nOriginal budget (B1) prepared was in January, and an updated forecast (F1) was prepared in March. Updated forecast prepared based on total group income trend rather than on a bottom up shop-by-shop basis \n\nF1 total income for April is forecast to be (say) $20 higher than it was in B1 \n\nIf the group has 5 shops and March income actual results versus budget were: \n- shop1 = +$7 \n- shop2 = +$6 \n- shop3 = +$2 \n- shop4 = -$1 \n- shop4 = -$3 \n\nHow would you allocate the $20 April total group forecast income excess versus budget based on March results? \n\nMy initial thoughts were that it could be done using a normal distribution, assuming that the shops performance versus budget follows a normal distribution. \n\nDoes anyone know how I would do this? \n\nThanks",
        "created_utc": 1526508751,
        "upvote_ratio": ""
    },
    {
        "title": "What's a good way to answer question 1a, without using the formula",
        "author": "Towlss",
        "url": "https://i.imgur.com/Os02O7M.png",
        "text": "",
        "created_utc": 1526504344,
        "upvote_ratio": ""
    },
    {
        "title": "Is it correct to say that a black person is 30x more likely to commit a violent crime against a black person than is a white person to commit a violent crime against a black person?",
        "author": "ej17178",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jy9ik/is_it_correct_to_say_that_a_black_person_is_30x/",
        "text": "I would like to discuss the data in this report. \n\nhttps://www.bjs.gov/content/pub/pdf/rhovo1215.pdf\n\nPertinent facts:\n\n3,679,410 violent crimes against whites of which 14.7% were by blacks = 540,873 cases of black on white violent crime\n\n850,720 violent crimes against blacks of which 10.9% were by whites = 92,728 cases of white on black violent crime\n\n~ 5:1 ratio (black on white violent crimes are over 5x more numerous than white on black violent crimes)\n\nGiven that in the US there are around 240 million whites and around 40 million blacks, the data appear to show that a black person is around 30x more likely to commit a violent crime against a white person than a white person is to commit a violent crime against a black person. \n\nIs this a valid conclusion to draw from the data? \n\nAre there statistical issues that need to be considered before communicating this information?\n\n\n",
        "created_utc": 1526502805,
        "upvote_ratio": ""
    },
    {
        "title": "Importance of reducing pair-wise correlations?",
        "author": "owl_14",
        "url": "https://www.reddit.com/r/statistics/comments/8jxeed/importance_of_reducing_pairwise_correlations/",
        "text": "",
        "created_utc": 1526500055,
        "upvote_ratio": ""
    },
    {
        "title": "Importance of reducing pair-wise correlations?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jxsqa/importance_of_reducing_pairwise_correlations/",
        "text": "[deleted]",
        "created_utc": 1526499273,
        "upvote_ratio": ""
    },
    {
        "title": "Do we have confidence at the 5% level that alternative hypothesis is overestimating?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jxmhq/do_we_have_confidence_at_the_5_level_that/",
        "text": "[deleted]",
        "created_utc": 1526497920,
        "upvote_ratio": ""
    },
    {
        "title": "Probability of getting first choice job",
        "author": "OriginalTinger",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jxgk9/probability_of_getting_first_choice_job/",
        "text": "My wife has applied for a job in medicine. She has been told that after the selection process she is placed 31 out of a total of 533 candidates nationwide. There are 16 jobs for 16 succesful candidates in our area that she has to rank in order of preference. What is the probability that she will get her first choice, one of her first two choices, one of her first three etc. Without knowing the rank of the other candidates in our area or how they might rank the jobs some assumptions need to be made but I'd like an idea of what her chances are.\n\nThanks!!!",
        "created_utc": 1526496693,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for Recommendations",
        "author": "The_Doc_P",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jxc85/looking_for_recommendations/",
        "text": "Hey all, Need some recommendations on a comprehensive basic statistics refresher. I'm starting my grad program in the Fall Semester in Applied Statistics and Data Analysis. I last took a statistics course in undergrad ~2013. Am looking for something I can complete over the summer, prior to September. Right now I'm looking @ Coursera's Basic Statistics course from University of Amsterdam, but am unsure if that is comprehensive enough. Any recommendations would be appreciated!\n\n",
        "created_utc": 1526495768,
        "upvote_ratio": ""
    },
    {
        "title": "Show that ∑ϵi(xi−x¯)=0",
        "author": "maskedself",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jwnfx/show_that_ϵixix0/",
        "text": "I hope this isn't the wrong sub to ask but i'm a little lost.\n\nI know that epsilon is a random variable so I can't just spread the summation inside the brackets\n\nI know what ∑\\(xi \\-xbar\\) =0 \n\n Much appreciation :\\) ",
        "created_utc": 1526490689,
        "upvote_ratio": ""
    },
    {
        "title": "Numerical data importing as string",
        "author": "thefilefolder",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jwhbl/numerical_data_importing_as_string/",
        "text": "I have my data in an Excel file and want to import it into SPSS. I have 78 variables measured for 160 participants. The data is numerical and I have made sure the cells are specified with the \"number\" category. My problem is that when I try to import the data into SPSS, all of my data is imported as string variables. When I try to change them to numeric in \"type\", my data gets deleted. I have tried saving the data as a CSV and XLSX file, but it makes no difference. It still comes in with string variables. What do I do?",
        "created_utc": 1526489490,
        "upvote_ratio": ""
    },
    {
        "title": "How to determine correlation between two data sets?",
        "author": "alvarkresh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jwcw7/how_to_determine_correlation_between_two_data_sets/",
        "text": "Problem: I have two sets of data with the same x value (two y's, one x) as below:\n\n    X1\tY1\tY2\n    1\t1.899\t1.431\n    2\t1.697\t1.023\n    3\t1.74\t1.082\n    4\t1.911\t1.103\n\nI have what amounts to first year university statistics, so I'm not very conversant with the details of ANOVA or other such tests. That said, I *believe* a one-way MANOVA would be the type that applies here, since I have two dependent variables and one independent variable.\n\nI'd like to find a quantitative measure of correlation between the second and third column. What should I do next?",
        "created_utc": 1526488602,
        "upvote_ratio": ""
    },
    {
        "title": "Choosing the appropriate statistical analyses",
        "author": "studioaz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jvayj/choosing_the_appropriate_statistical_analyses/",
        "text": "Hi, I had 2 questions in helping to prep a study of mine.\n\n1. If I am examining how categorical variables (gender, race, religion) impact substance related criminal charges - would I run a multiple logistic regression to examine how those factors impact various substance related criminal charges?\n\n2. In addition, what if I wanted to group each of those charges into 1 large encompassing category - substance use charges. What analyses would I use to look at something that has multiple IVS, but no DVS basically? Is that possible? My whole population has already been charged, so I'm trying to analyze predictors and significance essentially. I don't have a non-charged control group.\n\nThanks! Sorry, I'm not a stats whiz.",
        "created_utc": 1526480583,
        "upvote_ratio": ""
    },
    {
        "title": "Probability tree question",
        "author": "Baron-Zeppeli",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jv2qe/probability_tree_question/",
        "text": "I find this question somewhat tricky. How exactly do you calculate a general outcome in a probability tree?\n\nFor example, making a probability tree for a deck of 52 cards, how would you calculate all the chances of getting 2 Aces and one other card, regardless of order?\n",
        "created_utc": 1526478782,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone tell me the name of this formula using Log to determine sample size?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8juwgo/can_someone_tell_me_the_name_of_this_formula/",
        "text": "[deleted]",
        "created_utc": 1526477285,
        "upvote_ratio": ""
    },
    {
        "title": "logistic regression with two-sided covariate",
        "author": "funklute",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jun8x/logistic_regression_with_twosided_covariate/",
        "text": "I want to do a logistic regression, with multiple covariates, where at least one of the covariates is two-sided. When I say that a covariate x1 is \"two-sided\", I mean that values close to the mean of x1 are likely to be in class 0, whereas values far away from the mean of x1, in any direction, are likely to be in class 1. Furthermore, the distribution of x1 may not be symmetrical about the mean (for example, a high value of x1 might be somewhat indicative of class 1, whereas a low value of x1 might be extremely indicative of class 1).\n\nOne way to do this is to simply say that the actual covariate I give to the logistic regression is the absolute departure of x1\nfrom the mean. Another way is to create two such departure variables, to account for non-symmetry in the distribution of x1. A third way would be to use polynomials of x1. I'm sure there are other potential ways of doing this.\n\nIs there a common \"best practice\" for handling this situation? ",
        "created_utc": 1526474966,
        "upvote_ratio": ""
    },
    {
        "title": "Aggregation on multiple date ranges",
        "author": "Marleymdw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jsqwm/aggregation_on_multiple_date_ranges/",
        "text": "Im looking at aggregating a data set from its raw source to a datamart with the final output using either Tableau or PowerBI to present the data\n\nWhat would be the best way (both storage size and script efficiency) to handle different date periods?\neg: Hourly, Daily, Weekly, Monthly, etc\nThe original datetime field is standard (yyyy-mm-dd hh:mm:ss:mmm)\n\nMy first thought was for each level of date aggregation to have its own datamart but this seems overkill to me so i thought i would ask the lovely reddit\n\nAny suggestions would be amazing!\n\nThank you!",
        "created_utc": 1526451662,
        "upvote_ratio": ""
    },
    {
        "title": "Question regarding error propagation - mixing of standard deviations and absolute errors.",
        "author": "chinsalabim",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jrr2w/question_regarding_error_propagation_mixing_of/",
        "text": "Hi, thanks in advance for any help.\nI have a couple of questions which would help my understanding greatly.\n\nIn experimental error propagation where several measurements are made and then used to compute another value through a formula we use the errors of each component multiplied by their partial derivatives summed in quadrature to find the result's uncertainty. i.e. ΔC=√(∂C/∂b Δb)^2 + (∂C/∂a Δa)^2 ) \n\nMy question is regarding if one measurement, a, has an absolute error and is a quantity measured once and the other, b, is the mean of several measurements, then what do I use as Δb?\nIt seems to me that we are almost certain that true-a is within a+/-Δa but we are only ~68% certain true-b is within +/- the standard deviation of any one b measurement and ~68% certain it is within +/- the standard deviation of the mean(standard error) of b's mean. Do I use just SD as Δb or SE as Δb or 3*either of them to be 99%? \n\nOne other question is how sure then is the final result? Does ΔC act like a standard deviation? i.e. is +/-ΔC a 68% confidence interval and +/-3xΔC 99%?\n\ne.g.\na is measured as 5+/-2\n\nb is the mean of 1, 3, 6, 8, 13.\n\nTherefore mean of b, b_m = 6.2\n\nSD of b = 4.66\n\nSE of b = SD/sqrt(5) = 2.08\n\nC = 5axb^2\n\ntherefore ΔC =sqrt( (5x(6.2^2 )x2)^2 + (5x2x6.2xΔb)^2 )\n\nThen what is Δb here and what is ΔC?\nand how do i treat ΔC?\n\nThanks again.",
        "created_utc": 1526440411,
        "upvote_ratio": ""
    },
    {
        "title": "Stats for formative usability tests",
        "author": "cynderisingryffindor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jqxnb/stats_for_formative_usability_tests/",
        "text": "What kind of stat analysis would y'all suggest to use for formative usability tests? ",
        "created_utc": 1526433157,
        "upvote_ratio": ""
    },
    {
        "title": "Need help solving a Statistics Sample Test",
        "author": "RexItaliae476",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jqv1r/need_help_solving_a_statistics_sample_test/",
        "text": "Greetings, I am a student who has an upcoming Statistics midterm at my university, and could use some help in solving a sample exam to better prepare for the test. I'm not sure if this is the right place to ask, but if not, please direct me to the proper place!\n\nI have attached a OneDrive link to the sample test, and if you guys could get back to me as soon as possible, with some explanations of the various denotations and concepts, I would really appreciate it.\n\nThank you for your time and effort in this regard!\n\nhttps://diakoffice-my.sharepoint.com/:b:/g/personal/danielahmad1997_sulid_hu/ETvdLx8byANEvzfMNJvM2JIBwm3dqhDgaq22nh-4Zr8kTQ?e=yegCZa",
        "created_utc": 1526432524,
        "upvote_ratio": ""
    },
    {
        "title": "Probability of a difference between individuals.",
        "author": "twointhethink",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jqq7h/probability_of_a_difference_between_individuals/",
        "text": "So, a Z score tells me the probability that an individual value is at most some X. How does one calculate the probability that two values, chosen independently, have a certain difference? I'll rephrase below: \n\nA population is normally distributed with a standard deviation of s. You randomly pick two individuals from the population (M and N). What is the probability that the difference between M and N is equal to or greater than some distance y?",
        "created_utc": 1526431394,
        "upvote_ratio": ""
    },
    {
        "title": "Likelihood - expectation of a realisation of an RV",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jpkm0/likelihood_expectation_of_a_realisation_of_an_rv/",
        "text": "[deleted]",
        "created_utc": 1526421448,
        "upvote_ratio": ""
    },
    {
        "title": "Will anyone help me figure out what degree off the standard deviation Jerry Rice is on the NFL's All-Time Receiving Yards list? If this could be considered a teaser, he has almost 44% more receiving yards than the #2 of all time. More information inside. Please and thank you!!!",
        "author": "Jagsfreak",
        "url": "https://i.redd.it/oo9fytre43y01.png",
        "text": "",
        "created_utc": 1526418518,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting odds and risk ratios less than 1",
        "author": "catora",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8joqtn/interpreting_odds_and_risk_ratios_less_than_1/",
        "text": "Hello,\n\nI've been doing some reading and am getting a little confused with the information. \n\nFrom my understanding, odds ratios range from 0 to 1 in the lower end, and 1 to infinity on the upper end. But it has a symmetrical characteristic so that an OR and it's reciprocal technically have the same magnitude (if plotted on a log scale, they would be equidistant from 1).\n\nGiven this, what is the best wording to use for reporting odds ratios less than 1? For example, if OR=0.3, should I say 0.3 times the odds, or is it still accurate to say 0.7 lower odds or even 70% lower odds? (Can we use percentages when describing ORs?) \n\nSome references have suggested to always take the inverse and report on OR greater than 1.\n\nRisk ratios aren't symmetrical this way, and their upper limit depends on the risk in the unexposed group. (E.g., if the risk in the unexposed is 0.5, the max RR can only be 2 because that would be 100% of exposed having the outcome.) Therefore the lower and upper ranges still aren't equal. So is it still accurate to report a RR=0.5 as \"half as likely\" or \"50% decrease in risk\"? \n\nThanks so much in advance.",
        "created_utc": 1526415087,
        "upvote_ratio": ""
    },
    {
        "title": "Determining Probability of one team scoring more than another",
        "author": "me_for_president2032",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jnxbs/determining_probability_of_one_team_scoring_more/",
        "text": "I am trying to build a predictive model that will tell me the likelihood a baseball team will win a game. What it does is I'm using walk percentages and batting percentages of each player, and using that to generate an estimated number of runs that team would score. However, what I want to do is generate percentage probabilities that one team will score more than the other. Would using standard deviation be a good place to start, or is there a different type of test I should use?",
        "created_utc": 1526409078,
        "upvote_ratio": ""
    },
    {
        "title": "Single Measure vs. Average Measures Intraclass Correlation Coefficients: Which Do I Use?",
        "author": "nonlurkeraccount",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jnjgk/single_measure_vs_average_measures_intraclass/",
        "text": "Hello all!\n\nI want to validate a self-administered questionnaire, and for test-retest reliability, I don't know which intraclass correlation coefficient I'm supposed to use, single or average measures.\n\nThe questionnaire consists of 8 items, each in a five-point Likert scale. The test was first self-administered to 100 participants, and then repeated in 40 participants one month later. I've put the data into SPSS and ran a One-Way Random reliability analysis to obtain the ICC for each item separately (Item 1 vs Retest Item 1, Item 2 vs Retest Item 2...), and for the aggregate score (Total score after adding all items vs Retest total score).\n\nWhen I run the analysis, SPSS provides two measures of ICC: Single and Average. Which one am I supposed to report and why? I suspect it's the single-measure ICC based on what I've read online, but I still don't feel confident.\n\nCould anyone provide a clear example of when I should use vs. the other, preferably in the context of survey validation?\n\n\nThanks a lot!",
        "created_utc": 1526406206,
        "upvote_ratio": ""
    },
    {
        "title": "Explicit and implicit attitudes SPSS",
        "author": "almost_february",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jlwys/explicit_and_implicit_attitudes_spss/",
        "text": "Hello everyone,\n\nI did a small research on explicit and implicit attitudes towards people with mental health problems. I have 3 groups, everyone did Likert-style test measuring explicit attitudes and Single-Target IAT test. Is it possible to test if there is a relationship between explicit and implicit attitudes? Likert-style test had measures from 1 to 5 and Single-Target IAT is from -2 to +2. I don't know if it's clear but any suggestion would be appreciated! ",
        "created_utc": 1526393875,
        "upvote_ratio": ""
    },
    {
        "title": "variance",
        "author": "liananana",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jlexg/variance/",
        "text": "In linear combinations eg w=2x\\+3y and we find l mean and variance when do we square the coefficient of the variable of which we're calculating the variance?\n\n sometimes it's var\\(w\\)=2\\(var x\\)\\+3\\(var y\\) and sometimes var\\(w\\)=4\\(var x\\)\\+ 9\\(var y\\)\n\nhow do they differ?",
        "created_utc": 1526389564,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone ELI5 how to manually calculate the coefficients of a logistic regression model?",
        "author": "Astrobrony",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jl9j4/can_someone_eli5_how_to_manually_calculate_the/",
        "text": "Not that I would ever want to try to do it manually of course, but I find it pretty hard to get my head around where likelihood estimates and so forth actually come from.",
        "created_utc": 1526388208,
        "upvote_ratio": ""
    },
    {
        "title": "modelling multiple biological variables from the same dataset with different error structures",
        "author": "sonnyjimboyladdyman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jl6ga/modelling_multiple_biological_variables_from_the/",
        "text": "I have data collected from samples of fat tissue in birds, which have been analysed to see the relative amounts of different fat molecules in a few different migratory bird species. \n\nSo I have percentages for the amount of; Omega-3 fatty acids; Polyunsaturated fatty acid; Monounsaturated fatty acid; unsaturated fatty acid; saturated fatty acid; Linoleic acid. These variables are obviously bounded at 0 and 1, but all are strongly skewed (many 0 values/many values in the same small range, with a couple of big outliers).\n\nIn addition, we've also collected data on the amount of various different fatty acid chain species: methylenic subunits, olefinic subunits, average number of carbons in a chain, average number of protons, average molecular weight. These variables are only bounded at 0, but also strongly skewed.\n\nI first attempted to use GLM to compare between different model structures, error families and transformations, using diagnostic plots and AIC values. This led to some variables having completely different model structures (e.g. negative binomial, beta, quasipoisson) which I thought could still be useful, even though I realise it's easier to compare between some structures than others. It turns out that there are some strange results. For example the levels of PUFA and MUFA in one species are significantly different than others, but not in UFA, which seems strange to me. I think this could be down to the way I've modelled the dataset, or maybe I'm missing some other biological explanation for that. \n\nMy question is about whether or not I can use a range of different model structures for these data, and still be sure that the result I'm seeing is statistically accurate. Should the proportional data all be analysed with the same structure, even though a different one seems to fit each better than others? The same for the non-proportional variables. There doesn't seem to be anything I can do to transform such skewed data to get a really good model fit - all the diagnostic plots seem very messy despite everything I try.\n\nUnfortunately I'm just a lowly undergraduate intern, so everything I'm doing here I'm doing for the first time. Apologies if I've made any glaring errors in my description too, both biological and statistical. Please bear that in mind in your responses - go easy on me! Unfortunately my supervisor isn't able to offer much help, so any help from you is hugely appreciated.",
        "created_utc": 1526387361,
        "upvote_ratio": ""
    },
    {
        "title": "Correlation with repeated measures",
        "author": "lokistor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jl2qx/correlation_with_repeated_measures/",
        "text": "Hi, I read a paper and now I have questions regarding the statistical analysis.\nThe autors measured the electrical activation of muscles for different activation levels and the accompanying change in muscle thickness.    \nThen they put all data points together and computed the correlation coefficient between change in thickness and activation level.  \nNow I wonder if this is allowed. Aren't those repeated meassurements? My understanding was that this is okay if the correlation for just one subject is calculated. But not if all subjects are analysed together.  \nAm I wrong about this?",
        "created_utc": 1526386345,
        "upvote_ratio": ""
    },
    {
        "title": "what does zero term effect, self selection bias, falsification tests mean?",
        "author": "behappyftw",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jjzzn/what_does_zero_term_effect_self_selection_bias/",
        "text": "i am reading this paper about data on wines and how different factors (specifically certification) affect the price. They mention self selection bias and how they can fix it by looking at past data before certification happened and create a strong falsification test and how it should generate a zero term effect. I have no idea what that means and how that solves the problem.\n\nWhat i understand is that self selection bias is like people that know a lot of wine volunteer and thus affect the results because it isnt a random population? and falsification test is a test to disprove the data using past data (making a future prediction?).\n\nthanks",
        "created_utc": 1526373136,
        "upvote_ratio": ""
    },
    {
        "title": "Histogram vs line",
        "author": "Lizberry",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jjo4f/histogram_vs_line/",
        "text": "I want to make a distribution plot of grain sizes, so grain size vs counts. To me (I know nothing about statistics though) making a histogram would be logical, but someone I work with made it into a line plot with the probability density vs the grain size, and the line thus connects the centers of the bars of the histogram (but without showing the bars). Is this a correct way of showing the data? And maybe you can explain why this would be better or not? ",
        "created_utc": 1526368423,
        "upvote_ratio": ""
    },
    {
        "title": "Why is regression analysis only applicable in linear in parameter model?",
        "author": "Wickedmittal",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jje10/why_is_regression_analysis_only_applicable_in/",
        "text": "Why is it that it will work in \nY = 16 + 4X + U\nBut not in\nY = 4^2 + 2^2(X) + U.\nAren't they both essentially the same?\n",
        "created_utc": 1526364814,
        "upvote_ratio": ""
    },
    {
        "title": "Risk Assessment Calculation (Simple)",
        "author": "newphone2017",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jj0hz/risk_assessment_calculation_simple/",
        "text": "I am trying to do the following risk calculation (it's not for homework, it's based on a real example I'm faced with). Using hypothetical numbers, say:\n\n* You have a 1 in 100 lifetime probability of **dying of** a certain disease\n* You can undergo a fairly simple preventative procedure, but you have a 1 in 10,000 risk of death or serious injury as a result of the procedure\n* The procedure reduces your risk of **dying of** the disease from 1 in 100 to 1 in 400 lifetimes\n\nFrom a risk assessment perspective, is it worth undergoing the procedure?\n\nNo need to solve the problem for me, but guidance on how to approach would be appreciated. Thanks",
        "created_utc": 1526360363,
        "upvote_ratio": ""
    },
    {
        "title": "Help with hypoth test please",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jiw73/help_with_hypoth_test_please/",
        "text": "[deleted]",
        "created_utc": 1526359049,
        "upvote_ratio": ""
    },
    {
        "title": "SEM and SD overlap indicates no significance?",
        "author": "DonMatteo13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jit01/sem_and_sd_overlap_indicates_no_significance/",
        "text": "Hi guys,\nwas just reading a paper and I noticed that in one of the figures some of the SEM points overlap and yet the researchers state there was a significant difference. From some online reading I found out that if SEM overlap, the data is not considered significant. So from this stems two questions; why is this the case? and why would the researchers state significance if an analysis of the figure shows otherwise \nNOTE: no p-values were given for this particular data set - actually, a third question; what if the p-value is less than 0.05 but the SEM data points on the figure overlap?\nThanks for any help!",
        "created_utc": 1526358129,
        "upvote_ratio": ""
    },
    {
        "title": "Required sample size for systematic sampling",
        "author": "5k1rm15h",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jiox5/required_sample_size_for_systematic_sampling/",
        "text": "Hey All,\n\nI'm trying to calculate the required sample size for a given level of acceptable error [; \\epsilon ;] and confidence level based on the normal distribution using z. \n\nThe other information I have to work with is the population size and the estimated proportion in two groups, [; p_1 ;] and [; p_2 ;] . \n\nI'm free to select the sampling interval so sample size n will be given as N / k where k is the interval between samples taken. \n\nI'm not sure I know how to approach this. I don't believe I can make the assumption that the list is random, precluding the use of simple random sample size calculations,  and the only other formula I've come across refers to repeated systematic sampling. The example makes use of statistics I don't believe are available to me with what I have.\n\n&amp;nbsp;\n\nAny assistance on where I could begin approaching this would be greatly appreciated.\n\nThanks in advance!",
        "created_utc": 1526357006,
        "upvote_ratio": ""
    },
    {
        "title": "Linearity assumption with categorical predictor",
        "author": "cphmrk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jgg4y/linearity_assumption_with_categorical_predictor/",
        "text": "I have a linear regression model with an ordinary scaled categorical predictor with three levels ('low', 'medium', &amp; 'high'). Is it possible to test for linearity (Gauss-Markov)? I've done it visually and it looks fine, but I'm unsure whether linearity even applies to (non-binary) categorical predictors like mine. I've been told to use dummy variables, but won't this just mean that linearity is satisfied by definition?",
        "created_utc": 1526336530,
        "upvote_ratio": ""
    },
    {
        "title": "Can you combine two standard deviations?",
        "author": "twointhethink",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jfx23/can_you_combine_two_standard_deviations/",
        "text": "I have a sample size=n with standard deviation=s, mean=X. To that sample, I add some intervention that affects each individual. On average, the intervention has no effect (i.e. X1=X2=X); however, the effect size is normally distributed about zero with a standard deviation of G. On average, the new mean is obviously the same. What about the standard deviation? Is such a question answerable?\n",
        "created_utc": 1526332221,
        "upvote_ratio": ""
    },
    {
        "title": "What t test to use for my use case?Is my sample dependent or not?",
        "author": "HelloWorld1111111",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jd5sy/what_t_test_to_use_for_my_use_caseis_my_sample/",
        "text": "I am new to statistics, but I need to see if results of my findings are significant, but I’m not sure, which t test to use in my case.\n\nI have a product page, that may have from 1 to ~10 videos. So, I need to compare CTR(Click Through Rate - number of users who clicks on my video to number of users who saw this page with video).\n\nI need to calculate, if there is significant difference in CTR depending on video is alone on he page or video is located together with other videos.\n\nLet’s say I have 7 videos and I want to compare where those videos performed better (where CTR is better) : when they are alone on page or when they are together with other videos.\n\nSo my input for t test is looks like this:\n\nvideo \t    TRUE\t          FALSE\n\nvideo 1\t0.022888533\t0.009686095\n\nvideo 2\t0.115459271\t0.014216948\n\nvideo 3\t0\t                0.000904269\n\nvideo 4\t0.038639876\t0.003616088\n\nvideo 5\t0.046625481\t0.01061746\n\n\nwhere column TRUE - means that this videos is alone on page vs FALSE - video is not alone. CTR is in %\n\nFrom this table, I can see that all videos are performing better (meaning that has better CTR) when they are alone on page. But I need to confirm that this results are statistically significant.\n\nSo my question is which t-test I need to use to confirm this? As I understand, I cannot use independent sample T-test, since I am comparing same videos, but in different condition?",
        "created_utc": 1526311749,
        "upvote_ratio": ""
    },
    {
        "title": "Want to make an interesting yet easy simulation, for instance, to prove some theorem for any random process. Any ideas?",
        "author": "pompejid",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jco4w/want_to_make_an_interesting_yet_easy_simulation/",
        "text": "Hello everyone!\n\nRecently I was studying random processes (such as Markov chains, branching processes, Wiener processes and so on) and I also am studying python right now. I'd like to practice my skills by implementing statistics theory.\n Could you please give me any advice what could I simulate/model? \nI'm particularly interested in using simulation to prove some sort of theorem. Still I'm newbie in programming so I want to write a relatively easy code and make some pictures.\n\nI appreciate your help a lot!\nThanks to everyone!",
        "created_utc": 1526308055,
        "upvote_ratio": ""
    },
    {
        "title": "Lagrange multipliers spatial regression",
        "author": "ellieklg114",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8jb2no/lagrange_multipliers_spatial_regression/",
        "text": "I've carried out a spatial regression. The moran's I is telling me that there is spatial dependence. I have ran the lagrange multiplier diagnostics for spatial dependence to find out whether to apply the error model or the spatially lagged model. Both the LMerr and LMlag were significant so I looked at the more robust tests. But both RLMerr and RLMlag are non significant. How do I now decide which model to fit? ",
        "created_utc": 1526292001,
        "upvote_ratio": ""
    },
    {
        "title": "How to Build a Project Scoring Model?",
        "author": "quantumtool",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j9831/how_to_build_a_project_scoring_model/",
        "text": "Hello,\n\nI work for a construction company and am looking to build a statistical model that creates a score for each of our construction projects.  The score should indicate current project health and help identify those projects that are in bad health and need attention.\n\nThere are many factors that potentially affect project health such as:\n1 - Deficiencies - issues on the project site, maybe paint chips or cracks in walls\n\n2 - Requests for information (RFI) - workers in the job site may have to request information from other workers.  The longer it takes to get answers the more risk is incurred.  The more RFI's means perhaps more risk\n\n3 - Changes in project scope - the longer it takes to approve change orders the more risk is incurred\n\n4 - Schedule milestones - lateness of milestones poses risk\n\n5 - Many other factors to potentially include\n\nWhat are some statistical methods I could use to develop a model that creates some kind of project score?  I was thinking the higher the score, the worse the health of a project.  However, this is just a thought.\n\nAny thoughts/suggestions would be great!\n\nThanks.\n\n",
        "created_utc": 1526267655,
        "upvote_ratio": ""
    },
    {
        "title": "How to find sample size when given classes and frequencies.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j8kd4/how_to_find_sample_size_when_given_classes_and/",
        "text": "[deleted]",
        "created_utc": 1526260927,
        "upvote_ratio": ""
    },
    {
        "title": "Absolute Risk calculation",
        "author": "chewtoy0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j86x5/absolute_risk_calculation/",
        "text": "I'm trying to express:\n\n10.8 cases per 100 000 person years\n\nas a percentage of absolute risk during a lifetime.  I'm drawing a blank on how to do this, can someone point me in the right direction?",
        "created_utc": 1526257137,
        "upvote_ratio": ""
    },
    {
        "title": "Finding relationship between multivariate time-series",
        "author": "Atombe7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j7a8v/finding_relationship_between_multivariate/",
        "text": "I have multiple pairs of multivariate time-series that I know are related. Time-series 2 (call it R) is taken as the output of feeding time-series 1 (call it P) into a black box function, f.\n\nEffectively, f:P-&gt;R\n\nMy first goal is to find for a given r in R, the values p in P which most correlate; would partial correlation but for time-series be the most appropriate, or are there other technologies?\n\nSecondly, I want to perform clustering on R to identify different sets of P that produce similar Rs. My first thought is to take a specific r from all R, and cluster on that individual r. Does that still hold, given the multivariate nature of all R?",
        "created_utc": 1526248241,
        "upvote_ratio": ""
    },
    {
        "title": "Simulation Help",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j6v8c/simulation_help/",
        "text": "[deleted]",
        "created_utc": 1526244667,
        "upvote_ratio": ""
    },
    {
        "title": "Question on appropriateness of chi-square test for 2x2 observations.",
        "author": "b0dah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j6q57/question_on_appropriateness_of_chisquare_test_for/",
        "text": "I am comparing data on two binary variables; suicide status (lifetime ideation and lifetime attempt), and sex. \n\nThe suicide status variable leans heavily towards attempts (64% vs 36%); many more people think about it than act on it, so I'm realizing that this may be making the very high test statistic uninterpretable, as I am more interested in the differences in gender between these groups. Said otherwise, I *expect* ideation and attempt to be significantly different, and am not that interested in this difference. What I am trying to test is whether males and females belong to these groups more frequently.  Here's an image of the table I've set up for the numbers. \n\nAny feedback would be helpful: \n\n[https://imgur.com/a/Gg8RxaH](https://imgur.com/a/Gg8RxaH)\n",
        "created_utc": 1526243476,
        "upvote_ratio": ""
    },
    {
        "title": "What sort of statistical test would be best for this?",
        "author": "NT202",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j6jnp/what_sort_of_statistical_test_would_be_best_for/",
        "text": "We did an experiment comparing two exercise bike 5k time trial performances with the same participant.\nOne was with a placebo, the other a carbohydrate sports drink.\n\nI need to find out wether the carbohydrate drink improved  the time trial performances vs those under the placebo trial.\n\nThe data variables I have are PLACEBO TIME TRIAL IN SECONDS and CHO (carb drink) TIME TRIAL IN SECONDS.\n\nThanks!!\n\nEDIT: I realise it will depend on whether the data is or is not normally distributed, so if possible, could you tell which I’d need for this data for *both*? Then once I’ve run the normality tests I can pick whichever.",
        "created_utc": 1526241935,
        "upvote_ratio": ""
    },
    {
        "title": "Simulating sarima",
        "author": "mvaa12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j4z5q/simulating_sarima/",
        "text": "Hey, how I can simulate 1000 samples from SARIMA\\(1,1,0\\)\\(2,0,0\\)\\_12 model in R?",
        "created_utc": 1526229046,
        "upvote_ratio": ""
    },
    {
        "title": "How do I know if the sample size is adequate if I'm trying to derive normal range in a population?",
        "author": "hsihsadna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j4gh0/how_do_i_know_if_the_sample_size_is_adequate_if/",
        "text": "Hello.\nSuppose I'm trying to find the average height and standard deviation in a population of about a million. What sample size would I need, and what are the parameters I need to look at?\nHow would I look at the confidence interval?",
        "created_utc": 1526224640,
        "upvote_ratio": ""
    },
    {
        "title": "Beginner Question about reading correlations.",
        "author": "NT202",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j42k5/beginner_question_about_reading_correlations/",
        "text": "I generated [this correlation table ](https://imgur.com/a/0u65BSv) in SPSS for some normally distributed date.\n\nThe correlation is between training load in hours a week and performance on a time trial.\nI’m a beginner with statistics so have never done a correlation before, and can’t get my head around the above.\n\nI have been able to workout from the above table that the data is (I think) a significant, weak, negative correlation, but I’m not sure what this actually tells me about the data.\nIf it’s a negative correlation, If my understanding is correct, the two variables change in allowing directions.\n\nDoes this mean that time trial performance was better or worse in those with a higher amount of hours per week training load?\nI’m not really sure how to interpret it.\n\nThanks!",
        "created_utc": 1526221100,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate 99% Confidence Interval for a sample with known sample mean, Standard error, and sample standard deviation",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j3z1t/how_to_calculate_99_confidence_interval_for_a/",
        "text": "[deleted]",
        "created_utc": 1526220188,
        "upvote_ratio": ""
    },
    {
        "title": "Is it possible to calculate quartiles for qualitative ordinal data?",
        "author": "dxbdeedee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j3no2/is_it_possible_to_calculate_quartiles_for/",
        "text": "Specifically I have the following\n\nNone - 2 answers\nBasic teaching - 14 answers\nSecondary teaching - 13 answers\nUniversity teaching - 6 answers\n\nSince the cumulate fi already jumps to 45% on the basic teaching, how can I calculate Q1 And Q3?",
        "created_utc": 1526216916,
        "upvote_ratio": ""
    },
    {
        "title": "What Makes Naive Bayes Classification So Naive? | How Does Naive Bayes Classifier Work",
        "author": "LearningFromData",
        "url": "http://www.hashtagstatistics.com/2018/04/what-makes-naive-bayes-classification.html",
        "text": "",
        "created_utc": 1526214618,
        "upvote_ratio": ""
    },
    {
        "title": "How do I read this Pairwise comparisons chart?",
        "author": "NT202",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j3byw/how_do_i_read_this_pairwise_comparisons_chart/",
        "text": "[This is the chart.](https://imgur.com/a/8MPHNBk)\n\nBasically, I wanted to compare a bunch of variables, to see if there was a difference between genders for age, height, training load and years in sport.\n\nAfter some advice on here the test I went with was a repeated measures Anova.\n\nI’m new to statistics and so have never done one of these before. Equal variance can be observed through Mauchly’s test of sphericity at a significance value of .506, and when looking at the [gender section of the tests of within subjects effects table](https://imgur.com/a/k71XZdN) the significance of sphericity assumed is .001, indicating there is a significant difference.\n\nBecause if this I have been instructed to observe the Pairwise comparisons chart to see where the difference lies. The only problem is I don’t know how to read or interpret it!\n\nCan anyone help me out?\n\nThanks!",
        "created_utc": 1526212980,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical test to discern different year-groups",
        "author": "ExPrinceKropotkin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8j32n1/statistical_test_to_discern_different_yeargroups/",
        "text": "So my dependent variable is number of annual days of labor strike per country, and my dependent variable is the year. I want to test whether there are global trends in days of labor strike. If there is an increase in strikes in one country is there also an increase in strikes in others? In other words: how likely is it that the data points in the sample are drawn from year-groups with different means. So I guess the simplest way to do this is to treat the years as a categorical variable (would be fine for the stage I'm at). I've done a regression with dummy variables, but that produces a value for every year rather than for the model as a whole (unless I could do that a different way...)\n\nThanks a lot for your help! It's been a while since I've done statistics\n",
        "created_utc": 1526209146,
        "upvote_ratio": ""
    }
]