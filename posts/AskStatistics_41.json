[
    {
        "title": "OLS and goodness of fit",
        "author": "GimmeThoseCaps",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89t8dg/ols_and_goodness_of_fit/",
        "text": "I'm trying to figure out which are the best indicators to assess the fitness of my model. Can someone give me an overview about them? Just a simple explanation to make it easier for me to research.\n\n\nWhich indicator/indicators are more important or useful? How do I know the \"confidence level\" of my model - is Rsquare the best measure?\n\n\n",
        "created_utc": 1522874142,
        "upvote_ratio": ""
    },
    {
        "title": "hypothesis testing",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89t8c8/hypothesis_testing/",
        "text": "[deleted]",
        "created_utc": 1522874134,
        "upvote_ratio": ""
    },
    {
        "title": "What less common skills have you found most useful in statistics related areas",
        "author": "deaddogdude",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89sx4i/what_less_common_skills_have_you_found_most/",
        "text": "If you work in a field like statistics or anything related, are there any particular skills or software that are less commonly talked about but have been very useful/that you are glad you picked up? obviously for specialized areas certain things will not be the most applicable elsewhere, but feel free to list anything you think might pertain, even in specialized subfields.",
        "created_utc": 1522872066,
        "upvote_ratio": ""
    },
    {
        "title": "Simple probability question",
        "author": "commanderzilyana",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89rfm4/simple_probability_question/",
        "text": "If I have 20 cards with either a star, circle, wave, or square on them and ask someone to guess the shape on each card the probability the guess is correct is .25\n\nWhat would the probability be that the subject correctly guesses 10 of the 20 shapes?\n\nI don't know why, but this simple question has me lost. \n\nAll I have so far is P(X&gt;or equal to 10)",
        "created_utc": 1522862071,
        "upvote_ratio": ""
    },
    {
        "title": "Six variables, with a different number of responses, going into one index?",
        "author": "Sociological_Duck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89rbpf/six_variables_with_a_different_number_of/",
        "text": "I submitted a paper with six independent variables. They were all measuring attitudes toward immigrants and immigration, so a review recommended I merge them into one index. Three of the variables have four response values, while another three have ten responses. Do I need to take extra steps to standardized these? ",
        "created_utc": 1522861344,
        "upvote_ratio": ""
    },
    {
        "title": "Need a Stats refresher. TOC from Intuitive Introductory Statistics from Springer. Will this cover my bases?",
        "author": "[deleted]",
        "url": "https://www.pdf-archive.com/2018/04/04/intuitive-introductory-statistics-toc/preview/page/1/",
        "text": "[deleted]",
        "created_utc": 1522855377,
        "upvote_ratio": ""
    },
    {
        "title": "Is answering questions on r/askstatistics similar to being a statistics consultant in real life? Is it different? How so?",
        "author": "13ass13ass",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89qbrh/is_answering_questions_on_raskstatistics_similar/",
        "text": "",
        "created_utc": 1522854829,
        "upvote_ratio": ""
    },
    {
        "title": "Please help: Can someone double check the kind of test I should use?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89q6i7/please_help_can_someone_double_check_the_kind_of/",
        "text": "[deleted]",
        "created_utc": 1522853844,
        "upvote_ratio": ""
    },
    {
        "title": "Help with IBM SPSS 24 - I don't know which test to use!",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89p28r/help_with_ibm_spss_24_i_dont_know_which_test_to/",
        "text": "[deleted]",
        "created_utc": 1522845385,
        "upvote_ratio": ""
    },
    {
        "title": "[College Stats] Uniform distribution on a Parallelogram (Help me, guys!)",
        "author": "SAgirl101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89ovfs/college_stats_uniform_distribution_on_a/",
        "text": "Question: https://imgur.com/a/4zhCB\n\nPlease ignore the ζ\n\nI really don't understand what the \"hint\" means.\n\nPlease help.",
        "created_utc": 1522843708,
        "upvote_ratio": ""
    },
    {
        "title": "Real Life Statistics Question Re Fantasy World Surfing League",
        "author": "SlatersBallsack",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89om4y/real_life_statistics_question_re_fantasy_world/",
        "text": "As stated, I will draw my statistical question from my real world scenario regarding the World Surfing League (WSL). When playing my fantasy team, statistically speaking, am I better off if, let's say I have two remaining surfers in the semi finals, and option A) they face each other in the same heat, in which case one of my surfers is guaranteed to go to the finals, or B) It's better if each surfer is in a different heat, whereas in best case scenario they would face each other in the finals and I have a guaranteed winner. But conversely in B), neither could make the final. How do I statistically compute which is the better option?",
        "created_utc": 1522841243,
        "upvote_ratio": ""
    },
    {
        "title": "What is the best estimate of the population mean for data with one-sided errors?",
        "author": "stevenjd",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89o1ll/what_is_the_best_estimate_of_the_population_mean/",
        "text": "Have some data with one-sided random errors. So rather than each data point being some \"true\" value plus or minus some small error, the errors can only be added, never subtracted.\n\nThe measurements are the timing of a process which is expected to always take the same amount of time, since it is in principle perfectly deterministic. But outside factors can slow the process down. They can never speed it up.\n\nWhat is the best estimate for the time the process takes? What sort of statistics can I legitimately use on these measurements?\n",
        "created_utc": 1522834921,
        "upvote_ratio": ""
    },
    {
        "title": "Simple R programming help: need help calculating the B column.",
        "author": "[deleted]",
        "url": "https://i.redd.it/sv6wsau26up01.jpg",
        "text": "[deleted]",
        "created_utc": 1522823471,
        "upvote_ratio": ""
    },
    {
        "title": "Linear Programming help",
        "author": "VroomVroom415",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89mx2y/linear_programming_help/",
        "text": "Hi all, thank you in advance for helping me. My goal is to use linear programming to optimize the Golden State Warriors' lineup by each quarter (Q1, halftime, Q3, and Q4). For instance, knowing that Curry's average minutes per game, how can I maximize his time throughout all 4 quarters while satisfying all contraints (e.g. maximum number of minutes a player can play, maximum of 5 players on the court and they all need to play a different position and so on).  I've gathered the GSW players' stats (minutes played per game, points per game, and etc.) using basketball-reference.com. My plan for the next step is to find a way to assign a score (1-100) for each player. This way, it'll be much easier to execute my LP. I'm just not sure how. Since players all play a different position and some players play multiple, it'll definitely be an unfair advantage to compare a point guard and a center. My thought process is to compare a point guard on the GSW team with the \"greatest centers of all time\" (https://www.ranker.com/crowdranked-list/greatest-centers-of-all-time). This way, I can see where the center from GSW falls. What are your thoughts?   Please keep in mind that this is a very naive and broad project. I wont be to take in unforeseen contingencies such as injuries, trades and so forth.",
        "created_utc": 1522821619,
        "upvote_ratio": ""
    },
    {
        "title": "Where can I find good examples logistic regression models for binomial data?",
        "author": "purple-2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89metb/where_can_i_find_good_examples_logistic/",
        "text": " I'd like examples for when there is one factor \\(ex. heat\\) and two factors \\(ex. heat and time\\)",
        "created_utc": 1522816658,
        "upvote_ratio": ""
    },
    {
        "title": "Question about odds",
        "author": "learningtowalkagain",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89l3re/question_about_odds/",
        "text": "Something recently happened in my life, and I need some stats to show a person the low probability of a certain occurrence.  How would I go about pulling odds from the city population, the number of apt complexes in the city, and the odds of a person finding the right apt complex in question?  I feel only hard numbers will convince this person of the very low probability they were found at the exact apt complex they were at without being tracked in some way.",
        "created_utc": 1522806372,
        "upvote_ratio": ""
    },
    {
        "title": "How do I know what kind of weight I am applying?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89l2uf/how_do_i_know_what_kind_of_weight_i_am_applying/",
        "text": "[deleted]",
        "created_utc": 1522806151,
        "upvote_ratio": ""
    },
    {
        "title": "How to test if two variables are significantly different from each other?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89l0qs/how_to_test_if_two_variables_are_significantly/",
        "text": "[deleted]",
        "created_utc": 1522805741,
        "upvote_ratio": ""
    },
    {
        "title": "Am I reading this table correctly?",
        "author": "ly5ergic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89kwgg/am_i_reading_this_table_correctly/",
        "text": "I was looking at [this TABLE](https://imgur.com/gallery/Y57Kv) after reading news articles stating discontinuation of fluorinated water in Calgary, Canada in 2011 caused an increase in tooth decay. The news also stated tooth decay increased by 3.8% compared to a 2.1% increase in Edmonton, Canada.\n\n\nI'm not seeing this increase. It appears to me primary tooth decay increased by 8% and permanent tooth decay decreased by 12% in Calgary.\n\n\nPrimary tooth decay increased by 4% and permanent tooth decay decreased by 1% in Edmonton. \n\n\nWhat am I missing? Where is the 2.1% and 3.1%? It seems like discontinuation of fluoride had a positive effect. Also how could these numbers be significant or even attributed to fluoride with a 10% variation between 2004-2005?\n\nEdit: Source\n[Link to full study](https://onlinelibrary.wiley.com/doi/full/10.1111/cdoe.12215)",
        "created_utc": 1522804883,
        "upvote_ratio": ""
    },
    {
        "title": "Mean centered all the IVs, but vifs didn't change at all?",
        "author": "primeprime8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89kstm/mean_centered_all_the_ivs_but_vifs_didnt_change/",
        "text": "I'm using stata and I'm using an interaction term, so I subtracted the mean from all my independent variables, but the vifs are still the same. Any help is appreciated!",
        "created_utc": 1522804127,
        "upvote_ratio": ""
    },
    {
        "title": "What is the function that shows the expected value for a maximum of repeated IID samples?",
        "author": "NeuroBill",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89k1aj/what_is_the_function_that_shows_the_expected/",
        "text": "Say I repeatedly make IID samples from a Gaussian distribution with mean mu and standard deviation s. Every time I sample, if the value is larger than the last sample, I keep it as my new maximum. I continue to make samples like that over and over again. After doing that a 1000 times, I end up with a collection of maximums by sample number, and it looks like this:\n\nhttps://i.imgur.com/FCwrS2G.png\n\nHowever, what is the function of the expected curve? If I repeat the above experiment 1000 times, and plot the mean, the curve looks like this\n\nhttps://i.imgur.com/KlkCcJG.png\n\nBut how would you derive it? I don't even know where to start.",
        "created_utc": 1522799000,
        "upvote_ratio": ""
    },
    {
        "title": "Name of Sampling technique?",
        "author": "throwaway11111111231",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89jmcv/name_of_sampling_technique/",
        "text": "Hey there,\n\nFor my current proposal the sampling method that I'm using for my RCT is to advertise through print, clinics, and contact with physicians throughout my city and surrounding area to alert them of the existence of my trial. If patients have the condition and are not responding to conventional treatment for pain and meet the eligibility criteria they can volunteer be referred to my study for a new intervention at the discretion of their physician that has been treating them.\n\nWould this be best described as convenience sampling or is there some other term for it?\n\nThanks",
        "created_utc": 1522795935,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing multiple time variant datasets",
        "author": "BurtonGFX",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89izy1/comparing_multiple_time_variant_datasets/",
        "text": "I have multiple temperature vs. time datasets that I would like to compare to one another. Each dataset is compromised of several distinct temperature vs time trials of similar, but not identical, durations.\n\nTo perform the comparison I believe I need to begin by averaging all of the individual trials within each dataset, to obtain an average time variant trends. These average trends can then be compared to eachother...\n\nI am looking into dynamic time warping to obtain my dataset averages. Is this a reasonable method to use? Also, is my intended method for comparing these datasets flawed in any major ways?\n\n\nHere is an example of one of my datasets which I would like to obtain an average trendline for: https://imgur.com/a/H8xvL",
        "created_utc": 1522791825,
        "upvote_ratio": ""
    },
    {
        "title": "Which test to use when identifying average volume changes between two months.",
        "author": "thankkieu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89hrlu/which_test_to_use_when_identifying_average_volume/",
        "text": "I'm making a monthly report and I need to examine the daily average volume between two months. I used a two sample t-test with N as the number of days in each. Is this right?\n\nI also have another metric to examine between the two months. I have a proportion metric between two months that I need to examine. There are N1 samples from Month 1 and N2 samples from Month two. I used the Z score for 2 population proportions. https://www.medcalc.org/calc/comparison_of_proportions.php. Is this right?\n\nLastly, since I do this exercise fairly frequently and there are many levels (non-hierarchical) of daily average volumes and proportions to check across two periods. For example I also want to find out any significant differences across all (countries, countries within products, products) between the two months. Are there any good methods of doing these statistical tests in way that is efficient and show all the significant differences across every level combination?",
        "created_utc": 1522784942,
        "upvote_ratio": ""
    },
    {
        "title": "Please help - What sort of stats would I use to find the correlation between a search ranking and reviews.",
        "author": "skittles83",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89hkt9/please_help_what_sort_of_stats_would_i_use_to/",
        "text": "I've created a web scraper in python that gathers listings for a particular search term. I'm gathering the number of reviews per listing and trying to find the correlation between each search ranking and number of reviews. My question is, do I simply apply the Pearson formula to search rankings and number of reviews. I'm not sure how to approach this problem. I'm very rusty with stats. \n\nAny guidance or push in the right direction would be greatly appreciated! ",
        "created_utc": 1522783916,
        "upvote_ratio": ""
    },
    {
        "title": "Simple probability of two boolean variables",
        "author": "Cascades1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89gsf4/simple_probability_of_two_boolean_variables/",
        "text": "Hi,\n\nThis is from a past paper for my undergrad CS course, not homework, just trying to settle a dispute between the mark scheme/friends/me/the internet. We are given a probability distibution table of three boolean variables; a, b &amp; c:\n\nhttps://imgur.com/fl4W7Od\n\nI am asked to find the probability: P(a OR not c)\n\nI tried using the formula P(a) + P(not c) - P(a AND not c)... which worked out except...\n\nFor calculating P(a AND not c) i simply did P(a) * P(not c) = 0.6 * 0.22.... which gave me 0.132, but when you add up the correct cells in the table you can tell I should be getting 0.06....\n\nWhy is P(a) * P(not c) not giving me the overlap as usual?\n\nThanks",
        "created_utc": 1522779586,
        "upvote_ratio": ""
    },
    {
        "title": "Deriving sample size n required for a 2-sample z-test of proportions of a dichotomous outcome",
        "author": "AcetylBroA",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89g3en/deriving_sample_size_n_required_for_a_2sample/",
        "text": "Hi, I'm new here, but looking to see if the r/AskStatistics community can help me out!\n\nJust like the title says, I'm looking for a derivation of this equation: https://imgur.com/gHngP6o\n\nI'm sure this already exists somewhere on the web, but I'm having trouble googling to find it. If anyone here could point me in the right direction, just a link to a good derivation, I'd greatly appreciate it! Thanks!",
        "created_utc": 1522775691,
        "upvote_ratio": ""
    },
    {
        "title": "Any statistics on how many people engage in cults ?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89fc6u/any_statistics_on_how_many_people_engage_in_cults/",
        "text": "[deleted]",
        "created_utc": 1522771454,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a name for this principle?",
        "author": "Megatron_McLargeHuge",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89esru/is_there_a_name_for_this_principle/",
        "text": "Suppose there are two ways to get into XYZ University, be either really smart or really good at sports. Suppose those traits are independent in the population and the selection process isn't additive but just requires being top-1% at either. Then athletic ability and intelligence will be negatively correlated within the student body.\n\nIs there a name for this result?",
        "created_utc": 1522768435,
        "upvote_ratio": ""
    },
    {
        "title": "can you solve this???lottery probability",
        "author": "kockarnica99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89ej2w/can_you_solve_thislottery_probability/",
        "text": "in lottery 6/45 odds are 1 in 8,145,060.there are 45 balls and only 6 comes out.. Can you calculate what is the odds if we have combinations that includes sum of numbers 106-170, odd/even 3:3, 2:4, 4:2, ,high/low ( low is 1 -22, high is 23-45) 3:3, 2:4, 4:2 in one combination.. example:lottery 6/45 :3,17,21,30,36,40--we can se the sum is between 106-170, odd/even 3:3,high/low3:3 ..we reduce number of combination playing these 3 type/patterns in one combination..1:1 000 000? 1:2 500 000? how many combinations are there that includes these 3 type(106-170, odd/even 3:3,2:4,4:2,high/low 3:3,2:4,4:2.\n\nplease help?",
        "created_utc": 1522766881,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence interval for difference between means",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89drrd/confidence_interval_for_difference_between_means/",
        "text": "data : \n\n    a &lt;- c(45,32,36,32,40)\n    b &lt;- c(37,34,46,44,34)\n\n    mean-a = 37\n    mean-b = 39\n\n    sd a = 5.57\n    sd b = 5.66\n\nQuestion : **Calculate a 95 % confidence interval for the difference between the mean of A\nand B**\n\n# Answer\n\nConfidence interval for difference between means is computed as \n\n    (meanA - meanB) +- (t-statistic) * (standard error)\n\nThe degrees of freedom here is 8, as we have 10 samples.\n\nThe standard error for the difference between means is found as\n\n    sqrt(s1^2/n1   +   s2^2/n2)\n\nWhich is \n\n    sqrt(5.57^2/5 + 5.66^2/5) = 3.355\n\nThe t statistic for this Confidence interval is \n\n    T(sig = 0.95, degrees of freedom = 8) = 1.86\n\nThen the confidence interval is found as\n\n    (meanA - meanB) +- (t-statistic) * (standard error)\n\n    (37-39) +- (1.86) * (3.355)\n    \n    (-2) +- 6.24\n    \nWhich is \n\n    (-8.24, 4.24)\n\n# answer (to the question)\n\nhttps://i.imgur.com/Jp67kQB.png\n\n\n",
        "created_utc": 1522762318,
        "upvote_ratio": ""
    },
    {
        "title": "For the following set of data, find the linear regression equation for predicting Y from X",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89dlwj/for_the_following_set_of_data_find_the_linear/",
        "text": "[deleted]",
        "created_utc": 1522761262,
        "upvote_ratio": ""
    },
    {
        "title": "Degrees of freedom of a model (moderation analysis)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89cj2s/degrees_of_freedom_of_a_model_moderation_analysis/",
        "text": "[deleted]",
        "created_utc": 1522753394,
        "upvote_ratio": ""
    },
    {
        "title": "A die problem",
        "author": "radiax10",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89bpc0/a_die_problem/",
        "text": "Imagine a SINGLE regular untampered-with dice numbered 1-6.\n\nThree people have to choose a group of 2 numbers from  dice such as 1&amp;3 or 4&amp;5 or whatever, but they cant pick the numbers another person has picked.\n\nIf the dice rolls one of the 2 numbers a person has picked that person loses and the game ends.\n\nSo the way to win is to pick numbers that have the least probability of getting rolled.\n\nMy question is: Is it more advantageous to your chances if you pick 2 numbers which are ADJACENT to each other on the dice? Or numbers that are OPPOSITE of each other on that dice?\n\nI just came up with this in my head and cant stop thinking about it, but should it even matter? ",
        "created_utc": 1522745879,
        "upvote_ratio": ""
    },
    {
        "title": "Statistically speaking, about what percent of humans have counted to at least 1??",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/89araj/statistically_speaking_about_what_percent_of/",
        "text": "[deleted]",
        "created_utc": 1522737394,
        "upvote_ratio": ""
    },
    {
        "title": "Need help multipling out an expression",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8985tn/need_help_multipling_out_an_expression/",
        "text": "[deleted]",
        "created_utc": 1522723094,
        "upvote_ratio": ""
    },
    {
        "title": "How to find a Marginal Probability density function given a set of values for X and Y?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/895r4p/how_to_find_a_marginal_probability_density/",
        "text": "Hey all,\n\nSo I'm rather new to this. I've been given a set of probabilities for 2 variables X and Y. This is the entirety of the dataset so the probabilities all add to 1. I've been asked to find the marginal probability density function for each variable, but I'm not quite sure how to do it. A little bit of research led me to believe that I should be looking at integrating the conditional distribution with respect to each variable, but I'm not sure how to derive that particular formula.\n\nApologies if this is a noob question and thanks for your help!",
        "created_utc": 1522707030,
        "upvote_ratio": ""
    },
    {
        "title": "What is the best way to overcome missing data in a 2 x 2 Mixed ANOVA?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8959eb/what_is_the_best_way_to_overcome_missing_data_in/",
        "text": "[deleted]",
        "created_utc": 1522702518,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate degrees of freedom for interactions between subscales/dimensions in G*Power (for Moderation Analysis using PROCESS macro/SPSS)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8954dx/how_to_calculate_degrees_of_freedom_for/",
        "text": "[deleted]",
        "created_utc": 1522701946,
        "upvote_ratio": ""
    },
    {
        "title": "Please Help: t-Test: Paired Two Sample for Means (Pre and Post survey)",
        "author": "Training022",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8951m5/please_help_ttest_paired_two_sample_for_means_pre/",
        "text": "Hello,\n\nMy organization created Pre and Post surveys. The Pre surveys were distributed before training and the post surveys were distributed post training. The Pre and Post were labeled with a unique number allowing us to match them to the one specific participant. I.E. We know how the same person answered before the training and after the training. The survey is a ranking scale from (1 to 5) ex. how familiar are you with this training subject? where 1 is not likely and 5 is very likely.  \n\nUsing excel I created three columns of \"participant\" \"pre score\" and \"post score.\" I then clicked data analysis \"data analysis\" then \"t-Test: Paired two sample for means\" \n\nIn the \"variable 1 range\" i put my \"pre scores\" column\nIn the \"variable 2 range\" i put my \"post scores\" column\n\nI did not input anything for \"hypothesized mean difference\"\n\nand \"alpha\" is .05 \n\nMy results were very confusing see below:\n\nHypothesized Mean Difference\t0\ndf\t42\nt Stat\t-9.89036677\nP(T&lt;=t) one-tail\t7.78843E-13\nt Critical one-tail\t1.681952357\nP(T&lt;=t) two-tail\t1.55769E-12\nt Critical two-tail\t2.018081703\n\nI have a few questions that i'm hoping you can help with. \n\n*item 1 Did I use the correct statistical test? (I saw on this subreddit, that sometimes organizations don't care for the statistical significance of training, but would rather see the difference in before and after. if so how do I calculate for the difference?)\n\n*item 2 Did I do it correctly?\n\n*item 3 Why are my scores showing that there is no effect? \n\n*Item 4 Any more advice? or Thoughts?\n\nApologies if I made a mistake, I'm new to Reddit. ",
        "created_utc": 1522701639,
        "upvote_ratio": ""
    },
    {
        "title": "Determining Win Chance W/ Low Sample Size",
        "author": "Genje55",
        "url": "https://www.reddit.com/r/AskStatistics/comments/893m3j/determining_win_chance_w_low_sample_size/",
        "text": "Say you have a 5v5 team game where all members are randomly selected, and you want to determine the percent chance each team will win. Say also that you only know win/loss records of the players of one team. How would you go about solving this problem?\n\nFor example, say Player A on has a winrate of 67% with 3 games played, and Player B on the same team has a winrate of 60% with 200 games played. It would seem intuitive to place a higher \"value\" with Player B's record since he has a higher sample size of games played.",
        "created_utc": 1522696200,
        "upvote_ratio": ""
    },
    {
        "title": "Correlations between properties of network nodes?",
        "author": "ryandmars",
        "url": "https://www.reddit.com/r/AskStatistics/comments/891g0z/correlations_between_properties_of_network_nodes/",
        "text": "Hi r/AskStatistics. I've got a problem that I'm a bit stuck on: can we safely examine correlations between properties of network nodes?\n\nBackground: My research is on a graph theoretic approach to brain networks. Essentially, we have 233 well-defined regions of interest, we get their fMRI BOLD time series, and define our graph based on pairwise Pearson correlations between the time series of these 233 nodes. After we binarize based on something like the top 20% of strongest connections, we have an undirected binary graph to work with.\n\nFor each node, we further calculate two quantitative properties, and we're interested in seeing how well these properties correspond to each other. We'd like to simply take the correlations between these two 233-element vectors, but were not sure if that's valid, since the nodes of the network are definitionally not independent observations, and the properties are defined with respect to the nodes &amp; edges.\n\nIs this a real issue? And if so, how can I examine quantitatively the correspondence between these two properties?",
        "created_utc": 1522688965,
        "upvote_ratio": ""
    },
    {
        "title": "Performing pre-/post-event analysis of employee population",
        "author": "pboswell",
        "url": "https://www.reddit.com/r/AskStatistics/comments/890t3i/performing_prepostevent_analysis_of_employee/",
        "text": "I am currently trying to perform regression analysis to understand what affects Tenure of an employee. Particularly, I noticed that we gave an across-the-board pay raise on 8/4/2017 (the \"cutoff\").  I want to control for this event by distinguishing each employee as a \"Pre\" or \"Post\" employee.  Obviously, the problem is overlap.  Some employees started 1 month before the cutoff and stayed for 5 months after the cutoff.  Some employees started 2 years before the cutoff and never made it past the cutoff.  Some employees have spent basically 50-50 of their time before and after the cutoff.\n\nLet's say Employee 1 has records for:\n\n1/1/2016 - 10/31/2016\n10/31/2016 - 2/14/2017\n2/14/2017 - 4/1/2017\n4/1/2017 - 8/1/2017\n8/1/2017 - 10/1/2017\n10/1/2017 - 2/1/2018\n\nNormally, I use regression analysis for future predictive purposes, hence I only want to use a priori inputs to predict an a posteriori output. But here, this is more of a descriptive analysis, so I am unsure how much a posteriori input I am allowed to use.\n\nSo, clearly in this example, for PRE-cutoff I definitely want to include 2/14/2017 - 4/1/2017, and for POST-cutoff I want to include 8/1/2017 - 10/1/2017\n\n...BUT:\n\n1) Do I include all the employee's records, even if they fall entirely outside of the appropriate time bound? (e.g. 1/1/2016 - 10/31/2016) This will mean that some covariates are NULL due to lack of data prior to 2017.\n\n2) Do I truncate the end-points that fall outside of my bounds? (e.g. convert 10/31/2016 - 2/14/2017 to 1/1/2017 - 2/14/2017) This will result in a new calculation of Tenure for that record.\n\n3) Assuming my target is Tenure (in months), how should I calculate this? That is, assume an employee started 4/1/2017, so they were active for only 1 month within the PRE-cutoff period, but most of their tenure is in the POST-cutoff period. What should I do here?\n\n4) Should I just calculate what % of tenure someone spent in each respective period, and only include the employee where they spent the majority of their time?  Must they spend at least 60% of their time in a particular cutoff period?  Etc.\n\n5) Finally, any other considerations that y'all may have?",
        "created_utc": 1522687142,
        "upvote_ratio": ""
    },
    {
        "title": "Odds of drawings a specific card two games in a row.",
        "author": "Boosted_Concepts",
        "url": "https://www.reddit.com/r/AskStatistics/comments/890kbu/odds_of_drawings_a_specific_card_two_games_in_a/",
        "text": "A friend and I are having a debate about the odds of something that happened to me today. Those who play hearthstone with understand but I will word this in a way that doesn't require it. I want to know the odds of \"burning\" the same card twice in 2 games when I only burnt one card per game. \n\nBoth games I drew my \"best card\" on the only turn that it could hurt me. The first game it was the 26th card I drew out of a 30 card deck. The second game it was the 20th card I drew out of a 30 card deck. \n\n1st game: I played a card that draws 5 cards while there were only 10 cards left in my deck, I had 5 cards in my hand and the hand can only hold 10 cards. Knowing that I was drawing 5/10 remaining cards, my odds of finding the card I needed were decent. The risk was I would burn 1 card at the start of the next turn because my hand was full. Then I would have 4 remaining cards. Long story short I burnt the card I needed. \n\n2nd game: Was identical to the first game except I drew 5 cards with 15 cards left in my deck. Burning 1 and leaving 9 in my deck. I once again burnt the card I needed. \n\nConsidering I drew 25 cards and 20 cards with no success only draw it at the wrong time. I know I had a 10% chance to burn the card when I elected to draw 5 cards in the 1st game and ~6% in the second game; but what are the odds that I never drew this card in the first 45 cards (collectively) only to burn it twice? \n\nThanks and I look forward to some educational responses! ",
        "created_utc": 1522686050,
        "upvote_ratio": ""
    },
    {
        "title": "What statistical fallacy is this?",
        "author": "berserkerscientist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88zocr/what_statistical_fallacy_is_this/",
        "text": "I see this error made a lot, but I'm not sure what to call it. For example, say 90% of prisoner are smokers, that doesn't imply that people who smoke will commit crimes. Or say 100% of people who become addicted to opioids had a prescription for them doesn't imply that taking opioids will cause you to become addicted. Or say the news interviews a centenarian, and they say they smoked and ate red meat, that doesn't imply that smoking and eating red meat will make you live longer.\n\nI don't think this is \"correlation, no causation\", or base rate fallacy. I usually say, \"statistics can't swim upstream\". If you want to see the effects of smoking or opioid prescriptions, you need to start with those populations first, not some other population.\n\nI've included this [Venn diagram](https://i.imgur.com/SPNcjbD.png), because I think it helps.",
        "created_utc": 1522679095,
        "upvote_ratio": ""
    },
    {
        "title": "ANOVA or ANCOVA?",
        "author": "daisysneal",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88zh0c/anova_or_ancova/",
        "text": "I want to see whether implicit age bias (as measured by an age IAT test) is reduced as a result of intervention condition. I have four conditions (one control and three interventions), and everyone was randomly assigned. Each participant has a pre implicit bias score and a post implicit bias score. I am trying to work out if i need to analyse this using a two way mixed anova or an ancova with pretest as the covariate, condition as the predictor, and posttest as the outcome. I can't work out which test is best for this situation? ",
        "created_utc": 1522677384,
        "upvote_ratio": ""
    },
    {
        "title": "What is this stats equation? (see comments)",
        "author": "stevenjd",
        "url": "https://i.imgur.com/gARkzLY.png",
        "text": "",
        "created_utc": 1522667194,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical Analysis of pre/posttest changes in pain scale measures?",
        "author": "throwaway11111111231",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88xadl/statistical_analysis_of_preposttest_changes_in/",
        "text": "Hey there, I was wondering if you guys could give me any input or check my understanding for the following scenario to make sure I've got my head wrapped around it.\n\nI'm currently trying to develop an RCT and was looking for some input on if the statistical analysis makes sense. The framework of my study is a four armed trial, two arms are different doses of a new pain medication, one arm is the standard of care (old intervention) for the condition and the last arm is a placebo control.  \n\nI had two goals of the study: Firstly, to see if this new medication provided a significant reduction in the pain score test compared to baseline measures (i.e. pre/posttest changes) relative to the placebo and identify if there is a dose dependent response. \n\nSecondly, to identify whether there is a significant difference between the pre/posttest scores when comparing the new intervention to the old intervention.\n\nI planned to use a one way ANOVA to first determine if there was a difference between groups then using Fisher's LSD or Tukey's post hoc to A.) compare the two different doses of the new intervention to the placebo and B.) compare the higher dose of the new intervention to the old intervention to see if there is a significant difference.\n\nAny input or suggestions to improve or alter the project would be greatly appreciated",
        "created_utc": 1522649511,
        "upvote_ratio": ""
    },
    {
        "title": "Half variance?",
        "author": "procrastinatingornah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88woaf/half_variance/",
        "text": "Hey guys, so, a little while ago we were learning about tangency protfolios in my mathematical finance course. We were having a general discussion on the way we construct such a basket using a minimum variance portfolio. During the discussion someone asked why we would account for \"good variance\" as in variance in a positive direction from a risk free rate (we were using 5%) for a given year (we were using historical yearly data from the past 25 years from 3 different stocks). thus signaling better returns, saying who cares how far away from the mean it is as long as its above the risk free rate. Our professor told us about the half variance (I think that is what he called it) where you only use the variances from the years that a stock was less than the 5% risk free asset, and allow the variance to be 0 when a stock did better than the risk free asset, thus, not \"punishing\" a stock for doing better than expected when minimizing the variance. So my question to you, in theory how does this work? Is it a viable way to minimize variance to maximize profits by basically forgetting about variance in a direction that would benefit the portfolio? What are the problems that could arise from emitting variance data from years where stocks out performed? ",
        "created_utc": 1522641925,
        "upvote_ratio": ""
    },
    {
        "title": "mgf problem?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88wlik/mgf_problem/",
        "text": "[deleted]",
        "created_utc": 1522641075,
        "upvote_ratio": ""
    },
    {
        "title": "Dyadic Data Analysis (pre-post) with multiple IV's and DV's ???",
        "author": "Ilovelove89",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88vta0/dyadic_data_analysis_prepost_with_multiple_ivs/",
        "text": "Hi All - Looking for help from anyone who has analyzed dyadic data. I'm finishing my PhD in counseling psych, and worked myself into quite a pickle with my stats analysis. Sorry in advance for the long post.\n\nHere are the basics of my design: Experimental Couple’s Intervention (repeated measures) -Unit(case): Dyads (n = 38 [76 inds]) -IV: Treatment group (3 levels) (between) -IV: pre-post (within) Covariate(? may have to toss if too complex): GQ DV’s: grateful mood (GMS), relationship satisfaction (DAS), relationship maintenance behaviors (RMB)\n\nI randomly assigned couples to the three intervention groups. Breakdown of N in the groups is: group 1 = 14 couples, grp 2 = 12 couples, and grp 3 (control) = 11 couples.\n\nHYP: the intervention will increase GMS which will then increase rel. satisfaction DAS and RMB (related DV's) Also hypothesized the intervention groups (1 &amp; 2) would have higher outcomes than the control group (group 3). Taking it one step further I hyp that grp 1 would be higher than grp 2 and 3 because of how the intervention was carried out.\n\nBecause they are couples, I believe theoretically the data must stay in a dyadic organization in order to effectively account for the interdependence. I tried to organize it so that I could use the dyad as the case (first column) and the “partner” as differentiating husbands and wives.\n\nI originally planned to do a MLM within the actor-partner interdependence model but I did not achieve the full number of couples I expected I’d need (n = 51 couples). I know its really tricky to even estimate sample size so… idk? I heard the model wouldn’t even run if I didn’t have enough data in the sample? I’d planned on a 2 level model with level 1 = measures and level 2 = people (in the dyad). Anyone have really basic instructions on how to test and see if this would run or not?\n\nAnother option I’ve been exploring is a factorial MANOVA (or MANCOVA), but I am unsure how to handle the dyads in a way that is theoretically sound (not just summing or averaging). What I was thinking is that I may set up the MANOVA where IV = group, COV = GQ, and DV is four occurrences (no clue if that’s the right term) of each assessment (1 per dyad (2) at two time points = 4). Is that even a thing? Or am I not really accounting for the dyad if I try to set it up that way?\n\nAny and all help is welcome. I'm mildly familiar with SPSS and very basically familiar with R (so may need some step by step instructions if at all possible).\n\nThanks in advance for any thoughts/suggestions/help! ",
        "created_utc": 1522633161,
        "upvote_ratio": ""
    },
    {
        "title": "Not sure how to analyze this data...",
        "author": "baabmf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88usjs/not_sure_how_to_analyze_this_data/",
        "text": "Hey guys. So this is probably a pretty easy question but statistics was never my strong suit. I have this data where we are following antibiotic susceptibility (can the drug kill the bacteria) over time. Basically the data looks like this\n\nBacteria A\n2016: number tested (N) =367, number susceptible to drug =281 (76.6%), number not susceptible to drug =86 (23.4%)\n2017: N= 336, susceptible =269 (80.1%), not susceptible =67 (19.9%)\n\nBacteria B\n2016: N= 128, susceptible =116 (90.6%), not susceptible =12 (9.4%)\n2017: N= 123, susceptible =113 (91.9%), not susceptible =10 (8.1%)\n\netc \n\nObviously it does not look like the rates change any between 2016 and 2017, but what test would I use to prove that there has been no change? Looking for a 95% confidence interval too. Thanks",
        "created_utc": 1522623594,
        "upvote_ratio": ""
    },
    {
        "title": "Lottery Odds",
        "author": "pyrofox18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88ulpf/lottery_odds/",
        "text": "I heard that statistically your odds are better for winning if you pick different numbers each time. Is there math that supports that claim? Or is it better to stay with the same numbers each time?",
        "created_utc": 1522621853,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence level",
        "author": "lenaabb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88uc15/confidence_level/",
        "text": "I am using descriptive statistics that gives me 0.005 as the mean and 0.009 as the confidence level 95%. What does it mean? Does it mean that my result is reliable?",
        "created_utc": 1522619515,
        "upvote_ratio": ""
    },
    {
        "title": "Help! Why kurtosis can be so large!?",
        "author": "lenaabb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88u5ht/help_why_kurtosis_can_be_so_large/",
        "text": "One of my data sets shows kurtosis of 12 and the other shows kurtosis of 3. Can anyone tell what does it mean? ",
        "created_utc": 1522617955,
        "upvote_ratio": ""
    },
    {
        "title": "Difference between ANOVA use and t-test use.",
        "author": "fineshit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88u3rq/difference_between_anova_use_and_ttest_use/",
        "text": "Let’s say I’m comparing the means of two populations and all conditions are met, which should I use and why?",
        "created_utc": 1522617542,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics major looking for advice!",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88tzk7/statistics_major_looking_for_advice/",
        "text": "[deleted]",
        "created_utc": 1522616526,
        "upvote_ratio": ""
    },
    {
        "title": "Variance of error terms",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88tndi/variance_of_error_terms/",
        "text": "I don't really follow this : https://i.imgur.com/nCsSO95.png\n\nIt makes sense that we'd get zeros for all the covariances if the error terms are i.i.d\n\nBut I'm not sure what that \n\nvar(y) \n\nterm at the bottom is about. \n\nIt's not been defined. \n\nIf I take the y (when I say y here, I'm referring to the y bar) to be the prediction, then this isn't going to be equal to the above.\n\nIf I assume that y is the mean, then that's not really going to make sense... and If I take y to be the observed values, I don't think that makes sense either. ",
        "created_utc": 1522613595,
        "upvote_ratio": ""
    },
    {
        "title": "How best to analyze differences in how two groups respond to the same treatment.",
        "author": "Snuggle_Taco",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88th3d/how_best_to_analyze_differences_in_how_two_groups/",
        "text": "I'm creating a mock study for school in which I'm comparing whether musicians post-stroke respond better to a language stimulation technique better than non-musicians post stroke. The hypothesis is that musicians would respond better and overall experience better language improvements.\n\nIt would be a single subject design where each subject in the study would have their language skills taken at baseline, taken throughout treatment, and then taken 2 weeks after treatment ceased. \n\nI'm not really familiar with how to analyze the data since my experience is mainly with randomly assigning participants to two different treatment groups and comparing group averages.\n\nAny suggestions?",
        "created_utc": 1522612120,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical analysis for survey using categorical, ordinal, and interval variables.",
        "author": "Jah348",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88t6s9/statistical_analysis_for_survey_using_categorical/",
        "text": "I'm assisting my girlfriend in writing a paper (happy Easter), and I've run into a wall. The subject is on childhood obesity, and how to lessen it. She's created a survey that is supposed to be sent to parents, but we are not sure what to do with the data afterwards. Is there a method to use, or should we alter survey in the first place. \n\nI've taken a look at the UCLA webpage, and while helpful, it did not solve my problem. The dependent variable is BMI. Here are the questions and how the variables are categorized:\n\n* Categorical/nominal \n  * Race\n  * Gender\n\n* Ordinal\n  * education\n  * Importance of the structure of family meals\n  * Concern of nutritional value\n  * Concern of childhood obesity\n  * Do you think modeling healthy behavior impacts your hild\n  * How difficult is it for you to model healthy behavior\n\n* Interval\n  * income\n  * number of family meals per week\n\n\n[Here's  the survey\n](https://www.surveymonkey.com/r/TK33JHT)\n\nI believe the two interval questions could be moved to ordinal, simplifying the types of data we have. Though that still leaves nominal.\n",
        "created_utc": 1522609742,
        "upvote_ratio": ""
    },
    {
        "title": "Why are statistics so demonized?",
        "author": "tayezz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88ppfd/why_are_statistics_so_demonized/",
        "text": "A common rebuttal to being confronted with statistics that contradict your claim is some variation of the \"statistics are made up/ biased/ unreliable\" argument. This seems to happen very often with controversial issues. What are the general markers for reliable stats and what should we look out for? What conditions make it very difficult to extract accurate statistical data?",
        "created_utc": 1522575017,
        "upvote_ratio": ""
    },
    {
        "title": "[Help] I need help to read a table",
        "author": "legendcollector",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88pk6w/help_i_need_help_to_read_a_table/",
        "text": "I am medical student and need to understand this table. What SE and B stand for? and how I can say with adjusted data shellfish consumption is associated with higher LDL. Thank you in advance.\n\nTABLE: https://imgur.com/a/YihGE\n\nFull Paper: https://www.sciencedirect.com/science/article/pii/S0013935116309835",
        "created_utc": 1522572744,
        "upvote_ratio": ""
    },
    {
        "title": "Misunderstanding CLM/Law of large numbers",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88oazw/misunderstanding_clmlaw_of_large_numbers/",
        "text": "[deleted]",
        "created_utc": 1522556450,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical Conclusions From Data",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88nyqc/statistical_conclusions_from_data/",
        "text": "[deleted]",
        "created_utc": 1522552800,
        "upvote_ratio": ""
    },
    {
        "title": "Why is multicollinearity a problem?",
        "author": "integralfallacy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88nr6m/why_is_multicollinearity_a_problem/",
        "text": "Recently in classes I've been learning about multicollinearity, and from what I'm understanding it's when independent variables in a regression have a high correlation with one another. Why would this be a problem, and could you maybe give a real life scenario example showing how this affects the data? Thank you!",
        "created_utc": 1522550583,
        "upvote_ratio": ""
    },
    {
        "title": "What would be the best way to approach this dataset?",
        "author": "Lemres17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88n12h/what_would_be_the_best_way_to_approach_this/",
        "text": "Hey guys. New here and I have no clue if I'm asking this in the right place but I need help. Bear with me as I try to explain this. \n\nSo I'm working on a research project to analyze and rank the most contaminated areas in the state of Georgia. I managed to make a list of all the counties in Georgia that contain 1 or more toxic sites by compiling the data found in Georgia's EPA Hazardous Site Inventory. From this, I made a Top 5 list with which I will need help. So my goal is to analyze and rank which pollutants are most commonly found in the Top 5 county list. I would like to make a Top 10 list of pollutants from this analysis. So to do that, I would have to go through each individual site for all 5 counties and make a list of pollutants. For example, Site A for Cobb County might have 12 pollutants, while Site B has only 5. So Cobb will have 17 total and then I'd move on to the next county. However each pollutant in each site is further categorized by soil or groundwater. So it can be found in soil, groundwater, or both.\n\nLink to Hazardous Site Inventory: http://epd.georgia.gov/sites/epd.georgia.gov/files/2017HSI/10145.pdf\n\nMy professor introduced me to someone in the Statistics Department who was more knowledgeable in SAS. He was able to draw out a quick sketch of what the table should probably look like. Then he further went on to say that SAS would be the perfect program to crank out this kind of table and using proc freq is the way to go. He said that SAS has the ability to create both of these tables in the sketch. I'm just confused on what the code should be for both of them. I know proc.freq is used for the second one but I think it would need some tweaking that I'm not familiar with. For the code to make the first table to include all the variables, I have no clue on what the code would be.\n\nLink to table sketch: https://i.imgur.com/buHuqvi.jpg\n\nIndependent Variables = Site, County\nDependent Variables = Pollutant. Soil and Groundwater are dependent on Pollutant (?)\n\nEdit: Thought it would be more clear to see, but Pol = Pollutant. S and W means Soil and Water (Groundwater), respectively. For S and W, he used 0 and 1 to indicate whether the pollutant was found in soil or groundwater, just to be clear. Kinda like how 0 and 1 is used to indicate Gender.\n\nSorry if this sounds all over the place. I'm just really confused on how to get started on this. Any help would be REALLY appreciated.",
        "created_utc": 1522542997,
        "upvote_ratio": ""
    },
    {
        "title": "Preconditions for applying T-Test",
        "author": "97amarnathk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88l60q/preconditions_for_applying_ttest/",
        "text": "Okay, I am taking an introductory course in statistics and we are learning about hypothesis testing and student's t-test.\n\nAs a part of an assignment, I am supposed to verify whether t-test requires:\n\n1. Homogenity of variance\n2. Normal population\n3. independence of data\n\nHow do I go about verifying these requirements experimentally?",
        "created_utc": 1522525783,
        "upvote_ratio": ""
    },
    {
        "title": "Extracting a weighted subset from a mixture of datasets? Should be simple, but I'm at a loss...",
        "author": "ArrowThunder",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88g8sq/extracting_a_weighted_subset_from_a_mixture_of/",
        "text": "Please help, I'm fairly confident that this should be a simple problem but I'm struggling with how to solve it.\n\n#####The Problem#####\nLet *x* be some set of data of size N. Let *a* be a subset of *x* of size *k*, where *k* &gt; N/2. Let *b* be the subset of *x* which is not in *a*. Let *kmin* equal the lowest possible integer value of *k*. Let *ε*n equal our best estimate of the probability that some single value *x*n is in *a*. To that end, *ε* is a set (of size N) of numbers between 0 and 1.\n\n######Useful information#######\n\nWe know *a* is actually a sample of a normally distributed population of some **unknown** mean *µ* and some **known** standard deviation *σ*. \n\n######Objective######\nObtain both the best estimation of *a̅* (the mean of *a*) as well its corresponding *ε*. **Edit2** For clarification, *a̅* should be a weighted mean based on *ε*.\n\n#####What I have so far#####\nIf it helps, my approach so far is along the lines of:\n\n 1. Obtain the lower and upper bounds of *a̅* (by taking the mean of the lowest *kmin* values of *x* and the highest *kmin* values of *x*, respectively).\n 2. Define some function *f(a̅)* which, taking *a̅* as a guess, calculates the best *ε* and returns *∑(εi)*.\n 3. Find the maximum of *f* over the bounds found in step 1. If max(*f*) &lt; *kmin*, then we know that *x* does not fit the original assumptions and is therefore an invalid dataset.\n\nI've been stuck trying to come up with *f*.\n\n#####Conclusion#####\nFor the record, I am completely aware of the limited applicability of this problem (because you're effectively forcing a dataset to conform to your preconceptions), but the problem exists in a situation where the reliability of the preconceptions is infinitely higher than the reliability of the data (because of the known possibility of the presence of some subversive subset *b* in *x*).\n\nThanks for the help!\n\n**Edit:** Reformatted for easier reading\n\n**Edit 2:** Clarified goal.",
        "created_utc": 1522469008,
        "upvote_ratio": ""
    },
    {
        "title": "Best Approach for Interrater Reliability.",
        "author": "RyanChrist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88fxrt/best_approach_for_interrater_reliability/",
        "text": "I'm working with data that isn't of the traditional form for determining interrater reliability and I'm trying to determine the best approach. Any tips would be greatly appreciated.\n\nThere are a number of evaluation pairs (an instance of the same item being evaluated by two different raters) for a number of different boolean variables. There are just under 100 pairs of evaluations and each pair is a unique item being evaluated. There were about 6 raters in total, but the items were randomly assigned and I do not know which rater corresponds to which evaluation. \n\nHere is an example to illustrate:\n\nData ID | Q1  |  Q2  | . . .\n\n1     |       True  |  False\n\n1     |       True  |  True\n\n2     |       True  |  False\n\n2     |       False  | True\n\n3     |       True  |  True\n\n3     |       True  |  True\n\n. . .\n\nWhat I know for certain is that each item of a pair (the same Data ID) was evaluated by a different rater and each rater performed the same number of evaluations. \n\nIs there a way to proceed without identifying the raters and their corresponding evaluations?",
        "created_utc": 1522465454,
        "upvote_ratio": ""
    },
    {
        "title": "Unsure if I have significance or not",
        "author": "Looney-Lovegood",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88dqrj/unsure_if_i_have_significance_or_not/",
        "text": "I've ran a multiple regression and gotten a significance figure of 0.25: However I'm very tired and have a lot of anxiety with stats and I can't figure out if I have significance or not, and if I do how strong said significance is...can someone help? &gt;.&lt;",
        "created_utc": 1522444002,
        "upvote_ratio": ""
    },
    {
        "title": "PCA and Statistica",
        "author": "osteoblastah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88chqm/pca_and_statistica/",
        "text": "Howdy a..\nI am using Statistica to run PCA analyses as part of my dissertation project but have a few question as the program help files still leave me with questions.\nWhen thinking in terms of component scores/loading for each case, do you recommend using the factor scores by case option or factor coordinates by case option? I know the output for the variables is coordinate information but both factor scores and factor coordinates are available for the by case reporting, and I am not sure which to use. \nThank you very much in advance for helping me with this seemingly simple question but I can't seem to find a straight forward and comprehensible answer... thanks. ",
        "created_utc": 1522433924,
        "upvote_ratio": ""
    },
    {
        "title": "How can I select the best predictor variables in a multiple linear regression?",
        "author": "Pie_Flavoured_Pie",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88cdpr/how_can_i_select_the_best_predictor_variables_in/",
        "text": "I'm currently working on this assignment for a class where we have to do a multiple linear regression. The data set that we're using has over 500 variables and I have to select the variables that will most accurately predict the number of transactions of a restaurant depending on the nearby businesses. I've checked out a lot of articles and seen suggestions such as using a correlation matrix or running regressions and seeing how the adjusted p value changes when a new variable is added. Right now I'm inclining towards using correlation because there are too many variables for me to run regressions and check how the p value changes when I add or remove a variable. For this assignment we're using python if that helps. Any help would be appreciated. ",
        "created_utc": 1522433066,
        "upvote_ratio": ""
    },
    {
        "title": "Nested models discrimination",
        "author": "Armavica",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88bxxl/nested_models_discrimination/",
        "text": "**Context**\n\n* I have got temporal data of a process.\n* I have got two parameterized models of this process, `M0(a)` and `M1(a, b, c)` (`a`, `b` and `c` are the parameters), such that for all `a` and `b`, `M0(a) = M1(a, b, 0)` (the model `M0` is the same as the model `M1` with `c = 0`, in which case `b` doesn't matter).\n* I would like to know if it is more reasonable to think that my data can just be explained by the model `M0`, or if I need the model `M1`. (In reality I just want to discriminate between `c = 0` and `c != 0`.)\n\n**Question**\n\nI have read tutorials talking about the [F-test](https://en.wikipedia.org/wiki/F-test), which seems to be adapted to my problem. I understand that this test compares two SSR values (sum of squared residuals) and decides, also based on the number of parameters of the models, which one it is more reasonable to accept. In practice, if I understand correctly, I would need to first fit `M0` (find the parameters of `M0` which correspond best to the data) : `M0(a0)`, fit `M1`: `M1(a1, b1, c1)`, and compare the SSRs, which would tell me which of `M0(a0)` or `M1(a1, b1, c1)` is the most probable. But does it really answer my question? I have just compared two \"champions\" (`M0(a0)` is a specific instance of `M0`, and `M1(a1, b1, c1)` is a specific instance of `M1` as well), which does not tell me whether `M0` or `M1` is more reasonable, does it? If the F-test tells me to prefer `M0(a0)`, I just know that the *best* `M0` is more probable than the *best* `M1`, but I am not sure how it should follow that `M0` is more probable than `M1` *in general*.\n\nI would really appreciate if you could direct me to resources to understand this!",
        "created_utc": 1522429754,
        "upvote_ratio": ""
    },
    {
        "title": "How to use GDP as a control variable?",
        "author": "IamNowUpsideDown",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88bmui/how_to_use_gdp_as_a_control_variable/",
        "text": "I am writing a research paper on the relationship between level of democracy and environmental protection. I want to use GDP as a control variable, but am wondering how to do it. Should it be by annual growth, or GDP per capita? \nAnd also, where can I find data about GDP? World Bank and IMF both of course have loads of info about it, but I can't find a good downloadable dataset with GDP for 2017. Any ideas?",
        "created_utc": 1522427404,
        "upvote_ratio": ""
    },
    {
        "title": "Clarifying question about when we can use a t-test",
        "author": "speycrys",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88b2rb/clarifying_question_about_when_we_can_use_a_ttest/",
        "text": "So lets say I have a hypothesis distribution of observational units into 5 categories. And lets say that I have a sample with an empirical distribution of observational units into the same 5 categories.\n\nTo use a t-test on the amount of obs in a particular category, I need to have an assumed distribution of my population estimate, correct? Otherwise it makes no sense to do a t-test, since I know nothing other than the expected value of the estimate, and I need to know the standard deviation.\n\nSo in other words, in order to run a t-test on a sample parameter, I need to know the DISTRIBUTION of my hypothesis parameter, not just its expected value.",
        "created_utc": 1522422963,
        "upvote_ratio": ""
    },
    {
        "title": "One way MANOVA test in Graphpad Prism?",
        "author": "Statshelpme",
        "url": "https://www.reddit.com/r/AskStatistics/comments/88a0q0/one_way_manova_test_in_graphpad_prism/",
        "text": "Is it possible? I can't find an option for it in the drop down list, is there a way to run a one way ANOVA test as a MANOVA test? Are there any free softwares that will do a MANOVA test and produce p values and graphs?",
        "created_utc": 1522413498,
        "upvote_ratio": ""
    },
    {
        "title": "[Really stupid question about language] If one year 10% of the population is unemployed, and the next year 11% is unemployed, would it be accurate to say that the \"percentage of people unemployed\" rose 1% or 10%? What about if it was worded as \"unemployment percentage\"?",
        "author": "sge33",
        "url": "https://www.reddit.com/r/AskStatistics/comments/889nr3/really_stupid_question_about_language_if_one_year/",
        "text": "",
        "created_utc": 1522409346,
        "upvote_ratio": ""
    },
    {
        "title": "Help interpreting weak correlation coefficient yet 'significant' P-value",
        "author": "flairesmap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/889fqa/help_interpreting_weak_correlation_coefficient/",
        "text": "Hi all, sorry if this isn't the right place to ask this!\n\nI'm fairly new to statistics and trying to interpret the results from this study http://www.clinicalnutritionjournal.com/article/S0261-5614(16)00035-2/fulltext \n\nIn particular, under the 'results' section, the inflammatory dietary pattern (IDP) obtained using reduced rank regression positively correlates with IL-6 results with a r=0.08 and p&lt;0.001 in phase 3 clinical examination and a r=0.13 and p&lt;0.001 in phase 5.\n\nWould someone be able to explain simply how you are able to have a weak correlation coefficient yet a statistically significant P-value? Googling this seems to come up with answers relating to the population number, but are far too complex for my level of knowledge. \n\nThanks in advance",
        "created_utc": 1522406431,
        "upvote_ratio": ""
    },
    {
        "title": "What is the difference between forecast combination and forecast ensemble?",
        "author": "mertblade",
        "url": "https://www.reddit.com/r/AskStatistics/comments/888bdl/what_is_the_difference_between_forecast/",
        "text": "Hello, I am trying to understand forecast combination and forecast ensemble methods. To me, they look the same - both are trying to minimize the error between forecasts and validation data-.\n\nThere are also multiple terminologies to define these concepts - Forecast combination (combining forecast), Ensemble learning, forecast ensemble, ensemble forecasting and so on.\n\nAre there any difference between these two? Could you please recommend me articles that define the differences and provide clear description for both methods?\n\nforecast combinations: Timmermann, Allan. \"Forecast combinations.\" Handbook of economic forecasting 1 (2006): 135-196.",
        "created_utc": 1522390407,
        "upvote_ratio": ""
    },
    {
        "title": "Narrowing down the number of tests of association - is it appropriate?",
        "author": "bioinformaticsnewb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/886wwt/narrowing_down_the_number_of_tests_of_association/",
        "text": "I want to run a large number of tests of statistical association between a few categories (think Fisher's exact test). However, because I am running a very large number of tests, my multiple testing correction (Bonferroni) ends up quite strict, and so if possible, I would like to reduce the effective number of tests I am running.\n\nWould it be statistically inappropriate to test only combinations of observations that, prior to accounting for multiple testing, are significant?\n\nSay I have 100 observations, and they break down into a contingency table that looks like the following:\n\n    50    0\n    0    50\n\nA table like this quite obviously shows a significant association, prior to multiple test correction. Whether the p-value produced by a contingency table such as this one remains significant after multiple testing correction clearly depends on the number of tests run. However, consider the following table:\n\n    25    25\n    25    25\n\nThis one is obviously not significant, regardless of multiple testing correction or not.\n\nSo what about something like this?\n\n    32    18\n    23    27\n\nThe two-tailed Fisher's exact p-value for this one equal ~0.1, so not significant prior to multiple testing correction either.\n\nMy question is, is it inappropriate to exclude tests such as the one (immediately) above from counting as a test in multiple testing correction? The logic being that it has no chance in the first place of producing a significant value, and so there it is impossible for it to be significant after multiple testing correction.\n\nAn issue such as I describe above seems to be related to the limitations of certain statistical tests, extrapolating to regression and related methods. If a regression I run to test for association cannot possibly produce a significant value given the \"counts\" of the data, then is it at all appropriate to run such a test in the first place with the given method? My first thought is that no, it is not appropriate, and that a method without such a limitation should be chosen in order to test those cases.\n\nBut something about my logic seems well...wrong, as well. I really don't know if this would be an appropriate way to narrow down the number of tests performed, or whether it is a form of \"hacking\" to increase the chances of finding significant results, and is a big no-no in statistical analysis.\n\n",
        "created_utc": 1522375002,
        "upvote_ratio": ""
    },
    {
        "title": "What type of ANOVA?",
        "author": "anti_pope",
        "url": "https://www.reddit.com/r/AskStatistics/comments/886wh3/what_type_of_anova/",
        "text": "I'm unfamiliar with all the terms used for this type of statistical analysis... I have two groups. These groups have different sample sizes. There are two variables and Y is binned by X and each bin has different statistics. I want to know if the means of Y within the bins are significantly different between the two groups. It also shouldn't be assumed that the two groups have the same variance for each bin.\n\nIs this commonly stated as single factor with multiple treatments?\n\nParticularly, I would love to know how to do this test in MATLAB.\n\n",
        "created_utc": 1522374870,
        "upvote_ratio": ""
    },
    {
        "title": "Variances estimates - matrices",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/886oao/variances_estimates_matrices/",
        "text": "\nI'm not sure how to interpret these notes\n\n# context\n\nX ~ N(80, 225)\nY ~ N(70, 400)\n\ncorrelation between X and Y is 0.84\n\ncov(X, Y) = 252\n\nvariance covariance matrix : https://i.imgur.com/vqG73Sf.png\n\n# Second part - that I'm confused about \n\nhttps://i.imgur.com/2fuAotO.png\n\nI don't understand what we're doing here, I can see that we're heading towards\n\n    sd(X - Y) = 11\n\nBut I don't see why we have [this](https://i.imgur.com/2fuAotO.png) setup.\n\n**I don't see how to relate** [this](https://i.imgur.com/vqG73Sf.png) to\n[this](https://i.imgur.com/2fuAotO.png).  \n\nThis is my main question.\n\nIf anyone could explain that it would be appreciated",
        "created_utc": 1522372709,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing two intercepts from regression output",
        "author": "wcsbackwards",
        "url": "https://www.reddit.com/r/AskStatistics/comments/886now/comparing_two_intercepts_from_regression_output/",
        "text": "Hello, \n\nI have a data set of male and female morphological values for several different species. I am interested in knowing whether allometric patterns are different between males and females once their phylogenetic relatedness has been taken into account. If I didn't have to take relatedness into account, this would be a simple ANCOVA with SEX as a categorical variable, SIZE (mass) as the covariate, and SEX*SIZE as the interaction. \nThe program that accounts for phylogenetic relatedness recognizes data by species, so I can't code males=1 females=0 and run an ANCOVA. I can make the program run separate (phylogenetic-controlled) regressions for males and females that will output slopes and intercepts with their respective standard errors. \n\nDo I need to independently test the slopes and intercepts from these outputs (t tests for each male vs female slope and intercept), and if so how do I do that? Or, what is the best way to test the differences here? Am I SOL for running an ANCOVA?\n\nAny help is greatly appreciated!",
        "created_utc": 1522372552,
        "upvote_ratio": ""
    },
    {
        "title": "ANOVA vs. Mann Whitney U test for comparing data between two groups, when there are more than two groups in the experiment?",
        "author": "SEXPILUS",
        "url": "https://www.reddit.com/r/AskStatistics/comments/886f1g/anova_vs_mann_whitney_u_test_for_comparing_data/",
        "text": "Hi there,\n\nI need to choose a statistical test to analyse some mouse trial data, and there's a divide in my lab between people who use ANOVA (I'm assuming Kruskal–Wallis one-way) vs. Mann Whitney U test.\n\nHere's an example of the type of thing we're looking at:\n\nMultiple groups with 5 mice per group:\n\n* Group A - uninfected\n* Group B - infected organism 1\n* Group C - infected organism 1 gene knockout\n\nThe type of data we collect for comparison between groups:\n\n* weight\n* level of colonisation of the infecting organism\n* level of inflammation in the area of infection\n\nThe main thing we want to know is if the infected mouse data is significantly different from that of the uninfected, or gene knockout-infected mice. So in that way, we're only comparing two groups to each other at a time (e.g. uninfected vs, infected; and then knockout infected vs. infected).\n\nFrom what I understand, the Mann Whitney will compare two groups without consideration of the other group's data, but an ANOVA will take all the data into account, and I can add a Dunn's multiple comparison test to determine statistical significance between specific groups.\n\nAny ideas on which is the right way to go here? For reference, I'm using Prism Graphpad for my analyses, and I'm strictly a biologist and know very little about statistics, so if there's anything worth explaining here, please ELI5!",
        "created_utc": 1522370255,
        "upvote_ratio": ""
    },
    {
        "title": "What does it mean when there are two different percentages (P/A, %, %, n)",
        "author": "[deleted]",
        "url": "https://i.redd.it/r35chxxikso01.png",
        "text": "[deleted]",
        "created_utc": 1522368195,
        "upvote_ratio": ""
    },
    {
        "title": "Pearson's Chi Squared Test",
        "author": "uditmodi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/885o4s/pearsons_chi_squared_test/",
        "text": "Can you use a chi-squared test when the expected values are different? For example, I have a set of data with expected frequencies for a number of individuals in various income brackets based on the US population than an actual number derived from a survey. ",
        "created_utc": 1522363666,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting multinomial logistic regression- parameter estimates- Expected Bs far too high",
        "author": "daisysneal",
        "url": "https://www.reddit.com/r/AskStatistics/comments/885dvl/interpreting_multinomial_logistic_regression/",
        "text": "I've bitten off way more than i can chew here! And am trying to understand output for statistics that are far beyond what we get taught on my undergrad psych degree. I am trying to interpret all significant predictors to my variable of interest, and was doing fine(ish) until i got (exp)B values far far higher than 2, so e.g. one predictor was significant at .03 and the exp B was .183 then they're 18% less likely to choose the outcome of interest compared to the reference category (i think). But then some of my output shows significant at .000 and then exp B's are insanely high e.g. 12,375,466.78 and i have no idea how to interpret it. I'm guessing the results look so bad/strange because the standard errors are really high but i don't know how to fix it/ interpret it. Any advice would be so appreciated ",
        "created_utc": 1522361195,
        "upvote_ratio": ""
    },
    {
        "title": "Why is a sig test of sub-group vs. non-subgroup the same as sub-group vs. entire group(including subgroup)?",
        "author": "Adamworks",
        "url": "https://www.reddit.com/r/AskStatistics/comments/882mzl/why_is_a_sig_test_of_subgroup_vs_nonsubgroup_the/",
        "text": "I've run into a weird work problem where the client has asked us to make subgroup comparisons with the entire group e.g.,\n\n    A vs. A + B + C\n\nMy initial reaction was that is not right, you should rather do:\n\n    A vs. B + C\n\nBut they responded to show me that it was a \"industry\" standard (which it appears to be) and after further digging, it appears to be the same analysis just framed differently. It is also mentioned as one of the comments in this Stackexchange question: https://stats.stackexchange.com/questions/134627/compare-means-of-group-with-a-mean-of-a-subgroup\n\nNot sure I can wrap my head around this one. Can anyone provide insights?\n\n",
        "created_utc": 1522340684,
        "upvote_ratio": ""
    },
    {
        "title": "Help solve a debate between friends about the definition of correlation",
        "author": "xwordhelp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/882igu/help_solve_a_debate_between_friends_about_the/",
        "text": "So I got into an argument with some friends the other night about varying degrees of relationship between two variables. I'm no mathematician, so my apologies if I'm explaining this poorly.\n\n---\n\nTo start, we assumed and agreed that there are 3 levels of a relationship two variables can have (well, 4 if you count absolutely no relationship whatsoever, even a coincidental one)\n\n(1) causation, where there's clear and direct cause. Example: the number of hours per day I sleep directly correlates with the number of hours per day in which I'm awake.\n\n(2) indirect correlation, in which there isn't direct causation but there is some common link or systemic factor(s). Example: areas with higher poverty rates tend to have higher crime rates.\n\n(3) spurious correlations or coincidence, in which two variables happen to track each other but there is neither causation nor an underlying similar root. This is perhaps best exemplified by the site [SpuriousCorrelations](http://www.tylervigen.com/spurious-correlations) which, for example, shows that US spending on science &amp; technology correlates with suicides by hanging, strangulation, or suffocation with an *r* of 0.99789.\n\n**Person A** was stating that the definition of a correlation is simply that when one variable increases (or decreases), so does another variable. So person A declared that all 3 of these examples showed a correlation between two variables.\n\nIn contrast, **Person B** was stating that data that cannot lead you to accurately predict future outcomes by definition cannot constitute a correlation. In other words, a correlation can only exist if there is some actual real-world relationship between the two variables whether it be indirect or direct (aka, causal). And as such, he would say that the above scenarios #1 and #2 show a correlation but #3 does not.\n\n---\n\nThoughts?",
        "created_utc": 1522339791,
        "upvote_ratio": ""
    },
    {
        "title": "Variation between two-way ANOVA tests?",
        "author": "Statshelpme",
        "url": "https://www.reddit.com/r/AskStatistics/comments/881hjc/variation_between_twoway_anova_tests/",
        "text": "I'm gonna start by saying I know nothing about stats I've not done any in 6 years and even then I barely passed my classes. \n\nI've been told to do a two-way ANOVA on a set of data I have from a computer model that gives graphs with a series of peaks. I'm comparing two peak values and their respective times, mean of all peak values, median and the number of peaks produced, for 3 runs of a control, 3 runs of a variable that was doubled from the control value, and the same for x5 and x10 of the same variable. \n\nI put the data into Prism GraphPad (no matching, within each row compare columns, Tukey test) and it seemed fine but the graphs were too cramped so I split the data into 4 to have 4 different graphs (2 peak values, 2 peak times, averages, and no. peaks). The ANOVA test was automatically run on these split data sets and produced different p values. I also noticed this happened when I grouped all x2 data sets, x5 and x10, (where different variables were changed) on 3 separate tests. \n\nDoes anyone know why this is happening? Is there any way to stop it? Am I just running the wrong test and if so have you got any recommendations?",
        "created_utc": 1522332125,
        "upvote_ratio": ""
    },
    {
        "title": "Simple Question about two mean values",
        "author": "throwawayolli",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8818wi/simple_question_about_two_mean_values/",
        "text": "How do I test the level of significance  between to mean values? For example, if there is a significant difference between number of males in a test group v. Number of males in a control group",
        "created_utc": 1522330067,
        "upvote_ratio": ""
    },
    {
        "title": "Sum of Squares: Type I vs Type II vs Type Iii",
        "author": "JustJumpIt17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/880uix/sum_of_squares_type_i_vs_type_ii_vs_type_iii/",
        "text": "I’m wondering if someone could help me understand the differences between these types of sums of squares. More specifically as they pertain to ANCOVA models.",
        "created_utc": 1522326466,
        "upvote_ratio": ""
    },
    {
        "title": "Smoothing sparse data sets",
        "author": "kouhoutek",
        "url": "https://www.reddit.com/r/AskStatistics/comments/87zgs9/smoothing_sparse_data_sets/",
        "text": "I want to know how much rain falls on a typical rainy day throughout the year.  Note this is different than overall average rainfall, I don't care about dry days. Given it is raining, I want to know how much rain can I expect.\n\nI have a list of all the rainy days in the past 10 years, and have collated it into this kind of data:\n\nJulian Day|Rainy Days|Average Precip (mm)\n:--|:--|:--\n30|0|n/a\n31|0|n/a\n32|3|10.7\n33|1|20.0\n34|1|6.0\n35|1|8.0\n36|3|6.0\n37|0|n/a\n\nThere are days when it didn't ever rain in my data set.  That doesn't necessarily mean I can expect day 31 to get less rain than day 32, that might just be sampling error.  In the same way, I don't think 33 is more prone to a deluge the 34, it is an outlier.\n\nI've worked out a couple of ways to average days with their neighbors.  If I represent day 33 as the weighted average of days 30-36, that works out to more representative looking 93 mm.  I was thinking about adding a normal weighting on top of that, putting less weight on more distance days, but I realized I was just making it up as I went along.\n\nWhat would be the statistically proper way to smooth out this data?  I want to be able to say things like \"if it rains on day X, on average, we will get Y mm of rain\".",
        "created_utc": 1522309783,
        "upvote_ratio": ""
    },
    {
        "title": "Perform an appropriate hypothesis test using the p-value approach at alpha = 1% ? For starters, how the heck do I find the difference?",
        "author": "mirsss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/87yywq/perform_an_appropriate_hypothesis_test_using_the/",
        "text": "A company called QT inc cells ionized bracelets called q-ray bracelets, that it claims help to ease pain through balancing the body's flow of \"electromagnetic energy.\" The Mayo clinic decided to conduct a statistical experiment to determine whether the claims for the q-ray bracelets were justified.\n\nAt the end of the four weeks, of the 305 subjects who wore the \"ionized\" bracelets, 236 reported improvement in their maximum pain index. Of the 305 subjects who wore the placebo bracelet, 235 reported improvement in their maximum pain index. Perform an appropriate hypothesis test using the p-value approach at alpha = 1%.\n\nI need to use the formula to = (d bar - µ0)/sd/sqrt(n) \nI am so stumped :(\n\nCrossposted",
        "created_utc": 1522303479,
        "upvote_ratio": ""
    },
    {
        "title": "Stats on Card packs",
        "author": "angled_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/87xw4z/stats_on_card_packs/",
        "text": "I play a card game and am very frustrated with my card collection.\n\nWith a new expansion in the game coming out soon I began to wonder how many packs would it take me to collect every card in the new expansion.\n\nWith this in mind, I opened up excel and gathered the data necessary to answer my question.  \n\nImmediately after I put all the data in my spreadsheet I was lost. I have no idea where or even how to proceed. Can someone point me in the right direction? I'm treating this as a hobby so the answer is not my priority. I am more interested in the process of how to arrive at the answer.\n\nDATA \n(1 pack= 5 cards) - \n(1 rare  per pack) - \n(1 epic per 10 packs) - \n(1 legendary per 40 packs)- \n(135 cards in total)  of that total \n49 are common with a 71% chance of appearing\n36 are rare with a  22% chance of appearing \n27 are epic with a 4% chance of appearing \n23 are legendary with 1% chance of appearing\n",
        "created_utc": 1522291594,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate the adjusted misclassification rate from classification matrix",
        "author": "hmt28",
        "url": "https://www.reddit.com/r/AskStatistics/comments/87xr6q/how_to_calculate_the_adjusted_misclassification/",
        "text": "My original dataset had a target (1) of 1%.  I proceeded to oversample the data, and created the following classification matrix:  \n                                                 \n\t        0 (Actual)\t1 (Actual)\n0 (Predicted)\t310\t        90                                                                        \n1 (Predicted)\t130\t        270\n\nNow I have to calculate the adjusted misclassification rate of the model, but am unsure of how to adjust my numbers for the oversampling. \n\nI know misclassification can be calculated as: \n(FP+FN)/total\n\nAdditionally, why do I have to calculate the rate as adjusted versus just the misclassification rate as it stands now with the oversampled data?\n\nEDIT: sorry, I cannot get the matrix to appear better, but am working on it.\n",
        "created_utc": 1522290271,
        "upvote_ratio": ""
    },
    {
        "title": "Stats of opening card packs",
        "author": "[deleted]",
        "url": "https://i.redd.it/50shlewo3mo01.png",
        "text": "[deleted]",
        "created_utc": 1522289875,
        "upvote_ratio": ""
    },
    {
        "title": "Difference between scatter plot and scatter diagram?",
        "author": "Johnn_63",
        "url": "https://www.reddit.com/r/AskStatistics/comments/87xmyg/difference_between_scatter_plot_and_scatter/",
        "text": "",
        "created_utc": 1522289147,
        "upvote_ratio": ""
    },
    {
        "title": "Help me learn stats!",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/87xmo3/help_me_learn_stats/",
        "text": "[deleted]",
        "created_utc": 1522289071,
        "upvote_ratio": ""
    },
    {
        "title": "Cohen's Kappan over multi-class / label classifier?",
        "author": "crowoy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/87x9jg/cohens_kappan_over_multiclass_label_classifier/",
        "text": "I have 3 unbalanced classes. I am trying to calculate the Cohen's Kappa of the classifier. The classes are balanced as follows:\n\nClass A: 31.2%\nClass B: 40.8%\nClass C: 27.9%\n\nHow do you calculate the `expected accuracy` in the Kappa formula:\n\n`(observed accuracy - expected accuracy) / (1 - expected accuracy)`",
        "created_utc": 1522285636,
        "upvote_ratio": ""
    }
]