[
    {
        "title": "Which of these T-Tests? Paired vs Independent",
        "author": "Samuuuh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102owsc/which_of_these_ttests_paired_vs_independent/",
        "text": "I  would like your opinion on an assignment I am working on. We are tasked with measuring the performance of two systems, measuring the time each person takes to finish a task to work out whether one system is better than another.\n\nI  am arguing to use the paired sample t-test because it is the same person that we are measuring at two systems. (i.e. time, error, etc). \n\nMy friend thinks it is independent because each system is independent from each other, since they're built differently (one system uses a touchpad and another system uses keyboard and mouse, although the tasks are the same).",
        "created_utc": 1672792912,
        "upvote_ratio": 1.0
    },
    {
        "title": "Compare same population with different guidelines",
        "author": "cakecup24",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102njew/compare_same_population_with_different_guidelines/",
        "text": "Hi all, \n\nWas hoping to get an answer to this to help with an internal audit at our workplace. We have implemented a new guideline (10% increase after taking Ventolin) to see if people will be diagnosed with asthma. \n\nPreviously this was 12% increase after taking Ventolin. \n\nWe have found a 2.5% increase in people (145 people v 119 people) being diagnosed with asthma based on the new guideline. We would expect this as the criteria has been reduced to 10%. \n\nIs there a way to see if this change is significant?\n\nThanks all.",
        "created_utc": 1672789476,
        "upvote_ratio": 1.0
    },
    {
        "title": "Transform set of raw scores to percentile ranks in jamovi",
        "author": "PatWms",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102klvg/transform_set_of_raw_scores_to_percentile_ranks/",
        "text": " Can anyone suggest a formula to use in jamovi to transform a set of raw scores to their percentile ranks?",
        "created_utc": 1672782667,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is Chi-Squre appropriate here?",
        "author": "as3adtintin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102ft8w/is_chisqure_appropriate_here/",
        "text": "Hello,\n\nI ran a survey last year and a survey this year. Let's say I asked 100 people last year if they thought Object A was good or bad.\n\nLast year 80 said good, this year 85 said good.\n\nCan I just use the calculator below and plug in 80/20, 85/15. It says a p=0.35, so I would fail to reject the null that there is a difference, right? Assuming the survey samples are similar last year to this year.  \n[https://www.mathsisfun.com/data//chi-square-calculator.html](https://www.mathsisfun.com/data//chi-square-calculator.html)\n\nThank you!",
        "created_utc": 1672771409,
        "upvote_ratio": 1.0
    },
    {
        "title": "[QUESTION] \"Regressing out\" a categorical variable to build normative curves",
        "author": "ImGallo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102fad9/question_regressing_out_a_categorical_variable_to/",
        "text": "Hi,\n\nI  want to build normative curves of Y (Brain volume) according to X (age)  using Quantile Regression, something like \"QuantileRegression(Brain  Volume \\~ Age)\" then after train the model i predict the values for each  age value and quantile, that give the normative curves  but there is  another covariables that could influence like Z (resonator model) .  I  know that is possible build the model like \"QuantileRegression(Brain  Volume \\~ Age + C(ResonatorModel)\" but then how do i get normative curves  only according to age?. In some papers i read that one way of face this  problem is throught \"regress out\" using OLS or GLM,  whatever,  i dont  know how to do that.  I have read many forums but can understand how do  it for this case.\n\nAny idea about how to do it?\n\nThanks.",
        "created_utc": 1672770192,
        "upvote_ratio": 1.0
    },
    {
        "title": "I have a set of measurements and the uncertainties for each measurements. I would like to calculate the mean of that set and the corresponding error bars using the monte carlo method. How would I do this?",
        "author": "Cancel_Still",
        "url": "/r/AskPhysics/comments/101yo3e/i_have_a_set_of_measurements_and_the/",
        "text": "",
        "created_utc": 1672764145,
        "upvote_ratio": 1.0
    },
    {
        "title": "Explanation of SO answer about deviation coding",
        "author": "YourWelcomeOrMine",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102btrf/explanation_of_so_answer_about_deviation_coding/",
        "text": "The accepted answer to this question on StackOverflow accomplishes exactly what I need: [Comparing all factor levels to the grand mean: can I tweak contrasts in linear model fitting to show all levels?](https://stackoverflow.com/questions/72820236/comparing-all-factor-levels-to-the-grand-mean-can-i-tweak-contrasts-in-linear-m)\n\nHowever, I don't understand exactly what the process is doing. Could you explain, in a few sentences, how sum-to-zero contrast coding is being used for this purpose?",
        "created_utc": 1672761981,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS vs Prism for medical statistics with an emphasis on survival analysis",
        "author": "Systral",
        "url": "https://www.reddit.com/r/AskStatistics/comments/102bbso/spss_vs_prism_for_medical_statistics_with_an/",
        "text": "Hello smart people of the internet!\n\nAbsolute ultra noob here (close to 0 experience, only know the very few basics, even excel I haven't worked with a lot).\n\nI'm currently deciding on which statistic program to use for my medical doctoral thesis.\n\nIt's a retrospective cohort study about a specific type of metastasised tumour with a data set of 273 patients and a range of t0-t12 (each t being a new progress date (i.e. recurrence with new metasasis or growth of previous tumour mass) with new therapies,  lab parameters, general condition and a few others).\n\nThere's going to be a heavy emphasis on survival analyses with Kaplan-Meyer-curves, Cox-Regressions etc (or so I've been told :P)\n\nMy supervisor recommends SPSS (basically because that's all he's ever used) and an acquaintance may be able to help me with it. But most of my peers seem to use GraphPad Prism for its simplicity and ease of use and since I'm someone who tends to overcomplicate things that may be beneficial. I think with our uni license both will be around the same price.\n\nI know there's R and other programs too, but I currently don't have the time for a steep learning curve and to learn a new language, esp. considering it's probably going to be a one-time thing. If medical research is going to become a bigger part of my future I'd be very interested and excited to but as of now I'd like to focus on a simpler experience.\n\n&amp;#x200B;\n\nI'm very grateful for any input, thank you!",
        "created_utc": 1672760746,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS analysis:",
        "author": "Vanhkhamxpm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1025zw0/spss_analysis/",
        "text": "Which analysis methods should I use to answer this hypothesis:\n\" Regions with high population density have higher levels of income\"\n\n\n1 Chi Square\n2 Anova\n3 Logistic regression\n4 Linear regression",
        "created_utc": 1672746182,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are some useful metrics / statistics to use when analysing product returns rates?",
        "author": "daedric_dad",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1025mzi/what_are_some_useful_metrics_statistics_to_use/",
        "text": "No one has ever looked through our product returns and tried to see what information it might be telling us. \n\nI can get a report spanning back to 2006 containing every debit (return) we have on our system, including the date and a reason (which is from a set list of reasons and can be easily filtered). \n\nFor example, an entry might look like this:\n\nproduct, debit number, reason, user, date, notes\n\nI plan to do some general analysis looking at counts, averages, peaks and troughs over time and that sort of thing, but I would also be interested to use this opportunity to learn more about analysis and statistics. \n\nWhat sort of things might you recommend I look at in terms of getting useful or interesting statistics? It doesn't even have to be applicable to the business itself, it just seems that with this much data I'd be wasting an opportunity to learn more about using statistics and pulling information out of a data set so I wondered what you guys would suggest! \n\nHope that makes sense and apologies for my extremely limited understanding of data and statistics, this is purely an exercise of intrigue and desire to learn something new!",
        "created_utc": 1672745022,
        "upvote_ratio": 1.0
    },
    {
        "title": "Frustrating Advisor",
        "author": "dringdrin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1021ehx/frustrating_advisor/",
        "text": "Im a biostatistics Masters student and I am currently about to start by supposedly last semester of degree in Spring 2023. I had totally unrelated undergrad and I was done for good so I changed into MS biostatistics which started as MPH biostatistics. I am an international student with no experience of any kind of job before in statistics and tbh the public health department under which I am doing my degree doesnt have active projects going on to let me delve myself into the field, mostly allowing PhD students some sort of funding. Also, I am the only student in Masters programs for biostatistics as two other who transitioned with me from MPH moved back to MPH mid semester in Fall, thanks to Math Stat 1. I have Stat 2 and Bayesian left to take and I will complete my coursework. Coming to thesis, my advisor only wants me to work with some data analysis project using NIS data over years and present a report. I get it, whoever has done it, but it doesn‚Äôt require more than just modifying the variables and put the code in to get some results. For some reason, I cannot get myself to choose some data analysis project as my masters thesis as I am hoping for a good PhD program. Is it too ambitious for me? I want to do a thesis regarding something Bayesian but every time I bring this thing up, my advisor asks me to get the data myself and analyse it. I have access to gene data from a lab that I have worked but thats pretty much it. I want to work on some kind of methodology but my advisor is giving me no information on how to start.",
        "created_utc": 1672730564,
        "upvote_ratio": 1.0
    },
    {
        "title": "for any epiheads",
        "author": "Loose-Ladder-4760",
        "url": "https://i.redd.it/3mviwkelfs9a1.jpg",
        "text": "",
        "created_utc": 1672716041,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS Statistics",
        "author": "Technical_Law2224",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101sono/spss_statistics/",
        "text": "Hello there!\nI am having difficulties using SPSS. Specifically, I got 8 different groups of mice (one string variable) and 77 different proteins (each a numerical variable). I am trying to compare the expression levels of these proteins in each group and the generate a graph. I tried using some tests (like logistic regression and GLM) but had no success.\nI know I‚Äôm definitely doing something wrong cause it should be easier than what I am making it to be.\nCould someone please give me any advice? It would be greatly appreciated!\nThank you!",
        "created_utc": 1672705463,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to apply bootstrapping method to obtain Confidence interval for a distribution (bar plot)",
        "author": "YoarkYANG",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101pktj/how_to_apply_bootstrapping_method_to_obtain/",
        "text": "Hello, statisticians,\n\nI have a distribution that is represented with bar plots, (so all the bars add up to 1). I want to draw confidence interval on the bars. I have used bootstrapping in the past for simpler statistics like mean or median but have no clue on how to solve this one.\n\nI am assuming I might need to use the data that I use to construct the bar plot to do bootstrapping for each single bars? (each bar is constructed by a sum\\_prob/total\\_prob, where sum\\_prob is sum\\[prob1, prob2,...\\] under some constraint. In this case, I will maybe use \\[prob1/total\\_prob, prob2/total\\_prob ...\\] to do the bootstrapping? and the statistic is mean for this case?)\n\nThanks folks, and happy new year!",
        "created_utc": 1672697941,
        "upvote_ratio": 1.0
    },
    {
        "title": "I need help choosing a statistical test that would account for the influence of an additional factor on an outcome variable.",
        "author": "Character_Bid7515",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101n4dj/i_need_help_choosing_a_statistical_test_that/",
        "text": "Hey guys. This may seem pretty basic to some, but it's driving me nuts. I'm stuck and could use some help/ advice to choose a statistical test that accounts for the influence of an additional factor on the outcome variable.\n\nResearch Question: Is there a significant difference in wing length between mosquitoes at treatment sites vs. mosquitoes at control sites?\n\nBackground:\n\n\\-I am comparing captured mosquito wing lengths (3 different species) at treatment vs control sites. \n\n\\-There are 12 sites total (6 treatment, 6 control)\n\n\\-Sites are paired by county (6 counties, 1 treatment and 1 control site at each county).\n\n\\-I performed 11 collections per site throughout the summer (n=132 total collections).\n\n\\-The data set is spotty since we weren't able to capture mosquitoes or otherwise simply weren't able to measure the wing lengths at a few of the sites.\n\n\\-I've checked the data distribution at both the treatment/control as well as the individual site level and none of the data (wing lengths) are normally distributed. There is a wide variety of distributions by site.\n\n\\-I've found differences in means between treatment/ control sites.\n\n\\-I used a Mann-Whitney U test and found that these differences are not statistically significant (alpha= 0.05, p&gt;.1) for two species and are marginally/nearly significant (p&lt;0.1) for one species.\n\n\\-I ran a box plot wing lengths at all sites for each species and I observed similarities in means by county.\n\n\\-I'm concerned that the difference in means between treatment and control is being influenced by county-level differences.\n\n\\-I'm using Rstudio for analysis.\n\n&amp;#x200B;\n\nMy question is:\n\nWhat statistical test should I use in order to account for the county-level influence on the difference in in mean wing length between treatment/ control sites?\n\n&amp;#x200B;\n\nThank you in advance!",
        "created_utc": 1672692241,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it correct to compare the RMS error between two models if they output the same metric in different frames of reference?",
        "author": "BigDumDum00",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101mc75/is_it_correct_to_compare_the_rms_error_between/",
        "text": "To be more specific, I have two very simple models I've written for a personal project that takes in a particle's previous position (along with other metrics) and attempts to predict its future position(s) over time. It's less actual application and more proof-of-concept (just me messing around and getting used to the language and such) so in order to pretend I \"measure\" a particle's position I just add WGN to the true position - and if it's important I got the true path of the particle from a professor's old research paper or something.\n\nI'd like to compare the errors between the models, \\*but\\* one model is a 2-D system, so the output position is just along one axis, and the other is a 3-D model, where the position is (obviously) across two axes.\n\nObviously the 3-D model probably better predicts position, \\*but\\* if I wanted to compare them anyway, is it correct to use RMS error, or should I look at something different? For both models, I have \"true position\" and \"estimated position\" metrics, so for both models I do have error values to look at. I just didn't know if RMS was the best metric to use. I did seed the WGN to be the same between both models, so I'm assuming the errors should be similar, but I just don't know enough about stats to really know.",
        "created_utc": 1672690397,
        "upvote_ratio": 1.0
    },
    {
        "title": "Side hustles or part time jobs using stats?",
        "author": "newaccount0077",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101l2tm/side_hustles_or_part_time_jobs_using_stats/",
        "text": "Hi, does anyone use their stats knowledge to make some money - think sports betting or stocks. There are a lot of resources out there but it's hard to find legit resources due to the amount of nonsense people try to sell you in these areas! Can anyone point me in the right direction? Thanks",
        "created_utc": 1672687438,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question About Placebos",
        "author": "azure_1_5",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101jjzd/question_about_placebos/",
        "text": "\n\nMust a placebo necessarily be an unconscious effect? Or can it be an unassuming conscious approach. \n\nTo clarify: if I am a car salesman and I tell someone that one more expensive car has better brakes than the cheaper car but both breaks are the same, and the buyer treats the car differently as a result leading to the brakes' longevity and seemingly more valuable quality, is that a placebo?",
        "created_utc": 1672683747,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for data set with non-binary gender variable",
        "author": "Based_Scientist",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101fghe/looking_for_data_set_with_nonbinary_gender/",
        "text": "Hello  \nI am looking for a data set in which respondents were able to choose from more than just two gender variables (three would already be fine).  \n\n\nAdditionally, it would be great if the following conditions were met:  \n\\- N &gt; 5000 and more or less representative for the respective population\n\n\\- Age, income and education level included.\n\n&amp;#x200B;\n\nIf you know of any publicly available data set, I would greatly appreciate it!",
        "created_utc": 1672673734,
        "upvote_ratio": 1.0
    },
    {
        "title": "How would you calculate the RSS for the model that only has the constant term in?",
        "author": "BethStubbs",
        "url": "https://www.reddit.com/gallery/101eteb",
        "text": "",
        "created_utc": 1672672108,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does RMSE of 42.3671939 mean?",
        "author": "BackgroundSpirited67",
        "url": "https://www.reddit.com/r/AskStatistics/comments/101bxzm/what_does_rmse_of_423671939_mean/",
        "text": "Hello, I calculated the RMSE value of my data in excel and got 42.3671939. Can someone please explain what this means?",
        "created_utc": 1672664048,
        "upvote_ratio": 1.0
    },
    {
        "title": "Making predictions based on a sample of one.",
        "author": "HardlyAnyGravitas",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100yw3e/making_predictions_based_on_a_sample_of_one/",
        "text": "If I pick one red ball at random from a jar of 100 balls, is it possible to say anything about how many red balls are in the jar, other than the obvious fact that there was at least one?",
        "created_utc": 1672621158,
        "upvote_ratio": 1.0
    },
    {
        "title": "Laplace Transform of a negative binomial distribution",
        "author": "sonicking12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100yu9q/laplace_transform_of_a_negative_binomial/",
        "text": "I am trying to do this using the Poisson-Gamma mixture parameterization.  Please help.  I think the derivation is related to the derivation of the expected value.  But I have only seen how to derive the mean using conditioning, not from summation. Thanks!",
        "created_utc": 1672621012,
        "upvote_ratio": 1.0
    },
    {
        "title": "A/B Test covering entire population",
        "author": "JohnWCreasy1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100wunb/ab_test_covering_entire_population/",
        "text": "maybe an obvious question but i never stopped to consider this before: If the two cohorts in an AB test encompass the entire population, is a permutation test the only really appropriate tool in the toolbox?\n\nIf i sample 20k and 20k from a larger population, i have to consider sampling error.  If i'm splitting 40k into two groups of 20k, my only source of random error is in the sorting which is best assessed with a permutation test, no?",
        "created_utc": 1672615733,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why do we need different measures of central tendency and other statistics ?",
        "author": "al3arabcoreleone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100wkvs/why_do_we_need_different_measures_of_central/",
        "text": "Mean, geometric and harmonic, why do we have all these ?? and how exactly do we define central tendency ? do statisticians have controversial opinions on these measurements ?",
        "created_utc": 1672615022,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics question thats literally bothering me",
        "author": "Either-Equivalent867",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100tlm3/statistics_question_thats_literally_bothering_me/",
        "text": "This isn‚Äôt about politics. This is strictly on how statistics works. I was reading this article that made the claim 68% of Republicans think the 2020 election was rigged. At the end of the article, it stated The Reuters/Ipsos poll was conducted online, in English, throughout the United States. It gathered responses from 1,346 respondents, including 598 Democrats and 496 Republicans, and has a credibility interval, a measure of precision, of 5 percentage points. I think there‚Äôs around 40 million registered Republicans in the US. How can 496 republicans speak for the entire party? Is that a valid claim to make? I‚Äôm sure it has something to do with sample size and maybe the bell curve. But, I just don‚Äôt see how you can make the claim based on less than 500 people. Can someone give me a breakdown on why that you can make that claim?",
        "created_utc": 1672607570,
        "upvote_ratio": 1.0
    },
    {
        "title": "[introduction in Statistics]. How do I calculate descriptive statistics, correlation, etc. for data in a questionnaire with some values that are null because the question was not applicable to the respondent, should we include all the samples or for respondents whose question is relevant?",
        "author": "Rough-Plenty-7316",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100nkln/introduction_in_statistics_how_do_i_calculate/",
        "text": " \n\nI have this data, there are no missing values, but some questions are not relevant to some respondents, so they say no, for example, if they take alcohol, they say no, so the liters of alcohol per month are not filled. when calculating mean and other statistics, should I include all the samples and fill null cells with zeros or should I only include the samples where the respondent said yes?\n\nHere is display\n\nhttps://preview.redd.it/vt2tq8nhog9a1.png?width=138&amp;format=png&amp;auto=webp&amp;s=bc1ec089e8efc0e825ebcf5aafb834dfc7cb2fec",
        "created_utc": 1672591778,
        "upvote_ratio": 1.0
    },
    {
        "title": "With regard to linear regression, how do you interpret a significant independent variable with a low r-squared value?",
        "author": "BRENNEJM",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100lans/with_regard_to_linear_regression_how_do_you/",
        "text": "I found [this page](https://statisticsbyjim.com/regression/low-r-squared-regression/) which covers this in detail, but I‚Äôm not 100% convinced of the conclusion. The author says that r-squared isn‚Äôt necessarily important if you‚Äôre only interested in the relationship between the dependent and independent variables. They give the example of two charts, one with an r-squared of 0.143 and one with an r-squared of 0.865. The independent variable is significant at p &lt; 0.001 for both. \n\nHow does the low r-squared not change the interpretation of the independent variable? \n\nIn the first, the variable is correlated but explains very little of the variation. In the second, the variable is similarly correlated and explains most of the variation. So I wouldn‚Äôt conclude that both independent variables are equally important when understanding the dependent variable, right?",
        "created_utc": 1672585046,
        "upvote_ratio": 1.0
    },
    {
        "title": "About Linear Regression Model",
        "author": "GGneer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100hzlj/about_linear_regression_model/",
        "text": "What should be done when a single sample has multiple observations for its independent variable? Should I get the mean of the observations? Leave it as is? Or are there other ways to handle it?\n\n&amp;#x200B;\n\nTo illustrate, say, the stock market. The sample is a stock and the independent variable is yearly gross profit. The multiple observations come from the yearly observations of gross profit. What should be done to the observations of yearly gross profits?\n\n&amp;#x200B;\n\nThank you :)",
        "created_utc": 1672573460,
        "upvote_ratio": 1.0
    },
    {
        "title": "Moderating or Control Variables, which one to use?",
        "author": "TokioHot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100fr9e/moderating_or_control_variables_which_one_to_use/",
        "text": " I am currently conducting research about the motivation of sharing fake news and the likelihood of sharing it. I believe that social-demographics factors have affect in this study thus I would like to include them. But, I am confused should I use socio-demographic factors as MODERATING VARIABLES or CONTROL VARIABLES?",
        "created_utc": 1672563621,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the Instrumental Variables Estimator the only way to ensure estimator is consistent?",
        "author": "ProposalLeast3057",
        "url": "https://www.reddit.com/r/AskStatistics/comments/1009ffp/is_the_instrumental_variables_estimator_the_only/",
        "text": "",
        "created_utc": 1672538737,
        "upvote_ratio": 1.0
    },
    {
        "title": "What Psychometrics/ Econometrics papers should one-read?",
        "author": "ProposalLeast3057",
        "url": "https://www.reddit.com/r/AskStatistics/comments/100409d/what_psychometrics_econometrics_papers_should/",
        "text": "",
        "created_utc": 1672521725,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone help me understand what Confidence Intervals and Power represent in Equivalence Tests?",
        "author": "cyto_eng1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10006oe/can_someone_help_me_understand_what_confidence/",
        "text": "So I am developing a test that is looking at the difference between two measurements of the same sample via a bland Altman analysis. I want the differences between these two tests to be within +/-1.0 unit, so I‚Äôm performing an equivalence test.\n\nBelow are my assumptions about alpha, confidence intervals, and power of my test. \n\nI *believe* if I set my alpha at 0.05, then I‚Äôm using a 90% confidence interval (because I‚Äôm doing two one sided t tests). Let‚Äôs say my 90% confidence interval of my measurements is +/-0.8 units, then I would conclude 90 / 100 follow up measurements on the same sample should report a difference within +/-0.8 units. \n\nAnd because this confidence interval falls within my +/-1.0 unit criteria, I‚Äôd conclude that there is no difference between these two measurements. \n\nNow with power, I believe I‚Äôm measuring the probability of correctly rejecting the null hypothesis (i.e. detecting a difference in the two tests &gt;+/-1.0 units). So if I have 80% power then I‚Äôm saying 80 / 100 follow up measurements on the same sample will find a difference of &gt;+/-1.0 units if there is one. \n\nIs this all correct or am I missing something here?",
        "created_utc": 1672510719,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probability Question",
        "author": "pomegranatemilkers",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zzz3fm/probability_question/",
        "text": "Hi, i‚Äôm not sure if this is necessarily a stats question but i have no idea how to solve for this. If the chances of drawing a number x is 46.5%, and you have 3 chances of drawing that number x, what is the probability of you getting that number x?",
        "created_utc": 1672507751,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a statistical test that can be done to see if there is a big difference between the numbers per columns?",
        "author": "162739",
        "url": "https://i.redd.it/ugxws6qwk99a1.png",
        "text": "",
        "created_utc": 1672505883,
        "upvote_ratio": 1.0
    },
    {
        "title": "Calculating dice odds",
        "author": "Suitable-Yam7028",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zzuaqd/calculating_dice_odds/",
        "text": "Hi all, I am working on a dice game and I need the AI in it to be able to calculate the odds for a certain amount of dice in a pool of unknown dice that might have a certain face value, but I am having a hard time understanding some of the examples I found on the internet regarding the calculations. In particular I used the \"The agent\" section in the following blog post as reference:\n\n[https://www.r-bloggers.com/2018/11/liars-dice-in-r/](https://www.r-bloggers.com/2018/11/liars-dice-in-r/)\n\nWhat I managed to find is that my calculation should look something like this for the example of searching for 5 3's in a pool of 20 unknown dice:\n\n!(20) / !(20 - 5) \\* (1/6) \\^ 5 \\* (5 / 6) \\^ 15\n\nHowever this does not appear to be the correct equation so I am missing something here.",
        "created_utc": 1672493822,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about linear regression‚Ä¶",
        "author": "Puzzleheaded_Shoe_12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zzrhja/question_about_linear_regression/",
        "text": "Hi guys ! First i want to apologise for my bad english (i‚Äôm french^^). I‚Äôm a med student and this year, i started to learn statistics by myself and i have a question about a course i saw yesterday‚Ä¶ When you create a model for a multiple linear regression with categorial variables, you sometimes recode them into a binary variable 0,1 or -1,1. I have a big problem with the comprehension of this‚Ä¶could someone explain me why we do this ?  Thanks !",
        "created_utc": 1672483850,
        "upvote_ratio": 1.0
    },
    {
        "title": "drawing one 7 and one 2 out of a deck of cards",
        "author": "kobie1kenobie",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zzql9b/drawing_one_7_and_one_2_out_of_a_deck_of_cards/",
        "text": "Is it correct that the chance of drawing one 7 and afterwards one 2 out of a deck of cards is = (4/52) x (4/51) ?\nThanks!",
        "created_utc": 1672480324,
        "upvote_ratio": 1.0
    },
    {
        "title": "How does one determine if a two-sample t-test is applicable?",
        "author": "squidgyhead",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zzlzrp/how_does_one_determine_if_a_twosample_ttest_is/",
        "text": "I'd like to apply a two-sample t-test to some data.  From https://en.wikipedia.org/wiki/Student%27s_t-test, I can do this if data distribution has means that follow a normal distribution.  How can I test this?  The data itself isn't normal.  Can I boot-strap resample the mean and then plot it, convincing myself that it's normal by squinting at the histogram?",
        "created_utc": 1672463058,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is statistics/data science the same as doing research?",
        "author": "SwirlingLeave",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zzcn8y/is_statisticsdata_science_the_same_as_doing/",
        "text": "I have just graduated a Bachelor's in Economics, in which I, besides economic courses, had the courses Econometrics, Mathematics (my fav, no joke), Statistics an Policy Evaluation. The thing is, I HATED the economics courses, but I LOVED doing research. I have written three research papers during my study, using different types of methodologies (i.e. instrumental variables, AIPW, RDD, but learned many more), using Stata 16 and 17. \n\nI want to do this as a job. How do I get there?\n\nI am aware of the master's degree Statistics &amp; Data Science, but I am afraid that will be more focussing on coding in Python. \n\nAlso I think I would love to follow a PhD program after master's, but I have heard it's very hard to get in.\n\nFurthermore I am very interested in psychotherapy, but I am not able to take another bachelor's in Psychology for example. In my thesis I estimated the effect of non-clinical psychedelics use on anxiety level, using AIPW. I would love to perform this type of research more proffesionally.\n\nAnyone out there who is/knows a researcher and can give me some tips?",
        "created_utc": 1672437157,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can anyone check if i did this right,And can you get the P-value in this solution if so how?",
        "author": "ROO0II1",
        "url": "https://www.reddit.com/gallery/zzaizz",
        "text": "",
        "created_utc": 1672431894,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help! Basic question for upcoming technical interview",
        "author": "Acceptable_Sand4605",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz8zw4/help_basic_question_for_upcoming_technical/",
        "text": "Hello statistics wizards. Apologies in advance if this is an incredibly basic question!\n\nI have an upcoming technical interview, and the interviewers may ask about the ‚Äúdefinition of a statistic, and ways of finding statistics (e.g. likelihood functions)‚Äù \n\nIs this referring to the literal definition of statistics, or is there something more technical that I‚Äôm completely missing? Would really appreciate any pointers so I don‚Äôt look like a complete idiot when they ask üíÄ",
        "created_utc": 1672428184,
        "upvote_ratio": 1.0
    },
    {
        "title": "Astrophysicist needs help with sample size!",
        "author": "DiscoSloth69",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz8mnv/astrophysicist_needs_help_with_sample_size/",
        "text": "Hey statisticians! I'm an astrophysicist and I'm a bit confused with sample sizes and such, so this might be a stupid question, but a question nonetheless.\n\nI'm researching a certain phase (Compact obscuration) in galaxies that occur in a specific type of galaxies (Luminous infrared galaxies). I suspect from the small sample that we might be dealing with an inclination bias here, but since we're dealing with a small sample, it's hard to say. Currently, I have about 75 galaxies in total where only 9 has the phase property (thus I only have 9 galaxies with the property that I can use the inclination angle and other properties of). It has earlier been calculated that around 30-40% has this phase property and my question is: How would I go about calculating a preferred sample size to be able to draw conclusions with specific confidence about these galaxies (say a standard of 95%)?\n\nThankful for all of your help!",
        "created_utc": 1672427265,
        "upvote_ratio": 1.0
    },
    {
        "title": "No theoretical frequency in chi-square test should be less than 5.",
        "author": "saidFeck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz7nhg/no_theoretical_frequency_in_chisquare_test_should/",
        "text": "Why does the theoretical frequency in chi-square distribution should not be less than 5? What's the concept behind pooling the preceding and succeeding frequency to make pooled frequency more than 5?",
        "created_utc": 1672424821,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does the epsilon in multiple linear regression exactly stand for?",
        "author": "Impressive_Cold_9720",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz5evm/what_does_the_epsilon_in_multiple_linear/",
        "text": "[removed]",
        "created_utc": 1672419458,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can anyone help me understand why the adjusted R-squared value is increasing?",
        "author": "Apart-Safety336",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz3m3n/can_anyone_help_me_understand_why_the_adjusted/",
        "text": "I‚Äôm doing regression analysis and I have a model that consists of an independent variable that has a p-value of above 0.05 so I removed it since it is insignificant. However, when I removed this variable the adjusted R-squared value decreased by a couple decimal places, the standard error also increased, all the other variables in the model are significant (p-value below 0.05). Can anyone explain why this happened?",
        "created_utc": 1672415118,
        "upvote_ratio": 1.0
    },
    {
        "title": "Please help reading this - What is the median age of survival for young onset parkinson's",
        "author": "5AgXMPES2fU2pTAolLAn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz3676/please_help_reading_this_what_is_the_median_age/",
        "text": " [Increased Mortality in Young-Onset Parkinson‚Äôs Disease - PMC (nih.gov)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8490197/)   \n\n\nHi all,\n\nRemove the post if inappropriate. I am just not sure how to read this data.  \nI am trying to find the most common age of death for people with young onset parkinson's disease  \nAny help in finding the answer or pointing to other resources is appreciated",
        "created_utc": 1672414013,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Cohort study vs. case series? How to tell the difference for risk-of-bias analysis?",
        "author": "throwaway_usmle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz3092/q_cohort_study_vs_case_series_how_to_tell_the/",
        "text": " \n\nVery confused re: trial designs for a systematic review / meta-analysis I'm conducting. We have a set of studies that focuses on patients with disease X. The studies have what I thought to be a \"case series\" design (All patients in the study have disease X \\[almost always, the patients are retrospectively selected from a patient database, they are consecutive cases\\], they receive Y intervention, and the authors assess for Z outcomes 0-3 months following Y intervention. Some statistics are calculated like event rate of certain outcomes (some summary statistics of the patient sample, other times, event rate of a specific outcome Z following Y intervention, some studies will do a regression analysis of patient characteristics as independent vars vs. Z outcome as the dependent variable).\n\nI'm struggling to conceptually understand if these are truly case series or are \"single-arm cohort\" studies? The only reason I'm focused on this distinction is that I have to use the approrpriate risk of bias tool for publication bias in my systematic review and the tools differ for case series vs. cohort studies. This paper has me extremely confused ([https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0391-8](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0391-8)). I feel like it states that the same paper with a case series design can in effect be reconceptualized as a cohort study based on how you are analyzing the data at hand.\n\nThe studies do not have a control arm, and there is no selection based on \"exposure\" per se (with an assessment of outcome based on exposure status, as I guess would be expected with a cohort study). There is also no comparison group necessarily that is defined prior to the intervention Y (i.e. all patients receive intervention Y, and are generally similar in terms of prior exposures). The only time comparisons are made in these studies are when the patient sample is split based on characteristics that are observed *after* intervention Y, and compared . For example, some of these studies will split initial group of patients based on initial outcomes after intervention Y (e.g. some pts have positive initial response after intervention Y vs. others have poor initial response, and then the authors will compare these two subsets using the appropriate statistical method). Would a hypothetical study like the one just described be a cohort study or a case series?\n\nWe are using data to calculate the event rate for a specific outcome Z that these studies quantify. I'm inclined to say these are \"case series\" for our purposes because all the patients receive intervention Y and we're simply focused on the event rate of a specific outcome Z of the patients.\n\nHowever, to complicate things, we're only looking at a specific subset of patients that receive intervention Y (i.e. after patients receive intervention Y, some have a specific initial response, and we focus on the event rate of outcome Z in this specific subset of patients). Since we're only focused on this unique subset of patients defined by its characteristic that is only evident AFTER intervention Y, would this be an \"exposure\" / risk factor that we are focused on, and since we are looking for a specific outcome, this could be a cohort design (at least for our purposes)?\n\nThanks in advance!! apologies if this is vague and hypothetical, happy to provide more specifics / details.",
        "created_utc": 1672413591,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Paired or not?",
        "author": "hounstable_1869",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz1tjs/q_paired_or_not/",
        "text": "Hi everyone, I am sure this is pretty basic but nonetheless, I was not able to find a good answer. It would be great to hear your thoughts: I am analyzing data from a single individual where a metabolite was measured before and after a dietary intervention. However, the number of values in both categories is different. My initial idea was to use a paired t-test but that would mean censoring some measurements. Then I thought that these data are not paired per se, since nothing ‚Äûlinks‚Äú a value from before intervention to a specific one after intervention. Any thoughts/ideas would be greatly appreciated!",
        "created_utc": 1672410408,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interesting Data Engineering Topics",
        "author": "Data_Nerd1979",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zz16gj/interesting_data_engineering_topics/",
        "text": "[removed]",
        "created_utc": 1672408693,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Creating dummy variable when it is not categorical",
        "author": "bromsarin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zyykgi/q_creating_dummy_variable_when_it_is_not/",
        "text": "Hi everyone, I have a problem creating dummy variables. I have a dataset with a bunch of prices as dependent variables. I have prices for monday, tuseday and wednesday. I have done a regression for each of them before, now I want to have monday as baseline, (my dependent variable) and tuseday and wednesday as dummy variables. These are not catagorical (as in all prices are different) so my question is, how do i do this? I'm using the software Gretl. Sorry if this is a stupid question but all help is appreciated! I can give more information if needed.",
        "created_utc": 1672400586,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dumb question about data presenting in research",
        "author": "GwishingD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zyq6a6/dumb_question_about_data_presenting_in_research/",
        "text": " Hello!\n\nI was wondering if there is a way to present our data in our research regarding compressive strength \"statistically\"... or just present it as is. We were looking into ANOVA but it requires more groups and stuff but our research is pretty straightforward so it didn't work.\n\nAll samples are the of same composition. Three samples were subjected to a compressive test.\n\nSample 1 - 500 Psi\n\nSample 2 - 535 Psi\n\nSample 3 - 540 Psi",
        "created_utc": 1672372353,
        "upvote_ratio": 1.0
    },
    {
        "title": "A question of life and death - boardgame statistics.",
        "author": "Homo_Homini_Deus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zylix4/a_question_of_life_and_death_boardgame_statistics/",
        "text": "It is the post Christmas annual Risk duel between four very old friends. External factors have rendered us unable to perform simple math and silly homebrew rules made this simple, peaceful game a colloseum of carnage and hatred.\n\nThe following is the issue: We have decided that and attacker loses double the units upon attacking. The attacker may choose to attack with 3 units at a time, or 15 units in 5 man stacks. Thus again rolling three dice.\n\nThe argument was, wether or not it makes a difference if you attack once in a stack or multiple times for every single unit.\n\nPlease help us, 20 years of friendship and more importantly, bragging rights for being right are on the line.",
        "created_utc": 1672359904,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to figure out the range of income to accept for real estate?",
        "author": "EconometricsStudent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zyhqpz/how_to_figure_out_the_range_of_income_to_accept/",
        "text": "I'm working as an intern for a Real Estate company and I was tasked to figure out some insights to see what sort of tenants to accept or reject. I have a decent sample of current tenants (close to 100) and a relatively smaller amount of evicted tenants. The company is still fairly new and there isn't a really good way to organize the data (as for now).\n\nAnyways, given the information collected by current tenants and evicted tenants what would be a good method in finding out what sort of income range would be advisable to accept? (Not to make decisions purely on this statistic due to low sample size and other confounding factors but as a decent note for tough decisions.)\n\nThank you :)",
        "created_utc": 1672350990,
        "upvote_ratio": 1.0
    },
    {
        "title": "Making a Sampling plan for a printed (flexographic) process",
        "author": "SatsukiYone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zyhduc/making_a_sampling_plan_for_a_printed_flexographic/",
        "text": "Hey guys, I'm in an internship at the quality department for a food package manufacture company and one of my given tasks are to create a sampling plan for the printing process, the idea is to make it based on the meters produced and it doesn't have to be a lot of material taken from the product. The process I'm starting with printing (flexographic) process. Where should I start? I love statistics, but first time I have to put it in practice since all my knowledge comes from college (Industrial Engineering). \n\nWhat they basically want is (for example): for a 2000 meter order, you have to sample very 500 meters",
        "created_utc": 1672350136,
        "upvote_ratio": 1.0
    },
    {
        "title": "masters for a statistician",
        "author": "_lemonation",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zyaag4/masters_for_a_statistician/",
        "text": "I have almost completed my bachelor in statistics so recently I have started looking into getting a masters degree in data science. \nI would like to ask for some suggestions on good universities (preferably in Europe) that list my bachelor as a qualified one , since most of the unis I have browsed through seem to require a bachelor's degree in either engineering, computer science, or applied mathematics. \n\nThanks in advance",
        "created_utc": 1672333355,
        "upvote_ratio": 1.0
    },
    {
        "title": "Detecting outliers in non-normal data",
        "author": "DaLambSauce",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zyaaax/detecting_outliers_in_nonnormal_data/",
        "text": "I have a multi-year dataset with dozens of variables and I‚Äôm trying to calculate outliers boundaries to compare current datasets against to flag data as ‚Äúnon representative‚Äù of what normal conditions may be. \n\nMost of the data is highly skewed to the right but some are just passing a normality test (Shapiro-wilk test). \n\nI figured the easiest thing to do would just be to take the 5 and 95 percentiles of each variable to create the outlier boundaries. If current data is outside that range I don‚Äôt want to remove the outlier, just flag it as possibly non-representative. I figured percentiles would be the best option due to the skew and even with normal data since it would be similar to the standard deviation.\n\nIs this the best method for detecting possible outliers with skewed data?",
        "created_utc": 1672333346,
        "upvote_ratio": 1.0
    },
    {
        "title": "Moderator vs mediator - help needed :)",
        "author": "FailKitchen9912",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zy9ksh/moderator_vs_mediator_help_needed/",
        "text": "It's been years that i am struggling to remember which one is moderator and which one mediator. Do you have any tricks for me? Any mnemotechnics or other tricks?",
        "created_utc": 1672331600,
        "upvote_ratio": 1.0
    },
    {
        "title": "Model for relative bad or good performance for a sport race",
        "author": "TywinASOIAF",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zy900o/model_for_relative_bad_or_good_performance_for_a/",
        "text": "Let's say we have about 40 runners for runs of 400m sprint. Now there are plenty of tracks where you can run a 400 meter. Those tracks all have different conditions based on weather and terrain and therefore at certain tracks you can run 400 meter is less time (fast tracks). At an event not every runner is present and runners can participate in more than one event.  \nNow, which model can you use to determine if a event peformance (running time of players)  based on the playerfield and track conditions is better or worse than average.",
        "created_utc": 1672330166,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help! Measuring weight bias in college students.",
        "author": "kvetakabrokolice",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zy7f1c/help_measuring_weight_bias_in_college_students/",
        "text": "Hello lovely people of reddit, I sincerely hope there are some statistics fanatics out here.\n\nI am currently finishing my thesis and I am in sort of a picle when in comes to analyzing my data. I would very much appreciate the opinions and suggestions on problems presented below.\n\nI collected data from **830 college** students. I am trying to find out their weight bias against obese people.\n\n* age (number)\n* sex (M/F)\n* study major (study programme) (one choice out of 15 programmes available in my country)\n* BMI (in number or category)\n* year of study (one choice out of 6)\n* if they have a close obese person (yes/no question)\n* if they were obese in the past (yes/no)\n* how often do they meet obese classmates or teachers (Likert scale 1 to 5)\n* how pleasant were those interactions with people above (Likert scale 1 to 5)\n* how often do they witness derogatory humor and discrimination towards obese people (Likert scale 1 to 5)\n* to what degree they feel competent to treat obesity (Likert scale 1 to 4)\n\n\\+ I measured their **bias with two scales** ‚Äì Antifat Attitudes Questionnaire (AFA) and BAOP (Beliefs about obese people)\n\nNow I need to do the statistics and I am not exactly sure how to do it. I would like your opinion about my suggestions:\n\n**SEX**\n\nMen VS Women in AFA ‚Äì independent t-test (same for BAOP)\n\n**BMI**\n\nBMI categories in AFA ‚Äì one-way ANOVA (same for BAOP)\n\nCan I also correlate BMI (as a number, not an category) with AFA score?\n\n**STUDY MAJORS**\n\nStudy majors x AFA ‚Äì one-way ANOVA (same for BAOP)\n\n**YEAR OF STUDY**\n\nYear x AFA ‚Äì one-way ANOVA (same for BAOP)\n\n**OBESITY IN PAST**\n\nObesity in past (yes/no) x AFA - independent t-test\n\n**CLOSE OBESE PERSON**\n\nYes/no x AFA - independent t-test\n\nI have no idea what to do with **age**. Can I correlate it with something?\n\nI also do not know what to do with the **rate of meeting obese classmates and teachers**. I was wondering if meeting obese people is possibly in association with AFA or BAOP. The same applies for the nature of those interactions and seeing someone else being discriminatory towards the obese. **What statistical procedure would you use?**\n\nPerceived competencies are also a problem. I was wondering if they have any association with year of study, age or BAOP. What would you use?\n\nThank you so much for all opinions and suggestions!\n\nDesperate Psychology student",
        "created_utc": 1672326058,
        "upvote_ratio": 1.0
    },
    {
        "title": "Examining statistical significance between groups (questionnaire data)",
        "author": "Ill_Pineapple3927",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zy6k5d/examining_statistical_significance_between_groups/",
        "text": "I am wondering what statistical method to use for my questionnaire data. The questionnaire consists of two questions, called trustworthiness (T) and reliability (R). The participants are asked to read a statement and then say how trustworthy the person is and how reliable the statement is. On a scale from 1-9.\n\nI have two groups. Group A consists of Professors and other members of staff and Group B consists of Students. Within each group the participants were randomly assigned to two sub-groups. One group were told that the statement was made by a professor (treated) and the other were told that the statement was made by a Student (untreated)\n\nI have used a Mann-Whitney U test to test Group A treated vs. Group A untreated and Group B treated vs. Group B treated. \n\nI now want to test the difference between Group A and B. I found an effect on both groups, that the reliability increases when being told that a professor made the statement - now I want to see if the effect is bigger in Group A vs Group B.",
        "created_utc": 1672323736,
        "upvote_ratio": 1.0
    },
    {
        "title": "If the true linear model shouldn‚Äôt have an intercept but my model does, is my estimate still unbiased?",
        "author": "Bigg_UN",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zy5o4m/if_the_true_linear_model_shouldnt_have_an/",
        "text": "",
        "created_utc": 1672321225,
        "upvote_ratio": 1.0
    },
    {
        "title": "Calculating Cohen's d from test statistics",
        "author": "iamra447",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zy1ck3/calculating_cohens_d_from_test_statistics/",
        "text": "I am trying to get effect sizes from quite different studies with rather different study designs. \n\nI am aiming for Cohen's d - primarily from dependent t-tests and repeated measures ANOVAs (sometimes, with several degrees of freedom of effect (e.g. the F-statistic would look like this : F(13, 128) = 3.11, p = .03)).\n\nI sadly have no SD and ¬µ of the datasets, but almost always the F- or t-statistic. Is there a way to calculate Cohen's d?\n\nIf yes, a paper to cite or a formula would be very helpful! \n\nThanks a lot in advance!",
        "created_utc": 1672306814,
        "upvote_ratio": 1.0
    },
    {
        "title": "Time Series Forecasting",
        "author": "mr_greenTea04",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zy0i87/time_series_forecasting/",
        "text": "My thesis adviser said that I should use ARIMA or ETS models in forecasting poverty threshold and incidence in each region of the Philippines. There are 17 regions in the Philippines having data for the poverty threshold and incidence. I really find it very tedious having trials and errors in finding the best model in each region. Is there any way that I can have one model that represents all regions? If so, what statistical approach should I use considering that the main goal of my paper is to forecast values of poverty threshold and incidence for the years 2024, 2027, and 2030?",
        "created_utc": 1672303746,
        "upvote_ratio": 1.0
    },
    {
        "title": "My thesis adviser said that I should use Arima or ETS in finding the best forecast model in each region, but I find it very tedious. Is there any other way that I can only have one forecast model that describes all regions?",
        "author": "mr_greenTea04",
        "url": "https://i.redd.it/oiugn7foau8a1.jpg",
        "text": "",
        "created_utc": 1672302758,
        "upvote_ratio": 1.0
    },
    {
        "title": "I wanted to learn statistics from beginning to advance (I need this for machine learning), a suggestion on book required and if it is freely available as a pdf then please let me know",
        "author": "Brajesh_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxwz92/i_wanted_to_learn_statistics_from_beginning_to/",
        "text": "",
        "created_utc": 1672291946,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to analyse this demographics data",
        "author": "Ok-Philosopher2499",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxtq4c/how_to_analyse_this_demographics_data/",
        "text": "Hi guys. I need some help. But bear in mind English not my first language so there will be a lot of grammar error and awkward sentences. \n\nSo my department organized a training program through out this year. We had 10 batches for this year. The participants were the internal staff. So i have the data that consists of each participant's , organisational unit, division, position, business unit and registration mode. \n\nMy higher-ups ask me to do some analysis using this data. They didn't provide me with any questions or problems that i can solve. So i need to come up with my idea how to use this data to get some insight on the program that we did.\n\nI was thinking about doing a power bi dashboard of the summary but that's it. What else i can do with this data. What actually the question that i can ask. \n\nBtw i just graduated with a bachelor degree in Statistics and this is my first job so i don't have a lot of experience with real data. My jobscope have nothing to do with my degree but they want to give me a chance to use what i had learnt in uni to show something to them.",
        "created_utc": 1672282803,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with gathering stats for company marketing",
        "author": "cookiekam",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxrxmt/help_with_gathering_stats_for_company_marketing/",
        "text": "Good day to all Stats here!  \nAs I am in China and vpn is kinda wobbly, I have difficulties accessing big websites for global market statistics.  \nMy company (skin-care and cosmetics, Shanghai) is looking to expand on foreing markets, starting with Russia (many chinese companies are using the current situation to their advantage, as Russia has an underdeveloped inner production industry).  \nCan someone kindly help me gather some statistics for the Russian (and global if possible) market for the following:  \n**-skin-care products**  \n**-fillers**  \n**-skin rejuvenating products**  \n**-anti-aging products**  \n**-whitening/brightening products**  \n**-hydropeeling**\n\nIt would also help a lot to have some stats about consumer interests and priorities when buying such products. But honestly I'll take whatever you guys can help me with.\n\nMany thanks in advance!!!",
        "created_utc": 1672278099,
        "upvote_ratio": 1.0
    },
    {
        "title": "tukey test in spss software",
        "author": "safarin06",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxok1h/tukey_test_in_spss_software/",
        "text": "&amp;#x200B;\n\n[Image 1](https://preview.redd.it/enox9s7r2q8a1.png?width=728&amp;format=png&amp;auto=webp&amp;s=3f9e72187833e1ff2fac2f89ea6d25f239b52daa)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[image 2](https://preview.redd.it/llx0y8ss2q8a1.png?width=623&amp;format=png&amp;auto=webp&amp;s=dc5461f86d05703eaec6d3f178cd612529147be8)\n\n&amp;#x200B;\n\nhello,\n\ni did an experiment and then ran a two way anova test and then a tukey test. my dv is the size of zone of inhibition, my two iv's are the essential oil and bacteria. when i ran the tukey test, what i got was image 2. but this doesn't involve the bacteria in any way. however, when i scroll up i see something called pairwise comparisons and see that both of my iv's are involved. i used spss software. i am not sure why image 2 only gives me a tukey test for the essential oil and does not involve the bacteria. please help me ia m so confused :(",
        "created_utc": 1672269850,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the problem of aggregating (averaging) a feature if we don't know the underlying distribution?",
        "author": "maybenexttime82",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxlpg8/what_is_the_problem_of_aggregating_averaging_a/",
        "text": "Sorry if I'm asking it on a wrong sub, but this is mostly statistics part that I don't understand even if it focuses more on the SQL. Please do point me out to the best sub I can ask this question if this is the wrong one!\n\nSo in this video \n\n[https://learn.365datascience.com/courses/sql-for-data-science-interviews/mock-interview-1/](https://learn.365datascience.com/courses/sql-for-data-science-interviews/mock-interview-1/)\n\nat 10:58 interviewer asked what would be the problem if we aggregate (take the average) on the success rate on which she points out that it might be the problem not taking into account the actual distribution of that feature. To be honest, I don't know how to properly phrase this question but I would like someone to elaborate a bit more on that idea because I understand basic stats and all the terminology but I don't understand the 'ins and outs' of this particular thing of why we should avoid doing the aggregation she mentioned.",
        "created_utc": 1672263222,
        "upvote_ratio": 1.0
    },
    {
        "title": "Difference in differences and the control group",
        "author": "Agateasand",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxlo09/difference_in_differences_and_the_control_group/",
        "text": "Can difference in differences be used to compare two different interventions? If I have two interventions, Treatment A and Treatment B, can I use Treatment B as the control to see what would have happened if Treatment A participants received treatment B instead of Treatment A? In the literature for diff in diff, I only see comparisons being made against a group that received absolutely no treatment, so I‚Äôm wondering if the control group always has to consist of a population that received no treatment.",
        "created_utc": 1672263124,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do you calculate the odds for this modified Monty hall problem?",
        "author": "TurkeyLuver",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxlck4/how_do_you_calculate_the_odds_for_this_modified/",
        "text": "There are 3 doors, 1 prize, and 3 contestants.  The players take turns sequentially picking a door. After a door is picked the result is seen before the next contestant goes. \n\nPlayer 1 has a 1 in 3 chance of picking correctly. \n\nFor player 2 I‚Äôm finding that if they make an initial guess and player 1 is wrong that they are better off switching their guess to the other remaining door similar to the original Monty hall problem. \n\nMy question is how do you determine the odds?  I did a random simulation and found that sticking with the original guess wins 2/9 times while switching is 4/9 times. \n\nMy thinking now is that you take the 2/3 from the original Monty hall and multiply by 2/3 because player 2 only gets a turn 2/3 of the games.  Is that correct?",
        "created_utc": 1672262394,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to perform Power Simulations varying number of tests, number of samples, and replicates?",
        "author": "cyto_eng1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxkjl4/how_to_perform_power_simulations_varying_number/",
        "text": "Hello,\n\nI have a study design of A tests running B samples at N replicates each (so A\\*B\\*N datapoints) to predict a \"score\" value that ranges between 0-10.\n\nI am fitting this data to a mixed effects model (score \\~ sample + (1 | test)) to estimate the std\\_test and std\\_err.\n\n&amp;#x200B;\n\nI'm now hoping to do a power analysis simulating a tests, b samples and n replicates to predict power of my study at these given parameters.\n\n&amp;#x200B;\n\nI have generated a simulated dataframe based on these std\\_test and std\\_err using a=10, b=12, n=10. However, I'm a bit lost in implementation here. I'm not super savvy in R, but assuming I can leverage the simr package to run this?\n\n&amp;#x200B;\n\nAny help would be greatly appreciated.",
        "created_utc": 1672260522,
        "upvote_ratio": 1.0
    },
    {
        "title": "Completely stuck on how to build a forecasting model from this distribution (details in comments)",
        "author": "HavenAWilliams",
        "url": "https://i.redd.it/5623yj2a4p8a1.png",
        "text": "",
        "created_utc": 1672258087,
        "upvote_ratio": 1.0
    },
    {
        "title": "Advice Needed for Stratified Sampling Methodology",
        "author": "azdatasci",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxi77t/advice_needed_for_stratified_sampling_methodology/",
        "text": "I need a bit of help / nudge in the right direction. I am currently wrapping up my MS in Applied Statistics and have learned a ton about the subject. Recently I was approached by one of our business partners at work (I work in a data &amp; analytics team) asking about developing a solution for sampling from a population of worked alerts/tickets. In short - this team performs a QA/QC function on tickets that were closed for a given period of time. Just some basic info to provide more clarity:\n\n\\- They are applying stratified sampling - they are using the analyst/employee who closed the alert as one strata and the disposition of the alert and the second strata.\n\n\\- Currently they are sampling daily, but eventually want to move to weekly.\n\n\\- For sampling they want to make sure that all analysts and alert dispositions are represented in the sample - but noted if the number of analysts exceed the sample size, we'd include all analysts (aka, increase the sample size)\n\nThis is something they are doing themselves right now, but want my team to automate. After speaking to them, I have some concerns about their methodology, but wanted to see if the community could chime in. They are using the following calculator to determine sample size:\n\n[http://www.raosoft.com/samplesize.html](http://www.raosoft.com/samplesize.html)\n\nBased on the documentation, this assumes a normal distribution, which I am not entirely convinced we have. When I pulled the thread on this a bit, asking them why they chose this tool, I didn't get any compelling reasons why they did. They did say they are sticking to a 95-5 for the confidence level / margin of error. There seems to be no mathematical reasoning behind these choices either.\n\nNow, to my questions:\n\n1. For a data set like this - if I am performing stratified sampling across two strata (or could be more) - what is a good way to determine the distribution of these data?\n2. I ask #1, since I \\*think\\* I should probably know how the data are distributed in order to determine proper sampling methodology / size. If I do not need to know how the data are distributed, what is the proper way to determine sample size for these data?\n3. It seems off that they'd determine sample size, then decided to increase that if they have more analysts working alerts than the sample size. Is this normal or should those counts be considered before they determine sample size?\n\nA basic example could be that we have 1500 alerts closed in a day. Of these, let's say we have 20 analysts closing them with three different dispositions (Closed 1, Closed 2, Closed 3). In short, I am trying to:\n\n1. Consult to the business on proper sampling methodology so we can provide documentation to any auditing teams that my ask about it later. This was I can provide statistical evidence to the methodology.\n2. Understand the proper way to sample data like these, so I can implement it in our analysis.\n\nThanks in advance!",
        "created_utc": 1672255021,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the best metric to use to determine, how much income the average person in a country has?",
        "author": "What_The_Hex",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxhwlj/what_is_the_best_metric_to_use_to_determine_how/",
        "text": "The goal here is specifically to get an idea of: How much income does the average person take in, each year, within a given country? And this is ultimately to help figure out: How much disposable income do people in this country have?\n\nBased on the different ones I've looked at, median income seems to make the most sense -- because that's actually showing you, the dollar amounts that people are actually taking in each year.\n\nOther metrics like, GDP for example, seem like they could have tons of confounding variables at play. Looking at averages (vs. medians) also seems like it could distort the reality, since a rich minority could drastically skew the data and make it look like the average (median?) person is making way more than they actually are.\n\nI'm curious what others thoughts are on this, and if there's any sort of statistical consensus on this?",
        "created_utc": 1672254330,
        "upvote_ratio": 1.0
    },
    {
        "title": "How could I approach this Biostats problem? I originally merged all 3 datasets and used Master ID,potassium levels , and treatment. I then coded potassium levels over 5.5 as ‚Äúyes‚Äù for hyperkalemia and no for the other.",
        "author": "TAZ2532",
        "url": "https://i.redd.it/pzd5oquhvp8a1.jpg",
        "text": "",
        "created_utc": 1672249221,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you perform a non-parametric test on normal data",
        "author": "Relative_Credit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxe26s/can_you_perform_a_nonparametric_test_on_normal/",
        "text": "I have 7 different quantitative measurements I am trying to compare between two independent groups. I tested normality on them using a shapiro test and most of them are non-normal, but a couple are normal. \n\nBasically I'm torn between using a non-parametric test on all of them, or using non-parametric for the non-normal and a parametric on the normal. For \"simplicity\", I would rather perform the same test on everything, but not sure if this is bad practice.",
        "created_utc": 1672245297,
        "upvote_ratio": 1.0
    },
    {
        "title": "Independent Variables: THREE Discrete Ordinal (i.e. \"How overprotective is my parent\"), Dependent Variable: Discrete Nominal (i.e. \"Which picture do I like?\") - Statistical Test?",
        "author": "arcanehelix",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxcur5/independent_variables_three_discrete_ordinal_ie/",
        "text": "Hi guys, quick question on what kind of statistical test to use: just doing a small project on how students perceive their parent, and how they perceive adults of their ethnicity\n\n**Independent Variable(s): Discrete Ordinal (?)**\n\nThink of this variable like a test score of 3 dimensions. I'm actually recording how students perceive their parents on the following dimensions - how Overprotective, Emotionally Warm, and Abusive they are. \n\nScores are based on a questionnaire, so each participant will have 3 sets of score, one for each dimension.\n\n**Dependent Variable: Discrete Nominal (?)**\n\nThink of this variable as a single question - do I like photos of individuals that resemble my parent?\n\nThere are 3 options for every participant - a photo that is of the same ethnicity as their parent (A), a mixed-race photo that has 1/2 the ethnicity of their parent (B), a photo that is of different ethnicity as their parent (C).\n\n**What I want to know / my theory:**\n\nHigh Emotional Warmth = prefer photos of category A\n\nHigh Abusive = prefer photos of category C\n\nOverprotection = no effect on types of photo they prefer\n\nAny suggestions on what statistical test or model to use?\n\n\\-----\n\nMany thanks in advance from a hapless Psych student!",
        "created_utc": 1672242429,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to NOT feel culpable for studying Stats?",
        "author": "Proof-Combination334",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxctu7/how_to_not_feel_culpable_for_studying_stats/",
        "text": "Hey!\n\nSo I'm currently in my first year of college and will be taking an introductory statistics course next semester. I'm planning on studying epidemiology post-grad, so that means taking stat courses throughout my undergrad through post-grad. Even though I like numbers and did quite well in pre-calculus and calculus in high school, I am currently dipping my toes into R and SAS as a hobby. Statistics have been somewhat given a bad name due to the current pandemic and the quote from Disraeli that Mark Twain popularized, \"There are three kinds of lies: lies, damned lies, and statistics.\" How can I enjoy statistics while also not falling into the pitfalls of skewing and misrepresenting data?\n\nThanks in adavnce",
        "created_utc": 1672242370,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why do small samples inflate the effect size for a mean comparison?",
        "author": "_siggy__",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zxbzgj/why_do_small_samples_inflate_the_effect_size_for/",
        "text": "I had heard this and decided to simulate it, and it does seem to be the case (unless I made a mistake - would be interested to see others' simulation in R for example). But why does this happen, and why is it always in the direction of the effect being inflated , rather than also being shrunk? Because if it's just about there being larger errors, then surely it should be just as likely to underestimate the effect size?\n\nAnd is this really a big deal, or is it simply a case of pointing out that it will be a little inflated? Because using the code below, I can see it is only increasing the cohen's d by about 0.05 (or so) standard units. When people brought this up, it was as though they had committed an unforgivable sin.\n\n Here is the python code:\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from scipy import stats\n    \n    def makediffs(truediff, n, m):\n        # n: sample size\n        # m: number of simulated studies\n        null = np.random.normal(0, 1, (n, m))\n        diff = np.random.normal(truediff, 1, (n, m))\n        diffs = null - diff\n        result = {'mean': diffs.mean(axis=0), 'sd': diffs.std(axis=0)}\n        result['d'] = result['mean'] / result['sd']\n        return result\n    \n    def plot_results(res):\n        fig, ax = plt.subplots(1,2, figsize=(15,5))\n        ax[0].plot(res['mean'])\n        ax[1].plot(res['d'])\n        ax[0].axhline(0, lw=1, c='k')\n        ax[1].axhline(0, lw=1, c='k')\n        ax[0].set(title=f'mean (mean={res[\"mean\"].mean()})', ylim=(-1.5, 1.5))\n        ax[1].set(title=f'$d$ (mean={res[\"d\"].mean()})', ylim=(-1.5, 1.5))\n        plt.show()\n\nSo here I tested a true mean difference of 0.5 over 500 simulations with a small sample of n=10:\n\n    meandiff = 0.5\n    n = 10\n    nsims = 500\n    \n    res = makediffs(meandiff, n, nsims)\n    plot_results(res)\n\n&amp;#x200B;\n\n[n=10](https://preview.redd.it/motm65gfkn8a1.png?width=1213&amp;format=png&amp;auto=webp&amp;s=f610535122c96c1f383095d6685eb6a6c73bffdf)\n\nWhereas here I tested the same but increased it to n=50:\n\n&amp;#x200B;\n\n[n=50](https://preview.redd.it/r078jomikn8a1.png?width=1217&amp;format=png&amp;auto=webp&amp;s=a28e7058bbc6a199d6d35d1848b9338146ea7d29)",
        "created_utc": 1672240353,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any help to solve these 2 errors !!!",
        "author": "sakhrabdelsalam",
        "url": "https://www.reddit.com/gallery/zxbayd",
        "text": "",
        "created_utc": 1672238704,
        "upvote_ratio": 1.0
    },
    {
        "title": "ANOVA Effect Size",
        "author": "Sunshine-n-Happiness",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zx7uk9/anova_effect_size/",
        "text": "\nHello,\n\nI am writing up a lab project which was analysed using a two way ANOVA. \n\nComparing the effects of sex, treatment and sex*treatment on Alzheimer progression. Using invivostat, the Anova table generated lists: sex, treatment, sex*treatment and residual as rows in the effects column.\n\nI‚Äôd like to include an effect size for the significant effects. \n\nCan I use eta squared? Would this be appropriate? Is there a better method to work out effect size? How is this done? \n\nApologies if this is a silly question, my statistics teaching has been rather basic.",
        "created_utc": 1672228872,
        "upvote_ratio": 1.0
    },
    {
        "title": "How would you explain \"degrees of freedom\" to a non-stat expert?",
        "author": "mr_greenTea04",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zx725u/how_would_you_explain_degrees_of_freedom_to_a/",
        "text": "",
        "created_utc": 1672226066,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is a good way to measure the dissimilarity of a curve from other curves using statistics?",
        "author": "Choice-Drive-7350",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zx7018/what_is_a_good_way_to_measure_the_dissimilarity/",
        "text": "I have 10 datasets, each dataset consists of measured values of a parameter across 5 days. Now, I know that when each of these 5 days are plotted as 5 curves on a same 24 hour axis, 4 curves will exhibit a similar trend (shape of the curves will be similar but not magnitude). But 1 curve will show a different trend on the same 24 hour axis. This will be the case for all 10 datasets.\n\nI intend to present this observation using some mathematical methods. That way I can quantify how divergent this one day behaviour is compared to four days within its dataset. Then I could argue that the trend exists in all 10 datasets. This requires some mathematical analysis of the data. \n\nHowever, I have no formal training in statistics. I would be grateful if anyone can from this community can answer this problem or guide me to right resources to study relevant topics.",
        "created_utc": 1672225850,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can anyone help me with this problem,I really need it thanks!",
        "author": "ROO0II1",
        "url": "https://i.redd.it/jsaq2kwezm8a1.png",
        "text": "",
        "created_utc": 1672214218,
        "upvote_ratio": 1.0
    },
    {
        "title": "What statistical test to use for Randomized Study in Clinical Trial?",
        "author": "TAZ2532",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zx2sv4/what_statistical_test_to_use_for_randomized_study/",
        "text": "I have been re-reading up on my bio stats recently and came across an assignment where the general question is: In this randomized study, what is the effect of the drug on elevated potassium levels (i.e. hyperkalemia)?\n\nThen the actual question is:\n\n Is there evidence that the drug is associated with hyperkalemia?\n\nI did all my coding I needed to to prepare for the question but now I am kind of stuck on what test to use?\n\nI wanted to do some hypothesis testing for this or a relative risk?",
        "created_utc": 1672210035,
        "upvote_ratio": 1.0
    },
    {
        "title": "Significance in logistic regression",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwzrow/significance_in_logistic_regression/",
        "text": "What would the response surface of a logistic regression look like in an case such that: One variable is significantly different from zero in a logistic regression model but if and only iff it is added to a model with a specific variable already in the model? In other words variable #1 is significant only in the \"presence\" of another known variable #2. Is such a situation even possible in logistic regression? Can anyone provide an example of where this is seen?",
        "created_utc": 1672200303,
        "upvote_ratio": 1.0
    },
    {
        "title": "How would you solve this data analysis problem from start to finish? Any recommendations on a self-study approach so I can eventually accomplish this myself?",
        "author": "the_arcadian00",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwzozw/how_would_you_solve_this_data_analysis_problem/",
        "text": "Hi AskStats!\n\nTo start: I am not a data scientist, or even close, but I have a very small bit of statistics knowledge -- what's covered in an engineering degree plus self-study regression (time-series, multifactor, AR(n) models).\n\nProblem Statement: \n\nDataset: I have price data for various assets in 5-minute increments over a period of 20+ years (4+ million rows of data). \n\nObjective: \n\n1. I want to identify the n-highest/lowest price hours in a given day for each asset. For example, I'd like to identify the 4 highest hours and the 4 lowest hours on each date. \n2. I'd like to create a new table with the difference between the sum or average of the n-highest/lowest hours for each day, plus other statistics about that day (min/max price). I can then generate basic descriptive stats about that day (e.g., STDEV of spreads between n-highest/lowest hours)\n3. This isn't a one-off task, so I don't want to tackle it in Excel. Since there are infinitely many  assets I can analyze, I'd like to be able to perform this sort of analysis on new assets in the future. \n\nQuestion: What systems would you use for this? What's the easiest tool / platform / language for a non-data science professional to learn how to do this on their own over a reasonable period of time (3-9 months)\n\nContext: I've looked at visualization tools like like Power BI to see if I can jump to a solution... but I don't think these will provide what I'm looking for. \n\nThanks so much!!",
        "created_utc": 1672200071,
        "upvote_ratio": 1.0
    },
    {
        "title": "Did I perform and interpret these t-tests correctly?",
        "author": "valkaress",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwtadb/did_i_perform_and_interpret_these_ttests_correctly/",
        "text": "Been a long time since I did one of these, but now that I sort of work with statistics, I thought it would be good to get back into shape.\n\nI did these two two-sample t-tests with a personal dataset, and I was wondering if I did them correctly. The idea is to reject the null hypothesis that the two sample means are equal, in favor for the alternate hypothesis that sample 2's mean is *bigger than* sample 1's. In other words, I'm not trying to test that they're merely different.\n\n[Here's what I did,](https://i.imgur.com/oNAJg5T.png) though don't worry about the number themselves. I'm sure I transcribed the formulas correct, and if not, well, sucks to be me. Anyway, I'm only wondering if:\n\n1) The formulas at the top are correct.\n\n2) I'm interpreting the results at the bottom correctly. So, for the second t-test on the right, if I was merely testing that the means are different, I would input the t statistic 2.1942 on a two-tailed t-table with 73 df and find that I can reject the null at p = .95, but can't reject the null at p = .98. However, since I'm specifically testing if sample 2 has a bigger mean than sample 1, that means I use a right-tailed t-table, and thus I can reject the null at p = .98, but can't reject at p = .99.\n\nThoughts? Did I get it all right?",
        "created_utc": 1672182556,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] between theoretical and experimental probability, which one is the original idea or was calculated first. Also which is mostly used in pure science, medical, and psychology?",
        "author": "East-Ad1585",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwpxwu/question_between_theoretical_and_experimental/",
        "text": "",
        "created_utc": 1672174627,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Empirical evidence of endogeneity?",
        "author": "sonicking12",
        "url": "/r/econometrics/comments/zwob8s/q_empirical_evidence_of_endogeneity/",
        "text": "",
        "created_utc": 1672170667,
        "upvote_ratio": 1.0
    },
    {
        "title": "Imputing values for k means clustering in R",
        "author": "jetwing358",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwo64c/imputing_values_for_k_means_clustering_in_r/",
        "text": "Hello all,\n\nI have dataset I gathered as part of my PhD, basically a set of proteomics timepoint intensity values (12 replicates per 6 timepoints) for a set of identified proteins. I want to cluster the protein intensity profiles across time to look for correlated protein changes after an environmental stimulus.\n\nI basically did an FDR adjusted ANOVA to get significantly changing proteins across time, took the mean of significant proteins (ignoring NA replicates), and clustered this mean dataset (k means in base R) after omitting proteins with NA values at any time.\n\nThe problem is omitting proteins with an NA value at some timepoints reduces my dataset by about 70%. Is it acceptable statistically to impute values from the mean of each timepoint column (each of which do follow a normal distribution after log2 transformation) and then cluster this data? I guess to be more stringent I could only accept a protein that had a maximum of 1 timepoint value missing.\n\nThanks for any help,\n\nJack",
        "created_utc": 1672170297,
        "upvote_ratio": 1.0
    },
    {
        "title": "need help with a formula in RNG course",
        "author": "PurposeFeeling3050",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwnvgs/need_help_with_a_formula_in_rng_course/",
        "text": "My Uni teacher keeps using this formula but i have no clue why this holds or where this comes from? U is a uniformly distributed RV and this is a step somewhere in a proof regarding the acceptance rejection method if that would make any difference. Thanks in advance\n\nhttps://preview.redd.it/jbj5ndhosh8a1.png?width=536&amp;format=png&amp;auto=webp&amp;s=76e39a6234a89ac58a9879ec636ae0fb544e1919",
        "created_utc": 1672169564,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I create a \"smooth\" scaling distribution of numbers?",
        "author": "gabe736",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwn8ce/how_do_i_create_a_smooth_scaling_distribution_of/",
        "text": "Hello. I'm building a marketing calculator. To keep it simple, lets say I have a data set based on 5,000 Instagram accounts with related data like number of followers, growth rate, etc. The accounts range from hundreds of followers on the small side to 100,000+ on the big side, which just a few at the high end -- to acount for the fewer data points at the high end I'll likely just use a \"20,000+\" as the highest option on the slider.\n\nI'm using this as the data set to inform calculator outputs. E.g. people can set their follower count (with a slider) to see an estimate of their expected outcome based on the data set.\n\n**The problem**\n\nFirst, I tried manually separating the data into ranges, e.g. 0-2,000 followers, 2,001 to 5,000 followers, etc. The problem is that the data is quite lumpy (that's a real statistics term, right?) so the change from 2,000 to 2,001 followers just does not make sense. I think I need a way to \"smooth out\" the data with averages so it's still directionally correct, but consistent through the whole rage so there are no weird jumps.\n\nI've looked at a number of different Google Sheets formulas I've found, but realize the issue is I don't know what the right statistics approach/term is. If I had that I could probably piece the formulas needed together. I guess that's what I get for nearly failing stats in college :D\n\n**Ideal outcome**\n\n**I'm looking for a formula or approach in Google Sheets (I can switch to Excel if needed) that will give me a \"smooth\" distribution that's directionally correct.** I will then put this data into a free web calculator with a slider. So as people move the slider indicating their number of followers they will the corresponding rates increase as out put.\n\nIf it helps, here's a google sheet with the data: [https://docs.google.com/spreadsheets/d/13v2dxxBGxNu\\_E0ZKk09UZXEHad-5FmAFExyZZhtBX9s/edit?usp=sharing](https://docs.google.com/spreadsheets/d/13v2dxxBGxNu_E0ZKk09UZXEHad-5FmAFExyZZhtBX9s/edit?usp=sharing)\n\nI hope this makes sense and thank you.",
        "created_utc": 1672167939,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mathematical statistics Recruitment 2023",
        "author": "Educational_Code_555",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwmuj9/mathematical_statistics_recruitment_2023/",
        "text": "Anyone preparing for stat canada exam for Mathematical Statistics 2023. Or has anyone given one in the past and can share what topics to cover. I am studying sampling techniques right now and wants to know what else I should cover and how tough the exam would be.",
        "created_utc": 1672166953,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS - Column comparison and Fisher's exact test",
        "author": "Peach_Pie213",
        "url": "/r/biostatistics/comments/zwjev9/spss_column_comparison_and_fishers_exact_test/",
        "text": "",
        "created_utc": 1672158240,
        "upvote_ratio": 1.0
    },
    {
        "title": "Anyone with experience in Jamovi who'd like to help me with an exam? Willing to pay. Thanks!",
        "author": "Just_Apartment9940",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwg5ie/anyone_with_experience_in_jamovi_whod_like_to/",
        "text": "",
        "created_utc": 1672149248,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the most commonly used stats software in the US",
        "author": "phunterdot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwchpu/what_is_the_most_commonly_used_stats_software_in/",
        "text": "\n\n[View Poll](https://www.reddit.com/poll/zwchpu)",
        "created_utc": 1672136447,
        "upvote_ratio": 1.0
    },
    {
        "title": "\"Power Analysis\" of Kruskall-Wallis Test",
        "author": "GogaReborn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zwbhjp/power_analysis_of_kruskallwallis_test/",
        "text": "In a research on cell culture and disinfection methods. We have 3 methods of disinfection UV Rays, Antibiotic Spray and a untreated control group. The outcome of disinfection is measured by a continuous variable which is number of Colony Forming Units (CFU) of bacteria.\n\nWe can only practically afford a very small sample size thus we don't expect it to be normally distributed that's why we are shifting from ANOVA to Kruskall Wallis test.\n\nThe Question is how do we do the \"power analysis\" for Kruskall Wallis to see if it is even worth it to perform the experiment? I can't seem to find any calculator that offers that. I tried G*Power, nQuery, StatCalc etc. The one I couldn't access was PASS.\n\nShould I just do one based on ANOVA? Or some other test?\nWhat would be the effect size for Kruskall-Wallis test. We are expecting say 0 CFU in UV, 5000 in Antibiotic Spray and 50,000 in untreated group. So a relatively large effect size. But as I understand one needs \"mean ranks\" to calculate effect size of Kruskall Wallis but how can that be done before a sample size estimate?",
        "created_utc": 1672132473,
        "upvote_ratio": 1.0
    }
]