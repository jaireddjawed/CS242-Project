[
    {
        "title": "What is the probability that the chosen satisfactory bulb is of high, medium or low quality?",
        "author": "thatfailedcity",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95ei1c/what_is_the_probability_that_the_chosen/",
        "text": "The stock of a warehouse consists of boxes of high, medium and low quality light bulbs in respective proportions: 1 : 2 : 2. The probability of bulbs of three types being unsatisfactory is 0.0, 0.1 and 0.2 respectively. If a box is chosen at random and two bulb in it are tested and found to be satisfactory, what is the probability that it contains bulbs (1) of high quality, (2) of medium quality, (3) of low quality.\n\n(cross-post from r/statistics)",
        "created_utc": 1533668389,
        "upvote_ratio": ""
    },
    {
        "title": "Which test should I use to compare the same property of groups from different populations?",
        "author": "phfloz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95e2zx/which_test_should_i_use_to_compare_the_same/",
        "text": "Hello,\n\nI'm measuring the surface area (cm²) with a 3d scan of different kinds of rocks (granite, limestone, marble, etc.). By now, I have measured 6 types of rocks, with 20 particles each.\n\nHow can I compare these groups and evaluate if there is a statistical difference between them? Should I use t-test for 2 samples, 2 by 2? Or maybe one-way ANOVA?\n\nThanks",
        "created_utc": 1533665615,
        "upvote_ratio": ""
    },
    {
        "title": "Sample size and risk",
        "author": "thenickydshow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95djuo/sample_size_and_risk/",
        "text": "Hello again,\n\nLooking for some help here. I am looking to calculate the risk of using a certain sample size versus using a larger sample size for the same population. I’m not sure where to start here so anything will help. \n\nMy boss wants me to show that using a larger sample size provides us with a lesser risk of the population being “bad” than when we use our current, small sample size. Looking for some pointers in the right direction or the proper formula to calculate the previously mentioned risk. Thanks!",
        "created_utc": 1533662076,
        "upvote_ratio": ""
    },
    {
        "title": "How do I calculate standard error of proportions which accounts for changes as p approaches 0 or 1",
        "author": "FireBoop",
        "url": "https://www.reddit.com/r/AskStatistics/comments/95cw98/how_do_i_calculate_standard_error_of_proportions/",
        "text": "Hi,\n\nSo suppose probability of an event occurring 0.9. The standard error associated with this mean is 0.04. I don't think that a 95% confidence interval of 0.82-0.98 is valid. The higher value is too high and doesn't account for the fact that the difference between 0.99 and 0.98 is larger than the difference between 0.90 and 0.91.\n\nHow would I calculate a standard error which accounts for this?\n\nThanks",
        "created_utc": 1533657640,
        "upvote_ratio": ""
    },
    {
        "title": "Life Expectancy",
        "author": "JacobZhong",
        "url": "https://www.reddit.com/r/AskStatistics/comments/959vnn/life_expectancy/",
        "text": "As a layman to statistics, I never quite understood the concept of life expectancy. Say my life expectancy is x years. That means I am expected to live x years. But expected is not the same as guaranteed. So \"I'm expected to live x years\" =/= \" there is a 100 percent chance I will live x years.\" \n\nWhat then, is \"Expected\" in terms of numerical probability? Is it a 90 percent chance, or maybe 80 percent, or 70? \n\nCan somebody explain this to me? This may be a lot more complex than I realize, but I would like to get a general idea as a layman. Thank you!",
        "created_utc": 1533631060,
        "upvote_ratio": ""
    },
    {
        "title": "Correlating Votes in UN",
        "author": "dhumidifier",
        "url": "https://www.reddit.com/r/AskStatistics/comments/959ss0/correlating_votes_in_un/",
        "text": "So I have a data set that represents countries' votes in the UN. I would like to see how certain countries correlate. The possible values are \"Yes\" \"No\" \"Abstain\" or \"No Vote\". Now at first I thought it was very simple to just see how the countries correlate, but then I thought a bit deeper about it and decided an \"Abstain\" and a \"No vote\" could be similar to a \"No\". Some countries, however, are rarely present.\n\nMy idea is to create a scale like this: \"Yes\"=2 \"No Vote\"=1 \"Abstain\"=-1 \"No\"=-2\n\nI would also like to weigh \"No votes\" so that if a country has a high number of \"No vote\"s each one will count less when comparing to other countries' \"Abstain\" or No\"s\n\nI have a fair amount of math education, but very little formal statistics education so I apologize if this is blatantly obvious or poorly worded. ",
        "created_utc": 1533630058,
        "upvote_ratio": ""
    },
    {
        "title": "What is the use/purpose of Industrial Classifications (ISIC etc)?",
        "author": "Mr_AQ",
        "url": "https://www.reddit.com/r/statistics/comments/959pd7/what_is_the_usepurpose_of_industrial/",
        "text": "",
        "created_utc": 1533629895,
        "upvote_ratio": ""
    },
    {
        "title": "What statistical model should I use/how should I structure my sampling in animal disease project?",
        "author": "lemon-of-troy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/957evr/what_statistical_model_should_i_usehow_should_i/",
        "text": "I'm currently working on an observational study that involves taking tissue samples (nerve and muscle) and observing the degree of nerve damage/muscle atrophy in relation to the height of the animal. I've been given an assessment as part of my study that requires me to state how my sampling is structured (e.g. any groupings in data evident and how to eliminate any variation by arranging samples in a certain way). I've identified height as the independent variable as height determines nerve length which in theory determines nerve susceptibility to damage, and nerve damage/muscle atrophy as the dependent variables, but I am having trouble determining which statistical model to use as nerve damage is also an independent variable in relation to muscle atrophy. Is there a way to structure sampling such that this isn't an issue? What statistical model should I be using to analyse the results?",
        "created_utc": 1533606620,
        "upvote_ratio": ""
    },
    {
        "title": "statistics ignoramus in need of clarity",
        "author": "joshdes469",
        "url": "https://www.reddit.com/r/AskStatistics/comments/956klx/statistics_ignoramus_in_need_of_clarity/",
        "text": "My educated brother presented me with a statistics \\*rule\\* that doesnt satisfy my intuition and i would appreciate some clarity if possible.\n\nthought experiment goes as follows:\n\n\\-12 people are randomly selected to run a 40yard in 12 different lanes chosen at random with all lanes being equal and all participants being athletically diverse.  The process is replicated each time with new participants.  A graph is produced having each lanes value set to number of wins.  would the lane eventually level out and roughly how many replications would it take if one were to estimate.",
        "created_utc": 1533599697,
        "upvote_ratio": ""
    },
    {
        "title": "Weighted Engagement Rate.",
        "author": "Kajeed",
        "url": "https://www.reddit.com/r/AskStatistics/comments/954z5u/weighted_engagement_rate/",
        "text": "While working in social networks, I'm trying to figure out a way of establishing the actual relevance of differents posts within a month.  \nNormally we calculate the Engagement Rate es \"Post interactions\" / \"Post impressions\", but I'm wondering what's the correct way of showing something like \" (Post Interactions / Total Monthly Interactions\") / (\"Post Impressions\" / \"Total Monthly Impressions\").  \nAny help would be appreciated!",
        "created_utc": 1533587991,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence vs Prediction Interval Terms Used in Equation",
        "author": "Futuremlb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/953r6x/confidence_vs_prediction_interval_terms_used_in/",
        "text": "Hi everyone, my textbook provides a nice intuition as to why we do not include random error in the confidence interval since we assume the error is normally distributed and thus can be ignored when taking averages. However, it is extremely frustrating because no exact equation is provided in the book. When calculating a confidence interval, we do -&gt; value  ± t\\*SE. Where t corresponds to appropriate confidence interval. What standard error are we using though? Is this the standard deviation of our data(output) divided by sqrt of data size n?  And for prediction interval are we using the standard deviation of our data divided by sqrt data size n PLUS the standard deviation of our residuals? This is what I got from googling, but it is still not clear and not sure if I am right.\n\nThis is assuming multiple linear regression is used. Any clarification is greatly appreciated, thanks!",
        "created_utc": 1533579690,
        "upvote_ratio": ""
    },
    {
        "title": "Mean center discrete and ordinal variables when running cross-level interactions?",
        "author": "Sociological_Duck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/953qo3/mean_center_discrete_and_ordinal_variables_when/",
        "text": "I'm running two set of analyses. One is dichotomous by continuous (0-100), so I will certainly mean center that variable.\n\nThe other has a dichotomous variable interacting with a variable built from a likert scale. This one is a little tricky to me, because the individual level scale goes from 1-7. However, after creating a country level variable from it, it ranges from 1.782 to 4.061. The dependent variable is also a 5 item likert scale (strongly disagree to strongly agree). Do these need to be mean centered as well? And what about the other continuous variables in the model?",
        "created_utc": 1533579591,
        "upvote_ratio": ""
    },
    {
        "title": "Books for reading up on Survey weighting?",
        "author": "yinyang_zen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/952b5w/books_for_reading_up_on_survey_weighting/",
        "text": "",
        "created_utc": 1533569841,
        "upvote_ratio": ""
    },
    {
        "title": "Use of quadratic RM ANOVA with normed data points",
        "author": "expecto_patronum_1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94y8vh/use_of_quadratic_rm_anova_with_normed_data_points/",
        "text": "Hi,\nI've received excellent assistance from this subreddit before, and it has helped me very much. \nAs the title says, I have a question re: use of quadratic RM ANOVA when measuring performance in a verbal fluency task. The between subjects variable is has/does not have the Broad Autism Phenotype; the within subjects variable is a normative score (mean = 10, sd = 3) for a 60\" verbal fluency task. There are four levels--1st, 2nd, 3rd, and 4th 15\" intervals. \nI have run the ANOVA and it appears there is both a linear and quadratic effect, but no cubic effect. \nIs it appropriate to use this method of comparison when the scores have already been normed to consider the impact of time? That is, these are not raw scores which would drop probably both linearly and quadratically over time--(example: a group, on average generates 15 words in the first 15\", then 10 more words, then 3 more, then one more) these are scores that it would be expected, if the groups were perfectly average on all intervals, to have an average of 10, 10, 10, and 10 across each interval. (It seems, though that it would just test if it were NOT 10, 10, 10, and 10, and the problem would lie in interpretation). Thus, I was thinking that on the other hand, as I did see a significant linear and quadratic trend, does this mean the groups are even worse than would be expected on those last three intervals? That is, even considering the impact of time, giving them a normed score that reflected generating fewer words, they dropped below the mean?\nAgain, I greatly appreciate your help, and thank you in advance. \n*Edit: A couple punctuation marks and clarity of the next to last sentence.",
        "created_utc": 1533529697,
        "upvote_ratio": ""
    },
    {
        "title": "Linear Mixed Effects Model - Changing p-values",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94xzd6/linear_mixed_effects_model_changing_pvalues/",
        "text": "[deleted]",
        "created_utc": 1533527163,
        "upvote_ratio": ""
    },
    {
        "title": "Correlation test for finding viable features in mix of continuous and discreet features",
        "author": "RealMatchesMalonee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94v40r/correlation_test_for_finding_viable_features_in/",
        "text": "Hi. I have this data set on my hands that is a mix of discreet and continuous valued features. I want to find features in this list that are viable for predicting the label of a given test case. I think finding the correlation between the target feature (the label that we want to find) and all the other features should help clear up the picture a bit. But since the data has both continuous and discreet features, I'm not sure if correlation should be used. Am I right or wrong?",
        "created_utc": 1533502406,
        "upvote_ratio": ""
    },
    {
        "title": "How to determine if there is a linear relationship",
        "author": "Dabaumb101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94utli/how_to_determine_if_there_is_a_linear_relationship/",
        "text": "Hey guys,\n\nworking on some Econometrics problems, and one of the questions asks to determine is a given set of data demonstrates a linear relationship, and if it does, to calculate the function\n\n\nConsider the conditional probability distribution of Y, given X, shown in the table below.\n\n\nY f(y|X=0) f(y|X=1) f(y|X=2)\n\n\n0    0.20      0.20         0.20\n\n\n1    0.20      0.40         0.60\n\n\n2    0.60      0.40         0.20\n\n\nc. Is the conditional expectation function linear? If so write the linear function. \n\nI calculated the conditional function E(y given x) for x=0,1,2 and E(Y) and determined that Y is not mean-independent of X\n\nBut I'm struggling to determine if the conditional function is linear. My gut tells me no because if X=0 then Y=.2(0)+.2(1)+.6(2) and there is not X in the formula.\n\nAny and all help is greatly appreciated, thanks!",
        "created_utc": 1533500165,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing standard deviation to standard error?",
        "author": "playcdagain",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94u0qy/comparing_standard_deviation_to_standard_error/",
        "text": "In my study, I have reported the mean and standard deviation for a number of metrics. I wanted to compare my results to those of other studies that also looked at these metrics. My concern is that my standard deviations are too big. My problem is that all the other studies report SE (standard error) instead of SD (standard deviation). What does SE mean in a context where there is only one sample (and not a sampling distribution)? Also, how would I compare my SDs to their SEs? ",
        "created_utc": 1533493944,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting results of ordinal logistic regression (ordered logit)",
        "author": "camille_neuropsych",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94rx8k/interpreting_results_of_ordinal_logistic/",
        "text": "Using SPSS, I have run a generalized linear mixed model for repeated measures longitudinal data on an ordinal target variable: duration of hallucinations with levels of 0= \"N/A\" (no hallucinations), 1=\"Seconds\", 2=\"Minutes\", 3=\"Hours\" and 4=\"Continuous\". I used a multinomial distribution with a logit link function.\n\nThe model found a significant effect of [Event.Name](https://Event.Name), which is actually Time measured in weeks. FINAL.DIAG is my 5 different patient groups.\n\nhttps://i.redd.it/2vh9j5cx2ae11.png\n\nI am trying to interpret these findings and am struggling with what I am finding online. Some sources I've found seem to say that I can only state something like the following:\n\n\"For a one unit increase in time, the odds of observing any of the lower categories of duration (N/A, Seconds, Minutes, or Hours) versus the highest category of duration (Continuous) are 0.958 times less or decrease by 4.2% (1-0.958), given that the other variables in the model are held constant. Due to the proportional odds assumption of ordinal variables, the same decrease in odds of 0.958 times, is found between any category below a certain threshold and any category above the same threshold. For example, the odds of observing the lowest category of duration, “N/A”, are also decreased by 4.2% compared to observing any of the other categories (Seconds, Minutes, Hours or Continuous).\"\n\nBut other sources seem to say that I can state my findings like this:\n\n\"An increase in time (expressed in weeks) was associated with an decrease in the odds of a longer VH duration, with an odds ratio of 0.958 (95% CI, 0.958 to 0.978), t = -4.115, p &lt; .001.\"\n\nMy question is can I say that the odds of a subject being in a higher category of duration decrease with increasing time? Or can I only say the odds of observing any of the lower categories of duration (N/A, Seconds, Minutes, or Hours) versus the highest category of duration (Continuous) decrease by 4.2% as time increases but the same can be said for the odds of observing N/A vs any of the other levels in my ordinal variable? And what can I even conclude from that? The proportional odds assumption confuses me and I really would just like to say the second version of interpretation if that is also correct.\n\nThank you in advance to anyone who attempts to help me with this!\n\n(Btw, I have also posted this to Cross Validated - Stack Exchange and will post responses I get from there into this discussion.)",
        "created_utc": 1533476436,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting correlations of differences",
        "author": "yuppychan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94rk80/interpreting_correlations_of_differences/",
        "text": "Hi guys, student neuroscientist here.\n\nI am running a repeated-measures design experiment for drug A and B (active vs placebo) and I have some significant correlation coefficients across \\*differences\\* in neuroimaging parameters (essentially how active a certain brain region is for each individual) and \\*differences\\* in behavioural task scores across drug (so scores of drug A-B). Can anyone please give me a simple way of interpreting these? I think I am justified in saying that the drug's effect on the brain is correlated with the drug's effect on behaviour, and thus these effects appear to be linked, but is there any more to it than that or are there any caveats I am missing?\n\nAlso are there any advantages or disadvantages of doing what I described vs correlating across measures for each drug separately? In my view, my method is richer in that it takes into account the drug effect vs placebo, but I am all ears for comments!\n\nThanks for your comments in advance!",
        "created_utc": 1533472606,
        "upvote_ratio": ""
    },
    {
        "title": "Probability of collecting all unique blind bags",
        "author": "vinylandcelluloid",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94pmni/probability_of_collecting_all_unique_blind_bags/",
        "text": "I’m trying to wrap my head around the probability of blind bags.  Just looking at the problem in broad terms, not considering the makeup of boxes or different rarity tiers.  Say there are 30 blind bags to collect.  If every pick is independent and equal chance of any of of the 30, how many would I need to open to get all 30?  Or rather since it’s probabilistic, when would i be at 50% chance and 99% chance of having all 30 options?\n\nI know that each individual pick is 1/30, I’m struggling with how to express the decreasing odds of getting a unique pick/increasing odds of getting one you already have as you collect more of them.  \n\nThanks!",
        "created_utc": 1533446724,
        "upvote_ratio": ""
    },
    {
        "title": "Investment chances?",
        "author": "TheFox30",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94ogvo/investment_chances/",
        "text": "Assuming a startup chance to succeed are 10%\n\nAnd you have a choice to invest $100 in a single startup, \nOr to split the investment to $10 and invest in 10 different startups.\n\nHow to calculate the chance of getting any success? And what will be a better bet?",
        "created_utc": 1533434931,
        "upvote_ratio": ""
    },
    {
        "title": "Delete If inappropriate but I don't see any rules in the side bar, I'm stuck on a question and need a hint on how to solve it!",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94npz9/delete_if_inappropriate_but_i_dont_see_any_rules/",
        "text": "[deleted]",
        "created_utc": 1533427877,
        "upvote_ratio": ""
    },
    {
        "title": "Chance of facing off against team in competition.",
        "author": "eldizzy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94n1tw/chance_of_facing_off_against_team_in_competition/",
        "text": "In two months, my debate team will travel to regional competition and we want to know the possibility of facing another team in our league. We are randomly placed and no one is seeded. There are 16 teams, and four rounds of competition where no team faces the same team twice. Every team faces some other team during each round. What are the chances that we will face another team, let's call it team B. \n\nMy team is in a debate right now on the chances. We have answers ranging from 1/16 to somewhere approaching 1/3. If anyone has the answer, or wants to provide their best guess post below with an explanation! ",
        "created_utc": 1533421893,
        "upvote_ratio": ""
    },
    {
        "title": "How to build a likelihood when flipping two coins independently?",
        "author": "specific_account_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94mswd/how_to_build_a_likelihood_when_flipping_two_coins/",
        "text": "Hi, I am working on a problem. I have asked for help at the statistical consulting center on campus but the consultant got stuck and said the problem is outside his expertise. I hope someone here can give me a hand or suggest what to do next! Thank you for your patience in reading this.\n\nIn my problem, a person goes through a maze made of 32 T-intersections. At each intersection they can decide to turn either left or right; each time, on one side there will be a wall, while on the other side the maze will continue to the next intersection.\nTheir goal is to arrive to the end of the maze as soon as possible.\n\nAt each intersection, the person is aided by two cues, a visual cue and an auditory cue. **The cues are not always correct and not always in agreement**. Specifically, the cues can have different level of reliability. For example, in one maze the auditory cue may be correct 50% of the times, while the visual cue may be correct 25% of the times.  \n\nThe cues may be reliable at three different levels: 25%, 50% and 75%.  The reliability of cue does not change within one maze, but may change from maze to maze. Each new maze is a completely different situation. \n\nSince we have three different reliability levels, combining all of them we have a total of nine mazes.\n\n*Our main goal is to understand*:\n\n- Whether the person in themaze trusts more the auditory cue or the visual cue, or whether there is any bias toward one of the two modalities.\n- How long it takes for the person to learn the statistics of the environment\n\n**I was thinking about modeling the two cues (auditory and visual) as flipping two coins independently**. \n\nLet's say the auditory cue is 50% reliable; I will represent it with a fair coin. Let's say the visual cue is 75% reliable; I will represent it with an unfair coin that lands on head 75% of the times.\nAt this point I was inspired by the paper [\"Bayesian models of cognition\"](https://cocosci.berkeley.edu/tom/papers/bayeschapter.pdf) by Griffiths et al., to use a Beta prior to the Binomial distribution (see formulas 13 and 14 in the article and the discussion preceding them). This way I thought I could calculate my likelihoods separately for the visual and auditory cues, and then compare them at each turn. So I was planning to use three parameters:\n\n- 𝜃1, the probability that the first cue is correct \n- 𝜃2, the probability that the second cue is correct\n- *d*,  the number of successes (the observed outcome)\n\nAnd have these two likelihoods:\n\nP(d / 𝜃1) = 𝜃1^(d) (1 - 𝜃1^(d) )^(d)\n\nP(d / 𝜃2) = 𝜃2^(d) (1 - 𝜃2^(d) )^(d)\n\nBy then I talked to the statistical consultant and he told me that while a uniform prior is fine, the problem is very complicated and that I should use four additional parameters:\n\n- *d*^(audio), the probability that the person would follow the auditory cue when considering it alone (unobserved)\n\n- *d*^(visual), the probability that the person would follow the visual cue when considering it alone (unobserved)\n - 𝜂, combination of *d*^(audio) and *d*^(visual), leading to *d*, the decision I observe.\n\nThen I will have:\n H*_0_* : 𝜂 = .5 (no bias)\n H*_A_* : 𝜂 ≠ .5 (bias)\n\nThen he added I should actually consider two 𝜂s:\n\n𝜂*_agree_* in case the cues agree\n𝜂*_disagree_* in case the cues disagree\n\nNow I am not sure about how to build the likelihood for this model! Any ideas?\n",
        "created_utc": 1533419778,
        "upvote_ratio": ""
    },
    {
        "title": "What kind of analysis should i use?",
        "author": "FluffyTee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94m58x/what_kind_of_analysis_should_i_use/",
        "text": "Hi guys. i really need help on a thesis data analysis.\n\nThe study is to measure a balloon sizes(major and minor axis) in 3D images(on a desktop monitor) vs Virtual reality goggles/HMD.  measurement are performed in random order, twice permethod with 1 month interval between each reading\n\n60 Objects are used and there are 4 observers.\n\nI need a method to calculate intraobserver variability, interobserver variability and agreement between desktop monitor vs VR.\n\nWhat would be the best statistical analysis should i use?\n\n\\*edit i change balloon to object for now, as i realize balloons deflate over time.\\*",
        "created_utc": 1533414283,
        "upvote_ratio": ""
    },
    {
        "title": "Overlapning zones and confidence",
        "author": "LilleOel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94m11g/overlapning_zones_and_confidence/",
        "text": "I have two overlapping intervals. I have Area 1 (point a and b) and Area 2 (point b and c)\n\nThey are measured 100 times. E.g.\n\nid;a;b;c\n1;2.5;4;7\n2;3;5;8\n...\n\nArea1;area2\n2.5-4;4-7\n3-5;5;8\n...\n\nHow can i with x% confidence find the area2 where Area1 does not overlap?",
        "created_utc": 1533413315,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing baseline characteristics with a logistic regression versus chi-square/Mann-Whitney/T-tests, what is the difference?",
        "author": "durpyflurpy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94jxao/comparing_baseline_characteristics_with_a/",
        "text": "[This article](https://www.ncbi.nlm.nih.gov/pubmed/22569939) compares baseline characteristics with either linear or logistic regression.\n\n[This article](https://www.ncbi.nlm.nih.gov/pubmed/?term=Risk+factors+for+atherosclerotic+and+medial+arterial+calcification+of+the+intracranial+internal+carotid+artery) compares baseline characteristics with chi-square tests, T-tests, or Mann-Whitney tests.\n\nWhat is the difference? What is the decision based upon?",
        "created_utc": 1533396511,
        "upvote_ratio": ""
    },
    {
        "title": "[deleted by user]",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94hnpz/analyzing_rollercoasters_can_we_even_sufficiently/",
        "text": "[removed]",
        "created_utc": 1533370803,
        "upvote_ratio": ""
    },
    {
        "title": "Best practice around coding responses in Stata",
        "author": "thismustbeavailable",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94dis6/best_practice_around_coding_responses_in_stata/",
        "text": "Hi all,\nI've got some survey data in Stata, and one of the variables has several answers in the same cell (e.g. Answer 1/Answer 2/Answer 3). I've split (split var, p(\"/\") each of these into different variables.\nHowever, I now have up to 10 new variables and wanted to know the best way of coding these. I can code each of the new variables with the responses but how best should I combine these in order to do cross-tabs? Do I create binary variables for each of the 10 potential responses and then do tabs that way? E.g. tab BinaryVar Age? Thoughts very much appreciated, and please let me know if the above isn't clear.\nThank you!",
        "created_utc": 1533330967,
        "upvote_ratio": ""
    },
    {
        "title": "Graphing interaction terms and using different kinds of variables",
        "author": "KaleMunoz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94di7d/graphing_interaction_terms_and_using_different/",
        "text": "I have run some analyses with interaction terms. I’d like to graph them. Is there a simple guide to help us do this? I really hate Excel and forget how every time I put these equations into it. \n\nAlso, do I need to do anything special if I’m using different types of variables? For example, a dichotomous variable interacting with a continuous? I know in general I need to mean center continuous variables. ",
        "created_utc": 1533330859,
        "upvote_ratio": ""
    },
    {
        "title": "Simple Question about Calculating Standard Error",
        "author": "mapsandclocks",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94d5mi/simple_question_about_calculating_standard_error/",
        "text": "Hi r/AskStatistics. Thank you in advance!\n\nI have data from many subjects. Each subject has a different number of data points. Therefore to calculate a weighted mean I am calculating the overall mean by first finding the mean for each subject and then taking the mean of that.\n\nMy question is regarding how to then get the correct standard error? Do I just take the standard error of the set of individual subject means? \n\nCheers!",
        "created_utc": 1533328280,
        "upvote_ratio": ""
    },
    {
        "title": "Thinking outside the box",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/94abpo/thinking_outside_the_box/",
        "text": "[deleted]",
        "created_utc": 1533308622,
        "upvote_ratio": ""
    },
    {
        "title": "Wondering how to study hospital capacity as a function of length of stay.",
        "author": "MNAAAAA",
        "url": "https://www.reddit.com/r/AskStatistics/comments/949tn6/wondering_how_to_study_hospital_capacity_as_a/",
        "text": "The situation:\n\nA hospital is looking to reduce average length of stay for patients admitted to their hospital.  Shorter length of stay -&gt; reduced stress for patients, more efficient care, better outcomes, and ability to serve more patients.\n\nMy question:\n\nThe hospital would like to study how this change in average length of stay (say, from 5 to 4 days, assuming the variance would stay the same) would affect both their admission rate (how many unique admissions in a given period of time - less important) and their average number of patients in the hospital at any given day (more important).\n\nIf there's a way to study how the distribution of each characteristic (admission rate and length of stay) affects available capacity, or just general readings on how to combine distributions in the way I'm describing, that would be helpful as well.\n\nThanks for any help you can provide.",
        "created_utc": 1533304872,
        "upvote_ratio": ""
    },
    {
        "title": "Not sure which test to use to compare tissue donor differences. Help appreciated.",
        "author": "pun-a-tron4000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/949kye/not_sure_which_test_to_use_to_compare_tissue/",
        "text": "Hello All. I'm examining differences in protein production (in grams) between cells taken from different donors over time and I'm struggling to get the right analysis.\n\nMy data looks roughly like this (obviously with values):\n\n|Time (Days)|Donor 1|Donor2|Donor3|Donor 1 ctrl|Donor2ctrl|Donor3ctrl|\n|:-|:-|:-|:-|:-|:-|:-|\n|7|||||||\n|14|||||||\n|21|||||||\n|28|||||||\n||||||||\n\nI'm trying to determine any differences between donors at the same time points. I thought it would be a 2 way ANOVA as there are multiple groups and I want to compare results in each row but this showed no significant difference between positives and controls however a different analysis has confirmed at least at day 21+ there is a significant change. I'm using Graphpad (Prism) if that is relevant.",
        "created_utc": 1533302921,
        "upvote_ratio": ""
    },
    {
        "title": "Does it make sense to do a linear regression with a score variable as the dependent variable?",
        "author": "theatropos1994",
        "url": "https://www.reddit.com/r/AskStatistics/comments/947uv9/does_it_make_sense_to_do_a_linear_regression_with/",
        "text": "I have a survey where the score variables is calculated as a function of all the questions in it. A colleague has suggested that I use a linear regression to explain the contribution of each variable to the variation in score, which didn't make sense to me since they are all equally weighted when calculating the score meaning that they have the same contribution.\n\nAm I understanding it wrong ?",
        "created_utc": 1533285098,
        "upvote_ratio": ""
    },
    {
        "title": "How much more likely is an American to be killed by someone of their own race than another?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/945un5/how_much_more_likely_is_an_american_to_be_killed/",
        "text": "[deleted]",
        "created_utc": 1533264498,
        "upvote_ratio": ""
    },
    {
        "title": "Should I use Poisson? Sports model question.",
        "author": "222Botany",
        "url": "https://www.reddit.com/r/AskStatistics/comments/945dwv/should_i_use_poisson_sports_model_question/",
        "text": "I’m currently building a sports prediction model for Rugby league (NRL). I’m trying to predict the total number of try’s each team will score with in a game. Then produce the probability of it being more or less than a certain amount per game. Comparing to the bookmakers price and betting if I have a perceived edge. \n\nI’ve done some reading/YouTube videos etc... and looks like the Poisson distribution is what I need to use to calculate the probability of each out come based on the predicted number. This predicted number is a based on an average per game with some calculations applied to tweak it up or down depending on various factors.\n\n\nI understand Poisson is for whole number count data over a specific time. The predicted number I produce is normally a fraction and the bookies will always be over/under 6.5 tries as an example.\n\nIs Poisson still applicable here?\n\nShould I round my prediction/bookies number first then calculate the probability?\n\nOr use another distribution?\n\nThe model seems to be working so far this season with a good 60% odd strike rate and 28% profit. It’s a relatively small sample of bets so could be variance at this early stage.\n\nThanks for any advice",
        "created_utc": 1533260565,
        "upvote_ratio": ""
    },
    {
        "title": "How do i do Political Research?",
        "author": "ABlackKidOnTheNet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/945ahu/how_do_i_do_political_research/",
        "text": "Questions pretty straight forward, how do i filter out good studies from bad ones?",
        "created_utc": 1533259761,
        "upvote_ratio": ""
    },
    {
        "title": "How much did Ron Swanson affect Lagavulin 16?",
        "author": "heytherepookie",
        "url": "https://www.reddit.com/r/AskStatistics/comments/944j1m/how_much_did_ron_swanson_affect_lagavulin_16/",
        "text": "In Parks and Rec, character Ron Swanson (Nick Offerman) mentions his favorite whisky, Lagavulin 16, a kabillion times, and later, actor Nick Offerman did some commercials for them. \n\nHow did this affect the products sales and consumption?\n\nDid the price of the whisky go up?\n\nHas Lagavulin 16's image changed?\n\nI'd love to see some charts and data, as I hadn't heard of it before, but am now looking forward to trying it. Thanks.",
        "created_utc": 1533253467,
        "upvote_ratio": ""
    },
    {
        "title": "Analyzing Pre/Post Binary Outcomes - Should I use Logistic Regression or another approach?",
        "author": "throwaway_762",
        "url": "https://www.reddit.com/r/AskStatistics/comments/944hjs/analyzing_prepost_binary_outcomes_should_i_use/",
        "text": "**Background**: I am analyzing data on awareness of tobacco health risks before and after an intervention using R. Before the intervention, participants were asked if they knew health risks of tobacco (not my study). There were categorized as 0 (did not know) or 1 (did know about health risks). Then, a 5 minute video was shown. After the video, participants were again asked if they knew health risks of tobacco and again categorized as 0 (did not know) or 1 (did know about health risks).\n\n**Data**: In addition to before and after binary results for each participant, there are additional demographic data available. These data include Age (continuous), School (categorical variable: graduated high school/didn't graduate high school), and Gender (Male/Female) for each participant. There were n=150 randomly selected participants.\n\n**Goal**: Assess the impact of the video on awareness of tobacco outcomes.\n\n**My suggested approach**: Classify all participants into one of four outcomes.\n\nPRE 0, POST 0: *No Improvement*\n\nPRE 0, POST 1: *Improvement*\n\nPRE 1, POST 0: *No Improvement*\n\nPRE 1, POST 1: *No Improvement*\n\nThen, use logistic regression on the dichotomous outcome (Improved/No Improvement), while adjusting for Age, School, and Gender. The (1,0) group would be classified as No Improvement so that the outcome remains binary.\n\n**Questions:**\n\n1. In logistic regression, how do I get an \"overall\" impact of the intervention? I was intending to use the Intercept value from the model after adjusting for Age, School, and Gender.\n2. This approach defines (1, 0) as No Improvement, though in reality it is actually worsening. While this is unusual, I feel it needs to be accounted for. Hence there are technically three results: Improvement (0,1), No Change for both (0,0) and (1,1), and Worsening (1,0). What kind of models can be used for this type of situation?\n3. Does it matter that some people previously knew about health risks? In other words, does the model need to acknowledge a difference between (1,1) vs. (0,0) cases? I am not certain whether logistic regression inherently accounts for this or if it matters, or if there is another type of model to look.\n\nSorry for the long wall of text. Any help is hugely appreciated. Thank you.\n\nEDIT: Updated background and slight edits.",
        "created_utc": 1533253131,
        "upvote_ratio": ""
    },
    {
        "title": "Need Help Determining What Statistics/Methods to Use: proving a current sample size is inefficient and too small to use based on our population size (long read but please help)",
        "author": "thenickydshow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/942xw7/need_help_determining_what_statisticsmethods_to/",
        "text": "Good afternoon r/askstatistics, and I want to thank all of you who take the time to read this and try to help.\nI work for a biotech company and I, the intern, have been tasked with determining an optimal sample size to be used by our QC team. Currently we use a sample size of 20 \"tubes\" (the product) as a first step in QC inspection. If 1 of the 20 tubes fails, another 20 tubes are tested and if that test yields no failures of the second 20, the \"lot\" (population) of tubes passes and can go into final production to be sold. If a tube in the second set of tubes fails inspection, just 1 (similar to the first 20), the lot fails and cannot be sold. Essentially if 2 tubes of the 40 tube sample fail, the lot of 1200 or 2400 tubes also fails and are wasted.\n\nSome important info here is that our lot sizes range from 800 tubes to 2800 tubes. For this examples sake/my upcoming presentation's sake we are using two lot sizes, 1200 and 2400. \n\nI am looking to convince some of our senior members that our current sample size is too small, especially on our large lots (2400) and we are failing too many lots because we are not considering an appropriate margin of error (failure). What statistical approach could I take to help my case?\nMy proposed approach is to initially test 40 tubes and allow up to 2 to fail inspection (5% margin of error/failure being acceptable) and if 2 do fail inspection, we introduce a second set of 30 tubes and if 1 of those 30 tubes fails, then the lot fails.\n(I got my new suggested sample size (70) from http://www.raosoft.com/samplesize.html )\nIn essence, we go from an acceptable failure rate of 5% on a sample and up the ante if it is running the risk of failing to 3.33% to ensure that the determining sample is a good tell for the lot.\nIs this a statistically sound approach or do I need an even larger sample size for such large populations (lots)? Please let me know if there is a good approach to this or some formulas that I can use to give me a more accurate representation of the lots via samples and their sizes.\n\n****I just visited http://www.raosoft.com/samplesize.html and realized that I had unknowingly set the response to 5% instead of 50% (normal), upon fixing it, the website suggests a sample size of 292 on a 1200 tube lot and 332 for a 2400 tube lot. I guess at this point only consider my 70 tube sample as a potential low ball for the QC change. Regardless I need to know if there is a means to back this with formulas and calculations that will change the mind of the seniors at my company.\nThank you to those who can help!",
        "created_utc": 1533241832,
        "upvote_ratio": ""
    },
    {
        "title": "Correlation Question",
        "author": "adamlurie42",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9420lu/correlation_question/",
        "text": "Hey guys,\n\nI'm working on a project where we have one variable that we measure between 0-4 and we were hoping to see if that variable was affected by either the presence or absence of another variable (I've coded yes as 1 and 2 as no for if the other variable is present). I was wondering what test I could do in Excel to figure this out. Would a simple correlation be sufficient or do we need to somehow pair the first value with the yes or no values for that day?\n\nThanks!",
        "created_utc": 1533235578,
        "upvote_ratio": ""
    },
    {
        "title": "How to fit a sinusoid wave if I have only a noisy part of it?",
        "author": "zspasztori",
        "url": "https://www.reddit.com/r/AskStatistics/comments/941cnf/how_to_fit_a_sinusoid_wave_if_i_have_only_a_noisy/",
        "text": "Hello,\n\nI am interested if there is a good way of fitting to a part of sinusoid wave.\n\nThe wave is slightly noisy, it has unknown bias , its amplitude is not known, it also has phase shift. I know the frequency tough.\n\nI only have lets say 60 degrees of the wave, a random segment, but I know for sure that it is sinusoidal. \n\nIs it possible to fit this with least-square method or with FFT? What do you guys think?",
        "created_utc": 1533231112,
        "upvote_ratio": ""
    },
    {
        "title": "Are order statistics ever complete?",
        "author": "Darth_Marrr",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93vbc6/are_order_statistics_ever_complete/",
        "text": "I was curious if order statistics can ever be proven to be complete or by default are they never complete? Can someone supply a theorem to support this claim?",
        "created_utc": 1533176059,
        "upvote_ratio": ""
    },
    {
        "title": "What other metrics are there similar to mean, median, mode, harmonic mean, geometric mean, etc.?",
        "author": "o-rka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93qliz/what_other_metrics_are_there_similar_to_mean/",
        "text": "Looking for reduction stats that are similar to the aforementioned.  Preferably implemented in Python :) but if not I can reimplement. ",
        "created_utc": 1533141617,
        "upvote_ratio": ""
    },
    {
        "title": "Do any of you know where I can find data on the % of female accountants, lawyers, IT specialists and management consultants in the workforce in Spain?",
        "author": "Jacques_412",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93qamv/do_any_of_you_know_where_i_can_find_data_on_the/",
        "text": "",
        "created_utc": 1533139613,
        "upvote_ratio": ""
    },
    {
        "title": "It seems that the author has misused Jensen's Inequality near the end there. Please verify (http://cs229.stanford.edu/notes/cs229-notes8.pdf page 3, for those who need more context)",
        "author": "[deleted]",
        "url": "https://i.redd.it/izeepin6xhd11.png",
        "text": "[deleted]",
        "created_utc": 1533135706,
        "upvote_ratio": ""
    },
    {
        "title": "How safe is my safe?",
        "author": "bondsman333",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93po5n/how_safe_is_my_safe/",
        "text": "I have an old safe and the dial is pretty loose.\nNormally the safe requires 3 digits, from 0-99. However, in it's current state this mechanism allows for the digits to be +/- 1.\n\nExample: combo is 63-12-47 but the lock accepts 64-13-46.\n\nHow much less secure is this safe from one that would only accept the true combination? And what about a safe that was +/- 2 digits? There's a certain point where I would want to replace the dial mechanism, but I don't think I am there yet.\n\nThanks!",
        "created_utc": 1533135393,
        "upvote_ratio": ""
    },
    {
        "title": "Testing linearity when independent variable is dichotomous.",
        "author": "stugradent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93pmyy/testing_linearity_when_independent_variable_is/",
        "text": "I am reading-up on multilevel linear model statistics, and think it is what I need to use for my data. That being said, checking data for linearity is one of the data checks that should be done. Sad as it is, I am unsure how to do with a dichotomous independent variable. Any help is appreciated!",
        "created_utc": 1533135152,
        "upvote_ratio": ""
    },
    {
        "title": "The author has misused Jensen's Inequality in the last statement, hasn't he? (http://cs229.stanford.edu/notes/cs229-notes8.pdf , page 3)",
        "author": "[deleted]",
        "url": "https://i.redd.it/9ebeymn0whd11.png",
        "text": "[deleted]",
        "created_utc": 1533135092,
        "upvote_ratio": ""
    },
    {
        "title": "Converting logit model outcome to 0-1 probability",
        "author": "SenatusSPQR",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93o3ri/converting_logit_model_outcome_to_01_probability/",
        "text": "This has been killing me for a while now so apologies if I overlooked something simple but I really need some help.\n\nI'm attempting to use a model (specifically, benchmark 1, table 3, page 20 from this paper https://www.ecb.europa.eu/pub/pdf/scpwps/ecbwp1597.pdf). To do so, I am using the variables that are used in this benchmark model (capital ratio, tier 1 ratio, etc etc) that I have normalised using the mean / standard deviation found in this paper. So for example, my capital ratio is 0.1 and the mean / standard deviation found in the paper is 0.07 and 0.04 respectively, so my outcome is (0.10 - 0.07) / 0.04 = 0.75 (as an example). I do this for all coefficients listed and get a total sum of, for example, -2. However, in the paper they arrive at a probability that is in a range of 0 to 1.\n\nI can't seem to figure out how to convert my result (ranging from -23.09 to 64.54) to a probability that is between 0 and 1, and I can't help but feel that I am overlooking something simple. Any help would be very much appreciated because I have been stuck on this for days now :(\n\nEdit: What I think I need to do is calculate it as 1 / (1 + e ^ (-outcome), if that makes sense. But I'm sincerely hoping that is wrong because if that is the case then the probabilities are far different from what I expected.",
        "created_utc": 1533122629,
        "upvote_ratio": ""
    },
    {
        "title": "Kruskall Wallis for repeated measures?",
        "author": "staticseven",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93niqx/kruskall_wallis_for_repeated_measures/",
        "text": "Hi all,\n\nI conducted a longitudinal experience sampling study and want run Kruskall Wallis on some of the data. However, there are multiple measurements per participants spread over time. I was wondering if there's something similar to Kruskall Wallis I can run on categorical data that assumes repeated measures?\n\nI'm a total novice so please be as clear as possible. Imagine explaining it to a five year old. That's about my level.\n\nCheers",
        "created_utc": 1533116252,
        "upvote_ratio": ""
    },
    {
        "title": "Combining Classifiers under assumption of independence",
        "author": "Hoeschel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93mxo7/combining_classifiers_under_assumption_of/",
        "text": "I have a big dataset, containing the responses to multiple questionnaires. All questionnaires are trying to measure related concepts, but some concepts are closer to each other than others. I trained a classifier (neural network) to predict a binary outcome. The classifier was trained both on individual questionnaires as well as sets of questionnaires. What I am interested in is how much predictive quality the classifier gains by using information from multiple questionnaires simultaneously.\n\nFor simplicity’s sake, let’s say there are only three questionnaires. Then the classifier puts out four different estimates of the case belonging to the target class: P1, P2, P3 based solely on the responses to the individual questionnaires; P123 based on the combining all responses. I use accuracy to measure the quality of these predictions (Creating a binary prediction by cutting off the probabilities at p=.5) and I want to measure how much more accurate P123 is, compared to what would be expected, if the three questionnaires would share no information that the classifier could exploit.\n\nSo my idea is to pretend that P1, P2 and P3 are independent estimates of the target class and combine these estimates into a single estimate (P123_C). Then I can compare the accuracy obtained by using P123 to that using P123_C by taking the difference. That difference would then be a measure of the increase in predictive quality.\n\nSo here are my questions:\n\n1. Is that a sensible approach in general?\n2. How do I combine P1, P2 and P3 into P123_C?\n\n    My intuition tells me that I can obtain P123_C by calculating\n\n    P1\\*P2\\*P3 / (P1\\*P2\\*P3 + (1-P1)\\*(1-P2)\\*(1-P3)).\n\n    Is that correct? If so, why?\n\nAdditional information:\n\n* Each questionnaire was answered by at least 10000 people\n* All probability estimates were obtained through a logistic transformation\n* The distributions of the logits are very close to normal in all cases.\n* The binary class is approximately split 50-50\n* I think I have to use accuracy to keep the results comprehensible for the intended audience.",
        "created_utc": 1533109296,
        "upvote_ratio": ""
    },
    {
        "title": "Megastat's regression analysis",
        "author": "brainplease",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93mv5j/megastats_regression_analysis/",
        "text": "Hey there,\n\nWhenever I try to get a regression analysis, Megastat gives me an error message saying that the \"confidence levels must be between 0 and 1\". Regardless of which drop-down option I select (or try to type in because ugh), same outcome. I thought I would see if anyone knows what's up with this - Megastat's help isn't any new info, and the handful of forum answers I could find online all trail off (would posting the question and its answer be of help?).\n\nhttps://i.redd.it/ub5ccq7fpfd11.png",
        "created_utc": 1533108532,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics 2x2 Table",
        "author": "ferociouswhisperer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93megs/statistics_2x2_table/",
        "text": "https://imgur.com/a/60rExgS\n\nHow do I find B and D in both Boxes?",
        "created_utc": 1533103538,
        "upvote_ratio": ""
    },
    {
        "title": "Recommendations for Book/MOOC/Other on PCA, Factor Analysis, SVD...",
        "author": "BurkeyAcademy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93krzy/recommendations_for_bookmoocother_on_pca_factor/",
        "text": "This summer for fun I did the first four courses of [Harvard EdX Ph525](https://courses.edx.org/courses/course-v1:HarvardX+PH525.1x+2T2017/b60b30a885934cd5971b6fc620a41657/) The part I was really trying to dig into was the math behind Factor Analysis, which they ended up barely scratching the surface of.\n\nSo, I pretty much understand the basics of singular value decomposition, and how that relates to Principal Components. I am trying to help a friend do some custom work involving \"Confirmatory Factor Analysis\", involving restrictions (e.g., restricting the factor loadings for two sets of data to be equal, and testing the LaGrange multipliers/scores to see how much relaxing that restriction would improve the fit. (I am aware of and will be using the lavaan package in R- but need to understand exactly what is going on in the background.) Any suggestions of a good book or other resource that goes through the guts of the mathematics behind all of this?  \n\nI am decent enough at math/stats so that as long as the language they are speaking is in terms of matrix algebra and/or linear models, I'll be able to get it.  But, if there is a book that combines the theory with a little bit of applied context, that would be wonderful as well.  Thank you in advance for your time! ",
        "created_utc": 1533089045,
        "upvote_ratio": ""
    },
    {
        "title": "If 99 boxes contained $100 000, and 1 contained certain death, how many would you open?",
        "author": "chrisdab",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93jcc2/if_99_boxes_contained_100_000_and_1_contained/",
        "text": "I found this in a comment on another reddit post, and I thought there must be some some way to come up with the statistical odds of the correct number of boxes to open.  So I ask you reddit statistics.\n\n&gt;If 99 boxes contained $100 000, and 1 contained certain death, how many would you open?",
        "created_utc": 1533077636,
        "upvote_ratio": ""
    },
    {
        "title": "Tests involving a single group with multiple sessions",
        "author": "rgr1988",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93j2rh/tests_involving_a_single_group_with_multiple/",
        "text": "Hi everyone.\n\nI'm writing up my thesis but I'm finding it difficult to appropriately analyse my data. \n\nBriefly, I ran an experiment with 13 subjects. Each subject performed a task 6 times (6 sessions). I don't want to write a long post describing the task, but basically a game where they had to retrieve items from 10 boxes (5 opaque and 5 transparent). I recorded variables such as first choice, last choice, order of retrieval.\n\nIf I wanted to test if one box type was significantly chosen first over the other type, I would be tempted to perform a paired t test, since I have the same subjects. But the paired t test is only for 2 groups and I have 6, right? How can I go about it? Perform several t tests between each pair of sessions? Or an anova? I did get some advice from a professor saying that I should consider a variable for \"subjects\" as a random factor (but he didn't advise on what exact test to use)?\n\nAny help \\ tips appreciated!",
        "created_utc": 1533075651,
        "upvote_ratio": ""
    },
    {
        "title": "Which multiple lineair regression model is the best fit?",
        "author": "BEStudentThrowaway",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93i5mt/which_multiple_lineair_regression_model_is_the/",
        "text": "I ran 2 multiple lineair regression analyses. In the first model I found a significant result for a construct and in the second model I replaced that contruct for its 3 subscales/dimensions (to check for any differences), and none of the subscales had a significant effect anymore. I'm having trouble deciding which overall model is the best fit (so which analysis is more \"correct\"), as they are really similar otherwise (R2, significance,...). How do I decide this?\n\nScreenshot using the 1 construct : [Model 1](https://imgur.com/a/9E7PYpU)  \\-&gt; analysis 1\n\nScreenshot using the 3 subscales of said contruct: [Model 2](https://imgur.com/a/piMBJg0) \\-&gt; analysis 2\n\nEdit: I suck at explaining so added some more details",
        "created_utc": 1533069228,
        "upvote_ratio": ""
    },
    {
        "title": "Recommendation on best statistics book for learning to analyze survey data",
        "author": "SFDinKC",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93hsux/recommendation_on_best_statistics_book_for/",
        "text": "Title pretty much says it.  I have used various methods to analyze survey data (Likert scales, Net Promotor Score, general quant, yes/no, true/false type questions).  I feel like I need to expand my knowledge and put more formal rigor into the analysis and I am looking for recommendations of the best book or books for gaining that knowledge.  Thanks in advance for any responses.",
        "created_utc": 1533066806,
        "upvote_ratio": ""
    },
    {
        "title": "standard deviation n-1 versus n",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93gqji/standard_deviation_n1_versus_n/",
        "text": "[deleted]",
        "created_utc": 1533059626,
        "upvote_ratio": ""
    },
    {
        "title": "When it comes to mode as a measure of average, what other types of numbers would you typically need to see to feel like you weren’t misinterpreting that figure?",
        "author": "PortlandPerson94",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93gqe9/when_it_comes_to_mode_as_a_measure_of_average/",
        "text": "Most applications of average that I’ve seen typically use mean or median and I’m just trying to understand in what contexts would you use mode and also what else comes with mode in those applications so that you don’t skew any results either inadvertently or otherwise. ",
        "created_utc": 1533059598,
        "upvote_ratio": ""
    },
    {
        "title": "Quality Audit - Help me determine sample size",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93ertk/quality_audit_help_me_determine_sample_size/",
        "text": "[deleted]",
        "created_utc": 1533046042,
        "upvote_ratio": ""
    },
    {
        "title": "Do I need to run a factor analysis?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93e8rp/do_i_need_to_run_a_factor_analysis/",
        "text": "[deleted]",
        "created_utc": 1533041864,
        "upvote_ratio": ""
    },
    {
        "title": "Effect size interpretation",
        "author": "stats-help",
        "url": "https://www.reddit.com/r/AskStatistics/comments/93dk1e/effect_size_interpretation/",
        "text": "What  can I intepretate from a **0.5** cohens d effect size? ",
        "created_utc": 1533035495,
        "upvote_ratio": ""
    },
    {
        "title": "Extremely simple logic/statistics help",
        "author": "DavidAdamsAuthor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/939gjx/extremely_simple_logicstatistics_help/",
        "text": "Hi all,\n\nI am not a statistician but I need the sub's help.\n\nI am engaged in an extremely frustrating discussion with someone who repeatedly insists that I \"consult with a science or statistics sub\" regarding a claim I have made, and accordingly, I would like to do that. I apologize for bringing this here but that was their demand.\n\nI am not looking to have my claims refuted or supported, mainly to have the logic of them checked. Discussion and contributions are welcome, of course, but my primary goal is to meet this person's demands: that I consult with statisticians and have them verify my claim is logically sound.\n\nThe claim I am making is: \"Statistically speaking, poverty cannot be the sole cause of the disproportionate amount of crime committed by Sudanese residents in Australia.\" I am making no claims as to what the *true* cause is, only that it cannot be solely poverty.\n\nMy reasoning is the following:\n\n- The Sudanese community in Victoria is ~6,000 people. [Source](https://en.wikipedia.org/wiki/Sudanese_Australians). For the sake of this argument, I am assuming 100% of them are living in poverty, which is very flawed, but that is the assumption I am working with for the sake of the argument.\n- ~13% (specifically, 12.5%) of Australia lives in poverty. [Source](https://www.acoss.org.au/poverty/)\n- ~13% of the population of the Australian state of Victoria (5.791 million) is 752,830 people. [Source](http://www.abs.gov.au/ausstats/abs@.nsf/MediaRealesesByCatalogue/C508DD213FD43EA7CA258148000C6BBE?OpenDocument) Subtracting the 6,000 poor Sudanese, this leaves us with 746,830 poor non-Sudanese.\n\nConsidering the above, if poverty was the sole factor leading Sudanese residents in Australia to crime, then the levels of crime committed by poor Victorians (746,830 people) versus the Sudanese community (6,000 people) should be proportional (meaning, if the above facts are true, ~0.8% of all crime should be committed by poor Sudanese and ~99.2% of all crime committed by poor non-Sudanese).\n\nHowever, the current [ABS Crime Statistics show that Sudanese residents are responsible for 7.44 per cent of home invasions, 5.65 per cent of car thefts and 13.9 per cent of aggravated robberies in Victoria](http://www.theage.com.au/victoria/apex-fears-spark-concerns-about-racial-profiling-20161122-gsvete.html).\n\nAccordingly, at least when it comes to those three types of crimes, based on my logic above, poverty cannot be explained as the sole cause of those crimes and that even accounting for poverty, there are other factors at play here.\n\nAgain, I'm not looking for what these factors are, and am more than willing to acknowledge poverty as a contributing factor in the scheme of things, but it is not, and cannot be, the single sole factor because the numbers do not add up.\n\nIs my logic sound?",
        "created_utc": 1532995267,
        "upvote_ratio": ""
    },
    {
        "title": "finding the critical value a confidence interval that is not on my table",
        "author": "colatry",
        "url": "https://www.reddit.com/r/AskStatistics/comments/938kv4/finding_the_critical_value_a_confidence_interval/",
        "text": "For a confidence interval of 97.5% which, ^(z)a/2 = 0.0125. The values for ^(t)0.0125 are not listed in my supplied t-distribution table. How would I find the critical values? In this specific question, degrees of freedom are not required. ",
        "created_utc": 1532988568,
        "upvote_ratio": ""
    },
    {
        "title": "How to communicate our need for better data to my boss.",
        "author": "statsnerd99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/936oxq/how_to_communicate_our_need_for_better_data_to_my/",
        "text": "Our data is pretty bad for doing statistical analysis. It's univariate data, mostly. Time series. Things like patient deaths. We have cross sections on very, very little variables (gender, race being two of the handful of corresponding variables). We also have sample bias, for example, as in those patients who do not have deaths are not recorded.... We do not even have census data available to our department.\n\nI'm looking to communicate this to my boss who has no training in statistics whatsoever. Any suggestions on how to communicate our need for covariates? Any other suggestions? Data analyst btw",
        "created_utc": 1532975773,
        "upvote_ratio": ""
    },
    {
        "title": "Question: How to calculate winning score probability?",
        "author": "Hikalu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/934xd7/question_how_to_calculate_winning_score/",
        "text": "A game is played by four players who each have a 40% chance to score a point, with four tries each to score a point. How do I calculate the probability that a winning score is 4, 3, 2, etc?",
        "created_utc": 1532963826,
        "upvote_ratio": ""
    },
    {
        "title": "Interclass Correlation Interpretation (Repeated Measures ANOVA)",
        "author": "Triteleia",
        "url": "https://www.reddit.com/r/AskStatistics/comments/934g3v/interclass_correlation_interpretation_repeated/",
        "text": "For [these results](https://imgur.com/a/8Ozt2y7), does having an error of 407 mean that my variability sigma, or the variability between judges, is no different than error, and that to attribute any variability to subjects (varibility beta) or of the scale (variability alpha) it has to be a lot more than 407?\n\nAnyone know if an ICC of 0.68 for 227 subjects and 3 judges is particularly strong?\n\nFor more context, this a repeated measures on plants using a numerical, but qualitative scale with a confidence interval of 95% the upper limit is 0.79 and lower limit is 0.49",
        "created_utc": 1532960371,
        "upvote_ratio": ""
    },
    {
        "title": "STATA - I need help with internal validation of a predictive model.",
        "author": "tlbtc",
        "url": "https://www.reddit.com/r/AskStatistics/comments/930dwe/stata_i_need_help_with_internal_validation_of_a/",
        "text": "I have a multivariable logistic model, and I'll be using the beta coefficients for significant variables as my predictors.\n\nI would like to preferably split the data randomly into K folds, use k-1 for model building and then test the model on the final manifold, and then repeat K times.\nIn case this isn't possible, I would like to bootstrap my model.\n\nIf you'd recommend another option, I'm all ears!\nBut my real question is: how do I do this on stata?",
        "created_utc": 1532918694,
        "upvote_ratio": ""
    },
    {
        "title": "Help With Chi-Square Association Problem",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/930bdr/help_with_chisquare_association_problem/",
        "text": "[deleted]",
        "created_utc": 1532918055,
        "upvote_ratio": ""
    },
    {
        "title": "Mutual funds and stocks",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92ykgf/mutual_funds_and_stocks/",
        "text": "[deleted]",
        "created_utc": 1532902668,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical approach for thesis question",
        "author": "KOERNDOG",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92yjk5/statistical_approach_for_thesis_question/",
        "text": "Hi all, long post incoming:\n\n**Experiment Design**\n\nMy thesis is concerned with how freshwater mussels influence sediment properties (e.g. D50 median grain size) and dynamics (e.g. amount of scour) in streams. For my experiment, we installed 36 enclosures into a reach of a stream, and monitored the amount of scour (mm) within each enclosure, and looked at the size distribution of sediment particles at the end of the experiment in each enclosure (D50 particle size mm). We also sampled the sediment in each enclosure beforehand, so I have an initial size distribution of the sediment at the beginning of the experiment, and monitored the stream velocity at the enclosure locations throughout the experiment. As far as the experimental design goes, we had a factorial design consistent of two different abundances, and 3 species treatments for enclosures with mussels; sediment only control enclosures; and sham mussel controls (empty mussel shells) with two different abundances. I'm attaching a link to a figure to better represent the experimental design. \n(https://imgur.com/a/BgrVfbn)\n\n**Question**\n\nMy main questions with my data are (1) do mussels influence the amount of scour and median particle size within the enclosures and further (2) how does abundance and species composition influence scour and median particle size within the enclosures? \n\nMy initial approach was to run 2 one-way ANOVA's, one including all treatment groups and controls (n=9) as my factors and amount of scour (mm) as the dependent variable, to see initially if any groups significantly influenced the amount of scour in the enclosures; and then to run another one-way ANOVA, again using all treatment groups and controls as factors, but this time using median D50 particle size as my dependent variable, to see if any groups influenced the median particle size in the enclosure at the end of the experiment. To further investigate how abundance and species treatment influenced scour and D50, I then ran 2 two-way ANOVA's just using enclosures with mussels, using diversity and abundance as fixed factors. \n\nMy adviser suggested I use an analysis that also incorporates how the average velocity at each enclosure and the initial D50 median grain size within each enclosure influence the amount of scour and D50 particle sizes at the end of the experiment, and that is where I am lost. She suggested looking into using linear mixed models or logistic regression, and I have looked into them and I am not sure if they are even appropriate. \n\nAny help on how to approach the analysis of my data in regards to my research questions would be incredibly appreciated!!!",
        "created_utc": 1532902477,
        "upvote_ratio": ""
    },
    {
        "title": "Mutual fund total net asset",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92ye2m/mutual_fund_total_net_asset/",
        "text": "[deleted]",
        "created_utc": 1532901213,
        "upvote_ratio": ""
    },
    {
        "title": "Evaluated a simple trained model, simple parameter search vs nested cross-validation",
        "author": "ConfusionStatistique",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92xl2b/evaluated_a_simple_trained_model_simple_parameter/",
        "text": "Hello Redditors,\n\nI am in a bit of a confusion because of a recent debate with a professor of mine. As part of an experiment, I set up a cross-validation in order to evaluate a very simple model trained on only 2 variables (thresholds). My plan was simply to set up a 10 fold cross validation, and on the training data find an estimate of the optimal value using some heuristic search (probably coordinate descent in order to save some compute time vs grid search) and then evaluate it on the test data. Rinse and repeat for the 10 folds, then look at the distribution of the 10 results to see how we are faring. \n\nThe professor in question however is convinced that I need to do a nested cross-validation in order to first determine the  optimal parameter 1 (in the inner CV) and then the optimal parameter 2 (in the outer CV). While this makes sense for something like hyperparameter search in a support vector machine, I have a hard time understanding why it is necessary in a simple 2 variable model where both variables are independent and we could simply \"train\" all combinations on a training set and evaluate on the corresponding test set.\n\nI am hoping that someone more statistically educated can shed some light onto this for me!",
        "created_utc": 1532894712,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating a P-value when only given mean, standard deviation and sample size.",
        "author": "mannekes",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92w4w6/calculating_a_pvalue_when_only_given_mean/",
        "text": "I would like to know the process of finding a P-value when you`re only given mean, SD and sample size when you compare more than one group. Group 1: n = 60 , mean = 55.6 , SD = 5.20 ; Group 2: n = 65 , mean = 56.50 , SD = 6,16 group 3: n = 44 , mean = 54.99 , SD = 5,86\n\nI am supposed to calculate a P-value of 0.402 with these values, but I can't seem to find a solution. How can you calculate this when you`re dealing with more than 2 groups?",
        "created_utc": 1532883171,
        "upvote_ratio": ""
    },
    {
        "title": "Which statistics method to use.",
        "author": "accip",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92vszt/which_statistics_method_to_use/",
        "text": "Hi all,\n\nI've got my thesis data collected and I'm trying to figure out what statistical analysis to choose.\n\nI have conducted a between-groups design study, one indpedendent variable with 3 levels, which is condition and one dependent variable (credibility). However, I want to know if sex and education (categorical variables), and age (continuous variable) are significantly different between the 3 levels of the condition. I was wondering what statistical analysis I should use for these.\n\nMy initial thought was that I should use chi-square for the categorical variables, but I am unsure with the IV having 3 levels. For age, I was considering using a one-way manova.\n\nAll help is appreciated and I can try to further explain if I don't make sense!",
        "created_utc": 1532880403,
        "upvote_ratio": ""
    },
    {
        "title": "Question about post-test post-test analysis",
        "author": "HopefullyMPH",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92tp9t/question_about_posttest_posttest_analysis/",
        "text": "I am a grad student with basic statistical knowledge working on a study a bit over my head which conducted a KAP survey on three villages directly following an intervention (1 village received a single intervention, one village received two interventions and the third is a control village where no interventions were conducted.  The survey collected 23 knowledge attitude and practice variables and then a second survey was conducted two years later visiting the same individuals (matched with a caseid to the prior survey). This survey repeated 6 categorical questions on the KAP survey as well as collecting demographic data for the individuals.\n\nThere are two hypotheses. The first is that the village with two interventions would have the least fading of knowledge over the two years. The dependent measure is a count variable of the number of techniques specified (0-9) and the independent is a categorical variable of the three villages. \n\nThe second hypothesis is that the groups receiving interventions would have better knowledge attitudes and practices than the non-intervention village. The dependent variable would be a binary or a count depending on the KAP question and the independent would again be village.\n\nI am thinking that for the second hypothesis a simple regression could be done on just the first survey without using the second.\n\nI am not sure how to approach the first hypothesis. I have tried setting the data as survey in Stata and running analyses (xtreg), but I am having co-linearity issues and am not sure how to interpret the coefficients.\n\nThank you for reading, any guidance would be appreciated greatly.",
        "created_utc": 1532857406,
        "upvote_ratio": ""
    },
    {
        "title": "Stata lagged variable",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92tp5c/stata_lagged_variable/",
        "text": "[deleted]",
        "created_utc": 1532857349,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate population covariance given population mean and population variance?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92rjrd/how_to_calculate_population_covariance_given/",
        "text": "[deleted]",
        "created_utc": 1532830853,
        "upvote_ratio": ""
    },
    {
        "title": "Should you use standardised or non-standardised coefficients for dummy variables in a multiple linear regression analysis?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92raku/should_you_use_standardised_or_nonstandardised/",
        "text": "[deleted]",
        "created_utc": 1532828307,
        "upvote_ratio": ""
    },
    {
        "title": "Question about what stats to use",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92r50r/question_about_what_stats_to_use/",
        "text": "[deleted]",
        "created_utc": 1532826788,
        "upvote_ratio": ""
    },
    {
        "title": "Question about right stats tests in these scenarios",
        "author": "heyineedhelphaha",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92qaut/question_about_right_stats_tests_in_these/",
        "text": "I have 4 groups of patients all dealing with the same problem but differ in the manner in which they got evaluated for this one specific condition. Data is not actual data in study. It is made up but representative. \n\n***Question 1*** \n\nThere are 2 types of providers (Provider A, Provider B) that can provide an appropriate evaluation for this condition. \n\nOne group of patients saw provider type A only. \n\nAnother group saw provider type B only. \n\nThe third group saw both types of providers (A And B) \n\nThe fourth group saw neither type of provider. \n\nThe total number of patients was 400 \n\n73% (292/400)  saw provider type A \n\n14% (56/400) saw provider type B\n\n3% (12/400) saw both provider type A and B\n\n10% (40/400) saw neither type of provider\n\n*I'm trying to compare the four groups. Is a Chi squared test appropriate? And how would I set up the Chi Squared test between the four groups in excel in terms of CHISQ.TEST (Actual Range,Expected Range)? Not really sure what the actual/expected ranges are in this scenario.* \n\n***Question 2***\n\nI have a second question related to the above scenario I laid out. \n\nI am also comparing the times it took for the above patient groups to get evaluated. \n\nPatients who saw Provider type A took an average of 10 days for their work up to be completed.\n\nPatients who saw Provider type B took an average of 15 days for their work up to be completed.\n\nThose patients that saw both types of Providers took an average of 13 days to get worked up.\n\nThose patients that saw neither type of Provider took 8 days to get worked up.\n\n*Is an ANOVA test appropriate for comparing between the 4 groups since I'm dealing with 4 means? I tried setting this up in excel but got some weird values.* ",
        "created_utc": 1532819100,
        "upvote_ratio": ""
    },
    {
        "title": "Unable to understand the following statement about Normal distribution.",
        "author": "RealMatchesMalonee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92ofq9/unable_to_understand_the_following_statement/",
        "text": "I was reading an article that said that \" if ø is a vector that follows normal distribution,  and if ||ø1|| &gt; ||ø2||, then p(ø1) &lt; p(ø2)\". The norm used here is the Euclidean Norm.\n\nHow does the above statement follow?",
        "created_utc": 1532803749,
        "upvote_ratio": ""
    },
    {
        "title": "Meaning of the · symbol",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92lehh/meaning_of_the_symbol/",
        "text": "[deleted]",
        "created_utc": 1532776476,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing my collected data sample to the normative data from a standardized assessment",
        "author": "jbcanuck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92iqaw/comparing_my_collected_data_sample_to_the/",
        "text": "Hi there Reddit! I am trying to make sure I understand the statistics on a paper I am working on. I have collected data (n=24), and would like to compare it to normative data listed in an assessment I use. The sample sizes are different, and the variance is different. I don't have the raw data for the normative data I want to compare to, just sample size(n=200), and scaled scores (mean=10, SD =3). My research question is whether my collected data fits into the normative sample from the assessment.  My hypothesis is that you cannot use the normative data associated with this assessment when assessing the population I work with. So null hypothesis is that mean of the assessment's normative data is equal to my mean, and the alternative hypothesis is that the mean of the assessment's normative data is statistically significantly different from the mean of my collected data.\n\nI have initially used the welch t test, but I am not sure if this is right, though it seemed a better option than the student's, as there are unequal sample sizes and variances. Is there a different test I should be using, or does this make sense? Should I be using a single t test or unpaired t test? Do I need to delve into the world of Baysian Stats? I have used a combination of excel, r and graph pad thus far. I haven't done stats since university ( I am a clinician(speech therapist)), and am feeling a bit overwhelmed, so any help would be appreciated.\n\nThanks",
        "created_utc": 1532745186,
        "upvote_ratio": ""
    },
    {
        "title": "Say I have three six sided dice, and one 16-sided die marked 3 to 18; is the probability of getting the same value from each set equal?",
        "author": "SuchACommonBird",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92heby/say_i_have_three_six_sided_dice_and_one_16sided/",
        "text": "",
        "created_utc": 1532733365,
        "upvote_ratio": ""
    },
    {
        "title": "ANOVAS; Having a hard time picking out when to use which sort of ANOVA.",
        "author": "PaulGeyser",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92ep0r/anovas_having_a_hard_time_picking_out_when_to_use/",
        "text": "Our final is going to consist of all word problems and half of them will be ANOVAs. Looking at the review practise questions, I'm having trouble picking out which sort of ANOVA to use for which word problems.\n\nIn our course we have covered:\n\n-one-way within-subjects ANOVA\n\n-one-way between-subjects ANOVA\n\n-factorial between subjects ANOVA\n\n-factorial within subjects ANOVA\n\n-mixed between-within-(treatment)-subjects ANOVA\n\n-mixed between-within-(treatment x treatment)-subjects ANOVA\n\n-one-way ANCOVA\n\nCan anyone help (or point me in the direction of resources that can help) me make a list of key words / conditions to look for that would identify which ANOVA to use? I'd really appreciate it!\n\nFor other parts of the course I made a chart in our formula book with things like \"if it says _____ then it is a ______\" \n\nor \"if it says _____ it could be one of A or B. If it says ____ then it's A, if it says ______ then it's B\"",
        "created_utc": 1532713693,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a best practice for normalizing different rates",
        "author": "ValueBasedPugs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92dv7n/is_there_a_best_practice_for_normalizing/",
        "text": "**Goal in short:** I need to be able to directly compare unrelated rates by normalizing them in some way.\n\n**Description:** I am trying to compare apples to oranges. This means that I have 10 clinics which are each responsible for 10 different metrics related to membership. These metrics range from hospital readmissions to the concurrent use of certain drugs. Let's call them metrics 1-10. These metrics will have very different average rates, different denominators, and ultimately be very difficult to compare. But I want to compare them across clinics and then compare them across measures. So, I would like to be able to say \"Clinic #1 is the best clinic in Measure #3 but only 2nd for overall quality.\"\n\nObviously, I need to find a way to normalize these measures so that they can be compared directly like this.\n\nAny suggestions?\n\nI'm even open to suggestions around whether I'm even asking the right questions or framing this issue correctly.",
        "created_utc": 1532707997,
        "upvote_ratio": ""
    },
    {
        "title": "What is the intuition behind multiplying a probability and integer together?",
        "author": "billericayBatman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92dmq3/what_is_the_intuition_behind_multiplying_a/",
        "text": "Hi All,\n\nCheck out this data for the returns on a group of stocks I have:  \n\n&amp;nbsp;\n\n\n|          | Average Return | Frequency |\n|----------|----------------|-----------|\n| Up day   | 0.675916%      | 52.39%    |\n| Down day | -0.692779%     | 46.91%    |  \n\n&amp;nbsp;\n\nSo what this shows is that if these stocks have an up day, it will give a return of 0.675916% (a gain).  Furthermore, an up day happens 52.39% of the time\n\nSimilarly, if these stocks have a down day, it will give a return of -0.692779% (a loss).  And a down day happens 46.91% of the time.\n\nWhat my friend is doing is multiplying the Return and Frequency together, and he believes this gives the average return of an up day (or a down day).\n\nIntuitively this does not make sense to me.\n\nSurely if 52.39% of the time these stocks give an up day, then on those days there will be an average return of 0.675916% ?  \n\nBut my colleague is convinced I am wrong.  He says to find the average return on an up day you need to multiply Average Return and Frequency.  But intuitively this doesnt make sense to me.\n\nWould anybody be able to clarify this for us please?\n\nThanks very much!",
        "created_utc": 1532706333,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone ELI5 these hieroglyphics?",
        "author": "nn30",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92dkcm/can_someone_eli5_these_hieroglyphics/",
        "text": "https://imgur.com/gallery/Uc1WES0",
        "created_utc": 1532705859,
        "upvote_ratio": ""
    },
    {
        "title": "How would I write a trivariate cumulative distribution function where each variable has equal weight?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92cmv3/how_would_i_write_a_trivariate_cumulative/",
        "text": "[deleted]",
        "created_utc": 1532699106,
        "upvote_ratio": ""
    },
    {
        "title": "Does it make sense to apply both t-test and Mann–Whitney U-test for different variables?",
        "author": "UKBua",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92c1a5/does_it_make_sense_to_apply_both_ttest_and/",
        "text": "Hello everyone,\n\nI would like to know if it is sensible to use t-test and Mann–Whitney U-test on my inferential statistics to compare each pair of two variables depending whether they are normally distributed or not? I have acknowledged that if I use histogram to check normally distribution I can see if each variables are normally distributed or right/left skewed (I am using panel data) However, there are some variables I have now and some of them are from \"very normal to relatively right skewed\" I plan to use t-test for those with normal distribution and Mann–Whitney U-test on those with non-normal distribution, I have checked many articles and they did not mention about \"reason\" of their selection but purely showed a table of their mean with either t-test or Mann–Whitney U-test. I would like to ask if it is possible for me to do as what I mentioned confidentially? I am writing my master thesis and I need help, please help me. Thank you very much!",
        "created_utc": 1532694123,
        "upvote_ratio": ""
    },
    {
        "title": "Mixed model random effects and repeated measures",
        "author": "insufferablemoron",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92auc5/mixed_model_random_effects_and_repeated_measures/",
        "text": "What’s up my dudes. It’s that time again. \n\nI’ve got a data set of around 2800 observations of about 2100 individuals. So about a quarter of individuals have repeated measurements.\n\nI want to run a mixed model using individual ID as a random effect to account for pseudo-replication.\n\nIs this possible or is it a bad idea considering most individuals are only represented by a single observation?\n\nI’m assuming the alternative is to run a multiple regression but remove repeat observations of individuals? \n\nCheers",
        "created_utc": 1532681133,
        "upvote_ratio": ""
    },
    {
        "title": "Testing difference in proportions / counts (binary and/or multi level factors) with multiple observations (non-independence)",
        "author": "craoloro",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92aels/testing_difference_in_proportions_counts_binary/",
        "text": "Say, I have multiple observations per person over a span of a number of years. For example, the number of carnival entries for some persons over five years. I then have various variables that describe these persons where some of these variables are binary or multi-level factors. As for instance, whether the person was wearing shorts or not, or what colour hair a person had at that select carnival entry. \n\nNow, given my desired unit of analysis is carnival entry rather than the person, I would like to compare carnival entries by gender. For instance, how do entries differ by gender whether persons wore pants or not or by their hair colour (brown, blonde,  black, red, etc. ). \n\nObviously I cannot use classical tests (e.g., fisher’s, chi-square, etc. ) given the violation of independence (multiple entries per person). I would still like to provide p values in my frequency table for the comparison by gender. Therefore I am considering applying mixed effect logistic regressions (binary or multi nominal where appropriate as in the case of hair colour [or subset per hair colour and continue with binary]) to test for a difference in the variables and acquire desired p-values. I am writing to see whether this thought process makes sense. For instance, modelling whether shorts (yes/no) is a function of gender considering a random intercept for the persons (cluster/block as per preferred terminology)  Yet, since there is a time component, perhaps using a random slope, where the persons vary per time; as often seen (time|subject)when using various R packages, would be appropriate? \n\nIf another redditor here could comment on my thought process it would be appreciated, or alternatively to offer some suggestions. \n\nThis is obviously an alias for my project, but the concepts are identical. I am aware of the controversies behind p-values but unfortunately, sometimes we gotta comply with tradition to get published. \n\nThanks in advance! ",
        "created_utc": 1532675773,
        "upvote_ratio": ""
    },
    {
        "title": "[Inter-Rater Reliability] Intraclass correlation/Kappa causing problems when most scores are 0",
        "author": "Chocobuny",
        "url": "https://www.reddit.com/r/AskStatistics/comments/92a0fy/interrater_reliability_intraclass/",
        "text": "Hey,\n\nI have some data where two different raters are rating 12 of the same cases on a scale consisting of 10 items. The items range from 0 to 2, but the majority of cases are a 0. For example, it would not be unusual for a single case to be rated with nine \"zeroes\" and a single \"two\".  \n\nThis is causing issues when I run inter-rater reliability analyses, I've used both Kappa and ICC and get the same problems, one item I have one rater who rated all ten items as \"zero\" whereas the other person rater nine \"zero\" and one item as \"one\". This has caused both Kappa and ICC to say there is zero agreement. However, there is agreement because on the other nine cases we agreed and rated it as zero! I'm wondering if there is a way to deal with this type of data for inter-rater reliability analyses?",
        "created_utc": 1532671464,
        "upvote_ratio": ""
    },
    {
        "title": "kidnapping stats?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/928cwb/kidnapping_stats/",
        "text": "[deleted]",
        "created_utc": 1532655921,
        "upvote_ratio": ""
    }
]