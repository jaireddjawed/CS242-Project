[
    {
        "title": "Adjusting the t statistic for rolling returns in a time series regression",
        "author": "idledalian",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bboqr/adjusting_the_t_statistic_for_rolling_returns_in/",
        "text": "Hello! I am running a regression that uses rolling two year returns as the independent variable.  Now using just those parameters, I get a t stat for that coefficient of \\~16 with 200 observations, but all of those 200 observations share data points, in that the first observation shares 23 (monthly) returns with the second observation.  I believe that I need to adjust the t-stat for that, though I am not sure how I would do that aside from dividing the number of observations by 24 to calculate the t statistic.\n\n​\n\nAny suggestions or help? Thanks in advance!",
        "created_utc": 1535565982,
        "upvote_ratio": ""
    },
    {
        "title": "Correlating two variables within two groups with unequal sample sizes.",
        "author": "jDawgLite",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bazfj/correlating_two_variables_within_two_groups_with/",
        "text": "Hi all,\n\nI'm examining sex differences in the kinetics and psychoactive effects of a drug, and I'm struggling with how to approach the analysis. I've demonstrated that the concentrations of drug in blood are higher in males, yet the self-reported \"high\" associated with the drug is not different between males and females. I would like to correlate drug concentration to self-reported \"high\", but the problem is the numbers of males and females are not equal. The result is that the Pearson correlation coefficient is significant in males but not females, yet the r is actually higher in females. Is there some way of handling this situation?\n\nThanks in advance!",
        "created_utc": 1535561287,
        "upvote_ratio": ""
    },
    {
        "title": "What is wrong with my understanding here? (Maximum Likelihood)",
        "author": "buddho1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b9da1/what_is_wrong_with_my_understanding_here_maximum/",
        "text": "Hi everyone, \n\nI'm [learning about logistic regression](http://www.sfs.uni-tuebingen.de/~ddekok/dl4nlp/logistic-regression.pdf) now and I keep reading about maximizing the likelihood function to make the observed data as likely as possible. \n\nHowever, I'm having trouble understanding how there can be a single set of parameters that can maximize the likelihood of the data. [Here](https://i.imgur.com/R7N62ON.png) is a simple MS paint example illustrating what I mean (the data are symmetrical around mean 0). \n\nIt would seem that if we simply choose increasing values for \"a\", the likelihood of the observed data (the green points) goes up! And because the function will never actually reach the points 1 or 0, you can keep increasing the value of \"a\" without upper bound. \n\nMy current understanding is that we are trying to fit the curve \"as close as possible\" to the green points. But evidently, there is no best value for \"a\" in this case. Because the literature always reiterates that there is a unique optimum for the parameters, there must be something wrong with my understanding of how maximum likelihoods work.\n\nI'd appreciate an intuitive explanation of where I'm going wrong. How am I supposed to think about this concept?\n\nThanks!\n\n",
        "created_utc": 1535549968,
        "upvote_ratio": ""
    },
    {
        "title": "Synergistic or additive?",
        "author": "Cassie_Bio",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b7tyr/synergistic_or_additive/",
        "text": "I need some help analysing some biological data, \n\n&amp;#x200B;\n\nI give cells siRNA to knockdown a gene, I knockdown two genes, and both in combination (A, B, A+B), then I measure a response, \n\n&amp;#x200B;\n\nI want to know if the effect of using both treatments is additive, or synergistic, is there a statistical test for this?\n\n&amp;#x200B;\n\nI thought a two way anova might be the way to go but am not sure any more (mostly using R for my stats but very beginner!) \n\n&amp;#x200B;\n\nmy data looks something like this...\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n|Treatment|Response|N (biological repeat number)|\n|:-|:-|:-|\n|Control|10|1|\n|Control|11|2|\n|Control|12|3|\n|Control|9|4|\n|A|14|1|\n|A|15|2|\n|A|14|3|\n|A|15|4|\n|B|14|1|\n|B|13|2|\n|B|15|3|\n|B|15|4|\n|A + B|30|1|\n|A + B|26|2|\n|A + B|24|3|\n|A + B|17|4|\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThank you!",
        "created_utc": 1535536015,
        "upvote_ratio": ""
    },
    {
        "title": "A football probabilistic model",
        "author": "yourmom100",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b6srb/a_football_probabilistic_model/",
        "text": "As part of a school assignment, I'm creating a probabilistic model to predict the winner of the 2018 FIFA World Cup starting from the semi-finals onwards. I found some methods online using a Poisson distribution to do this. I have gone ahead with that and have collected data on average goals scored or conceded per game and have created a value for the offensive and defensive strength of each team. \nThen Poisson distribution is used: Team 2's expected goals is = Defense strength of team 1 x offense strength of team 2 x average goals per game. \nThis number is used as rate of success. \nMy question is regarding how I can integrate tactical data into this. I have taken the different formations used historically, and the number of goals scored or conceded in each as well as the ranking/rating of opponents it was used against. How can I combine this data with the attack/defense strength data to give an overall prediction?  Thanks in advance!",
        "created_utc": 1535524050,
        "upvote_ratio": ""
    },
    {
        "title": "Can you think of a study that would use ordinal/ranked variables?",
        "author": "FruitPopsicle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b48wb/can_you_think_of_a_study_that_would_use/",
        "text": "I learned about them in class today. I was told that assigning pain to a scale from 1-10 makes it a measurement variable. So now I'm trying to find a study with variables that can be ordered, but don't have numerical values assigned to them. Are there studies that would use labels such as \"biggest to smallest,\" \"first to last,\" etc?",
        "created_utc": 1535501085,
        "upvote_ratio": ""
    },
    {
        "title": "Can't seem to make sense of results from mediation analysis",
        "author": "Pocshi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b3xoe/cant_seem_to_make_sense_of_results_from_mediation/",
        "text": "Hi redditors,\n\nCurrently running through some mediation analyses using R with package 'mediation'. So I am running into some results that I am struggling to explain. We love to hear what everyone else thinks.\n\n1)  Average causal mediation effect and average direct effects are significant but total effect is not statistically significant. Does this mean that the independent variable is affecting the mediator in the opposite direction to it's direct affect. But this doesn't seem to make any sense because when I run correlations between IV and mediator and DV, it seems to imply that worse IV = Increase mediator = worse DV, so shouldn't the total effect be additive and not counteracting one another?\n\n&amp;#x200B;\n\n2) When I add more mediators into the model, the direct effect appears to decrease but the average mediation effect seems to improve slightly. I'm not too sure what this implies.\n\n&amp;#x200B;\n\nThanks guys!",
        "created_utc": 1535498627,
        "upvote_ratio": ""
    },
    {
        "title": "Construction Forecast Help",
        "author": "ForecastThrowaway",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b3he7/construction_forecast_help/",
        "text": "Hey guys I recently got assigned a pretty big task and would appreciate some help or input.  I currently have an idea on how I want to approach the problem and im asking for someone with more experience in the field to push me in the right direction.\n\nSome background info I have a degree in pure mathematics we only took 1 stasticis class and maybe 1 numerical analyses class but ive been out of college 6 years so god knows how much ive retained.  My job is in AR but were growing and my job is starting to expand in resoponsibility and I really want to do an impressive job on this project.\n\nThe company is a construction company, we do many types of jobs, high end residential, commercial, assisted living facility, country clubs, condos, jobs from 2-100 milllion.  Theyve got some big projects in the works our current gross monthly sales is about 6 mil, theyre expecting to hit between 13-20 million a month next year.  My job is to provide as accurate of a forecast as possible.\n\nI have a lot of data, I have all our jobs dating back to 2015, their budgets, what we billed and got paid etc.  My plan is to make a large data set of all the jobs, may take me a while but I can do it.  My plan is to categorize the jobs on similar types, IE some jobs are renovations and others are ground up, some are hotels some are condos, the billing structure is radically different from one type to the next.  \n\nMy plan so far is to plot all like jobs in monthly intervals vs gross income for that job that month.  I then want to average out all these similar jobs to try and get some sort of standard plot for jobs of those types.  I would then like to apply that plot to future jobs to get a decent forecast as far as hross monthly revenue from those future jobs.\n\nMy biggest road block right now is my data set isnt huge, i got maybe 35 jobs total so some catagories may only have 3 or 4 data sets.  To further complicate the matters jobs have different lengths on order of 12 to 36 months mostly.  Obviously jobs on a shorter time scale bill more agressively, given im using a static interval of month or 30 days Im wondering if its possible to try and normalize that vs a similar type of job but may be on a longer time schedule.  I doubt its possible but hey I figured id ask.\n\nSorry for the wall of text, if anyone has any experience or could point me to some literature it would be much appreciated! I really want to do a good job on this",
        "created_utc": 1535495276,
        "upvote_ratio": ""
    },
    {
        "title": "Does this analysis strategy make sense?",
        "author": "TempUN18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b18ls/does_this_analysis_strategy_make_sense/",
        "text": " \n\nHey there, I've been trying to figure out the best way to perform some analyses, and I'm kind of uncertain; any input would be helpful!\n\nI'm analyzing a large data set, examining various sites of damage in tissue in a large-ish number of patients. I'm trying to see if there are certain locations of damage that predict a particular symptom, so I am running binary logistic regressions with the binary dependent variable \"symptom present/symptom absent\", as well as a damage site (scale of severity) and a couple of covariates to reduce confounding. The a priori hypothesis is pretty clear: damage (or worse damage) in a particular area might predict developing the symptom (so, using the beta from the regression to calculate the increased odds ratio for the symptom as the severity increases in region X, etc).\n\nThe major issues here are obviously which sites to include and the risk of false positives from multiple comparisons. I want to avoid falsely claiming significance from Type I error. To avoid this, I'll correct for the number of significant findings using a Benjamini Hochberg FDR correction.\n\nEverything seems to go fine, but the issue is that a handful of the sites show a negative beta that's statistically significant; basically, those with the symptom had LESS damage present in that one site. This isn't really an interpretable result; it's not that it would go against a hypothesis, but that it functionally couldn't be possible for \"less\" damage to cause the symptom, hence the a priori hypothesis above. These 'negative beta' sites could be due to some factors between the groups that aren't quite controlled for in the covariates, or more likely just random Type I errors resulting from multiple comparisons, but there's nothing I can really glean from them.\n\nWhat I had been doing post-analysis (for the FDR correction etc) is to only consider those significant findings with a positive beta, and then correct for that number of findings. This would be analogous to doing one-tailed tests with an a priori hypothesis (in that you'd have to ignore findings that were significant in the opposite direction than expected), except I wouldn't be reducing the alpha p value. I'd obviously report this in the methods of the paper, in case it's relevant or there's something sketchy about it that I'm missing. But I want to make sure this is actually valid to do, so I thought I'd ask around to make sure.\n\n(So, basic gist: running a series of regressions and then correcting for multiple corrections, have a clear a priori hypothesis and anything in the opposite direction isn't functionally interpretable, so is it okay (as long as I report it) to ignore these negative-beta variables or consider them non-significant, as one would with a one-tailed test?)\n\nAnyway, just trying to make sure I'm not unintentionally doing something sketchy. Please let me know if you have any comments. Thanks a lot in advance!",
        "created_utc": 1535479565,
        "upvote_ratio": ""
    },
    {
        "title": "How to conduct a moderated multiple regression with two (or more) moderators?",
        "author": "BorisMalden",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b0s9v/how_to_conduct_a_moderated_multiple_regression/",
        "text": "I've been trying for a while to understand how to perform this type of analysis, but I can't seem to find any literature or even forum posts about it, so any help or guidance anybody can offer would be greatly appreciated.\n\nSuppose I have five continuous predictors (X1-X5) and one continuous output (Y). I'd like to test whether the effects of X1-X5 on Y are moderated by two continuous variables (M1 and M2). Specifically, there is prior evidence to suggest that the effects of X1-X5 on Y will be stronger at higher levels of M1 and M2.\n\nWhat is the correct way to analyse the data to test these predictions? Some options I've considered are:\n\n * A single long regression equation including all of the interaction terms (i.e. Y = β1X1 + β2X2 + β3X3 + β4X4 + β5X5 + β6M1 + β7M2 + β8X1M1 + β9X1M2 + β10X2M1 + β11X2M2 + β12X3M1 + β13X3M2 + β14X4M1 + β15X4M2 + β16X5M1 + β17X5M2 + C + e)\n\n * Conducting separate multiple regression analyses for each moderator (i.e. Y = β1X1 + β2X2 + β3X3 + β4X4 + β5X5 + β6M1 + β7X1M1 + β8X2M1 + β9X3M1 + β10X4M1 + β11X5M1 + C + e **AND** Y = β1X1 + β2X2 + β3X3 + β4X4 + β5X5 + β6M2 + β7X1M2 + β8X2M2 + β9X3M2 + β10X4M2 + β11X5M2)\n\n * Converting the two moderators into a single polychotomous nominal moderator (e.g. splitting each moderator into 'High' and 'Low' and using these to create four groups: Low M1 Low M2; Low M1 High M2; High M1 Low M2; High M1 High M2).\n\nIs anybody able to offer any guidance on which of these, if any, would be the correct approach? Any references to textbooks or webpages with more information would also be very welcome.",
        "created_utc": 1535476436,
        "upvote_ratio": ""
    },
    {
        "title": "Help with normalization of data",
        "author": "badfish8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9b0lof/help_with_normalization_of_data/",
        "text": "I should preface this by saying I know very little about stats beyond the basics. I am looking for help with normalizing some data for a research project. I am trying to report on number of a certain kind of lawsuit per state and want to normalize by state population. The issue is that the data is over a 30 year period of time over which state populations have of course changed. Can anyone suggest a good way to do this?",
        "created_utc": 1535475204,
        "upvote_ratio": ""
    },
    {
        "title": "Non-parametric, one sample, single tailed independent t-test alternative. Mann Whitney? Wilcoxon matched- pairs? ...other?",
        "author": "eyeballjunk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ayxz6/nonparametric_one_sample_single_tailed/",
        "text": "I have 45 measurements from a population, and I want to test if the mean value is significantly different from a value of zero. The data has many 0 values, so the assumption of normality is violated.  I'm unsure whether wilcoxon matched pairs or a mann whitney is appropriate.  \n\n**Bonus points for an example in python!**\n\nThe test value of zero allows us to test the hypothesis that the presence of non-zero values within that condition are due to noise noise.\n\nThanks, guys!\n\nUpdate:  \n- The data represent counts and therefore contain no negative values. ",
        "created_utc": 1535463137,
        "upvote_ratio": ""
    },
    {
        "title": "Which test should I perform to see the relation between the two variables(nominal-ordinal)?",
        "author": "Mohamedsharif",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aybyy/which_test_should_i_perform_to_see_the_relation/",
        "text": "Hi, I am working on a project on SPSS. The table contains 30 columns and is about techniques to improve the services provided. Now, I want to test the relation between two variables one of them is a binary nominal (Yes/NO) The other is ordinal( 1-2-3-4-5).\n\n**I tested the correlation using the eta test, but I am not sure if this is the right test.**\n\nI think of using the Chi-square test or the logistic regression or even Mann whitney test.\n\nSo What is the most appropriate test to perform?",
        "created_utc": 1535458010,
        "upvote_ratio": ""
    },
    {
        "title": "Model fitting for several dependent variables",
        "author": "GinDingle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9awobf/model_fitting_for_several_dependent_variables/",
        "text": "Hi,\n\nI'm working on analyzing some gene expression data and have what is probably a really silly question which I'm not able to find an answer to (maybe because it's so obvious...).\n\nI have five continuous dependent variables measured over three treatment groups, with three repeated measures per subject. I have several missing values, making a RM-MANOVA unsuitable unless I want to exclude a bunch of cases. I'm in the process of fitting separate linear models for each DV instead.\n\nHere's the potentially silly part. Is it best that I keep my covariance structures for the models the same for each DV, or does it not matter as long as the model setup is otherwise the same? Different DVs fit better with different covariance structures, but I'm not sure whether it's common to allow each DV a different structure or pick the one that has the best balance between all of the DVs.\n\nDoes anyone have any advice on how to proceed with fitting these models? Or if there is another way which might work better?   ",
        "created_utc": 1535439061,
        "upvote_ratio": ""
    },
    {
        "title": "Request: Helping my daughter with her science fair project",
        "author": "mrvalor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9avofw/request_helping_my_daughter_with_her_science_fair/",
        "text": "Howdy!\n\nMy daughter is in 9th grade and is looking for some help coming up with science fair project ideas, which is a pretty big part of her year in my city/state.\n\nShe didn't have to do one in 7th or 8th grade, and before that she always did one around her mother's field (geology). I'm in information sciences, but I'm far from a statistician. I do documentation, project management, other light things. I suggested a very simple \"simulation\", something we could probably do in just an excel spreadsheet using random number generators and formulas. Although now thinking about it I might need to use Python. I have a background in C++ although my knowledge is quite dated.\n\nI suggested things like:\n\n* Pulling up real statistics about relationships between weather and traffic accidents\n* Health statistics such as smoking or diet and health concerns\n\nAnd then running a simple simulation with the data she can find in public records over given timelines. From there we can look at outliers and averages to see what the data looked likes. It won't be anything near what AI or machine learning methods look like, but it could be a fun father/daughter project.\n\nHere's my goal:\n\n* I want her to do the project. Not me. But I have enough light coding I should be able to help research and guide her through some of the harder parts.\n* She has some limited coding knowledge from public school.\n* I'd like to have a simple design around the project, something to get her excited about statistics.\n\nAside from what type of simulation to consider, any suggestions about statistics for youths/teens, programs, or easy to pick up languages that might be good for this type of thing?\n\nI'll continue to do my own research, but just thought I'd drop something on this sub. Thanks everyone!",
        "created_utc": 1535428282,
        "upvote_ratio": ""
    },
    {
        "title": "How to work out Post Hoc Power for Hierarchical regression",
        "author": "anandoknows",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9atqfe/how_to_work_out_post_hoc_power_for_hierarchical/",
        "text": "Hello, Im trying to conduct a power analyses on my Hierarchical regression model and I'm not sure how to do this using G\\*power. Should I even be using G\\*Power?\n\n&amp;#x200B;\n\n&amp;#x200B;",
        "created_utc": 1535411256,
        "upvote_ratio": ""
    },
    {
        "title": "MLE vs OLS",
        "author": "fazio92",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9at601/mle_vs_ols/",
        "text": "Hi, sorry if this is a stupid question, but I am quite new to statistics/econometrics. \n\nI am currently reading about estimations for the natural rate of interest and I notice that many of the estimations use Maximum Likelihood Estimation to estimate the parameters of the models. \nFor example in Holston, Laubach and Williams (link: https://www.frbsf.org/economic-research/publications/working-papers/wp2016-11.pdf) they use a state-space model with normally distributed error terms, and estimate the parameters with maximum likelihood estimation.\n\nI am mostly familiar with estimating parameters with OLS, and I´m not really familiar with MLE. My question is therefore: What are the advantages of using MLE over OLS to estimate these parameters? \n\nThank you for any answers",
        "created_utc": 1535406748,
        "upvote_ratio": ""
    },
    {
        "title": "What's the best way to learn from this data?",
        "author": "kvswim",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ar9cj/whats_the_best_way_to_learn_from_this_data/",
        "text": "Hi there /r/AskStatistics ,  \n\nI hail from r/overclocking, where I have collected ~225 samples of a processor being overclocked (Intel Core i7-8700K). The data is located [here on this Google sheet](https://docs.google.com/spreadsheets/d/1B0ONHqbmd-pWt2Aoh48-Z4uv8znhlHYqWk-ECPygq4Y/edit?usp=sharing). The \"aggregated\" sheet is where the numbers are, and the \"scratch\" sheet is the result of me poking around in excel. Related threads are [here](https://www.reddit.com/r/overclocking/comments/99igbp/community_coffee_lake_overclocks/) and [here](https://www.reddit.com/r/overclocking/comments/99rc74/community_8700k_overclocks_preliminary_results/) but they're not very important.  \n\nI have collected an independent variable (voltage input to the processor) and a dependent variable (resulting stable frequency). At least, I think that's where I was going with this when I first started... My goal is for a novice overclocker to get a decent ballpark starting voltage for a desired frequency (based on real world examples), making it easier to dial in the correct voltage for maximum stability/minimum heat.  \n  \nDue to the silicon lottery, any given processor's overclocking ability will differ from another of the same model. Getting to 5.0 ghz requires anywhere from 1.2v to 1.4v depending on your luck. I think I'm trying to say that there is a percentile scale for how good a processor is - the best will require the lowest voltage to get to the highest frequencies.  \n  \nI tried running this through Excel's data analysis regression tool, but I don't think I'm doing it right. ANOVA numbers, t-test and p-test values are astronomical. Maybe a linear regression is not what I want to use? I've tried brushing up on statistics, but I'm just as confused as when I started.\n  \nIt's been a couple years since I took Probstat in college, so this is a quite rusty for me. Can anyone help? I can elaborate on anything you want to know.  \n\ntl;dr I nerded out and collected a bunch of data from reddit, but now I don't know how to best process it. ",
        "created_utc": 1535392994,
        "upvote_ratio": ""
    },
    {
        "title": "What Kind of Statistical Analysis Would You Recommend For Research Data (details inside)",
        "author": "Danmish21",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9apu93/what_kind_of_statistical_analysis_would_you/",
        "text": "The research regards the recruition of neural ensembles in the substructures of the hippocampus at different learning stages. With three different substructures and 6 learning stages, I'm at a loss of how to analyze the data. The variable would be the different learning stages. I ordered the data into a stacked bar graph comparing the average cell counts of the 3 substructures in each learning stage, however I would like to show some type of statistical significance from the cell counts. We calculated the mean and SD of the cell counts and ratios for each substructure of all 6 learning stages. The control would be the tissue samples from the subjects that received no external stimuli. The sample size for each learning stage is n=5 at best and n=4 at the smallest (it's a small lab so it's hard to get a large sample size). My limited understanding of statistics leads me to believe that a t-test is out because of the small sample size, however I had read on a research forum that for smaller sample sizes a Mann-Whitney U-test is recommended for cell counts with smaller sample sizes. I watched a few videos on it but I don't really understand both how to do one and the statistical significance of one. The lab head can be frustrating to ask help from so I turn to you stranger in my hour of need. \n\n&amp;#x200B;\n\nHelp me Obi-Wan you're my only hope. ",
        "created_utc": 1535383315,
        "upvote_ratio": ""
    },
    {
        "title": "Need to compare accuracy of different ways to visually estimate something?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aoisb/need_to_compare_accuracy_of_different_ways_to/",
        "text": "[deleted]",
        "created_utc": 1535373048,
        "upvote_ratio": ""
    },
    {
        "title": "Can I create a dummy variable if the reference has no responses?",
        "author": "anandoknows",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aod25/can_i_create_a_dummy_variable_if_the_reference/",
        "text": "Hello, I am conducting a regression analysis and one of my questions is \"Do you practise mindfulness meditation?\" to which participants responded as either \"Daily, Weekly, Monthly, Rarely or Never\". Now i want to add this in to my regression model but I am a bit confused on how to do this because I have 60 participants total and none of them responded with Monthly, and 2 and 3 responded with daily and weekly with the rest of the participants choosing Rarely and Never. I am unsure how to approach doing this and would appreciate any kindness and guidance on whether making Never the reference would make sense or if I should do something else. ",
        "created_utc": 1535371589,
        "upvote_ratio": ""
    },
    {
        "title": "Anyone here knows how to do Biserial and Point-Biserial correlation using SPSS? Send help pls 😀",
        "author": "[deleted]",
        "url": "https://i.redd.it/ui7djtulkki11.png",
        "text": "[deleted]",
        "created_utc": 1535346738,
        "upvote_ratio": ""
    },
    {
        "title": "R vs Python",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aj1w2/r_vs_python/",
        "text": "[deleted]",
        "created_utc": 1535318006,
        "upvote_ratio": ""
    },
    {
        "title": "Can you code ranges of numbers for analysis?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aiytq/can_you_code_ranges_of_numbers_for_analysis/",
        "text": "[deleted]",
        "created_utc": 1535317342,
        "upvote_ratio": ""
    },
    {
        "title": "Hello, is there any research on this topic (finance crime by ethnicity)",
        "author": "isry7123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aindx/hello_is_there_any_research_on_this_topic_finance/",
        "text": "Tight of the bat i want to say i am not racist or antiemetic, in fact i am a jew myself and live in israel. But i lately become interested in finance crime and i noticed that a lot of jewish names pop up. Could someone link me a research about ethnicity in finance crimes? ",
        "created_utc": 1535314842,
        "upvote_ratio": ""
    },
    {
        "title": "which statistical test to use for pharmaceutical data?",
        "author": "Nanek1990",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ahm1w/which_statistical_test_to_use_for_pharmaceutical/",
        "text": "I have data from 3 different formulations (A,B &amp; C) and I stored them in two different temperatures: 40c and room temperature and then I took several measurements such as pH, viscosity, osmolarity, each in tripiclates at different time points e.g. day 0, day 1, day 7 etc. \n\nWhich statistical test is best to use to understand the relationship between value measured and time? value measured and temperature?",
        "created_utc": 1535307065,
        "upvote_ratio": ""
    },
    {
        "title": "Confused about Linear Models/Regression?",
        "author": "fattyfatfatfuck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ah5yf/confused_about_linear_modelsregression/",
        "text": "Hi Guys,\n\nI'm currently reading through [this](https://blog.stata.com/2011/03/03/understanding-matrices-intuitively-part-1/). And the author has this paragraph\n\n&gt;Begin by imagining a case where it just turns out that (X‘X)^-1 = I. In such a case, (X‘X)^-1 would have off-diagonal elements equal to zero, and diagonal elements all equal to one. The off-diagonal elements being equal to 0 means that the variables in the data are uncorrelated; the diagonal elements all being equal to 1 means that the sum of each squared variable would equal 1. That would be true if the variables each had mean 0 and variance 1/N. Such data may not be common, but I can imagine them.\n\n\nI'm confused he says *the off-diagonal elements being equal to 0 means that the variables in the data are uncorrelated; the diagonal elements are all equal to 1 means that the sum of each squared variable would equal 1*. I get that since it's the identity matrix, the diagonal elements are 1 and the off-diag are all 0. But I have no idea how he's making these conclusions about the data from this?\n\nI'd love it if someone could elaborate or maybe post some resources that delve into why this is true? Thanks a lot!",
        "created_utc": 1535303681,
        "upvote_ratio": ""
    },
    {
        "title": "Need help to answer this question",
        "author": "younsomoom",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9agto3/need_help_to_answer_this_question/",
        "text": " There are about 1000 Malcreek Highschool students. Teachers want to interview a sample of 50 students about their studying habits. A list of all 1000 students with their names and contact information is available.\n\n​\n\nFor the question:\n\nIn order to obtain a sample that is representative of the population, should Teachers use the convenience sampling method or the simple random sampling method?\n\n​\n\nFor the answer I think only simple random sampling method is representative sample, but I'm not sure. Help me please",
        "created_utc": 1535301123,
        "upvote_ratio": ""
    },
    {
        "title": "Can I run Linear regression on this?",
        "author": "sholopinho",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9afu9f/can_i_run_linear_regression_on_this/",
        "text": "Hello,\n\nI'm willing to run a Linear regression analisys on my data. The Shapiro-Wilk shows 0.0 sig, and Skewness -.080(std error=\t.188) Kurtosis -.432 (std error .375).\nWhen I tried to run the linear regression on the data it did show significance. What do you think I should do? \n\n\n\n",
        "created_utc": 1535293108,
        "upvote_ratio": ""
    },
    {
        "title": "Does a moving average on a time series cut the curve into equal halves?",
        "author": "kiwiheretic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ae4ys/does_a_moving_average_on_a_time_series_cut_the/",
        "text": "I was just thinking about this hypothetically.  I've been looking at a couple of time series with large seasonal effects.  Would a moving average on the time series approximately cut the curve into two halves that have equal areas that become more equal the longer the series runs or am I barking up the wrong tree?",
        "created_utc": 1535272144,
        "upvote_ratio": ""
    },
    {
        "title": "What test should I use for comparing the results of the same group of people in different settings?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9adf8u/what_test_should_i_use_for_comparing_the_results/",
        "text": "[deleted]",
        "created_utc": 1535262145,
        "upvote_ratio": ""
    },
    {
        "title": "What test do I use?",
        "author": "ymmvs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aaxit/what_test_do_i_use/",
        "text": "Hi,\n\n&amp;#x200B;\n\nI am attempting to write a medical article and lack fundamental understanding. I've spent some time researching but am still unclear on how I know when and where I can use a test. For example, say I have different age groups suffering from an injury\n\n&amp;#x200B;\n\ngroup 1: 10 total injuries\n\ngroup 2: 11\n\ngroup 3: 20 injuries\n\ntotal: 41 injuries\n\n&amp;#x200B;\n\nI calculate each group group 1 as comprising 24.3% of the total injuries, group 2 as 26.8%, and group 3 as 48.7 % of the total injuries.  This is just a hypothetical example. \n\n&amp;#x200B;\n\nThe part I am having trouble with is what statistical test do I run to know if this data is significant? Or does significance not apply because these aren't \"means\"? Thank you for any help. I greatly appreciate it",
        "created_utc": 1535237679,
        "upvote_ratio": ""
    },
    {
        "title": "Test to compare the accuracy of different ways to measure/estimate something",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aao3n/test_to_compare_the_accuracy_of_different_ways_to/",
        "text": "[deleted]",
        "created_utc": 1535235420,
        "upvote_ratio": ""
    },
    {
        "title": "Need help with understanding representative sample",
        "author": "younsomoom",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9aabqv/need_help_with_understanding_representative_sample/",
        "text": "Do you think both convenience sampling and random sampling of a specific population is considered as representative sample?",
        "created_utc": 1535232615,
        "upvote_ratio": ""
    },
    {
        "title": "How can I find an association between a dichotomous variable and a continuous variable?",
        "author": "anavsc91",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9a9suc/how_can_i_find_an_association_between_a/",
        "text": "I have many dichotomous independent variables and one continuous dependent variable. Is there any way to determine an association between both? I wanted to avoid categorizing the numerical variable.\n\nIs it possible to calculate Spearman's rho if I have dichotomous variables?",
        "created_utc": 1535228225,
        "upvote_ratio": ""
    },
    {
        "title": "Chi Squared Sampling Distribution?",
        "author": "fattyfatfatfuck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9a9pz6/chi_squared_sampling_distribution/",
        "text": "So,\n\nMy understanding is that if you take a sampling distribution of anything, it should come out to be a normal distribution due to the central limit theorem. I'm confused as to how the Chi Squared Sampling Distribution plays into this? I'd appreciate anyone correcting me on where my thinking is incorrect. Thank you!",
        "created_utc": 1535227603,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing 1 month post intervention to 6 months pre intervention",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9a70z5/comparing_1_month_post_intervention_to_6_months/",
        "text": "[deleted]",
        "created_utc": 1535206122,
        "upvote_ratio": ""
    },
    {
        "title": "Probability of winning a best of five.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9a5odi/probability_of_winning_a_best_of_five/",
        "text": "[deleted]",
        "created_utc": 1535190468,
        "upvote_ratio": ""
    },
    {
        "title": "Test for relation between categories within two variables",
        "author": "Eyokiha",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9a5o9q/test_for_relation_between_categories_within_two/",
        "text": "I have two variables (categorical/nominal). Both contain two or more categories. Each line represents a case (a sentence) where the two variables each contain one category (a descriptive word). I want to determine if there are any relations or correlations discernable.\n\nI want to know for each category of var 1 if it has a significant correlation with any category of var 2. So I want to know which categories are related to each other. The chi square test would be an option, but this only tests if var 1 has a relation with var 2, and does not test for each category in each var separately.",
        "created_utc": 1535190424,
        "upvote_ratio": ""
    },
    {
        "title": "How to find a truly random but static number without electronics, machinery, writing it down, or even memorizing it?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9a4kgl/how_to_find_a_truly_random_but_static_number/",
        "text": "[deleted]",
        "created_utc": 1535175424,
        "upvote_ratio": ""
    },
    {
        "title": "Guessing the probability of heads tossing two biased coins",
        "author": "specific_account_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9a1tqj/guessing_the_probability_of_heads_tossing_two/",
        "text": "I posted a similar question some time ago but I think this is a much better formulation of the problem.\n\nA game is played at a computer, as follows:\n\nThere are two coins, one silver coin and one gold coin, in a box.\nThe computer “shakes” the box, and then tells the player whether the two coins landed on the same side or not. \nAt this point the player must guess whether the coins are both heads or both tails (if they landed on the same side), or which of the two coins, silver or gold, is head (if they landed on different sides).\n\nNow, if the coins were both fair, the player would just guess. The problem is that each coin can be of three types:\n-\tFair coin (lands on head 50% of the times)\n-\tBiased coin coming up head 25% of the times\n-\tBiased coin coming up 75% of the times\n\nThe player does not know the exact probabilities; he (or she) only knows that the coins are not necessarily fair.\n\nThe game is repeated nine times with all the possible coin combinations.\n\nDuring one game session the coins are tossed 32 times. The goal of the player is to make as many correct guesses as possible. \n\nNow, since this is a computer game, the sequence of results is predetermined. When I say that a coin comes up head 75% of the times, it means that the coin will be head 24 times out of 32 (again, the player does not know that, he only knows that the coins are not necessarily fair). Since the sequences are all predetermined, I can compare different players, as they will all go through the same sequences.\n\nThe problem has two parts.\n\n**PART ONE**\n\n**DATA**\n\nMy data is the sequence of coin tosses. In the following example:\n-\tthe gold coin has 25% chance on landing oh head\n-\tthe silver coin has a 50% chance of landing on head.\n\n1: landing on head \n0: landing on tail\n\n    Gold:  0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0\n    Silver:0,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,0,1,1,0,1,1,1,0\n\n**QUESTION**\n\nI would like to model what an ideal observer would do. The ideal observer would start the game with a beta prior and the update it at each coin toss. I would like to see the updating at each toss, and what’s the final posterior after 32 tosses.\n\nIs Stan a good way of doing this? I looked at the Rate1 example from the book Bayesian Cognitive Modeling: A Practical Course (2014) by Michael Lee and Eric-Jan Wagenmakers:\n\n    // Inferring a Rate\n    data { \n      int&lt;lower=1&gt; n; \n      int&lt;lower=0&gt; k;\n    } \n    parameters {\n     real&lt;lower=0,upper=1&gt; theta;\n    } \n    model {\n      // Prior Distribution for Rate Theta\n      theta ~ beta(1, 1);\n  \n      // Observed Counts\n      k ~ binomial(n, theta);\n    }\n\nIn the book example k, the number of successes, is 5 and n is 10. The code calculates the posterior distribution centered at 0.5. In my case I would like to see how the prior is updated step by step.\n\nI also looked at the example “Hierarchical Partial Pooling for Repeated Binary Trials” on the Stan website, but again it considers an overall rate of successes.\n\nAnother thing I would like to know is which initial prior (comparing different types of beta priors, or other kinds of priors) would yield the best score (out of a max of 32, when one is making the correct decision at each turn).\n\n**PART TWO**\n\n**DATA**\n\nMy data is the player’s actual choices. For example, the following is a list of a subject’s choices in the case we considered above, according to the following code:\n\nThe coins landed on the same side, the subject bets on heads: 1\nThe coins landed on the same side, the subject bets on tails: 0\nThe coins landed on the opposite side, the subject bets the gold is head: 1\nThe coins landed on the opposite side, the subject bets the silver is head: 0\n\nThe line after the subject's choices shows his score: 1 if he guessed right, 0 if he guessed wrong\n\n    Gold:  0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0\n    Silver:0,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,0,1,1,0,1,1,1,0\n    Guess: 0,1,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,0,1,1,0,0,1,1,1,0,1,0\n    Score: 1,0,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,0,0,1,0,1\n\n**QUESTION**\n\nI would like to reverse engineer the process: given the choice, what was the player prior? \nHow did it change through the trials, and is the player biased toward the silver of gold coin?\n",
        "created_utc": 1535149632,
        "upvote_ratio": ""
    },
    {
        "title": "Do all explanatory variables have to be differenced in an ARIMAX model (for consistency) even if only some (not all) are non-stationary?",
        "author": "Hazza385",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99yyke/do_all_explanatory_variables_have_to_be/",
        "text": "",
        "created_utc": 1535128680,
        "upvote_ratio": ""
    },
    {
        "title": "How do I calculate the odds of an outcome of an event for the event itself is subject to certain odds that it will occur ?",
        "author": "Amygdaloidal_Dream",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99yam8/how_do_i_calculate_the_odds_of_an_outcome_of_an/",
        "text": " For instance,   Let’s say that I have a one in 1 million chance of winning the right to play a game of chance. If I win that right I am permitted  to play a game of chance in which I have a one out of 100 chance to win.\n\nHow do I calculate my overall odds of winning both the right to play the game of chance as well as winning the game of chance itself? how do I calculate overall odds of winning both the right to play the game of chance as well as winning the game of chance itself ",
        "created_utc": 1535124046,
        "upvote_ratio": ""
    },
    {
        "title": "quick question about probability distributions",
        "author": "anarcho-monarchist2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99uxel/quick_question_about_probability_distributions/",
        "text": "idk if this is the right place to post this, but what the hell, y'all seem chill.\n\nhow would the distribution resulting from rolling four six-sided dice (sides numbered 1-6), ignoring the lowest, and summing up the highest three be different from... \n\nthe distribution resulting from rolling three four-sided dice (sides numbered 1-4) and one ten-sided die (sides numbered 1-10), ignoring the lowest die (of either type) and summing the rest? \n\nI normally do this kind of bullshit in anydice but can't figure out how to lump dice types. so asking the smart people here.\n\nfor the curious, trying to think of a new way to generate character stats in D&amp;D. I like rolling four-sided dice better than six-sided dice. ",
        "created_utc": 1535090280,
        "upvote_ratio": ""
    },
    {
        "title": "Data/table manipulation in SAS/R",
        "author": "ComputerCrashing",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99sk6t/datatable_manipulation_in_sasr/",
        "text": "I'm given a table which is formatted as:  \n&amp;nbsp;\n\n1 | 222  \n2 | 189  \n3 | 140  \n4 | 100    \n\nWhere these values represents there are 222 of \"1\"s, 189 of \"2\"s, etc etc. In this format, I'm unable to run any statistical analysis with it. How can I turn this table into a usable form without needing to type out 222 \"1\" manually.   \n\nI was attempting to google this, but wasn't entire sure what this is called aside from table manipulation.  ",
        "created_utc": 1535068717,
        "upvote_ratio": ""
    },
    {
        "title": "Data normalising",
        "author": "Shorse_rider",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99sewe/data_normalising/",
        "text": "I don't know if I've used the correct term here, but I am trying to level the playing field before conducting analysis due to a data skew. \n\n&amp;#x200B;\n\nI'm comparing social media mentions of different models of cars within car brands for a share of voice analysis. However, some brands have more models and therefore, it is likely that there will be more social media mentions for the brand with the most models being tracked by our tool. \n\n&amp;#x200B;\n\nFor example\n\n&amp;#x200B;\n\nBrand 1: 5 models\n\nBrand 2: 1 model\n\nBrand 3: 3 models\n\nBrand 4: 1 model\n\nBrand 5: 10 models\n\n&amp;#x200B;\n\nObviously brand 10 could quite easily have the largest share of voice. When all the social media mentions are pulled in by the tool for each brand's car models, is there a way I can weight the numbers of mentions (adding additional/subtracting) based on number of car models being analysed. \n\n&amp;#x200B;\n\nMany thanks!",
        "created_utc": 1535067509,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting a significant overall Kruskal Wallis with no significant differences between groups",
        "author": "les_mort",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99s0il/interpreting_a_significant_overall_kruskal_wallis/",
        "text": "Kruskal.test shows significance, but post hocs (dunns test, kruskalmc, pairwise Wilcox) with p adjustments (tried different ones) show no significant differences between groups.\nHow to describe this?\nThe difference is significant but groups don't differ? Dafuq?",
        "created_utc": 1535064238,
        "upvote_ratio": ""
    },
    {
        "title": "Analyzing meta-game health via competitive game drafting - what is the ideal stats representation for this?",
        "author": "Veeshan28",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99owzl/analyzing_metagame_health_via_competitive_game/",
        "text": "Hi folks,\n\nI'm a fan of the game DOTA 2 with a curiosity for stats and data analysis. Currently the game is having it's largest tournament of the year, and I'd like to compare draft statistics from this year to past years. \n\nMy suspicion is that there are more substantially underrepresented heroes than in some years past. Conversely, I also suspect there are more substantially over-represented heroes than in some years past as well. How would I ideally go about measuring, and representing this?\n\nHere's a few key details:\n\n* Matches are 5 v 5\n\n* Currently there are 115 heroes to select from in any given match.\n\n* For a match, each team picks their 5 heroes, but also bans 6 heroes (5 heroes in years past). \n\n* These tournaments have roughly 180 games played per year.\n\nA few thoughts I've had so far:\n\n* Given the number of hero picks, hero bans and the available hero pool, heroes would ideally be contested (picked OR banned) in roughly (22/115) games, or 19.1% of games.\n\n* With this many heroes, it's not surprising that some are popular, and some are niche picks, but at some point in either direction it's extreme. \n\n* I'd assume I'd want to focus on ends of the spectrum, perhaps by percentile?\n\n* I've got a passing knowledge of using tableau, and I've got some data already ready in excel tables. \n\n-----\n\nBasically, I'm looking for advice on how best to statistically find extremes in video game drafts from one annual tournament to the next, and then how best to represent it visually. \n\nAny advice is greatly appreciated.\n",
        "created_utc": 1535042181,
        "upvote_ratio": ""
    },
    {
        "title": "What test should I use to see if a binary distribution is different than natural chance?",
        "author": "abcbrakka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99ofbp/what_test_should_i_use_to_see_if_a_binary/",
        "text": "Hi, this is probably a newbie quesiton but I am having a hard time deciding what test to use.\n\n&amp;#x200B;\n\nI am testing to see if people with a stroke have a stenotic artery leading to the area of stroke. The way I see it I have 2 conditions: stroke with stenosis, stroke without stenosis. If there is no relation between stroke and stenosis the ratio between the two should be 0.5. So I think I should test if my ratio differs from 0.5. Is this correct? And what is the best way to test this?\n\n&amp;#x200B;\n\n&amp;#x200B;",
        "created_utc": 1535038694,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting Kest statistics in R",
        "author": "bugnerd87",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99nszb/interpreting_kest_statistics_in_r/",
        "text": "Hi everyone, I'm using spatstat in R to run spatial stats on a few datasets that contain information about insect attacks on trees. When I plot the Gest, Fest, and Kest all three indicate clustering (plot linked [here](https://imgur.com/a/zQX66O2)). When I run the stats for the envelope function, however, the significance level is 0.09 which seems incorrect considering just how far outside the envelopes my observed values are. This is my first time working with these tests so I'm assuming I'm just interpreting the information wrong but the online documents aren't helping me understand. Here is my R output for the dataset that corresponds to the photo linked.\n\n&amp;#x200B;\n\n\\&gt; at7.env3\n\nPointwise critical envelopes for K(r)\n\nand observed value for ‘at7.ppp’\n\nEdge correction: “iso”\n\nObtained from 20 simulations of CSR\n\nAlternative: two.sided\n\nSignificance level of pointwise Monte Carlo test: 2/21 = 0.0952\n\n.....................................................................\n\nMath.label     Description                                      \n\nr    r              distance argument r                              \n\nobs  hat(K)\\[obs\\](r) observed value of K(r) for data pattern          \n\ntheo K\\[theo\\](r)     theoretical value of K(r) for CSR                \n\nlo   hat(K)\\[lo\\](r)  lower pointwise envelope of K(r) from simulations\n\nhi   hat(K)\\[hi\\](r)  upper pointwise envelope of K(r) from simulations\n\n.....................................................................\n\nDefault plot formula:  .\\~r\n\nwhere “.” stands for ‘obs’, ‘theo’, ‘hi’, ‘lo’\n\nColumns ‘lo’ and ‘hi’ will be plotted as shading (by default)\n\nRecommended range of argument r: \\[0, 45\\]\n\nAvailable range of argument r: \\[0, 45\\]\n\n&amp;#x200B;\n\nI know I'm missing something here but cannot figure out what. Any help is greatly appreciated!",
        "created_utc": 1535034217,
        "upvote_ratio": ""
    },
    {
        "title": "Question about constraining correlations in CFA",
        "author": "snowbabiez",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99m57j/question_about_constraining_correlations_in_cfa/",
        "text": "Do researchers ever constrain values of correlation among latent variables to any value other than 1 or 0? I have not yet come across any papers that mention this.\n\nWhat is the interpretation for constraining correlation to 1? That the two latent variables are essentially equal to each other? Or simply that they are correlated (to some extent)?\n\nIf constraining correlation to 1 just means that they are correlated to some extent, what is the difference between constraining it to 1 and freeing it up (not constraining)?",
        "created_utc": 1535019179,
        "upvote_ratio": ""
    },
    {
        "title": "What should I know as a statistics graduate when applying to positions?",
        "author": "statsMajorNeedWork",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99ld23/what_should_i_know_as_a_statistics_graduate_when/",
        "text": "Hi I graduated with a statistics degree and have been having trouble finding a job for the past few months. I apply to \"data analyst\" or \"analyst\" roles. My data science/analyzing experience mostly comes from the classroom and personal Kaggle projects I have worked on. \n\nI have worked IT for the school and another position where I was hired by the school to build a simple messaging webapp for reminders through python. I also interned as a sales analyst for a nearby company hoping to gain some data experience but ended up mostly doing data entry work so there really isn't much to talk about. These three things are on my resume. \n\nSome of the coursework titles I have taken: Linear Modeling, Computational Statistics in R, Statistical Programming in SAS, Stata, SPSS &amp; R, Computation and Optimization for Statistics, Machine Learning,, Monte Carlo Methods, Statistical Consulting, and Experimental Design.\n\nThe general advice I usually get is work on more Kaggle projects or go to UCI Machine repo to look for something of interest. \n\nWhat should I highlight in interviews? I usually get asked about the python app I wrote and maybe 1 or 2 kaggle projects. \n\nAlso open to taking suggestions on what I should know for certain positions in certain industries so I know how I should work on myself.\n\nI apologize if this post seems very sporadic. Just some late night thoughts",
        "created_utc": 1535009786,
        "upvote_ratio": ""
    },
    {
        "title": "Can I compare a single group on two different variables?",
        "author": "iamkylekatarnama",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99khl0/can_i_compare_a_single_group_on_two_different/",
        "text": "I feel like I am violating a major stats rule here so I just want to check in. I have 30 depressed people in my study and I ask them 11 yes-no questions, 6 relating to beliefs about happiness and 5 relating to beliefs about sadness. I give them each a total happiness score and a total sadness score (e.g. 1 point for a yes, 0 points for a no). I expect the happiness score to be lower than the sadness score, can I test this or should I just be reporting the average scores by themselves?",
        "created_utc": 1535000107,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for some guidance about a Formula 1 project",
        "author": "ParadeShitter",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99kdxz/looking_for_some_guidance_about_a_formula_1/",
        "text": "I'm looking to tackle a project I have been thinking about for the last few months now that I have a bit of free time on my horizon.\n\nThe idea started with wanting to quantify which driver was best at which track. I want to try and best adjust for car performance and get a result that better reflects which driver performs above (and below) average at which track to determine what tracks drivers are and were most/least comfortable at.\n\nThe end result would be some type of rating system that would quantify each drivers performance at each grand prix. It would take into account (or adjust for) the larger picture of car performance for that season as well as results in comparison to a teammate in the same car.\n\nIdeally, if a driver had like 10 unremarkable seasons but managed to finish significantly higher than his teammate or consistently higher than where the car would be expected to finish at one particular track every year, that'd lead me to think that track suited the drivers driving style and was a noticeable strength of theirs.\n\nBasically I'll just be web scraping each season, each race and storing the: race results (driver, team, position, gap, points) and qualifying results (driver, team, position, time) which will give me the ability to have start/finish position and positions +/-, race wins, fast laps and race dnfs. I'll be using python if it matters as it's the language I'm most comfortable in. I still haven't decided how to best store the data but that's part of the fun anyway.\n\nI'm big into basketball stats and for some (likely foolish) reason I'm drawn to Hollinger's player efficiency rating (PER) because it allows for positive and negative stats and boils it down to a single number.\n\nIf you want to flat out tell me how you'd go about modeling this, feel free but I'm more looking for some jumping off points to research this myself. I'm sure there's some terms or rating systems that are similar that I could look at as reference or guidance. I have a bit of background in math up to Calc 2 and a degree in Finance but that all feels like it was in another life. \n\nThoughts? Ideas? Comments? Critiques? Life tips? I honestly got a few ideas just trying to put my thoughts in a semi-coherent post so I got that going for me already.",
        "created_utc": 1534999107,
        "upvote_ratio": ""
    },
    {
        "title": "How to report Benjamini-Hochberg corrected values?",
        "author": "slipstitchy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99jkr9/how_to_report_benjaminihochberg_corrected_values/",
        "text": " In the text body I give the FWER I used , and which values survived correction. I'm assuming I need to list the critical values for each *p*, can I just put these in the tables I'm using to report the t-test results and throw it in the appendix? I currently have a column for critical values alongside each *p*, do I need to list the rank of each value as well? ",
        "created_utc": 1534991563,
        "upvote_ratio": ""
    },
    {
        "title": "What is a principal components analysis used for?",
        "author": "vaguely_specific1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99j4ne/what_is_a_principal_components_analysis_used_for/",
        "text": "I'm having trouble understanding the justification for a principal components analysis in simple terms- explanations like the one used on wikipedia are very mathematical and abstract but I don't understand specifically how it is used in real life, or what to do with the output of the analysis.  Can someone help me understand in simple terms?\n\nFor reference, wikipedia describes it as:\n\n&gt;  a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables[clarification needed] into a set of values of linearly uncorrelated variables called principal components\n\nand gives the following further explanation:\n\n&gt; PCA can be thought of as fitting an n-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component.\n\nI understand the basic concepts of taking a lot of variables and creating from them fewer, more distinct variables, but I'm struggling to understand how to apply this in real life.",
        "created_utc": 1534987684,
        "upvote_ratio": ""
    },
    {
        "title": "Mediation, Moderation, &amp; Interaction Terms",
        "author": "dwyerswansong",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99j2s1/mediation_moderation_interaction_terms/",
        "text": "I have a hypothesis about a mediation effect between X and Y through M, where X is binary and Y and M are continuous. I understand Baron &amp; Kenny's causal steps approach to testing mediation, and the steps do support partial mediation. \n\nHowever, out of curiosity, I also conducted a moderation test, where Y=X+M+X\\*M. Here I found a negative sign on X\\*M, with a significant p-value. This was quite unexpected, and makes no sense theoretically.  \n\nCan someone shed light on this outcome. I realize that mediation is not tested through interaction terms, moderation is. But is this what you might expect an interaction to look like given a mediation effect?",
        "created_utc": 1534987230,
        "upvote_ratio": ""
    },
    {
        "title": "Non-independence of almost complete sampling.",
        "author": "Modularva",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99iavt/nonindependence_of_almost_complete_sampling/",
        "text": "I feel like I've read about this issue before, but apparently I'm using the wrong phrases to search for it. Essentially, the idea is that observations can only be assumed to be independent when you're sampling only a small portion of the population of interest. However, I don't really understand the argument why this should be true. Hopefully it's clear what I'm trying to talk about.",
        "created_utc": 1534980791,
        "upvote_ratio": ""
    },
    {
        "title": "Trying to understand how to calculate the probability in relation to a time-series",
        "author": "TheFakeMatt",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99h2xo/trying_to_understand_how_to_calculate_the/",
        "text": "Please forgive me as my statistics is a bit rusty and I'm way out of practice. I'm trying to research a method to help solve the following problem.\n\n​\n\nGiven an ordered sequence, I'd like to predict the likelihood that the next value will occur within X of the last value.\n\n​\n\nFor example, based on this pattern {2,4,6,8,10} the likelihood of the next value occurring within 2 of the last value is 100% because the standard deviation between each value is 0.\n\n​\n\nOn the other hand, if I had a pattern like this {1,2,6,10,11} then the next value occurring within 2 is something else because the standard deviation between each value is &gt; 0. So how confident can I say that the next value might be something in the range of 11 to 13?\n\n​\n\nNot sure if there's a name for this type of problem, but anything that might point me in the right direction would be greatly appreciated!",
        "created_utc": 1534971472,
        "upvote_ratio": ""
    },
    {
        "title": "Why do the odds for logistic Regression need to be symmetrical?",
        "author": "PMmeURFaveBook",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99gz4y/why_do_the_odds_for_logistic_regression_need_to/",
        "text": "When walking through the steps to turn probability into log odds, what's the reasoning behind taking the natural log? Why do we want the odds to be symmetrical?",
        "created_utc": 1534970684,
        "upvote_ratio": ""
    },
    {
        "title": "Basic question about using the term percentile for data",
        "author": "o-rka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99gkpl/basic_question_about_using_the_term_percentile/",
        "text": "I am doing an operation where I am grabbing the upper 50th percent of the data, then the upper 90th percent of the data.\n\n&amp;#x200B;\n\nIf there were 100 observations, would the top 50 be the  50th percentile and the top 10 be the  90th percentile?​​\n\n&amp;#x200B;\n\nAlso, is there a single letter that is typically used to denote these types of measures? For example, when representing the top 10% of the data, or the 90th percentile, would it make sense to use q\\_90?",
        "created_utc": 1534967787,
        "upvote_ratio": ""
    },
    {
        "title": "Questions about Firth logistic regressions",
        "author": "TempUN18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99fsmx/questions_about_firth_logistic_regressions/",
        "text": " \n\nHi there, I'm a bit confused about usage of Firth regressions, and I was hoping to get some input.\n\n​\n\nI  have a large dataset with many variables, and the outcomes I'm  currently interested in are binary, with simple scales as independent  variables and a couple of covariates.\n\n​\n\nIn  my first analysis pass, I analyzed (in SPSS) using binary logistic  regressions for each of the scales of interest (plus 2 covariates).  Everything seemed to go smoothly, except I noticed that some of my  variables had massive betas and SEs. I realized this was due to complete  or quasi-complete separation in some of my variables. Some research led  me to Firth logistic regressions, which can compensate for complete or  quasi-complete separation.\n\n​\n\nI guess what I'm confused about is the following:\n\n​\n\n\\-  Is it more correct to just run the \"standard\" logistic regressions,  forget about the complete separation variables, and just report the  non-\"separated\" significant results, or to use the Firth regressions?\n\n\\-  If using Firth regressions, is it more correct to run Firth regressions  for EVERY variable of interest (for consistency), or only those  variables with complete separation (as the non-separates don't need  correection)\n\n\\- In SPSS, for the logistic  regression the main p value I was looking at to determine significance  was just the p value for the independent variable amongst the covariates  in the \"Step 1 Variables in the Equation\" box. For Firth, the SPSS  output also specifies a \"significance\" value above, for the test as a  whole. Is it relevant whether this value is &lt;0.05 (or whatever alpha)  if I'm only concerned with my one independent variable?\n\n​\n\nThanks greatly in advance for any input, I really appreciate it!",
        "created_utc": 1534962236,
        "upvote_ratio": ""
    },
    {
        "title": "Assumption of Normaility issues.",
        "author": "girlagainsthumanity",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99fczr/assumption_of_normaility_issues/",
        "text": "I have two independent groups (Group T, N = 67 and Group S, N = 70) in one column on SPSS. Normality tests show Group S is **not** normally distributed. I've tried to do Square Root Transformation but this then affects Group T, making them not normally distributed. \n\nIs there a method I can do that will fix Group S without disrupting Group T?\n\nOr should I just do a non-parametric test (Mann Whitney in this case)?",
        "created_utc": 1534959252,
        "upvote_ratio": ""
    },
    {
        "title": "Choosing the correct statistical test for 4 groups in SPSS",
        "author": "1299638",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99e5hm/choosing_the_correct_statistical_test_for_4/",
        "text": "I have 4 different groups in my experiment, but not all of these have a normal distribution. I want to compare each group with each other, so I thought of using a one-way ANOVA. However, not all my groups have a normal distribution. So I thought of using the Kruskal/Wallis test or do I need to perform some transformations?\n\nDoes anybody have any idea of which statistical test I should use? I want to know what groups have a significant difference and which don't.",
        "created_utc": 1534950706,
        "upvote_ratio": ""
    },
    {
        "title": "Unsure how to pick variables to include in SEM analysis...how to decide between multiple measures of a construct",
        "author": "jmpsychresearcher",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99drno/unsure_how_to_pick_variables_to_include_in_sem/",
        "text": "I'm fairly new to Reddit so please forgive me if I don't follow any conventions!\n\nBasically, for my thesis (psychology) I am examining several constructs that are thought to be associated with a type of disordered eating behaviour. One of these constructs is multifaceted, so I'm using multiple methods to measure it, and for each of the other construct the measure has multiple factors. Each construct will make up a chapter of my thesis, in which I will examine group differences between control and the eating disorder group and also use regressions to show which elements of the construct best predict severity of eating disorder. \n\nI have sketched out a theoretical model of how these constructs may work together to impact eating disorder severity and want to test this using a SEM analysis, but I'm not sure how to go about deciding which measures of each construct/factors of each measure would be best to include in the SEM. \n\nCan anyone provide any advice on how to go about this? I've heard about parcelling but am not sure what this entails. One idea I had was to run the regressions and see which of the factors appear to be important in predicting ED severity, and just use those in the SEM. But what if there is more than one? How do I select and distil these?\n\nI would really appreciate advice on this! Thanks in advance\n\n&amp;#x200B;\n\nAdditional info for clarity - I'm measuring 4 constructs and the measures are outlines below:\n\nConstruct A: 4 measures\n\n* Self-report measure (15 items, 3 factors &amp; a total score)\n* Self-report measure (59 items, 5 factors)\n* 3 Behavioural measure (each produces 1 DV)\n\nConstruct B: 1 self-report measure (16 items, 1 factor)\n\nConstruct C: 1 self-report measure (18 items, 5 factors &amp; a total score)\n\nConstruct D: 1 self-report measure (35 items, 5 factors and a total score)",
        "created_utc": 1534947887,
        "upvote_ratio": ""
    },
    {
        "title": "How do weight evidence from a study in a meta-analysis with means and variances derived from bayesian analysis?",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99ctuy/how_do_weight_evidence_from_a_study_in_a/",
        "text": "Given that I am interested in estimating the true mean effect of some parameter on Y, the normal procedure would be to calculate the mean of all study means, weighted by their variance within the respective study.\n\nThis is rather straightforward in a frequentist setting where each mean and variance is derived from the data of the respective study, but how do I include bayesian estimates of the mean and variance that also depend on the prior?\n\n​\n\nLets say I have three studies. Each is (nearly) exactly using the same setup.\n\n* Study one estimated the effect of X on Y *with a frequentist regression*\n* Study two estimated the effect of X on Y *with a bayesian approach* and used the *results of study one to build priors*\n* Study three estimated the effect of X on Y *with a bayesian approach* and used the *results of study one as well to build priors*\n\n&amp;#x200B;\n\nNow, If I where to calculate the mean of means, I would implicitly assign way too much weight to study one since the results of study two and three are influenced by study one. What is the recommended procedure here?",
        "created_utc": 1534940249,
        "upvote_ratio": ""
    },
    {
        "title": "Recognize patterns in time series",
        "author": "guglicap",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99civp/recognize_patterns_in_time_series/",
        "text": "Hello, \n\nso with the premise that I don't know anything about statistics (I'm willing to learn though), here's what I'm trying to do.   \nI have a time series of my house energy consumption, every minute. So I'll have the consumption at 00:00, 00:01, 00:02 ... 23:59. \nThen I have a time series of the energy consumption of an house appliance, let's say the dishwasher, also every minute. So I'll start the dishwasher and I'll have like energy consumption during first minute, second, third, etc.   \nWhat I'd like to do is analyze the house energy consumption and detect the appliances pattern to find out when I used the dishwasher, or dryer or whatever.   \n\nHow would I go about doing this? What should I look into?  \n\nI hope I explained myself, please ask if you need to know anything else. \n\nThank you. ",
        "created_utc": 1534937342,
        "upvote_ratio": ""
    },
    {
        "title": "Should I restate my hypothesis? (interaction)",
        "author": "MrKimblet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99c2me/should_i_restate_my_hypothesis_interaction/",
        "text": "Suppose I have the following model:\n\nY = *α + β*1∗education + *β*2∗gender + *β*3∗gender∗education\n\nGender = 1 if male (reference group = 0 female)\n\n​\n\nBased on the theory I develop the following two hypothesis:\n\n* H1) Higher educated males have a significant positive association with Y\n* H2) There is no significant relation between educated women and Y\n\n&amp;#x200B;\n\n**Question**\n\nWhat I know is that\n\n* *β*1 = the effect of education for females\n* *β*3 = the difference in effect of education for males (interaction)\n* *β*1 + *β*3 = the effect of education for males\n\nIs my hypothesis consistent with my model i.e. can I draw a  conclusion on the hypothesis purely based on the p value of coefficients of *β*1 and *β*3 in the model? Or do I have to restate my hypothesis to something like ''...the effect of education for males is stronger than females...'' or run two separate regressions to make a conclusion on this hypothesis?\n\n​\n\nFor Hypothesis H1 I can directly read the Pvalue on the coefficient of *β*1. But how would that work out for *β*3? I dont think I can combine it in some way with *β*1",
        "created_utc": 1534932451,
        "upvote_ratio": ""
    },
    {
        "title": "Correct steps to examine 3x interaction?",
        "author": "sleepy-o",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99bskd/correct_steps_to_examine_3x_interaction/",
        "text": "Hi all,\nI'm trying to find out what statistical models to build to answer my research questions. The data are longitudinal and correlated so I'm going with GEE.\n\nDV is continuous.\nIV1, IV2, and IV3 are dichotomous.\n\nQuestions\n\n1. Does IV1 affect DV?\n\n2. Does IV2 affect DV?\n\n3. Does the effect of IV1 on DV depend on IV2?\n\n4. Is this interaction further dependent on IV3?\n\n\nNote that I'm not interested in the effect of IV3 per se, only the 3x interaction.\n\nI'm working with R and package geeM. Would these models suffice to answer my research questions? I've left out all but formulae from the following arguments for the sake of clarity.\n\n1. geem(DV ~ IV1,...)\n2. geem(DV ~ IV2,...)\n3. geem(DV ~ IV1*IV2,...)\n4. geem(DV ~ IV1*IV2*IV3,...)",
        "created_utc": 1534929149,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics on bacterial counts.",
        "author": "traitoro",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99bouf/statistics_on_bacterial_counts/",
        "text": "Apologies if this isn't appropriate for this subreddit. \n\nI have some data that is number of bacteria found in two different samples. These numbers are large and have 100's - 1000 x differences between the samples.\n\nOn the face of it, there is a huge difference between both samples and yet these differences don't seem to be significant. I'm obviously not looking to shoehorn conclusions into my report but I just wanted to check I was using the appropriate statistical analysis as I am very surprised at the results. \n\nThe experiment was conducted in triplicate, the data isn't normally distributed and I conducted a Mann Whitney U test on the samples. I have added some example numbers to show the kind of difference I am talking about. \n\nSet 1 3000, 100, 30.\n\nSet 2  3000000. 400000. 40000.\n\nAny help would be super appreciated, I'm definitely not asking anyone to do the analysis but any guidance as to what test I could undertake would be greatly appreciated. \n\nThanks a lot! \n\n\n\n\n",
        "created_utc": 1534927822,
        "upvote_ratio": ""
    },
    {
        "title": "MyStatLab help",
        "author": "bodyinspectorG",
        "url": "https://www.reddit.com/r/AskStatistics/comments/99a7k3/mystatlab_help/",
        "text": "so i paid for the code and supposedly that allows me to use an online textbook but i cant for the life of me find how to get the book once i logged in.\n\ni am using \".mathxl.com\" to get to it btw since the regular mystatlab link doesnt load for me.",
        "created_utc": 1534911127,
        "upvote_ratio": ""
    },
    {
        "title": "What is the appropriate comparison to do after a mixed effects regression?",
        "author": "NeuroBill",
        "url": "https://www.reddit.com/r/AskStatistics/comments/999qap/what_is_the_appropriate_comparison_to_do_after_a/",
        "text": "I am recording data from individual brain cells. From each cell I sample the size of the inputs to the cell. Each cell has many inputs (\\~10,000), so for each cell I record many samples (100). Then I record from from many cells. Each cell does not have the exact same size inputs, i.e. the size of the inputs of each cell is drawn from a different population. Finally, these cells come from two different populations of brains: health brain or sick brains. The hypothesis I want to test is, are the means of the sizes of the inputs different between the sick and the health brains different. (The size of the inputs to each cell is log-normally distributed, so I log the values before fitting the model)\n\n​\n\nThe data looks \\*like\\* this\n\n[https://pastebin.com/QTK05UR7](https://pastebin.com/QTK05UR7)\n\n​\n\nI believe I have to perform a mixed effects linear model because I have recorded many events from a single subject (cell) which are therefore not independent, so cell needs to be random effect, and healthy/sick is the fixed effect.\n\n​\n\nSo in R I perform a mixed effects model, with cell providing a random intercept\n\n​\n\n`model.full = lmer(amp ~ healthy + (1|cell_id), data=d)`\n\n​\n\nHowever, now I want to know whether the healthy coefficient significantly effects size of the inputs. Is the correct thing to do fit this model:\n\n​\n\n`model.null = lmer(amp ~ (1|cell_id), data=d)`\n\n​\n\nAnd then perform ANOVA to compare the models?\n\n​\n\n`anova(model.null, model.full)`\n\n​\n\nThanks",
        "created_utc": 1534906860,
        "upvote_ratio": ""
    },
    {
        "title": "Creating a 128 person double elimination tournament bracket and need help making sure the consolation bracket is randomized as much as possible.",
        "author": "cakefraustin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/998u7x/creating_a_128_person_double_elimination/",
        "text": "I apologize if this is the wrong place to post this, but I’m not sure who else would be willing or able to answer it. \n\nThe runs in the main bracket are numbered top to bottom; 1-64 in round 1, 65-96 in round 2, and so on. On the consolation side, I have L1-L64 top to bottom. They compete, 32 are eliminated with their second loss. 32 more, the losers of runs 65-96, are brought over to compete with the winners of round one of the consolation side. To prevent duplicate match ups, I flip these, bringing the loser of run 65 (L65) in on the bottom of the bracket, and working up to L96 at the top. The next 16, the losers of the winners bracket round 3, are again “top to bottom”, but start in the middle, so L105-112 is above L97-L104. \n\nThe question is, what comes next? Do I just go top to bottom, bottom to top, and flip the halves again?",
        "created_utc": 1534899376,
        "upvote_ratio": ""
    },
    {
        "title": "Appropriate Variable for ANOVA Test?",
        "author": "notevelvet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/997cw4/appropriate_variable_for_anova_test/",
        "text": "For my assignment I need to conduct an ANOVA test using Spps. \nThe independent variable needs to be “weekly consumption of alcohol” I’m struggling to find an appropriate dependent variable from my choices. I’ve tried to run the test with marital status, and separately region of residence. But I’m not sure that they are relevant, I’m thinking life satisfaction may be better suited or household income.\n\nAny help would be great thanks! ",
        "created_utc": 1534887847,
        "upvote_ratio": ""
    },
    {
        "title": "Testing the significance of thousands of varaibles",
        "author": "deathdonut",
        "url": "https://www.reddit.com/r/AskStatistics/comments/996j8e/testing_the_significance_of_thousands_of_varaibles/",
        "text": "I have a fair amount of health claims data with (ICD10) diagnosis codes.  Each of these claims can have multiple diagnosis codes associated with them and the first 3 digits tend to correlate into a specific category of diagnosis.  Let's call each of those 3-digit combinations \"prefixes\".  Each individual can have multiple claims.\n\n\n\nI would like to look for prefixes and combinations of prefixes that indicate higher claims risk, but we're talking about 1,700 prefixes (alpha-numetic codes), hundreds of thousands of individuals and millions of claims.\n\n\n\nRunning a multiple linear regression seems pretty worthless with so many coefficients even before running into stumbling blocks like trying to pivot out the prefixes into over 1000 columns.\n\n\n\nI have the data in SQL server and access to R and excel.  Is there a good way to look for statistically significant prefixes or combinations of prefixes?\n\n\n\nAny ideas would be appreciated.\n\n\n\n\n\n",
        "created_utc": 1534882095,
        "upvote_ratio": ""
    },
    {
        "title": "What's the best way to reduce the space required to store measurement data?",
        "author": "kutuzof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9956kv/whats_the_best_way_to_reduce_the_space_required/",
        "text": "I tried posting this on /r/AskMath but it's been downvoted right off the bat so I'm trying again here because maybe this is a better fit:\n\nI've got a tool that takes measurements at regular intervals. For example once a minute it measures and records an absolute value.\n\nThis data is then displayed as a line chart showing the measured values over time. After about 7 days of recordings my database is roughly 60% full. For recent days is the high resolution (1 / min) very useful however for older data it would be enough if I reduced the resolution to 1 recording per 15 minutes or even 1 per hour.\n\nMy question is what is the best way to do this? I currently see two option:\n\n1) I just delete the data points in between and the result is as if I had originally been measuring at 1 / 15 minutes all along.\n\n2) I delete the data points in between but first I averaged the values and I now store the averaged value in the one data point I'm keeping. For example I delete the data from 00:01 to 00:14 and store the average value in the data point at 00:15, which I keep.\n\nThe advantage of 1) is that the data is 100% accurate whereas the data in 2) is essentially \"faked\" because at that exact moment the measure in the database was not in fact the actual value.\n\nThe advantage of 2) is that if I have peaks or values between two points that I'm keeping those will be reflected in the averaged value. Whereas with 1) I would lose that information and on the whole my diagram would now be less accurate with respect to the actual reality.\n\nMy goal is essentially to have data that visually more than anything is similar to the diagram I had at the higher resolution rate.\n\nI'm definitely in over my head, any ideas?",
        "created_utc": 1534872728,
        "upvote_ratio": ""
    },
    {
        "title": "Why do dating sites like Tinder limit users who swipe right excessively?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/993z5v/why_do_dating_sites_like_tinder_limit_users_who/",
        "text": "[deleted]",
        "created_utc": 1534864373,
        "upvote_ratio": ""
    },
    {
        "title": "[Cluster Analysis] Can I use Hierarchical Cluster Analysis on Binary Data?",
        "author": "Chocobuny",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9930xz/cluster_analysis_can_i_use_hierarchical_cluster/",
        "text": "Heya  \n\nSo I have a dataset of two groups, 30 in one group and 30 in the other. Each of these people have been rated on a 10 item scale which is binary, either they score on the scale or they do not. From looking at the data I find that most people score on at least 2 variables (generally they score on 2 or 3 variables, and the rest are 0). I want to know if this is meaningful, if the people in the dataset can be grouped based on their scoring. I believe this means that I want to do a cluster analysis, but I'm a bit hazy about the implementation.  \n\nI've got the file ready to go in SPSS, I've selected Heirarchical Cluster Analysis, but in the method I'm a bit unsure. I've heard that for \"Cluster Method\" the best one to use for binary data is \"Between-groups linkage\", however in the \"Measure\" section I have selected Binary but have multiple choices such as Squared Euclidean distance, size difference, pattern difference, Jaccard, etc. Would anyone know which is the best to use for this type of data? I can provide more information if needed\n",
        "created_utc": 1534857240,
        "upvote_ratio": ""
    },
    {
        "title": "Best ways to combine normalized exam grades into a composite grade",
        "author": "susato",
        "url": "https://www.reddit.com/r/AskStatistics/comments/990jxn/best_ways_to_combine_normalized_exam_grades_into/",
        "text": "I must compute grades for a large course in which the 4 modules are taught by 4 different instructors whose equally-weighted module exams have wildly different means and standard deviations.   I propose to normalize the exam scores by replacing each student's raw exam score with the corresponding z-value (standard score) for that exam. (not a range based normalization b/c there are always a couple of ultra-low outliers). But I'm confused about two things: a) How can I combine the z-values into a composite distribution that will still have a standard deviation = 1 and b) Will moderate deviations from normality in the raw exam scores seriously undermine the fairness of trying to grade this way?  I'm seeking advice specific to this problem, plus search terms or pointers to relevant examples in books or online.  Many thanks!",
        "created_utc": 1534829996,
        "upvote_ratio": ""
    },
    {
        "title": "Where to get a dataset for risk of death vs age, broken down by cause of death, that also includes historical data?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98zza9/where_to_get_a_dataset_for_risk_of_death_vs_age/",
        "text": "[deleted]",
        "created_utc": 1534824366,
        "upvote_ratio": ""
    },
    {
        "title": "R ARIMA Significance",
        "author": "Kuyi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98y5eh/r_arima_significance/",
        "text": "When I run ARIMA in R I get some values and the standard deviation as an answer, but how can i say if there is a statistical significance or not?",
        "created_utc": 1534804140,
        "upvote_ratio": ""
    },
    {
        "title": "Generalized Procrustes Analysis and Discriminant Analysis?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98y3x5/generalized_procrustes_analysis_and_discriminant/",
        "text": "[deleted]",
        "created_utc": 1534803820,
        "upvote_ratio": ""
    },
    {
        "title": "Q-Q plot, is this a normal distribution?",
        "author": "RuffRyderss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98xlca/qq_plot_is_this_a_normal_distribution/",
        "text": "I searched online and looked video tutorials but I'm still not sure. Would you consider the below data normally distributed? I know the ideal fit in theory would be that most of the points are on the line. However data in the real world can be different. So would like to hear your opinion\n\nhttps://i.redd.it/srnt76ceebh11.png\n\nhttps://i.redd.it/kfq278w1kbh11.png\n\nhttps://i.redd.it/vx0p31j0qeh11.png",
        "created_utc": 1534800000,
        "upvote_ratio": ""
    },
    {
        "title": "Should I use a hierarchical regression? If not, then which test is best?",
        "author": "CeruleanCherry",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98xaoc/should_i_use_a_hierarchical_regression_if_not/",
        "text": "Hi everyone,  \n\n\nI appreciate anyone who takes the time to help me out with this!!!\n\nI am having a very hard time wrapping my head around this. I am doing my master's in psychology.\n\nLong story short. Previous research has associated depression scores to sociometric status (or subjective status) measures. Meaning how 'highly regarded' someone is.  \n\n\nI am testing a theory that says that this association is stronger WHEN someone's current status level is unwanted, when they do not accept this status level.  \n\n\nSo I have the sociometric status scale variable, the depression variable, demographics (age and gender) and a status dissatisfaction variable.  \n\n\nI thought I should do a hierarchical multiple regression (to predict depression scores) and add demographics first, then current status levels AND THEN add the status dissatisfaction variable. I've done this and the addition of the status dissatisfaction variable DOES result in a significant r change.   \n\n\nHowever, what is this telling me? Is it just telling me that status dissatisfaction HELPS predict depression scores OVER status levels AND demographics. If so, that isn't what I want. I want to almost say that dissatisfaction \"mediates\" the relationship between social status levels and depression.  \n\n\nI hope that was clear. I'm very happy to clarify if needed but would really really appreciate some help with this.  \n\n\nThanks so much in advance!",
        "created_utc": 1534797820,
        "upvote_ratio": ""
    },
    {
        "title": "Logistic Regression. Can someone simplify the odds ratio?",
        "author": "desperatesnowelf",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98x2e5/logistic_regression_can_someone_simplify_the_odds/",
        "text": "I cant wrap my head around the odds ratio. I can calculate it and use it in my model, but this way it feels sort of roted. Can someone break it down for me? Thanks in advance.",
        "created_utc": 1534796177,
        "upvote_ratio": ""
    },
    {
        "title": "Confuse",
        "author": "Iamnotdoneyet18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98wyn1/confuse/",
        "text": "Can someone give me a good book on statistics for beginners? I am taking a Statistic 1 class. I am a Psychology major on the Ph.D. level",
        "created_utc": 1534795488,
        "upvote_ratio": ""
    },
    {
        "title": "Difference between post-hocs (Dunnet's LSD and HSD) on 1-way ANOVA and multifactorial ANOVA??",
        "author": "EmmasDragon",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98vbms/difference_between_posthocs_dunnets_lsd_and_hsd/",
        "text": "My study guide for my midterm next week says to know how they're applied in a 1-way independent and a multifactorial ANOVA. \n\nARE THEY NOT APPLIED THE SAME? find the Crit value and compare the means?? What am I missing.",
        "created_utc": 1534784227,
        "upvote_ratio": ""
    },
    {
        "title": "P(X) = P(Y) in joint normality distribution?",
        "author": "mcloffin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98u3ho/px_py_in_joint_normality_distribution/",
        "text": "https://imgur.com/a/oznBgTR\n\n\nGiven X ~ N(0,1) and Y = (2B-1)X, where B~Bern(0.5).\n\n\nHow to show that P(Y &lt;= y) = P(X &lt;= y)?\n",
        "created_utc": 1534775702,
        "upvote_ratio": ""
    },
    {
        "title": "Which test should I use to compare two means",
        "author": "mynameisway2long",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98t9fp/which_test_should_i_use_to_compare_two_means/",
        "text": "I have a group of 20 patients who do a walking test (see how far they can walk in 6 minutes), then undergo a few months of rehab, then retake the walking test. Unfortunately data is incomplete, to the point where there are no matched pairs (say results are only available for the first 10 patients before the interventions and the second 10 patients after the intervention). So I now have 2 sets of results that are not matched but I want to compare them to see if the rehab helped by comparing the means. What test should I use. Assume it is normally distributed and the results are in metres. ",
        "created_utc": 1534769137,
        "upvote_ratio": ""
    },
    {
        "title": "Dixie State University math 1040, intro to statistics, Any advice on staying on top of the work?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98ri5j/dixie_state_university_math_1040_intro_to/",
        "text": "[deleted]",
        "created_utc": 1534749394,
        "upvote_ratio": ""
    },
    {
        "title": "How to obtain normal distribution CDF?",
        "author": "mcloffin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98q6a0/how_to_obtain_normal_distribution_cdf/",
        "text": "Given X~N(0,1), the normal distribution would be 1/sqrt(2pi) * e^(-x^2)/2). How do I obtain its CDF from here (not sure if I can integrate)?",
        "created_utc": 1534735064,
        "upvote_ratio": ""
    },
    {
        "title": "Is it possible for sensitivity, specificity, AUC (c-statistics), and positive/negative predictive value to be higher but the accuracy lower?",
        "author": "Google_-_Ultron",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98q1fd/is_it_possible_for_sensitivity_specificity_auc/",
        "text": "I evaluated two models and one has a higher overall accuracy but each of the aforementioned performance statistics is lower. Is this an error, or is it possible?\n\nEdit: Everything except from the accuracy AND c-statistic/AUC is higher",
        "created_utc": 1534733847,
        "upvote_ratio": ""
    },
    {
        "title": "Another question regarding cross-validation",
        "author": "ConfusionStatistique",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98n9ym/another_question_regarding_crossvalidation/",
        "text": "Hello Redditors,\nA quick follow-up on another post which helped me understand what I didn't understand a little better. My question is the following:\n\nAssuming I want to find the optimal values for two parameters, p1 and p2, for a given algorithm. Note that p1 and p2 are simple parameters, not hyperparameters, and the algorithm is a simple classification algorithm. I perform a 5 fold cross-validation in which, on each training set, I do a search for the optimal p1 and p2 values, which constitutes my training step. I then use these values on the corresponding validation set, and get an error value for that particular setting. I repeat the process for the 5 train/validation sets and end up with 5 sets of optimal &lt;p1, p2&gt;.\n\nNow the standard approach would be to use the best-performing &lt;p1, p2&gt; on an external test set which was not used in the cross-validation, and I cannot wrap my head around why. The cross-validation as well as the generation of the outside test set are, afaik, done using stratified sampling. They are also sampled from one single dataset. The best-performing &lt;p1, p2&gt; was therefore tested on a validation set which wasn't observed during training either, as is the outside test set. So if both are issued from the same data, distributed similarly (w.r.t classes), and unobserved, what more information can be gained from an outside test set than from the original validation set that was used in the CV loop? And moreover, since I am dealing with an extremely small dataset, is that information worth the reduction in data sizes that I would have to endure by removing a hold-out test set in the beginning?\n\n",
        "created_utc": 1534710344,
        "upvote_ratio": ""
    },
    {
        "title": "What are good educational resources for comparing 3 or more groups?",
        "author": "gggb777",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98n5iw/what_are_good_educational_resources_for_comparing/",
        "text": "Looking for intro to intermediate level learning materials for ANOVA and other methods to compare 3 or more groups. I periodically see posts like this asking for helpful resources on various statistics topics and I’m hoping I can find the same in this case as well. ",
        "created_utc": 1534709390,
        "upvote_ratio": ""
    },
    {
        "title": "To regress or not to regress.. That is the question.",
        "author": "Allnaturalstrapon",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98k8qm/to_regress_or_not_to_regress_that_is_the_question/",
        "text": "Dear Stats Gods,\n\nI'm in the middle of writing my MSc dissertation and am ashamed to admit that I'm stuck on something pretty basic. Please bear with me.\n\nI have 3 IVs (all ordinal) and want to show that 2 of them are better predictors of an ordinal DV than the other one. I have run bivariate correlations and found that only 1 of the IVs is significantly correlated with the DV while the others are insignificant. Knowing this, would it make sense to now run a regression with my two hypothesised 'better' predictors on the DV? \n\nI thought it was worth a shot and ran the regression. I found that when I ran the regression with my two 'better' IVs, the model was not significant but one of the IV's betas went up compared to when I run it as a linear univariate regression..\n\nMy mind is all muddled so I may not be making myself clear at all but would greatly appreciate any advice on this! \n\nThanks so much!",
        "created_utc": 1534685390,
        "upvote_ratio": ""
    },
    {
        "title": "Multivariate hypothesis testing for proportions",
        "author": "Optimesh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98k7w1/multivariate_hypothesis_testing_for_proportions/",
        "text": "Hello,\nIt's very common in website optimisation to run A/B tests for conversion rates (e.g. 'buy' button is green by default, would the conversion rate from 'visit' to 'buy' be significantly larger if we made the button orange?).\n\nLet's say you want to test multiple variants - e.g. green (default), orange, blue, purple.\n\n1) Would you use Chi-sqr test, using the proportion of the green button conversions as the expected proportion for the rest?\n\n2) How would you calculate the required sample size for a given desired effect size, p-value and power?\n\nTIA!",
        "created_utc": 1534685157,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing event counts with national averages",
        "author": "duskmars",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98jphi/comparing_event_counts_with_national_averages/",
        "text": "Apologies if this is a basic question, I have been struggling all weekend with selection of my statistical tests, and I have read the sidebar link to no avail. \n\nI have data of length-of-stay, and various comorbidities. I also have data of the national average, adjusted for the same comorbidities. For example, Patient A has comorbidity X and Y, and stayed 7 days. The US average for patients with X &amp; Y, who are admitted for this reason, stay 6 days. I want to compare whether the set of patients I am analysing is significantly different from that predicted by the US average. \n\nAs well as this, I have data of complications. For each admission, these are binary outcomes. However, for the US average data, these are percentages. I.e. Patient B developed Complication R. The US average for patients like B is 42% developing R. Is there any way of analysing whether our complication rate is higher than expected?\n\n",
        "created_utc": 1534679302,
        "upvote_ratio": ""
    },
    {
        "title": "Using probit regressions coefficients to derive probabilities?",
        "author": "jplank1983",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98gypc/using_probit_regressions_coefficients_to_derive/",
        "text": "Sorry if this isn't the right place. I'm reading a paper (here https://www.dropbox.com/s/0cgys2elm1nvmiy/ae2016-2.pdf?dl=0) that uses a Probit Regression. I'm not entirely familiar with how this works. But, I'm wondering if there's a way to use the coefficients from Table 3 to derive the probabilities indicated in Tables 4, 5 and 6. And, if so, how I would do that. Thanks.",
        "created_utc": 1534644842,
        "upvote_ratio": ""
    },
    {
        "title": "Controlling for confounding variables in a test for proportions?",
        "author": "Optimesh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98g58v/controlling_for_confounding_variables_in_a_test/",
        "text": "When running a t-test for difference of means, I saw ([e.g.](https://stats.stackexchange.com/questions/153396/students-t-test-with-a-covariate))  that if you suspect there's a confounding variable you can just run a multiple regression with the suspected confounding variable as one of the IVs, hence it will be 'controlled' in the regression. If the coefficient for the variable you are truly interested in knowing if it has an effect on the DV comes out with a low p-value, then there's an effect. E.g. IV in question is a dummy variable with 1 for 'new drug' and 0 for 'placebo' (control group/baseline).\n\nHow do you control for confounding variables in a test for proportion? Something similar only with a logit model? What's a common approach in practice?\n\nSpecifically asking for problems in the tech world, where you'd like to measure differences in conversion rates (=proportions) with A/B testing.\n\nTIA for your help!",
        "created_utc": 1534637100,
        "upvote_ratio": ""
    },
    {
        "title": "Confused with what stats I should do and normalising data.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/98ezhx/confused_with_what_stats_i_should_do_and/",
        "text": "[deleted]",
        "created_utc": 1534626652,
        "upvote_ratio": ""
    }
]