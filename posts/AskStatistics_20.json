[
    {
        "title": "I need to find an estimate amount of views with this data",
        "author": "rrckie",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yy1hjr/i_need_to_find_an_estimate_amount_of_views_with/",
        "text": "So I have delivered 8,800 videos through my agency this year. I need to know how many views they generated approximately. This is the data I have but not much else:  \n\n\nFEW videos get around 500 views.\n\nMOST videos get around 5,000 views\n\nFEW Videos get 100k views\n\nA couple of videos get 1M+ views.\n\n&amp;#x200B;\n\nCan anyone help me?",
        "created_utc": 1668718889,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can someone do some math for me",
        "author": "wangsmilk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yy0dab/can_someone_do_some_math_for_me/",
        "text": "Can someone do some math for me\n\nThere is this netflix show called perfect match. There are ten girls and ten boys. They all have their corresponding ‚Äúmatch‚Äùand have to get all ten matches to win the prize over the course of ten weeks. What is the probability of them all getting perfect matches 10/10 on the first try. \n\nI tried to do the math but forgot how. Plz help",
        "created_utc": 1668716213,
        "upvote_ratio": 1.0
    },
    {
        "title": "What graph should I be using?",
        "author": "Educational-House382",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxzuae/what_graph_should_i_be_using/",
        "text": "Hi there,\n\nThe data I want to graph is the following;\n\nI have age of the voice the participant hears (adult or child shown as 1 or 2)\n\nI have the level of familiarity the participant has with the imagery (shown as levels 1-5)\n\nand the level of commitment the participant to conserving water ( 1-5)\n\nI need to make sure my graph shows the effect (if any) that both the age and the familiarity has on the commitment level. I have tried out a few different types of graphs and I am just not sure what works best.",
        "created_utc": 1668715018,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dummy variables in an xgboost model",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxzgcu/dummy_variables_in_an_xgboost_model/",
        "text": "Not sure the proper way to code them. \n\n  \n\nI have a variable with 3 categories:\n\n* \"1\"\n* \"2-4\"\n* \"5\"\n\n&amp;#x200B;\n\nthat I want to include in my xgboost model.\n\nShould my variables be coded as \n\n* 1, 0\n* 0, 1\n\nOR \n\n* 1, 0, 0\n* 0, 1, 0\n* 0, 0, 1\n\n?",
        "created_utc": 1668714094,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best way to analyze multiple different time series?",
        "author": "limjimsthegod",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxz823/best_way_to_analyze_multiple_different_time_series/",
        "text": "Hello all,\n\nCurrent working on a project at work where we're essentially trying to explore the relationship between multiple different time series to a reported KPI. We are attempting to do this for multiple KPIs, \n\nSo to simplify we have roughly 50 different KPIs we are trying to explore, and each KPI has roughly 10-15 different time series that we suspect may have a relationship with. We are hoping to explore the relationships between these different time series ( and some lagged versions of these time series ) to get a top-down of view which of these series would be most relevant for modeling purposes. \n\nWhat is the best way to approach this? \n\nI primarily code in python so would appreciate any resources/packages you would use that work on this process.",
        "created_utc": 1668713562,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting principal component analysis with variables",
        "author": "marns16",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxz7yt/interpreting_principal_component_analysis_with/",
        "text": "Hello :)\n\nI am having difficulty understanding PCA's and how to interpret each principal component in relation to my variables. Per my understanding, I use the the Eigenvectors to see which values are the highest for that specific PC and those variables are the ones that correspond? Is this correct or am I completely off in my understanding?\n\nThanks so much!",
        "created_utc": 1668713556,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] longitudinal panel model",
        "author": "majorcatlover",
        "url": "/r/rstats/comments/yxywa6/longitudinal_panel_model/",
        "text": "",
        "created_utc": 1668712845,
        "upvote_ratio": 1.0
    },
    {
        "title": "meaningful stats with a small dataset",
        "author": "Express-Level8511",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxwxil/meaningful_stats_with_a_small_dataset/",
        "text": " \n\nI would like to gather a meaningful confidence level and other statistics from the screenshot below, however, I'm used to large datasets and this small one is throwing me off.. would I use a t-dist instead of norm.dist? I'd like to provide something a business user can understand and of course, is as accurate as can be.",
        "created_utc": 1668708154,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] This is histogram. I'm using it to show distribution of order status. I have no idea how to explain. The question is, what is the distribution of the order status?",
        "author": "Mr_Insomnia_",
        "url": "https://www.reddit.com/gallery/yxugam",
        "text": "",
        "created_utc": 1668702135,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this what really mean the chapman-kolmogorov law or is just a joke?",
        "author": "Surimimimi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxtli1/is_this_what_really_mean_the_chapmankolmogorov/",
        "text": "In a tv show I saw a guy that said that because the chapman-kolmogorov law the safest way to flight in an airplane is wearing explosives, because 2 persons wearing explosives is near impossible to occur, so you avoid a terrorist, is that true or he was joking?",
        "created_utc": 1668700053,
        "upvote_ratio": 1.0
    },
    {
        "title": "What are your chances, at birth, to have at least one child of your own?",
        "author": "cmossoi",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxt3m3/what_are_your_chances_at_birth_to_have_at_least/",
        "text": "[removed]",
        "created_utc": 1668698790,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question regarding logistic regression",
        "author": "Holiday_Snow_2734",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxslw7/question_regarding_logistic_regression/",
        "text": "Thank you for taking the time helping me out! \n\nI have a nominal dependent variable (binary) and i have multiple independent variables. Some of these are binary and some of which is ordinary with several levels. \n\nCan i conduct a multiple logistic regression or should i use a ordinal logistic regression? Since my dependent variable is nominal i could use a ordinary logit model right? \n\nI am completely new in the field of logit regressions.",
        "created_utc": 1668697541,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the difference between AR model and linear regression model?",
        "author": "Worldly-Category-755",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxrt6o/what_is_the_difference_between_ar_model_and/",
        "text": "Is an auto regressive model simply a linear regression with previous repose variable as a new feature?\n\nSuppose I want to use an autoregressive model to predict tomorrow‚Äôs ice cream sales. Other than today‚Äôs sale, can I also put today‚Äôs temperature as another feature into the regression?",
        "created_utc": 1668695479,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bhattchary they a mean can lawyer",
        "author": "Independent-Couple49",
        "url": "https://twitter.com/SaniKhalsa/status/1593237408476119042?t=-ITbxkBH6b6SlllAoQKl1w&amp;s=34",
        "text": "",
        "created_utc": 1668692437,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help choosing appropriate analysis",
        "author": "onofrio93",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxpapo/help_choosing_appropriate_analysis/",
        "text": "Hello evereyone. Statistics noob here (using SPSS).\n\nI've been anallyzing a cohort of 79 patients, assessing whether some clinical-laboratory predictors are associated with a greater likelyhood of developing some outcomes (including mortality, infarction, CNS complications, need of dialysis, etc.).\n\nOut of the 79 pts, 4 developed infarction, with 3 of them dying. No other patient died.\n\nSo it is quite obvious that since out of the 3 patients who died, all of them suffered from infarction, the two events (infarction and death) could be correlated.\n\nThe thing is, I'm having some trouble choosing the right statystical analysis to investigate this. I am aware that the sample is small. Is there any kind of statystical analysis that would still be considered of any value at all?\n\nSo here's my doubts:\n\n\\- Does it make sense using logistic regression even in presence of a very low number of events per outcome (3)?\n\n\\- Is it more appropriate to use a non-parametric test (chi square, Fisher exact test), in order to compare frequency of death between the 2 groups (infarction vs No\\_infarction).\n\nOr else, what should I do?\n\nThanks in advance.",
        "created_utc": 1668689008,
        "upvote_ratio": 1.0
    },
    {
        "title": "Selection of the observation period in Regression Analysis",
        "author": "v33p0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxmrrx/selection_of_the_observation_period_in_regression/",
        "text": "Currently I am working on my thesis, the main focus of which is to focus on a relationship between the change in unemployment (from before COVID-19 to COVID-19 period) and the excess mortality rate (taken as an indicator of COVID-19 severity). Overall, the variables that I want to use in my model are:\n\ny = change in unemployment (2019 - 2020)\n\nx1 = Excess mortality rate (2020)\n\nx2 = Stringency Index (2020)\n\nx3 = Government expenditure on health (2020)\n\nThe question of mine is whether the selected periods of observation for the explanatory variables make sense, taking into account that my response variable is unemployment in 2019 minus unemployment in 2020.",
        "created_utc": 1668680826,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with 2d binning in python histograms",
        "author": "SKYNET__1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxlxfg/help_with_2d_binning_in_python_histograms/",
        "text": "I m a student and I m in a huge trouble, I am trying to visualize a plot of 2 variables x and y the number of data points in x and y are like 100 thousand or similar and the data in is float and almost with 3 decimal places for eg 0.002 or 12.003.\n\nWhen i try to plot it using plt.hist i get weird lines in my plot when i give it different bins sometimes some bins work perfect. I have attached the image of plot and code below pls help. [Like this one is perfect](https://i.stack.imgur.com/8x4tC.jpg) [One line appears here](https://i.stack.imgur.com/bkLXa.jpg)\n\n[More lines appear here](https://i.stack.imgur.com/SCVTk.jpg)\n\nand so on.. **it just happens when i increase binning number in my code like bins =\\[46,46\\] and bins =\\[47,47\\] and so on and then there will be a specific bin number where it would appear okayish and perfect i dont know why this is happening**\n\n    plt.hist2d(df3.Yaxis, df3.YError,bins =[45,45]) plt.colorbar() plt.title(\"EMF OFF X- axis\") plt.xlabel(\"mm\") plt.show() \n\nPls help me explain this\n\nI tried many other libraries in python all have the same problem while plotting. I tried to do the same stuff in matlab and it works flawlessly but pls make me understand why python is like this",
        "created_utc": 1668677776,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to interpret the difference in R^2 between these two models...",
        "author": "empirical-sadboy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxkx02/how_to_interpret_the_difference_in_r2_between/",
        "text": "I have one additive model:\n\nY \\~ X + Z\n\nand another mediation model with the same variables:\n\nX -&gt; Z -&gt; Y\n\n&amp;#x200B;\n\nThe additive model explains more variance in the dependant/outcome variable than the mediation model. Can someone explain how this is the case, and what the implications are for the predictor (X) and 'mediator' (Z)?",
        "created_utc": 1668674150,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Is there a way to calculate internal consistency with a binary outcome and repeated trials?",
        "author": "picturesofyou",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxf4s9/q_is_there_a_way_to_calculate_internal/",
        "text": "I have a AX same/different task. There are 32 different combinations of stimuli. Participants are tested on all 32 combinations three times each for a total of 96 trials or data points per participant. I want to calculate some sort of internal consistency measure, but I'm not sure how to handle the binary outcome and repeated nature of the task. Should I just take a random sample of the 96 so there are only 32 unique trials?",
        "created_utc": 1668655428,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Chi Square or other approach for assessing image model",
        "author": "Colddustmass",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxdpa9/q_chi_square_or_other_approach_for_assessing/",
        "text": " Wondering about the appropriate test to assess the performance of different models. I have an image represented as a 2D array. Each element in the array is the flux or brightness of a given pixel.\n\nAs a second step, portions of the image are masked (pixel value removed) and then I have used a variety of interpolation methods to model potential values over those masked regions. This results in a second 2D array with the same dimensions as the original image. This is the 'model' and represents the background signal behind the masked features. \n\nI would like a way to quantitatively compare the performance of the various models, to make sure for example that I am not over-fitting. Asking around I'm told that a chi square method would make sense but I can't see to wrap my head around if that is really a valid approach and if so how that would be done.\n\nSo the questions are twofold. 1) Can a chi square approach be applied here, and 2) if so, how?\n\nWould the \"observed\" be the flux at a given pixel in the original image and the \"expected\" be the modeled flux? I then use the Chi Square formula summing up the result at each pixel? Null hyphothesis is that \"Model A\" is no different than ground truth and the model with the highest chi square value is the best? Like I said, I'm led to believe by folks smarter than I am that this is the way to go, but I can't seem to wrap my head around it. Appreciate any input as to what I'm not seeing",
        "created_utc": 1668651521,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Uncertainty regarding an appropriate statistical test",
        "author": "BoricuaPerdido",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxdapf/q_uncertainty_regarding_an_appropriate/",
        "text": "I want to conduct a study on patients with diabetes and peripheral arterial disease that have to undergo a transmetatarsal amputation followed by a skin graft. Prior to the surgeries, they will have to receive a lower extremity arterial study (which is typical for these patients) and an ankle-brachial index. I will measure the timeline to full healing after the skin graft and want to gauge the predictive value of each of these two tests to see if an ankle-brachial index would suffice as a preliminary exam in comparison to an arterial study.",
        "created_utc": 1668650431,
        "upvote_ratio": 1.0
    },
    {
        "title": "type 1 error inflation",
        "author": "Pr1nc3ss_P1ckl3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxc8on/type_1_error_inflation/",
        "text": "why is it important to control for type 1 error inflation?",
        "created_utc": 1668647593,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to compare mediators?",
        "author": "empirical-sadboy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yxa8hz/how_to_compare_mediators/",
        "text": "Hey all,\n\nI have an effect of a categorical variable (3 groups) on a continuous DV, and I am testing many mediators of this relationship using SEM path modeling. Is there a way for me to compare each mediator, so that I can say which is the \"best\"? The models each include a different mediator, so the models are not nested and I can't use a Chi-squared difference test.\n\nShould I compare them based on the R\\^2 for the DV? That's what I was thinking of doing...",
        "created_utc": 1668642359,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about the 3 types of Mann-Whitney U for different values of n",
        "author": "PiniataLad47",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yx7gik/question_about_the_3_types_of_mannwhitney_u_for/",
        "text": "Hello\n\nI'm doing a stats module and we're being taught Mann Whitney U. As I understand they teach us there's different ways to perform it for \n\nn&lt;9, n&gt;9, and n&gt;20. These seem a bit weird.\n\nI previously self taught a method where you number all the values you have, account for duplicates, then do some addition for each of those values, and you get a U-value.\n\n[https://www.youtube.com/watch?v=BT1FKd1Qzjw](https://www.youtube.com/watch?v=BT1FKd1Qzjw)\n\nAs seen in this\\^\n\nCan I just apply that method for all values of n or should I learn all 3 methods?",
        "created_utc": 1668635626,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does a Hierarchial Multiple Linear Regression need a Bonferroni Correction?",
        "author": "H4RRY29",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yx6yrv/does_a_hierarchial_multiple_linear_regression/",
        "text": "Thank you.",
        "created_utc": 1668634474,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for a Source of Analytic Bayesian Practice Problems (Not homework help)",
        "author": "helloheyhowareyou",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yx4lu7/looking_for_a_source_of_analytic_bayesian/",
        "text": "I'm pretty sure this doesn't violate the rules as I'm not asking for help with specific class work. If I'm wrong I apologize. \n\nI'm in a 400 level Bayesian inference class. The class is equal parts theory and computational. The computing component is evaluated through assignments and the theory portion is evaluated through exams. I have no issues with the computational aspect of the class; I've been getting nearly 100% on the assignments so far. I (and the rest of the class) did terribly on the midterm exam. Most of the practice problems given to us have focused on the computational aspect of problems solving, and I would like more practice from the analytic aspect. For example, on our midterm we had to identify and work with a gnarly (to an undergraduate) posterior. I want more of that kind of practice. Maybe even just practice identifying unusual forms of the common distributions. For instance, I struggled to identify that \\\\theta x\\^{\\\\theta - 1} is a beta(\\\\theta, 1) distribution. \n\nI have the following textboks: Casella &amp; Berger; Hogg, McKean, &amp; Craig, *Introduction to Bayesian Statistics* by Bolstad &amp; Curran; *Applied Bayesian Statistics* by Cowles; *Bayesian Computation with R* by Albert. I'm open to getting more, if required.\n\nI have asked my prof for additional sources as well.\n\nThanks in advance!",
        "created_utc": 1668629310,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Help Choosing the Correct Method for Forecasting Lagging Data",
        "author": "NotBatman81",
        "url": "/r/statistics/comments/yx1jcp/q_help_choosing_the_correct_method_for/",
        "text": "",
        "created_utc": 1668624870,
        "upvote_ratio": 1.0
    },
    {
        "title": "How could I account for individual variation in a between subjects design? Could I use a mixed effects model and add participant as a random effect?",
        "author": "ifyouseekamy17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywzvgi/how_could_i_account_for_individual_variation_in_a/",
        "text": "I am having some trouble designing my research project. My study would ideally benefit from a within-subjects design, because this would deal with the problem of individual variation, and would also increase statistical power. However, using a within-subject design makes the study too complicated which creates other problems, so I am really trying to find a solution to the individual variation problem. If I were to switch to a between subjects design, would it be possible to account for individual variation by using a mixed effects model and adding participant as a random effect? I know that it is only possible to have a random effect of participant when having within subject designs, so is there any way to do that in a between subjects one? Or would adding random intercepts and slopes do the same thing? I really do not know how to deal with this, and it is really important to be able to account for personal variation due to the nature of the study I am creating... Any suggestions will be helpful! Thanks in advance!",
        "created_utc": 1668619474,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to combine a frequency variable with an orientation variable to form one variable?",
        "author": "greenyouth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywzbnm/how_to_combine_a_frequency_variable_with_an/",
        "text": "I have two variables. The first is the frequency of comparing upward comparisons (comparing self to someone perecieved to be better). This uses a 5 point frequency scale. The second is comparison orientation, or how predisposed one is to compare themselves to others. This uses a 5 point agreement scale. \n\nI would like to combine the 2 variables to form one variable of how predisposed one is in comparing themselves to a person they perceive to be better than them. How would I go about doing this?\n\nThanks in advance",
        "created_utc": 1668618360,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to deal with several significant interactions?",
        "author": "yuzaR-Data-Science",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywyia6/how_to_deal_with_several_significant_interactions/",
        "text": "If my final model has 4 significant interactions, how do I interpret them? This model has then a severe multicollinearity. So, my approach is to take those 4 interactions and make 4 models for each of them for easier interpretation, but the results differ. So, does anyone know how to deal with several significant interactions? And some references / book, where it's clearly written and I can cite. Thanks forward!",
        "created_utc": 1668616717,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical analysis of combined normally and non-normally distributed data",
        "author": "Brilliant_SaIad",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywxjuw/statistical_analysis_of_combined_normally_and/",
        "text": "So, I have got an extensive data set with information on pollution levels in a range of species. The data set looks like something in the table below (but far larger). My aim is to analyse whether the levels of the different pollutants correlate in the evaluated species, and I started off by assessing whether my data was normal distributed or not. I ran a Shapiro-Wilks test on the data, which showed that the pollution data for species C was not normally distributed. I then log10-transformed the data from all species and ran the test again. I again found the data from species C to not be normally distributed.\n\nWhat do I do now? Do I \"give up\" and run non-parametric tests (e.g., Spearman correlation analysis) or is that wrong since the other variables actually are normally distributed?\n\n&amp;#x200B;\n\n|Species|Pollutant X (¬µg/g)|Pollutant Y (¬µg/g)|\n|:-|:-|:-|\n|A|9|21|\n|A|7|32|\n|A|8|19|\n|B|1|3|\n|B|0.5|6|\n|B|1|2|\n|C|2|8|\n|C|1|12|\n|C|3|6|",
        "created_utc": 1668614795,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need some help on factor analysis",
        "author": "Scared-Famous",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yws7jn/need_some_help_on_factor_analysis/",
        "text": "Hi,\n\nI am trying to reduce 7 survey items (which are supposed to represent a construct) into a single variable. My aim is to then use this new variable as a covariate in a OLS analysis. In case it is of relevance to interpreting/accepting the numbers below (like it can be with regression coefficients), I am in the social sciences, and the items measure self-reported teaching activities (sample is teachers).\n\nUsing SPSS, I've conducted a principal component analysis, which yields 3 components like so (sorry about the mess):\n\nComponent Matrix\t\t\t\n\tComponent\t\t\n\t1\t2\t3\nT12A\t,308\t,632\t-,137\nT12B\t,321\t,518\t-,003\nT12C\t,387\t,630\t-,088\nT12D,766 -,306\t-,398\nT12E\t,781\t-,339-,320\nT12F\t,437\t,010\t,750\nT12G,596 -,184\t,511\nExtraction Method: Principal Component Analysis.\t\t\t\na 3 components extracted.\t\t\t\n\nWhen saved as variables, it yields 3 variables, and this is where I struggle.\n\nMy question is: which component, if any, should I use? The factor loadings (I think its called) does not seem ideal in any of the three, especially the last two. Should I weed out A, B, C and F and run a new analysis? And just accept that about half the items does not fit the intended construct? Does one use all three or something? How do I go about this? I would be very thankful for any help at all.\n\nThank you in advance!",
        "created_utc": 1668602157,
        "upvote_ratio": 1.0
    },
    {
        "title": "Clarification on Odds Ratios - Inverse interpretations",
        "author": "monkeydace",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywngs0/clarification_on_odds_ratios_inverse/",
        "text": " Hello, I am reviewing odds ratio and I had question about interpretation:\n\nGiven that my independent variable is \"Overweight\" and my dependent is \"Cancer\" (hypothetical). Where 1 = overweight/has cancer, and 0 = not overweight/no cancer respectively.\n\nI ran a logistic regression controlling for all other variables and got an Odds ratio of 1.24.\n\nI interpreted this as:\n\nIndividuals who are overweight are 1.24 times as likely to get cancer as individuals who are not overweight.\n\nOr in other words: individuals who are overweight are 24% more likely to have cancer as individuals who are not overweight.\n\nWould I also be correct in saying:\n\nIndividuals who are overweight are 0.76 times as likely to not have cancer as individuals who are not overweight.\n\nOr in other words: Individuals who are overweight are 24% less likely to not have cancer than individuals who are not overweight?\n\nIs it accurate to state the inverse in such a way for odds ratios? Thank you for any clarification.",
        "created_utc": 1668585295,
        "upvote_ratio": 1.0
    },
    {
        "title": "please help me answering our reviewer for our long exam about elementary statistics esp. about sampling distrubution. thank you!",
        "author": "Euphoric_pjs",
        "url": "https://www.reddit.com/gallery/ywng53",
        "text": "",
        "created_utc": 1668585230,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should I analyse my data using a glm or a glmm?",
        "author": "VastInevitable7130",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywl52a/should_i_analyse_my_data_using_a_glm_or_a_glmm/",
        "text": "I have a dataset composed of movement data for 35 polar bears. These bears are categorized into 4 groups: adult females, adult males, subadults and females with cubs.\n\nI am building a model to determine what variables affect bear speed. The variables I am including are year, date of sea ice breakup, mean longitude, and the groups described above.\n\nShould I build a glm with group as one of the fixed effects, or a glmm with group as the random effect?",
        "created_utc": 1668577338,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should I just apply to grad program (MS), even if I miss a pre-requisite?",
        "author": "VastDragonfruit847",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywjgq9/should_i_just_apply_to_grad_program_ms_even_if_i/",
        "text": "So for context, I can submit 3 programs in a single application. I really want to go the traditional route because I want to have a foundation training as I come from a CS background. But the other two options (Appplied Stats &amp; DS) assume some stats courses, and are not even funded.\n\nI was wondering if it's okay to fill them? The Applied course lets you make your own major from various departments along a Central theme. It seems like a nice option but just has 4 stats courses in total (2 being Into to Mathematical Statistics). DS is okay but again has other courses too and it's more of a professional program when I feel I would be going for research programs(or maybe a PhD later).\n\nI was wondering if I should apply anyway? The nothing-to-lose mindest, but do I? üëÄ Will it affect my chances(MS STATs- traditional)? What if there's no seperate committees?(UW Madison) for Master's",
        "created_utc": 1668572261,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this an inappropriate measure of dispersion using the Gini coefficient?",
        "author": "giscard78",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywixp4/is_this_an_inappropriate_measure_of_dispersion/",
        "text": "I have a set of neighborhoods in one county. In 2010, there were 5,000 households participating in a program across 200 neighborhoods, some of which had zero and others which had many. The Gini coefficient for 2010 was 0.25. The years between 2010 and 2020, changes to the program made and were *intended* to disperse households across more neighborhoods. In 2020, there were 6,000 households participating in the program across the same set of neighborhoods. The Gini coefficient for 2020 is 0.32. \n\nTo me, this says that the households are dispersed into fewer neighborhoods, the opposite of the intent of the changes. Is there any error with this situation? I realize there are other measures of global dispersion and spatial autocorrelation (eg Global Moran‚Äôs Index).",
        "created_utc": 1668570738,
        "upvote_ratio": 1.0
    },
    {
        "title": "Misuse of the CLT",
        "author": "CarelessZebra1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywiqfq/misuse_of_the_clt/",
        "text": "The Central Limit Theorem says that if we have a sequence of random samples, the distribution of their sample means converges in distribution to a normal. However, one of the underlying assumptions is that the random samples are drawn from populations with finite means and standard deviations. So the CLT says nothing about samples drawn from populations that have some sort of pathological distribution (like a Cauchy or a Pareto w alpha less than 1).  \nDoes this sort of thing happen in real life? Are there examples of people misapplying the CLT to situations where the underlying populations don't have means or standard deviations?  \n\n\nThanks for your thoughts!",
        "created_utc": 1668570174,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting differences in coefficient and se for different cases",
        "author": "sillypelin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywhxko/interpreting_differences_in_coefficient_and_se/",
        "text": "Hello. \nI‚Äôm working with panel data in Stata with a bunch of dummy variables and got the value of my coefficient of interest and it‚Äôs robust standard error. I then ran it again, but this time for just one year. Then again (for t,‚Ä¶, T) but with a fixed effect. The values for the coefficient and se for the three cases are given below. How do I interpret this difference in values in general? I can provide context and the study in question if it‚Äôs necessary. \n\n(coeff., se)\n(0.0334979, 0.0018111)\n\n(0.0337106, 0.0056216)\n\n(0.0351337, 0.0263726)",
        "created_utc": 1668567951,
        "upvote_ratio": 1.0
    },
    {
        "title": "Flip a coin X times to represent the roll of a 1dY Dice/Chance",
        "author": "MrTheWaffleKing",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywhft2/flip_a_coin_x_times_to_represent_the_roll_of_a/",
        "text": "Is there an algorithm for converting multiple 1/2 chances into 1/Y chances?\n\nFor example, I can flip a coin, heads is A, tails is flip again. Heads is B, tails is C. As you can tell, A has a 50% chance while B and C only have 25% chance.\n\nIs there any algorithm like this to successfully convert odds into other odds? clearly 1/2 to 1/4 or 1/8 is easy, but I cannot think of a way to do nondivisible numbers.",
        "created_utc": 1668566625,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to ensure a Bayesian credibility interval with 100% probability is exactly at one point?",
        "author": "ChungusZilla",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywendf/how_to_ensure_a_bayesian_credibility_interval/",
        "text": "[removed]",
        "created_utc": 1668558956,
        "upvote_ratio": 1.0
    },
    {
        "title": "An \"appreciative\" t-test?",
        "author": "slammaster",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yweh2w/an_appreciative_ttest/",
        "text": " I'm reviewing a grant (that is otherwise well written) and this one word is confusing me\n\n&gt;An **appreciative** t-test and rank-sum tests may be performed to determine the statistical difference between the groups\n\nDoes anyone know what an appreciative t-test is? I don't think it's anything, but I want to make sure, as it is their proposed analysis for the primary outcome.",
        "created_utc": 1668558436,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Theoretical unfair coins",
        "author": "6uvn0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywd3w5/q_theoretical_unfair_coins/",
        "text": "So I know that its possible to take a large number of coin flip samples and use them to say whether its likely that a coin is fair or not, but considering that the definition of an unfair coin is more or less anything that isn't fair I would think that it's possible to consider a hypothetical unfair coin that satisfies certain conditions that we decide (like flips are dependant). \n\nEx. coin that each flip alternates sides i.e. HTHTHT...HTHT... \nEx. coin that each even number flip decides the result of the next flip to be the opposite i.e. HT HT TH...HT TH... \n\nWhen flipping these unfair coins the number of heads to tails approaches a 1 to 1 ratio, so the statistical test involving the binomial probability density function would determine that this coin is very likely to be fair, however we know this is not the case.\n\nMy plan for determining something like this would be to consider the distribution of 2-flips, in which each option (HH, HT, TT, TH) has an equal probability of 0.25 in a fair coin. So in the first example since the frequency of flipping HT is a 1 at any sample size, it's very likely that that coin is unfair. Increasing the 2-flips to an arbitrary n-flips seems like it would take alot of resources especially if we consider all of them simultaneously.\n\nIs there a name for this method or is it just something thats common sense to do in these sorts of problems? Also, are there any well known methods to find unfair coins like this, with or without considering infinite time and resources? If so, I would appreciate any information about them!",
        "created_utc": 1668554948,
        "upvote_ratio": 1.0
    },
    {
        "title": "Power/sample size with multiple raters",
        "author": "cwm84",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywa8bx/powersample_size_with_multiple_raters/",
        "text": "Hi all, does anyone have a good resource to help me figure out how to determine the number of raters required in a study where test scores are being scored by *N* number of raters where we have an estimate as to the reliability between raters? I know that increasing the number of raters would decrease the number of subjects required; however, at what point might one decide to use only 1 rater (or 2 vs. 3, etc.). Any help would be appreciated!",
        "created_utc": 1668547945,
        "upvote_ratio": 1.0
    },
    {
        "title": "Questions about the calculations of chi-square values in the paper ‚ÄúThe Melodic Arch in Western Folksongs‚Äù",
        "author": "MusicologyStudent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ywa6qb/questions_about_the_calculations_of_chisquare/",
        "text": "Hello everyone.\n\nI am reading this paper ‚ÄúThe Melodic Arch in Western Folksongs‚Äù written by David Huron.\n\nHere is the paper: [http://www.music.mcgill.ca/\\~ich/classes/mumt621\\_20/papers/huron96melodic.pdf](http://www.music.mcgill.ca/~ich/classes/mumt621_20/papers/huron96melodic.pdf)\n\nIn brief, the paper statistically proves that arch-shaped melodies (first go up then go down in pitch) are of typical in western folk songs.\n\nThere are some parts about chi-square tests that I struggle to understand.\n\nSpecifically, here are the questions:\n\n1. In page 10, why is the chi-square value equal 6244.1?\n2. I don‚Äôt know how the chi-square values in Table 2 are calculated.\n3. In page 13, why are the chi-square values equal 74.78 and 21.82?\n\nI have managed to figure out my 1st question, and the detailed calculations are shown here.\n\n| Category|Observed Frequency|Expected Frequency|(O-E)\\^2|(O-E)\\^2/E|\n|:-|:-|:-|:-|:-|\n| Concave| 3496|17422\\*0.50   = 8711| 27196225|3122.055447|\n| Convex| 13926|17422\\*0.50   = 8711|  27196225|3122.055447|\n| Total| 17422|||6244.111|\n\n&amp;#x200B;\n\nHowever, I am still struggling with my 2nd and 3rd questions.\n\nIt will be very helpful to my studies if someone can provide insights on how to calculate the chi-square values.\n\nThanks a lot.",
        "created_utc": 1668547842,
        "upvote_ratio": 1.0
    },
    {
        "title": "Comparing specific variable values between two groups.",
        "author": "Rlphyg",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yw8g0y/comparing_specific_variable_values_between_two/",
        "text": "Hello, just trying to work this out - forgive me if it is in the wrong place. \n\nI have collected a set of data pertaining to frequent visitors to a clinic over a year - each visit/record includes a variable for the primary reason of the visit (categorised as cardiac, respiratory, etc).\n\nI have created a new variable with two groups - heavy users who have 5-9 visits each year, and super heavy users who have 10+ visits each year. Overall there are around 900 separate visits within these, accounting for 100 individuals, split at 80:20 heavy:super. The records themselves are split at 60:40, heavy:super. Individuals often have separate visits for different reasons. \n\nI would like to somehow compare/contrast these groups and each primary visit reason, so as to see/predict that if you are in a certain group you are more likely to be visiting for a specific reason (e.g. heavy users are more likely to be respiratory, or cardiac if you are a super heavy user), or if there is no difference between groups and visit reasons. \n\nHow would this best be achieved? As an example, this study shows what I would like to do using odds ratios (granted there are 3 groups rather than two) but I am unsure how to go about doing so or if that is the best way or if it is even possible:\n\n[https://sjtrem.biomedcentral.com/articles/10.1186/s13049-019-0624-4#Fig4](https://sjtrem.biomedcentral.com/articles/10.1186/s13049-019-0624-4#Fig4)\n\nI am using SPSS. Many thanks.",
        "created_utc": 1668543764,
        "upvote_ratio": 1.0
    },
    {
        "title": "What kind of statistic test? thanks in advance!",
        "author": "Cheap-Judgment-2375",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yw82ax/what_kind_of_statistic_test_thanks_in_advance/",
        "text": "I am working on a project and wondering what kind of statistical testing I can do using the data. I‚Äôve calculated the avg number of ER visits before and after enrolling in a health promotion program at 30, 60 and 90 days. There is a percent decrease at each interval but not sure what kind of test to do from here.\n\nExample:\n30 days - 10 patients\n\navg # of ER visits before program: 16\nAvg # of ER visits after program: 8\n\n16/10 = 1.6\n8/10 = 0.8\n\n50% decrease in ER visits after enrollment \n\nI‚Äôll take any tips I can get to go in the right direction! Thanks :)",
        "created_utc": 1668542869,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Can a mediator influence the direct effect?",
        "author": "ApricotImmediate9114",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yw5wzp/q_can_a_mediator_influence_the_direct_effect/",
        "text": "For my masters I had a linear regression in my first experiment, where I had 2 dummies present who each had a marginal effect. Now that I introduced a mediator, the direct effect became highly significant and I can't really work out why. Anyone have an idea where I might have made a mistake?",
        "created_utc": 1668537844,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to ensure confidence intervals have zero error?",
        "author": "ChungusZilla",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yw4dox/how_to_ensure_confidence_intervals_have_zero_error/",
        "text": "I'd like to make my intervals have zero error and want perfect estimates for my data. How would one go about doing this?",
        "created_utc": 1668534392,
        "upvote_ratio": 1.0
    },
    {
        "title": "I just watched this youtube video and I'm wondering, how did he find the number of coin flips it would take to satisfy the first 2 goals/conditions? Here's the video link: https://youtu.be/XTcP4oo4JI4",
        "author": "FireWolf133",
        "url": "https://i.redd.it/kv02kwp6y60a1.png",
        "text": "",
        "created_utc": 1668533314,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Can you show statistical significance on the frequency of a single variable?",
        "author": "Arrhythmania",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yw1aaa/q_can_you_show_statistical_significance_on_the/",
        "text": "I‚Äôm new statistics and trying to get my head around them as part of my masters. I‚Äôve come across some research that looks at the prevalence of anxiety amongst students and offers a percentage of students affected by anxiety. Is there any way to show statistical significance of the number of students with recorded anxiety? Or would you need another data set to compare against, such as the general public? Same for any frequency of results in a quantitative survey, another example being the number of participants who agree with a statement. Any advice would be great!",
        "created_utc": 1668528041,
        "upvote_ratio": 1.0
    },
    {
        "title": "Scaling mean and standard deviation",
        "author": "yonac42778",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yw12k6/scaling_mean_and_standard_deviation/",
        "text": " Let's say weekly demand for a product has a mean of 10 and standard deviation of 2.\n\nIf I want to keep a four-week supply, can I say that that the mean is 40 and the standard deviation is 8?",
        "created_utc": 1668527621,
        "upvote_ratio": 1.0
    },
    {
        "title": "SPSS Statistics v. 29 - MAC OS Ventura M2 chip",
        "author": "LSDM1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvz3f5/spss_statistics_v_29_mac_os_ventura_m2_chip/",
        "text": "Sorry guys &amp; girls you are my last hope. IBM doesn't offer anything helpful and to live-chat you need an account.  \n\n\nMy Uni has an SPSS licence for students for free, however, I am thinking about upgrading/switiching to an Macbook air m2 chip (and with that ventura, I guess). Does anyone know if SPSS 29 (only version uni has) will run on this setup?  \n\n\nMany, many thanks!",
        "created_utc": 1668523149,
        "upvote_ratio": 1.0
    },
    {
        "title": "Please take time to take this survey. This is for class and will be considered by Starbucks. Thank you for your time!",
        "author": "breitbach15",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvz1gr/please_take_time_to_take_this_survey_this_is_for/",
        "text": "[https://docs.google.com/forms/d/e/1FAIpQLSfaGbr-bWPUGH7L4M7YJKtjhn7x660covAAvkCqJiQI1Voyng/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSfaGbr-bWPUGH7L4M7YJKtjhn7x660covAAvkCqJiQI1Voyng/viewform?usp=sf_link)",
        "created_utc": 1668523022,
        "upvote_ratio": 1.0
    },
    {
        "title": "Test for difference of individual relative proportion and group proportion",
        "author": "OktavianAugustus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvpo9v/test_for_difference_of_individual_relative/",
        "text": "Hi everyone, \n\nI work at a cancer registry and am currently stuck with the following problem: I analyze the data quality of reporting institutions and individual doctors reporting from within those institutions. I have a binary indicator for a certain aspect of data quality, and each individual scores a relative proportion on that (for example: 60% fully complete reports). In one institution we have several individuals with differing amounts of reports. An example table could look something like this:\n\n|ID|Amount of reports|Proportion of complete reports|Average of Institution|Total number of reports in institution|Difference in proportions|\n|:-|:-|:-|:-|:-|:-|\n|1|2000|0,6|0,6|9000|0|\n|2|3000|0,7|0,6|9000|0,1|\n|3|2500|0,5|0,6|9000|0,1|\n|4|1000|0,2|0,6|9000|0,4|\n|5|1500|0,55|0,6|9000|0,05|\n\nI now want to know, whether all in all the relative proportions of individuals differ significantly from the average of the respective institution. None of the usual tests seem to fit. Do you have any idea how to go about something like this? Anything is highly appreciated!\n\nPlease forgive a possibly unspecifc question title, I did not really now how to put it any better ;)\n\nThank you so much for your time!\n\nMaximilian",
        "created_utc": 1668496003,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics Help",
        "author": "Designer-Buy9564",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvofim/statistics_help/",
        "text": "Looking for someone who‚Äôs fluent in statistics. Learning it for the first time and really struggling.\nThanks",
        "created_utc": 1668491726,
        "upvote_ratio": 1.0
    },
    {
        "title": "In K nearest neighbor models, why does a smaller k value mean a larger variance and lower bias, instead of the other way around?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvlfti/in_k_nearest_neighbor_models_why_does_a_smaller_k/",
        "text": "Like, isn't the idea of higher bias what happens when we consider less information about the training set?  For example, all else equal, a linear regression model will be more biased than a quadratic regression, because the latter considers more information, in the sense that it , right?  It's a more complicated model and, if I understand correctly, more complicated models tend to be less biased but have higher variance.  I tend to think about it like this: overfitting/using a model with high variance is like missing the forest for the trees, while underfitting/using a highly biased model is like zooming out so far you can't even tell the forest is made of trees anymore.\n\nSo, if I choose a larger k value, say, k=10, that's using more information classify each value, than if I used, say, k=5.  So why should k=10 give me a larger bias instead of a smaller one?  \n\nI feel like it's maybe related to the idea of sample size, in that the k nearest neighbors to some point are sort of like a sample of convenience and larger sample tend to give more accurate results.  But that's really only true with random/pseudorandom samples and only when we're sampling from a population, so I'm not sure I'm even on the right track thinking in those terms.",
        "created_utc": 1668482595,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is the number of days discrete or continuous?",
        "author": "Z_Gunner",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvinxn/is_the_number_of_days_discrete_or_continuous/",
        "text": "I'm writing a report and I'm measuring the number of business days it takes for a document to be written every month. I've been looking online but I'm not sure if I should report this variable as discrete or continuous. I'm also calculating the average time it takes for this document to be written (by looking at all the months). What would it be in this case?",
        "created_utc": 1668474982,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to apply statistics to DNA genealogy research?",
        "author": "px6657jq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvhjef/how_to_apply_statistics_to_dna_genealogy_research/",
        "text": "Hello group,\n\nI would like to share with you a spreadsheet I have been working on for a couple of years. In my family tree, my paternal great-grandfather has been a complete brick wall, and with the help of a 1st cousin once removed (closest living ancestor to my paternal great-grandfather); I believe I have isolated his lineage. The question I am trying to answer is: Who are the biological parents of my paternal great-grandfather? \n\n&amp;#x200B;\n\nUsing several DNA genealogy sites, I have a list of 63 matches I believe are part of this lineage. In case you don't know anything about DNA or genealogy, relatedness is measured in centimorgans (cM). As you can see on this spreadsheet, my daughter is on top sharing 3,437cM and then the amount goes down rather quickly. \n\nWhat are some statistics functions I can use to better understand this data? How would standard deviation apply to this data set? How can I crank out some useful statistics information? I would appreciate any help. Thank you!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4cz81o53e00a1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=67d5299cda3c441751450830726be6d01554dbea",
        "created_utc": 1668471923,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to apply statistics to DNA genealogy?",
        "author": "px6657jq",
        "url": "https://i.redd.it/sv149dhrd00a1.png",
        "text": "",
        "created_utc": 1668471815,
        "upvote_ratio": 1.0
    },
    {
        "title": "Get CAGR from monthly data and forecast sales",
        "author": "tena_city",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvgthe/get_cagr_from_monthly_data_and_forecast_sales/",
        "text": "I have sales data for 17 months and I can calculate the month-over-month growth rate percentage. I was required to calculate the CAGR using these 17 months. Then, I know from MoM growth rate I can get CMGR, is it the same as CAGR? How can I get the CAGR? and how can I forecast future months sales using either MoM or CMGR or CAGR?",
        "created_utc": 1668470063,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it correct to say that overfitting corresponds to a high variance, while underfitting corresponds to a high bias?",
        "author": "dcfan105",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvftqi/is_it_correct_to_say_that_overfitting_corresponds/",
        "text": "I just want to make sure I'm correctly understanding the idea of bias-variance trade-off and how it relates to over/underfitting:\n\nIf I understand correctly, the idea of bias-variance trade-off is that if a model has too many parameters (e.g. a high k value in k nearest neighbors, a lot of coefficients in a polynomial regression, etc) then it will likely perform well on the training data (i.e., have a low in-sample error rate) but perform poorly on the test data.  That would correspond to having a high variance and would be an example of overfitting, yes?  Or, in simpler terms, overfitting is essentially \"missing the forest for the trees\", considering too many details of the data that happens to be in the training set, and missing the big picture relationships.  That results in a model that may give wildly different and completely inaccurate results for out-of-sample data because the model is picking up on patterns that are specific to the training set and don't exist in the relevant population.  That is, an overfit model give results that greatly _vary_ between the train and test data and hence it corresponds to a high _variance_.  Is that right?\n\nIn contrast, if the model has too few parameters (e.g. a particularly low k value, or maybe using a linear regression when a quadratic regression is clearly more appropriate, etc), the overall error rate (in sample and out of sample error combined) will be higher, but the two error rates will be more similar.  Or,  to be more specific, if we consider, say, only 1 or 2 factors when the relevant relationship actually has, say, 4 important factors, our model is leaving out important information, and so the factors we _do_ consider seem more important than they are.  In other words, the results are _biased_ towards the parameters (or, maybe \"parameter\" is the wrong word, since bias-variance trade-off applies to non-parametric models as well as parametric ones, but then I'm not sure what the correct word is) we did include.  That results in a model that performs more _similarly_ on the train and test data, but that doesn't perform _well_ on either of them.  Correct?\n\nAnd the idea of the tradeoff is that we don't typically want a model that fits _either_ extreme, but rather, we ideally want to minimize both bias and variance, as much as is reasonably possible.  But the problem is that decreasing one tends to increase the other, so we have to balance them, trying find a model somewhere in the middle, that isn't overfit _or_ underfit.  Right?",
        "created_utc": 1668467505,
        "upvote_ratio": 1.0
    },
    {
        "title": "how to calculate linearized rate",
        "author": "toniowned",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvecg7/how_to_calculate_linearized_rate/",
        "text": "hello, I am approaching to write a scientific paper and my tutor asked me to calculate the linearized rate of certain complications following a surgery. To give an example: I have a cohort of for example made by 350 patients followed along a period of time, of these 20 developed the complication one at 2 years, one at 4, etcc and I should calculate then this linearized rate but I can't figure out how:\n\ncan someone help me",
        "created_utc": 1668463957,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sampling Technique Method",
        "author": "Haskz-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yvboxd/sampling_technique_method/",
        "text": "Hi all,\n\nI'm super interested in an article about electoral reform in Canada, but I cant seem to identify the sampling technique used. Here is the closest I could find to a reference about how they achieved their sample. \"To test the impact of the reading level of information provided, 474 student participants were recruited from an Ontario college through their instructors, who allowed the researcher or the researcher‚Äôs assistant to administer the experiment (using traditional pen-and-paper texts and surveys) during the final 20 minutes of a class during the fall semester of 2018. The classes were diverse in their levels (ranging from first year to fourth year) and discipline (including classes in nursing, communications, business and policing).\"\n\nWhat type of sampling technique would this be? I can provide the link below.\n\n[https://www.cambridge.org/core/journals/canadian-journal-of-political-science-revue-canadienne-de-science-politique/article/abs/abcs-of-electoral-reform-the-impact-of-reading-levels-on-knowledge-interest-and-opinion/A301B782EF74D2E92C69ECD0080D6387](https://www.cambridge.org/core/journals/canadian-journal-of-political-science-revue-canadienne-de-science-politique/article/abs/abcs-of-electoral-reform-the-impact-of-reading-levels-on-knowledge-interest-and-opinion/A301B782EF74D2E92C69ECD0080D6387)",
        "created_utc": 1668457867,
        "upvote_ratio": 1.0
    },
    {
        "title": "I'm a little rusty with stats. if I have group a and b being tested pre and post intervention, what test should I use to analys the data?",
        "author": "Feeling-Comedian-751",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yv9nc2/im_a_little_rusty_with_stats_if_i_have_group_a/",
        "text": "",
        "created_utc": 1668453684,
        "upvote_ratio": 1.0
    },
    {
        "title": "Power Curve Simulation Question",
        "author": "LionsBSanders20",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yv5mn6/power_curve_simulation_question/",
        "text": "This question is related to the OP [here](https://www.reddit.com/r/AskStatistics/comments/yqxb9m/requesting_guidance_on_calculating_an_n/?utm_source=share&amp;utm_medium=web2x&amp;context=3).\n\nI've decided to move forward with a data simulation for this problem but I'm having trouble understanding how best to code the random variables and what the outputs for 'n' ultimately mean.\n\nSo to back up a bit, my colleagues want to better understand the effect that 4 binary variables (Operator, Location, Method, &amp; Instrument) have on a continuous response Y, and they're also interested in if interactions of those variables come into play. They want to know what 'n' is needed in order to have sufficient statistical power, where 'n' equals the number of total data points in the dataset.\n\nThe premise of the model looks like this:\n\n Response \\~ Beta0 + Beta1\\*Operator + Beta2\\*Location + Beta3\\*Method + Beta4\\*Instrument + error\n\n**Question 1**. In coding this simulation in R, the data sim for the variable is currently coded as follows:\n\n`X = rbinom(sample_size, 1, 0.5)`\n\n`) %&gt;%`\n\n`mutate(Y = effect*X + rnorm(sample_size, mean = 0, sd = 3))`\n\n`model &lt;- lm(Y ~ X, data = tib)`\n\nThe way I understand this is that this example is the instance where we simulate and model each of the 4 independent variables individually. The resulting output, therefore, is the total number of data points each variable needs to have associated with it. So if the calculated 'n' = 300, then each binary level of that variable needs to have 150 data points in the set. Is this correct?\n\n&amp;#x200B;\n\n**Question 2**. I'm not sure I'm coding this correctly to account for the full model + interactions. As I said above, the project team is interested in all 4 variables and their interactions. So wouldn't the full model actually be - \n\nResponse \\~ Beta0 + Beta1\\*Operator + Beta2\\*Location + Beta3\\*Method + Beta4\\*Instrument + **all 2nd order interactions** \\+ error?\n\nAnd wouldn't the related code in R actually look as follows? - \n\n`X1 = rbinom(sample_size, 1, 0.5),`\n\n`X2 = rbinom(sample_size, 1, 0.5),`\n\n`X3 = rbinom(sample_size, 1, 0.5),`\n\n`X4 = rbinom(sample_size, 1, 0.5)`\n\n`) %&gt;%`\n\n`mutate(Y = effect*X1 + effect*X2 + effect*X3 + effect*X4 + rnorm(sample_size, mean = 0, sd = 3))`\n\n`model &lt;- lm(Y ~ X1 + X2 + X3 + X4 + X1*X2 + X1*X3 + X1*X4 + X2*X3 + X2*X4 + X3*X4, data = tib)`\n\nMy understanding here is that the above simulation accounts more accurately for the combined effect all variables, and their interactions, have on the continuous response. If this is true, how do I interpret the resulting 'n'? For example, if the ideal 'n' from this calculates to 800, do I divy this 800 evenly among the 4 variables and say each variable needs 200 data points, therefore each level of the binary variables need 100 data points? Or is there a different interpretation?",
        "created_utc": 1668445538,
        "upvote_ratio": 1.0
    },
    {
        "title": "T-Test help",
        "author": "lawdsybe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yv3t2a/ttest_help/",
        "text": "Hey y‚Äôall! So I have not done statistics in 7 years so I‚Äôm struggling. I want to compare voters‚Äô registered party to actual party vote. So I have the number of registered democrats in each parish and the number of votes for democrats. For example: Acadia Parish has 18811 voters registered as democrat and 5638 actual votes for democrat.\nIs there a way to use a t-test for this? Or do y‚Äôall recommend something else?\n\nThanks in advance for yalls help!!!",
        "created_utc": 1668441957,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which test to do - Student's t-test or Welch's t-test?",
        "author": "SurgMMA",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yv1nrz/which_test_to_do_students_ttest_or_welchs_ttest/",
        "text": "I'm trying to see if there is a statistically significant complication rate between 2 populations using prism. 1 cohort has 30 complications in a sample size of 1000. The other cohort has 30 complications in a sample size of 200.\n\nWhat's the optimal test to see if there's a statistically significant difference between the 2 cohorts?",
        "created_utc": 1668437526,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to analyze with regression using the variables I have?",
        "author": "FatherOfGoldfinch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yv0qw1/how_to_analyze_with_regression_using_the/",
        "text": "Hello,\n\nI've been searching a lot for an answer but I can't seem to find a clear one and the more I looked the more confused I got, I hope someone can help me.\n\nMy independant variable (IV) is the Satisfaction With Life Scale (SWLS), which is a 5-item Likert scale of 1-7, and my dependant variable (DV) is the weekly phsyical activity in minutes. \n\nHow do fit the SWLS scores to work for a regression analysis? Can I dummy code it or is that only for dichotomous variables? \n\nAlso, what type of test do I need if I want to analyze the inverse: Physical activity as IV and SWLS as DV?\n\nThanks in advance.",
        "created_utc": 1668435549,
        "upvote_ratio": 1.0
    },
    {
        "title": "Measure similarity between bivariate distributions",
        "author": "denitaden",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yv0e10/measure_similarity_between_bivariate_distributions/",
        "text": "Hello! What methods would recommend for deciding if two bivariate histograms are similar? The data represents latitude and longitude coordinates, and I want to see if the two sets actually represent the same area or not. Thank you!",
        "created_utc": 1668434757,
        "upvote_ratio": 1.0
    },
    {
        "title": "HELP!! HYPOTHESIS TEST QUESTION",
        "author": "Extra-One4008",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yuy5x7/help_hypothesis_test_question/",
        "text": "I need some help from the math gurus out there! \n\nI am trying to do a hypothesis test for the following: \n\nGetting the influenza vaccine or wearing masks. Which method do most healthcare professionals at hospitals believe is more effective in preventing the flu? \n\n1. I am aiming to prove the claim that most healthcare professionals (i.e at least 75%)  believe that getting the influenza vaccine is more effective than wearing masks, in preventing the flu. \n\n2. The population I will be collecting the data from will be healthcare professionals in various fields. I will focus on healthcare professionals who work at G.S hospital, on my unit and that are on my personal Facebook page. It will be in the form of a survey. What questions would be appropriate to ask so as to make sure my results are QUANTITATIVE in nature and not qualitative?\n\n3. What hypothesis test would be appropriate? \n-Two sample hypothesis test for proportions (I‚Äôm thinking this would be appropriate?) \n-Two sample hypothesis test for means\n- Goodness of Fit hypothesis test\n- Test of Association hypothesis test\n\n4. Can someone clarify what my variables would be or I could use in this case? They need to be variables that are QUANTITATIVE. \n\n5. What would be by null and alternate hypotheses? \n\nAny help would be greatly appreciated.",
        "created_utc": 1668429578,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] Exercise time series acceleration data",
        "author": "JasonH06",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yux29w/question_exercise_time_series_acceleration_data/",
        "text": " I have a dataset which tracks bench press exercise with 25 repetitions across time. I want to use this data for classification purposes.\n\nI am confuse whether the data is stationary or not. I am having trouble wraping my head around whether the time series has seasonal or cyclic characteristic.",
        "created_utc": 1668426808,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with economics and stats quiz today. I'll pay if I need to",
        "author": "DepartmentSavings329",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yusx8p/help_with_economics_and_stats_quiz_today_ill_pay/",
        "text": "Hi guys if any of you are professional I would like to ask for help with a quiz today. My module is techniques of economic analysis. 2nd year university. The quiz will take place at 13:00 gmt and will end at 14:00. I will be able to send the paper then and there. I have an example paper also. Please DM/message me As Soon As Possible!",
        "created_utc": 1668414570,
        "upvote_ratio": 1.0
    },
    {
        "title": "Science meets Industry - Data science research internship",
        "author": "ProMetronics-Dev",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yusbgv/science_meets_industry_data_science_research/",
        "text": "[removed]",
        "created_utc": 1668412211,
        "upvote_ratio": 1.0
    },
    {
        "title": "Data elasticity of demand to the price germany",
        "author": "Extension-Engineer82",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yus45h/data_elasticity_of_demand_to_the_price_germany/",
        "text": "Hello, I am carrying out studies on the elasticity of demand at the price of Germany on products such as sweet and savory biscuits, etc.  of germany in recent years.  The problem is that I cannot find reliable data on the internet\n Can anybody help me?\n Thank you",
        "created_utc": 1668411401,
        "upvote_ratio": 1.0
    },
    {
        "title": "is it normal to get high variance once you do interaction?",
        "author": "Naj_md",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yur6b6/is_it_normal_to_get_high_variance_once_you_do/",
        "text": "WHenever I add an interaction term, I get higher variance. This time I used VIF to evaluate my model, nad I'm not sure what is happening. \n\nWithout interaction\n\n&amp;#x200B;\n\nhttps://preview.redd.it/flyzwe6v3vz91.png?width=1237&amp;format=png&amp;auto=webp&amp;s=668559bc6232e1a0e4098eb8d88e3e33309d80f8\n\nwith interaction\n\nhttps://preview.redd.it/wa2ro3lr3vz91.png?width=1347&amp;format=png&amp;auto=webp&amp;s=c0d8187f48c70be2a7bd632577086df03f832eb7\n\nShould I proceed forward? any other way to evaluate more suitable transactions?",
        "created_utc": 1668407939,
        "upvote_ratio": 1.0
    },
    {
        "title": "Fantasy Football Probability of Getting A Score",
        "author": "obex42",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yupy6n/fantasy_football_probability_of_getting_a_score/",
        "text": "Okay! Here is the situation I am in, and I would love to know how to approach this.\n\nBased on a set of numbers (scores from a player from the last 8 weeks) I was wondering how to calculate the likelihood they score 14.54 points tomorrow, which is below their average. I know the sample size is small, but I am going down a hole and would love to know how to do this.\n\nIs this a conditional probability problem? It seems to be that I should be able to calculate how many standard deviations 14.54 is away from the mean? I have no idea how to approach this, and would love some help!",
        "created_utc": 1668403791,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the difference between a distribution, probability distribution, frequency distribution, probability distribution function, probability mass function, and a probability function?",
        "author": "ferrrrrrral",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yupliw/what_is_the_difference_between_a_distribution/",
        "text": "I'm just totally confused with all of the different terms and how different sources use them differently.\n\nI always thought a distribution was outcomes or observations mapped to their respective frequency (frequency distribution) or probability (probability distribution). But then I see visualizations of distributions called PDFs, PDFs called PMFs, PDFs called probability distributions, distributions called probability distributions, etc.",
        "created_utc": 1668402633,
        "upvote_ratio": 1.0
    },
    {
        "title": "Binary Dataset Question",
        "author": "Educational-Town2295",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yuoe6y/binary_dataset_question/",
        "text": "Hi there, sorry if this is basic or silly but I am not the most fluent in statistics and have to get this down for my thesis. I am trying to \"predict variables associated with a mass shooter.\"\n\nI have a dataset that is analyzing mass shooter. I have my independent column as mass shooter (which is all 1, because in the dataset I just have data from mass shooters). All of the other columns are dependent variables such as history of mental health, history of abuse etc, with a 1 representing positive history or 0 representing not present. \n\nHow can I analyze my dataset in R, what model (I think logistic regression) should I use, and how do I analyze the P values for all of these variables (50+ variables)",
        "created_utc": 1668398842,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Question] Hi! Please help if you are able, I joined my class a little late so I would appreciate any help!",
        "author": "Dizzy-Incident-4588",
        "url": "https://i.redd.it/7a23bw6j1uz91.jpg",
        "text": "",
        "created_utc": 1668395047,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I put a range of values on a T-Test Calc. for 2 independent means?",
        "author": "Mundane-Lettuce-292",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yukzkj/can_i_put_a_range_of_values_on_a_ttest_calc_for_2/",
        "text": "So if my data is like 30.-4.0 and 1.0-2.0 (collecting gpa) can I put these ranges into the treatment groups and press enter or would the data I receive be wrong? I just wanted to make sure before I put this into writing. Thank you!\n\nExample\n\nTreatment group 1:\n\n3.0,4.0\n\n1.0,2.0\n\n1.0,2.0\n\n2.0,3.0\n\nTreatment group 2:\n\n3.0,4.0\n\n3.0,4.0\n\n3.0,4.0\n\n1.0,2.0\n\n[https://www.socscistatistics.com/tests/studentttest/default2.aspx](https://www.socscistatistics.com/tests/studentttest/default2.aspx) \n\n&amp;#x200B;\n\nI'm so sorry if my question makes no sense\\^",
        "created_utc": 1668389065,
        "upvote_ratio": 1.0
    },
    {
        "title": "question about hypothesis testing for two populations",
        "author": "saucersss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yui5t7/question_about_hypothesis_testing_for_two/",
        "text": "How do i write the null and alternative hypothesis of a problem for two populations when the question is \"is there enough evidence in these samples to declare that operation a takes significantly longer to perform than operation b?\"\n\nIs it like\n\nH0: ¬µ1&gt; ¬µ2\nHa: ¬µ1&lt; ¬µ2",
        "created_utc": 1668381577,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mixed models and geostatistics : interpretation",
        "author": "Riies_black",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yud8wn/mixed_models_and_geostatistics_interpretation/",
        "text": "Hi, I thank in advance all persons who can help me to understand my problem more clearly.\n\nlet's say we have 20 air quality sensors that crisscross a city. Resulting in N observations (each sensor has n\\_k observations), each observation z is linked to the coordinates where it was collected.\n\nfirst, I use a gaussian process to model my data: I assume that the hidden process Y (which represents the dispersion of the pollutants) follows a gaussian process with an exponential function as covariance function, and that the observations of the sensors are just noisy independent observations, classical in geostatistics : \n\nZ/Y \\~ N( Y,  **œÉI** )\n\nY \\~ N( ¬µ , R( ùû° ))\n\nWhere  ¬µ  can be a constant or a regression with other covariate collected by the sensors ( ex : proximity to highway ) ,and R(ùû°) is an exponential function of the distance depending on several parameters ùû°,  and **œÉ** represent the mesurment error. Resulting in the marginal model : \n\nZ \\~ N( ¬µ , R( ùû° ) + **œÉI**)\n\nThis model work just fine, but does not take into account the fact that my data comes from unreliable ,different sensors. \n\nI want my model to be able to have several intercepts in Z for each sensor k, this is where i came across linear mixed models : \n\nZ\\_k/Y \\~ N( Y + a\\_k,  **œÉI** )\n\nY \\~ N( ¬µ , R( ùû° ))\n\na\\_k \\~N( 0 ,  **Œ≥**  ) \n\nI understand that the a\\_k must be centred at 0, or they will not be distinguished from  ¬µ , and the model will be unidentifiable.\n\nIn the first model (geostatistics), the resulting covariance matrix  **Œ£**  is equal to R(xi,xj) for any input not in the diagonal, and R(xi,xi) + œÉ in the diagonal. \n\nIntegrating out of the a\\_k in the second model results in a more complex covariance matrix  **Œ£** , and we do not directly observe the a\\_k. \n\nHere  **Œ£**  is equal to R(xi,xj) for every 2 observations not from the same sensor, R(xi,xj) + **Œ≥** for every two observations from the same sensor and R(xi,xi) + **œÉ** \\+ **Œ≥** on the diagonal. (is this correct ?) \n\nMy understading is that **Œ≥** capture all the variation within the sensors.\n\nMy questions are :  1) Is my second model correct?\n\n2 ) We have seen that the **Œ≥** present between 2 observations from the same sensor in the covariance matrix, can be interpreted as different intercepts in the model for each sensor k, centered on 0. What is the interpretation if, instead of putting the same parameter, I put **Œ≥\\_k**  in the covariance matrix for each sensor k, thus adding (k-1) parameter (each sensor has its own variance).\n\n3) Same question if i simply put different  **œÉ\\_k** on the diagonal for each observation from sensor k.\n\n \n\nIf anyone has a lead or an idea, a paper or a book chapter that could help me, I would be very grateful.\n\nThank you.",
        "created_utc": 1668371592,
        "upvote_ratio": 1.0
    },
    {
        "title": "Do the coefficients reported from a linear probability model represent the log odds ratio?",
        "author": "Gorillasdontshave",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yub1rz/do_the_coefficients_reported_from_a_linear/",
        "text": "Basically, as the title says, do the coefficients resulting from the linear probability model represent the log odds ratio? I have has read that they represent the marginal effect, but it's not clear to me.",
        "created_utc": 1668367125,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should I remove this huge outlier from my birding data? These were flocks that flew over me. More info in comments",
        "author": "DauphDaddy",
        "url": "https://i.redd.it/k3ws61oq3rz91.jpg",
        "text": "",
        "created_utc": 1668359491,
        "upvote_ratio": 1.0
    },
    {
        "title": "How does standard deviation of rates scale?",
        "author": "mxk912",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yu3p55/how_does_standard_deviation_of_rates_scale/",
        "text": "Say if we have a random variable velocity with mean 600 m/minute and standard deviation 60 m/min (and is normally distributed - not sure if this matters or not). If I want to change units to m/s then the mean is trivially just 600/60 = 10 m/s. In my head the standard deviation should also just be 60/60 = 1 m/s in these different units. The problem is when I am running a simple Monte Carlo sim to look at distance travelled when I use increments of 1 second with the m/s values the variance in the results is much smaller than running simulations with increments of 1 minute. I feel like this should be trivial to solve using basic definitions but somehow I can't wrap my head around this issue.",
        "created_utc": 1668350822,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Is ‚Äúregression adjustment‚Äù in Causal Inferences achieved by running ANCOVA?",
        "author": "sonicking12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yu33fl/q_is_regression_adjustment_in_causal_inferences/",
        "text": "",
        "created_utc": 1668349469,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I use biserial correlation for repeated measures? (i.e., comparing a value at 2 time points)",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ytyy3l/can_i_use_biserial_correlation_for_repeated/",
        "text": "",
        "created_utc": 1668338726,
        "upvote_ratio": 1.0
    },
    {
        "title": "Correlation in Liner Mixed Model",
        "author": "Worldly-Category-755",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ytwvdf/correlation_in_liner_mixed_model/",
        "text": "I‚Äôm confused about how linear mixed model eliminate correlations between subjects. \n\nSuppose I want to predict people‚Äôs marathon time with their ages as predictors. What I do is to put there ID as cluster labels. I‚Äôm confused how putting them into separate clusters eliminates the correlation. In one cluster, isn‚Äôt that person‚Äôs current age depend on that person‚Äôs previous years age? It looks like we only filtered out the independent predictors, for example person A‚Äôs age and person B‚Äôs age are independent but we won‚Äôt put them into one cluster.\n\nSecondly, suppose clustering do eliminates the correlation. If I want to look at the random effects within the location cluster. Is it okay if I pool runner‚Äôs data into location clusters? Won‚Äôt it reintroduce the correlations and violates the assumption of independent?",
        "created_utc": 1668332275,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bayesian/frequentist question",
        "author": "Timely-Ordinary-152",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ytwc0w/bayesianfrequentist_question/",
        "text": "Have I understood this correctly: Frequentist approach can always reach the same conclusion as Bayesian in terms of confidence intervals can always end up being the same as credible intervals. So the difference between the approaches is that we can use bayes theorem and change P(A given B) to P(B given A) and through that not have to redo the whole experiment if we want to be more confident, but rather update the old experiment with more data? My knowledge is limited to statistics used to create these intervals and basic use of random variables, so the question should be taken within this context.",
        "created_utc": 1668330466,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I sample pairs (or triplets, etc.) from a list without replacement, in proportion to some value?",
        "author": "--MCMC--",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yto05w/how_do_i_sample_pairs_or_triplets_etc_from_a_list/",
        "text": "And by how, I mean a quick, elegant, flexible way to do this -- eg is there a distribution that has this property that's well understood? \n\nBasically, suppose I have an even numbered list of length *n*. Maybe *n* = 6, and my list is the set {a, b, c, d, e, f}.\n\nI can generate `n! / 2^(n/2) / (n/2)!` unique sets of pairs in this list, eg of the form {ab, cd, ef}, or {ac, df, be}.\n\nIf I wanted to sample uniformly from these 6! / 2^3 / 3! = 15 possible sets of pairings, I could do so in many easy ways, eg randomly permute the order of the list and take successive sets of 2.\n\nBut suppose I want the probability of some pair appearing in my sample of pairs to be proportional to some value. For example, say I'm hosting a social event and have a 1-on-1 icebreaker activity planned, and separately know where all my attendees fall on some similarity or distance metric, say the extent to which they prefer Star Trek to Star Wars on a continuous scale in (0,1). I want conversation to flow, so I want to maximize some aggregated \"mainstream sci-fi conversational compatibility\" score of pairs of participants -- maybe I want to minimize the average distance between members of a pair on their (0,1) value across all pairs (eg, seat 0.1 w/ 0.1, 0.9 w/ 0.9, and 0.5 w/ 0.5). But I also believe in destiny, so I want to sample pairs from a distribution centered on this value, rather than finding some strict optimum. How do I do that?\n\n(actually, I think the optimum would just be sorting the pairs and then taking successive sets of 2 -- any possible \"trade\" would reduce average compatibility. This would also generalize to triplets, quadruplets, etc.)\n\nI can imagine lots of iterative algorithms that would let me do this, e.g. pick two from a discrete uniform, check their distance, sample a bernoulli random variable with probability f(distance), if it's a 1 remove them from the population, if it's a 0 return them to the population and try again, repeat until only 2 people are left. But this potentially leaves me w/ very incompatible individuals in my final pool, which I do not want.\n\nI can also exhaustively list out all 15 sets of pairs, compute their average compatibility within pairs for each set, and sample a set from a multinomial with probability some function of that vector of average compatibilities. But if n = 100, that gives me around  2.7 \\* 10^(78), which would present considerable computational challenge.\n\nI could also write a markov chain to swap members of a pair of pairs with probability proportional to the difference in average distances of the swap, maybe initializing from the optimum and then bouncing around it.\n\nWhat else can I do? Ideally in a way that results in nice, tidy properties, e.g. to be able to make statements of the form \"x units greater distance between elements corresponds to y units lower log-odds of their ending up in the sample set of pairs\".\n\nAlso, if possible, how could I extend this to larger groupings than pairs, like triplets or quadruplets? What if, in that case, I wanted to satisfy some other objective, like the variance between members of a group -- maybe it's a mentoring event with ugrads, grad students, postdocs, and faculty, and I want to seat tables of 6 who are on average the most dissimilar -- not as much opportunity for mentoring if it's all people at the same career stage. IDK what the optimum there is, but it would be easy enough to find numerically -- but is there something else I could do?)\n\nUltimately seems like a pretty straightforward problem, but googling has failed to turn up any canonical solutions!",
        "created_utc": 1668302156,
        "upvote_ratio": 1.0
    },
    {
        "title": "Mnemonic for specificity, sensitivity, precision, recall etc?",
        "author": "cowox93112",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ythtay/mnemonic_for_specificity_sensitivity_precision/",
        "text": "I need help, even after years I keep confusion and having to look up the definitions of those terms... I understand their interpretations and purpose etc., but I am looking for a mnemonic to stop mixing them up.",
        "created_utc": 1668285776,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] exploratory factor analysis with binary variables",
        "author": "majorcatlover",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ytg2gw/q_exploratory_factor_analysis_with_binary/",
        "text": "How do you select the number of factors when you are doing a exploratory factor analysis that includes some binary variables?",
        "created_utc": 1668281250,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I use a chi-square test if I have two independent variables, both categorical &amp; independent-groups, where one of the IVs (gender) has two levels (male/female), and the other IV (treatment) has 3 levels (short length, medium length, long length)?",
        "author": "francescar182",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yte0rn/can_i_use_a_chisquare_test_if_i_have_two/",
        "text": "",
        "created_utc": 1668275986,
        "upvote_ratio": 1.0
    },
    {
        "title": "order of integration?",
        "author": "fiendish_adversary",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ytbjbv/order_of_integration/",
        "text": "hi regarding panel unit root tests, how can i know the order of integration of variables? If a variable is stationary at second difference, does it mean it's I(2)?",
        "created_utc": 1668270188,
        "upvote_ratio": 1.0
    },
    {
        "title": "I got a time series of a discrete values, but this is outside of course scope, how can I transform?",
        "author": "ObligationAntique147",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yta8jb/i_got_a_time_series_of_a_discrete_values_but_this/",
        "text": "Hello, I have a exercise where I have the number of visits in a restaurant , the scope is time series, but GLM time series is out of question. Is there any transform that I can use so I can fit SARIMA models? thinking in something like (number of visits in a day)/(mean) maybe a log of that. Any suggestions? ( not worth any grade, is a challange question)",
        "created_utc": 1668267247,
        "upvote_ratio": 1.0
    },
    {
        "title": "Estimate value in non-normal distribution from other value?",
        "author": "RagnarDa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yt8atm/estimate_value_in_nonnormal_distribution_from/",
        "text": "If I had a measurement x and want to estimate a likely value y in another distribution based on known correlation r and assuming both are standardized I would use y = x \\* r\n\nBut what if the value we try to estimate are not normally distributed and skewed and the mean of one distribution is nowhere near the median which is the assumption here? For example, if I hade a measurement of someones height and given a correlation with wealth I would probably be very off trying to estimate that persons wealth since a averagely long person would likely have a wealth much lower than the average wealth? Should I standardize around the median instead then?",
        "created_utc": 1668262715,
        "upvote_ratio": 1.0
    },
    {
        "title": "Considering a statistics major, anything I should keep in mind?",
        "author": "Classic-Asparagus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/yt1iqf/considering_a_statistics_major_anything_i_should/",
        "text": "The only statistics class I‚Äôve taken so far is an introductory course, which I‚Äôve found to be quite easy, but also very engaging. So I am wondering: for those who have pursued a stats major, how have the more advanced courses differed from the introductory material? Anything else I should keep in mind about this major? Thank you!",
        "created_utc": 1668243568,
        "upvote_ratio": 1.0
    },
    {
        "title": "What do statisticians/data scientists do with SQL and Python and what advantages do these have over R?",
        "author": "hifrom2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ysyun1/what_do_statisticiansdata_scientists_do_with_sql/",
        "text": "  Curious about how SQL and python can be helpful",
        "created_utc": 1668235550,
        "upvote_ratio": 1.0
    },
    {
        "title": "What does a X% correlation (and r/r¬≤) actually mean?",
        "author": "Chigi_Rishin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ysv8cn/what_does_a_x_correlation_and_rr¬≤_actually_mean/",
        "text": "For example, research says that some 30% of lifespan is genetic (is this r or r¬≤ by the way?). What does this actually mean? Assuming an equation can be used to determine lifespan, how would that equation be like? For example, say Lifespan=A\\*B. Say we investigate the graph L x A, while B changes randomly and we don't have any idea what it is. If I graph L x A from 1 to 1000, and B is varying between 2 and 200 (hidden), I get r¬≤ around 0.55. Given that B is hidden, it stops me from finding that A is actually perfectly correlated with L, by the factor B. How to interpret this r¬≤? How does that point me towards finding an equation that fits?\n\nOr say a drug is effective with only 90% correlation. On 10% of people it does not work?\n\nIn the end, correlation is not 1 because we cannot control for all the other variables, so how can we truly find out what is happening, given such imperfect correlation? What if the equation is something like L=log(A)\\*B? Surely the effect of A is diminished, and expressing it in terms of percentages makes little sense.\n\nFeel free to write a lot, thank you.",
        "created_utc": 1668223375,
        "upvote_ratio": 1.0
    }
]