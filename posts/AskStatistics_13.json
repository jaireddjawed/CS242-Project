[
    {
        "title": "I wonder if anyone who has access to Statista Premium can share me with the graph contained in the link below? I shall be much obliged to you.",
        "author": "Bright-Plantain-7940",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zw8532/i_wonder_if_anyone_who_has_access_to_statista/",
        "text": " https://www.statista.com/statistics/663314/gregs-bakery-customer-satisfaction/",
        "created_utc": 1672120289,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help doing this proof",
        "author": "TrovadorAngolano",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zw6cfx/help_doing_this_proof/",
        "text": "I'm confused on solving the following proof:\n\nIf X is a random continuous variable with a CDF F(X) and Y=F(X) what is the CDF of Y?\n\n&amp;#x200B;\n\nThanks guys",
        "created_utc": 1672114635,
        "upvote_ratio": 1.0
    },
    {
        "title": "Textbook Recs?",
        "author": "Ok_Bathroom6659",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zw69or/textbook_recs/",
        "text": "Does anyone know of any good textbooks to refresh my basic stats knowledge? Also, any recs for general business stats textbooks (interested in learning how stats is applied in this context).\n\nFor context, I completed AP stats a couple of years ago and did well but haven't used it too much since. I recently got into some statistics and programming again because I took a Financial Economics class, but that did not require any deep knowledge of those topics. Ended up skating by because the math in this class was easy enough for me to learn without actually remembering too much of how it works.\n\nAny resources or textbook recs are appreciated! Thank you!",
        "created_utc": 1672114397,
        "upvote_ratio": 1.0
    },
    {
        "title": "post hoc test for two way anova?",
        "author": "safarin06",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zw2kkb/post_hoc_test_for_two_way_anova/",
        "text": " hi there,\n\nso i performed a two-way anova test and got my results which are statistically significant. now i want to perform a post hoc test but i am having some trouble. i want to do a tukey's hsd test but i have absolutely no idea how. i've been looking on websites and i can't seem to find anything. is it even possible to do a tukey test from a two way anova test? are there any other post hoc tests i could do? how can i do a tukey test in excel from two way anova data?",
        "created_utc": 1672103629,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the distribution of the average of iids that are normally distributed?",
        "author": "Kamal_Ata_Turk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvy502/what_is_the_distribution_of_the_average_of_iids/",
        "text": "",
        "created_utc": 1672091969,
        "upvote_ratio": 1.0
    },
    {
        "title": "is the normality of the residuals still a valid assumptions in non linear regression?",
        "author": "aveterotto",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvtrge/is_the_normality_of_the_residuals_still_a_valid/",
        "text": "Hi, i  have read that for linear regression there is the assumption that the residuals are normally distributed. Is this assumption still valid in case of non linear regression? the residuals calculated on the evaluation set   through my neural network  shows that they are normally distributed but i was wondering if it still meant something. Is it normality used to estimate the confidence bands?",
        "created_utc": 1672080395,
        "upvote_ratio": 1.0
    },
    {
        "title": "Improve prediction model",
        "author": "Confident-Eye8474",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvriet/improve_prediction_model/",
        "text": "Hi  everyone, I work in a warehouse where we distribute products, but it is   impossible to audit all of them. So we take a sample of around 1.5% of   the orders and from there we take a coefficient of error (-0.11% in  the  image).\n\nWith this we  calculate the total to be paid or collected from the  error. Is it  possible to find a more elaborate or exact model? I have  tried using  the r2 of linear regression but it gives very different  values to the  error that we currently calculate\n\nSample (1.5% of total rows)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3fgi6hzvx98a1.png?width=337&amp;format=png&amp;auto=webp&amp;s=c6e3cccf8b8717e0f718ca93586ee0180b2c4d98\n\nSample of full database\n\nFull Database\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4ast231xx98a1.png?width=337&amp;format=png&amp;auto=webp&amp;s=7c28ab8884f9277878dcc394d0e183cb9f5bdfd0",
        "created_utc": 1672074336,
        "upvote_ratio": 1.0
    },
    {
        "title": "Fitting a straight line to heteroscedastic data with autocorrelation",
        "author": "Ruff-Riff",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvqj2l/fitting_a_straight_line_to_heteroscedastic_data/",
        "text": "Hello all, \n\nI've a question regarding model fitting of some laboratory data.\n\nThe data in question measures how well some laboratory procedure can quantify some target analyte in a sample. My independent variable is thus \"analyte added\" and the dependent variable \"analyte recovered\".\n\nDuring the laboratory work, we prepared two samples ¬´with added¬ª analyte per day and measured our ¬´recovery¬ª. In addition, we had four different levels of ¬´analyte added¬ª.  We did this across four days. This means that the sample results have \"pair-wise\" behaviour. That is, for every day, the two measurement recorded on that day are very similar. However, when you compare all four pairs at some level, the differences between the pairs can differ quite a lot. I added a sketch at the bottom to illustrate the data shape. \n\nDue to conventions in my field, it is customary to report the \"linearity\" of the data. Linearity here meaning that you can fit a straight line through the \"analyte added and recovered\" data. \n\nAfter collecting all the data, I started to work on model fitting, trying to fit a straight line. Firstly, I noted that the data is considerably heteroscedastic, so I needed to apply WLS. After fitting the model, I then applied the Durbin-Watson test to the residuals and found that the data also possesses autocorrelation at lag 1. \n\nCorrelation does make intuitive sense, as if the measurement from one particular day is above the regression line, the other measurement on that day is also likely to be.\n\nNonetheless, I am quite uncertain on how to proceed from here. I have tried several weights for my model. I‚Äôve done 1/X , 1/X\\^2 and 1/s\\^2 . The latter of which I‚Äôve estimated from both the sample data and iterative reweighted least squares techniques. Nonetheless, they all provide approximately the same regression coefficients to a model on the form y = b0 + b1x. Also, data transformation is not an option here.\n\nIn my case, I cannot be satisfied with only unbiased regression coefficients because I also need to use their standard errors in future error propagations. Therefore, my question becomes: how can I fit a straight line to this data such that I get both unbiased regression coefficients and standard errors? Should I add some additional term to account for the correlation? Should I perhaps use a linear mixed-effects model? Any help is very much appreciated!",
        "created_utc": 1672071588,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to analyze the survey results?",
        "author": "BathTimeKevin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvm7kl/how_to_analyze_the_survey_results/",
        "text": "\n\nHello everyone i am a newbie looking for some advice, i did a short online survey with randomized groups, the structure looks like this;\n\nrandomly assign participant to one of the four groups:\n\n-Control Group\n-Treatment 1\n-Treatment 2\n-Treatment 3\n\nin each group participants have to choose between only two options (binary categorical data). \n\nAll treatment groups contain the same nudge but phrased differently, the control group contains no nudge.\n\nThe 4 groups have an almost equal amounts of responses.\n\nMy goal is to analyze if the nudge was effective and to see if there were any notable differences between the treatments. Do you have any advice on statistical test? I am grateful for every piece of advice i can get :)",
        "created_utc": 1672057786,
        "upvote_ratio": 1.0
    },
    {
        "title": "How could neural networks out perform tree models?",
        "author": "Worldly-Category-755",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvj4qy/how_could_neural_networks_out_perform_tree_models/",
        "text": "I feel like neural networks are large linear regressions that is able to capture all interactions and non linear relationships. However, tree models like random forests are also able to capture these relations. How could deep neural networks be better?",
        "created_utc": 1672045000,
        "upvote_ratio": 1.0
    },
    {
        "title": "How would you test Ordinal (Likert; Emotion Questions) and Nominal (Yes/No) Data?",
        "author": "Helpful-Sock-9810",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvef45/how_would_you_test_ordinal_likert_emotion/",
        "text": " Hello, I have conducted a questionnaire that had questions that pertain to emotions (such as Joy, Fear, etc.) that are to be compared with a Yes/No question (also called Dichotomous/Binary or Nominal afaik?).\n\nThe Likert scale ranges from 1 -- Not at all, 2 -- Slightly, 3 -- Moderately, 4 -- Very, 5 -- Extremely.\n\nThe Yes/No is a response to the question ''Would you recommend this . . . to your friends and family?\".\n\nThe goal is to see if, for instance, an increase in Joy leads to an increase in recommendation (word of mouth), essentially if \"Yes\" is chosen as an answer.  \nAnd conversely, if increase in Fear leads to an increase of the answer \"No\". (I don't know if I should mention this but this is for hypothesis testing).\n\nI'm a complete beginner and thought that analyzing categorical variables is going to be more feasible, but it has turned out to not be the case :/  \nWould it be appropriate to use Fisher's Exact Test, considering that the sample size is only 140 and some questions like \"Not at all\" have less than 5 answers?  \nBased on similar questions and answers, I have seen Chi-squared, Fisher's, Spearman's rho, and Ordinal Regression as recommended tests. I really can't attempt regressions as it is far too complex. I'm looking for a simpler solution even if it's not the best option overall.\n\nThank you in advance!\n\nP.S.: I'm aware that this should have been already considered when designing the questionnaire but the project that I'm working on is very limited time wise and a big chunk of time had to be spent on other research as well so it left very little for the survey/analysis :( And I hope this is not against the rules as I'm not asking for help directly.",
        "created_utc": 1672026907,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can I use a ratios for hypothesis in a chi square GOF test?",
        "author": "brianplusplus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvcobt/can_i_use_a_ratios_for_hypothesis_in_a_chi_square/",
        "text": "Here is a pet example to make my question clearer.\n\nI have three countable categories of data: A,B, and C.   I do not know the total number of counts I will end up with but I want to test the hypothesis that the ratio of A:B:C will be 2:5:3.  Can I just do the following (in pseudo code):\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n    o = get_data() # get counts for three groups\n    total_found = sum(o)\n    e = [.2*total_found,.5*total_found,.3*total_found] \n    chisquare(o,e)\n\nOr do I need to exact counts beforehand in order to use chi square?\n\nAlso, is this exactly the situation where I should just use multinational theorem?",
        "created_utc": 1672020957,
        "upvote_ratio": 1.0
    },
    {
        "title": "Outcome probability from multiple conditions",
        "author": "jrdubbleu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvcd6d/outcome_probability_from_multiple_conditions/",
        "text": "If I have, for example, the last 50,000 days of weather conditions:\n\nHumidity %, Temperature, Barometric Pressure, Amount of Precipitation\n\nWhat conditional (I think) probability formula would I use to determine something like, on a day when Humidity is 50%, Temperature is 30C, Pressure is 1000mbar, what is the percent chance of 2cm of precipitation.\n\nI know I have to work back from the amount of precip I want, right? 2cm, and then the total number of days of each condition where 2cm happened, right?",
        "created_utc": 1672019952,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which is better, head-to-head vs individual rating?",
        "author": "elatedate",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvbxmh/which_is_better_headtohead_vs_individual_rating/",
        "text": "Working on a dating app that includes a feed to rate individual photos from random profiles so we can build up a model of what you like in terms of looks.\n\nRight now we just record whether you like/dislike each photo but we're thinking about switching it to a choice of two random photos of different people and you pick which one you prefer. \n\nWanted to get some educated thoughts/opinions on that approach...",
        "created_utc": 1672018528,
        "upvote_ratio": 1.0
    },
    {
        "title": "Regression on Residuals",
        "author": "Safe_Drop6122",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zvbq6j/regression_on_residuals/",
        "text": "I have read (eg. [1](https://besjournals.onlinelibrary.wiley.com/doi/10.1046/j.1365-2656.2002.00618.x), [2](https://gking.harvard.edu/files/gking/files/mist.pdf)) that it is inappropriate to perform regression on residuals (in attempting to estimate marginal regression coefficients) as this procedure will necessarily produce biased estimates. I am confused about whether or not this is always the case or only under certain assumptions and also how it relates to the Frisch-Waugh-Lovell theorem. Any clarification would be great.",
        "created_utc": 1672017862,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to avoid the frustration of learning theoretical statistics?",
        "author": "EmergentPhysics",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zv4v7n/how_to_avoid_the_frustration_of_learning/",
        "text": "I study applied statistics but some subjects are focused on the theory. The problem I have is that I never had subjects like these, and it is making me very frustrated because I don't understand how I am even supposed to study it.\n\nThe lectures and the literature consist of an equation, followed by showing that the equation is equal to something else. Which is equal to something else, which equals something else. And all it tells me is that these values are equal. And while it may be important that they are equal, it doesn't feel that way.\n\nAt the end, it seems like the entire point is to go through a number of long chains all showing that some values are equal to other values.\n\nSo, when I try to study, I encounter two problems. First is understanding why particular value equals another value. And the second problem is, once I actually confirm that one value is equal to another, I am left with a deep sense of \"So what?\"\n\nI am overwhelmed because I don't understand at a most basic level, whether I am simply supposed to understand how one step leads to another and nod along with the satisfaction of knowing that no mistake was made in the process, or whether I am supposed to actually know how we obtained each result.\n\nI don't doubt there is purpose to what we are doing, but I have no idea how to actually learn this. Do I just look at equations and go through them until I can recognize them?\n\nOr is this something to push through and memorize?\n\nDid this happen to you or am I completely misunderstanding the situation I am in?",
        "created_utc": 1671996833,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics question for game grinding",
        "author": "hi_im_frank88",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zv2xo4/statistics_question_for_game_grinding/",
        "text": "Hi all, I‚Äôm playing this game Genshin Impact and wanted to calculate the probability I would get a desired artifact. I‚Äôll try to remove the flavor and reduce it down to a stats problem:\n\nWhat is the probability, when picking 4 balls, of picking specific balls A and B, which each have 7.89% probability of occurring? This is Without Replcement. There are 9 balls with differing distribution (two are 7.89%, five are 10.53%, two are 15.79%).\n\n(If anyone is interested in the specifics, this is asking what the chance a Flower of Life will have both Crit rate and Crit damage)",
        "created_utc": 1671990976,
        "upvote_ratio": 1.0
    },
    {
        "title": "HELP !! any help would be appreciated",
        "author": "sakhrabdelsalam",
        "url": "https://www.reddit.com/gallery/zv2kk6",
        "text": "",
        "created_utc": 1671989845,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should I submit my GRE Subject Math score for PhD in Stats/Operations Research applications?",
        "author": "Last-Essay-521",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zv1g7e/should_i_submit_my_gre_subject_math_score_for_phd/",
        "text": " Hi all,\n\nI am applying to top 10 Ph.D. programs in Stats/OR in the U.S, and I am wondering whether I should submit my GRE subject Math score or not. I got 830 in the test 3 years ago, and my undergrad major is not math. Would this score add any value to my application at all? Thanks in advance!",
        "created_utc": 1671986391,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dice question",
        "author": "Alive-Reaction-7266",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zv0foy/dice_question/",
        "text": "In a game of Monopoly, each player must go round the board twice before being able to buy properties.\n\nHow statistically low are the chances that someone can get stuck on the second go round and keep ending up in jail? I've rolled 3 doubles twice and landed on the 'Go to Jail' square once. My husband and daughter both have properties and my husband is crying laughing. \n\nI feel like the dice may be weighted (they are the pair that came with the game). \n\nHow shit is my luck today? üòÇ",
        "created_utc": 1671983129,
        "upvote_ratio": 1.0
    },
    {
        "title": "is n=2 ok?",
        "author": "Pretend_Shame_2399",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zuv377/is_n2_ok/",
        "text": "Hi there! I write a paper right now and here's the question. I have 3 groups to compare. One of them contain only two observation. And i really can't add at least one more coz i don't have another 1.5 y.o. mice. The other two contatins 4 and 3 (so it's 2, 4, 3). I use the Kruskal-Wallis test as non-parametric criteria for ANOVA. Can i do this? Or i have to take 3 obs minimum for *every* statistical analysis?",
        "created_utc": 1671961753,
        "upvote_ratio": 1.0
    },
    {
        "title": "How many repetitions?",
        "author": "yertipy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zuu6my/how_many_repetitions/",
        "text": " \n\nHi everyone,\n\nI've got this process I'm running which involves some randomness. So in order to get representative results, I run it multiple times and provide averages and intervals of confidence. The process is quite expensive to run, both in terms of time and computing resources. I'd like to find out how many repetitions are enough so I can save on costs while still providing somewhat representative data.\n\nI'm looking for pointers to techniques I could use to achieve this. Can anyone recommend anything? I was thinking of looking into things like ANOVA or cross validation. Would either of these be a good starting point? Or maybe I could do it manually by calculating the intervals of confidence on increasingly large sets of repetitions and see if there's an inflexion point somewhere. But I keep thinking I'm not the first one to need something like that and there ought to be tools out there doing exactly that.\n\nThanks a lot!",
        "created_utc": 1671957655,
        "upvote_ratio": 1.0
    },
    {
        "title": "Non-standard card odds",
        "author": "Sorin_Markov_1947",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zuta24/nonstandard_card_odds/",
        "text": "This is a real-life scenario that's *just* out of my statistics skill level.  Probably pretty basic to anyone who does actual statistics work though.\n\nI have a deck of 60 cards (the extra 8 are jokers).  I need the following to be in the top 7 cards of the deck:\n\n* Three jacks\n* One queen\n* One king\n\nThe other two cards in the top seven are irrelevant, they could be anything.  Obv there are four jacks, queens, and kings in the deck.  What are the odds of getting this scenario?",
        "created_utc": 1671953663,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is the difference between ordered logit models and ordered probit models?",
        "author": "Sewblon",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zuqx84/what_is_the_difference_between_ordered_logit/",
        "text": "I know that they are both for estimating effects of some independent variable on some ordinal dependent variable, as in, some dependent variable where the distance between outcomes 1 and 2 and 2 and 3 are not known, and not known to be the same or different magnitude. But, what is the difference between an ordered logit model and an ordered probit model? In what cases does it matter?",
        "created_utc": 1671944016,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Abstract Algebra or Stochastic Processes",
        "author": "pmorri",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zucpwu/q_abstract_algebra_or_stochastic_processes/",
        "text": "I'm going into my final year and I want to make sure I take the right courses. I'm a Stats and Computer Science major, I've finished all my stats classes and now focusing on the higher level CS classes, for a math elective I have the choice of either Stochastic Processes or Abstract Algebra, I can't take both. Both sound great but Abstract sounds better for CS, also I'm leaning more towards ML engineer for a profession. What's the best choice here?",
        "created_utc": 1671897848,
        "upvote_ratio": 1.0
    },
    {
        "title": "Accounting for incomplete data in time-dependent analysis",
        "author": "wateredmalone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zu8ww2/accounting_for_incomplete_data_in_timedependent/",
        "text": "Hi! I'm working on an analysis of how long it takes my team to close tickets. I have all of the data for tickets we closed in the first half of 2022. For the second half of 2022, we implemented a goal for time to close tickets, and I want to assess whether it has been effective in reducing the time to close them. \n\nThe problem is, I need to run this analysis now, but the second half of 2022 isn‚Äôt complete, so there are still tickets that are open. When I‚Äôm evaluating ‚Äútime to close a ticket‚Äù for H2, how should I account for tickets that are still open, as excluding them presents a huge bias since still-outstanding tickets are the most likely to go against my hypothesis that the goal has been effective in reducing time to close them. \n\nI‚Äôve considered something along the lines of replacing outstanding tickets with a ‚Äútime to close‚Äù that is the nearest standard deviation above what it currently sits at, based on the H1 dataset. However, I was wondering if there‚Äôs some standardized statistical practice to account for this in time series analysis? Thanks!",
        "created_utc": 1671885013,
        "upvote_ratio": 1.0
    },
    {
        "title": "Main 8 Descriptive Statistics | Main descriptive statistics | What are the four descriptive statistics",
        "author": "ayshu12100",
        "url": "https://statisticme6.blogspot.com/2022/10/main-8-descriptive-statistics-main.html",
        "text": "",
        "created_utc": 1671883945,
        "upvote_ratio": 1.0
    },
    {
        "title": "Declustering spatial data and kriging",
        "author": "Alone_Bee_6221",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zu83in/declustering_spatial_data_and_kriging/",
        "text": "I am doing a simple project on a mining dataset and came across some problems:\n\n1. Should I use the declustered values to model the variogram?\n2. Do I apply ordinary kriging to the original or to the declustered values?\n\nAlso, it is worth saying that I applied the cell method for declustering and eventually, as the more densely sampled regions are near to weight 0, I feel like the deposit has changed a lot in terms of continuity. It feels wrong using the declustered values, but in theory, I believe it is correct, as it does penalize the regions which are more densely sampled.",
        "created_utc": 1671881584,
        "upvote_ratio": 1.0
    },
    {
        "title": "MS in applied statistics or data science?",
        "author": "Conscious_Bumblebee8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zu1wfb/ms_in_applied_statistics_or_data_science/",
        "text": "Hello, I‚Äôm currently an undergrad systems engineering and psychology major with a minor in mathematics who has hopes of going to grad school. I want to be a ‚Äúdata scientist‚Äù (quotes because the role can be different titles based on the company). For the longest time I‚Äôve been interested in the computational and statistical aspects of data science, as well as computational social science and behavioral data science. I am also heavily interested in the intersection of social and computational science, but can‚Äôt seem to find a direct degree path for such a career. Through my engineering program, i‚Äôve had experience with R, Python, SAS, MATLABs, and some brief machine learning projects. We‚Äôve also covered research design and experimentation. \n\nI was thinking about what program I‚Äôd like to go for, and for the longest time I was thinking applied statistics. However, I noticed that I myself spend a lot of my time learning the software side of data science that I don‚Äôt completely  from my classes and wanted to expand on my interest in machine learning applications to social sciences.  I enjoy math (specifically statistics), but I just see a pattern in myself right now that i put an emphasis on learning tools outside the theory, which makes me wonder whether an applied stats or data science degree would be the better option (or if there is some alternative?)\n\nI‚Äôve heard some applied stats programs do have a software aspect and it‚Äôs not all theory, but many seem to be lacking in heavier computer/data science concepts. I am also mainly looking for online programs so I can gain working experience though my graduate studies and there seems to be a plethora of options related to data science but they have less ‚Äúhistorical backing‚Äù than those related to applied statistics. \n\nWhat do you all think? Would a computational social science degree even be worth looking into? For those of you have have done either an applied stats/stats masters or a data science masters, what can you speak on the programs? I know it comes down to what I‚Äôm interested in, but where do some of these programs fall short/benefits? What may be differences in career outlooks? \n\nThank You!",
        "created_utc": 1671856920,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] comparing points allocation at 3 different time points, within-subject, what stat test do I need?",
        "author": "scootergrl2010",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zu1obz/q_comparing_points_allocation_at_3_different_time/",
        "text": "I‚Äôm working with a data set sort of like this:\n- participant IDs 1-150\n- at time pt 1, all 150 people allocated points into four buckets (bucket A, B, C and D). Each person had to allocate all 100 points. \n- at time pt 2, all of the same 150 people again allocated a total of 100 points to the same four buckets. \n- at time pt 3, ‚Äú ‚Äú\n-the only thing that differed between time pts was a priming context. \n\nNow I want to assess if that priming context affected the way people allocated points across the different buckets at time pts 1, 2 and 3. I have all the descriptive stats but don‚Äôt know how to assess ‚Äústatistical significance‚Äù. \n\nSo the IV is: priming version A, B or C (which people got in a randomized order)\nThe DV is: average pts allocated to each bucket. \nWithin subjects. \n\nWhat type of statistical test would you use for this? Bonus points if you can give example SAS code. (I‚Äôm also trying to understand what the non-parametric options are, in case the data fail a test of normal distribution).",
        "created_utc": 1671856141,
        "upvote_ratio": 1.0
    },
    {
        "title": "the average of medians or weighted medians for summarizing data?",
        "author": "Inside_Acanthisitta6",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ztu6cr/the_average_of_medians_or_weighted_medians_for/",
        "text": "I am researching the effect of rental prices on the number of homeless people in the U.S. at the community level. Each community comprises different counties within the same state.  The homeless data comes from the HUD point-in-time count with different communities. The median gross rent comes from ACS at the county level and I have the sample size of it for weighing. I am trying to summarize the median gross rent from counties for communities.  Would it be more appropriate to use a weighted average median gross rent or just average median gross rent for communities?",
        "created_utc": 1671833618,
        "upvote_ratio": 1.0
    },
    {
        "title": "Need some advice understanding what kind of t test applies to an unusual data set",
        "author": "Bodark",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zts4af/need_some_advice_understanding_what_kind_of_t/",
        "text": "Hi everyone, this isn't a homework problem and it's really more of a broad question as I tend to struggle with conceptualization, so I'm not sure if this breaks the rules of this sub or not.\n\nSo I am working on a side project for myself and I'm a bit stumped and I'm worried if I'm going about this all wrong.\n\nI have a large population of mortgage holders and their FICO scores that are provided by the three different credit bureaus. The credit-worthiness of the pool of people is calculated (in an aggregate) by taking each person's median FICO, then finding the average of those medians, and then proceeding on with the calculation with that number.\n\nWhat I want to know is, is there a statistical difference between using the mean of all the people's median scores (the way it is done now) or, just using the mean FICO score from just one of the provided credit bureau, and skipping the median part.\n\nA step further would be to take whether or not a person falls delinquent on their mortgage in their future, and then performing logistic regression to determine if the median scores, or the scores from just one of the credit bureaus do a better job of calculating credit-worthiness, or if there's no difference at all.\n\nThanks in advance, and again, I apologize if this is a rule-breaking post.",
        "created_utc": 1671828171,
        "upvote_ratio": 1.0
    },
    {
        "title": "i need some help with sample size calculation",
        "author": "ahmed_fawzy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ztoo3v/i_need_some_help_with_sample_size_calculation/",
        "text": "i am using this equation to calculate the sample size for a study the expected prevalence is a bit confusing for me  \n\nbasically, it's a questionnaire given to a certain group of people who are indicated for a procedure\n\nin P should i use the no of people indicated / total population or no of successful procedure / indicated \n\nhttps://preview.redd.it/tr7hpd6eto7a1.png?width=685&amp;format=png&amp;auto=webp&amp;s=9572e27e91a4153f506cec219da22fcfa0a9c745",
        "created_utc": 1671819071,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does anyone have an intuitive explanation of over-parametrization?",
        "author": "ScaredByStatistics",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ztiv74/does_anyone_have_an_intuitive_explanation_of/",
        "text": "I just ran across a passage about polytomous factors in multiple regression in my GLM textbook which talks about how you can subdivide your data into groups (parallel slopes in this case) and then estimate intercept values to get a vertical translation of the group. \n\nThe passage says it's best to use the intercept alone for your baseline group and to calculate additional gammas for other groups (as opposed to calculating the intercept and a gamma-value for every group) because otherwise, you're calculating more parameters than there are categories in your predictor (intercept + gammas for every category) which would give you a non-unique solution.  \n\nI can't really imagine what that means in an intuitive (or mathematical) sense, and in simple regression, for example, you have one category but also have to calculate an intercept and slope even though you only have one category. \n\nIn my mind, it seems like this concept is adjacent to the concept behind degrees of freedom, but to be honest, I don't fully understand that as much as I should yet either. \n\n&amp;#x200B;\n\nI just switched studying statistics from a different field in science and am embarrassed about not being great at math yet, so please be gentle with me, and thanks so much in advance for the help if you have time to answer!",
        "created_utc": 1671808489,
        "upvote_ratio": 1.0
    },
    {
        "title": "Name for a situation where the probability for an event is very low but a test for the event accuracy is very high",
        "author": "PushingPesbians",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ztgmyw/name_for_a_situation_where_the_probability_for_an/",
        "text": "I've been trying to remember the name for a certain situation but can't remember now can I seem to Google it. I was hoping someone here could help me.\n\nIt's mostly applied to medical statistics and forensics but has other applications, in this scenario I will use cancer.\n\nThere is a type of cancer that is very unlikely to occur in humans, a 0.001% chance to occur. Thankfully, there is a way to test for this cancer. This test is 99% accurate.\n\nMost people would assume, due to the high probability of the test being accurate that a positive result means you almost certainly have this cancer. However, due to the exceptionally low chances of having this cancer it is more likely that this test is instead inaccurate.\n\nI keep jumping to Bayes Theorem but I swear there is another name for this. Can anyone help me with the name and explanation as to why this holds (if it even does?)",
        "created_utc": 1671804936,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Time variable in longitudinal data - fixed or random effect? dummy code or not?",
        "author": "permanenthouseguest",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zter3d/q_time_variable_in_longitudinal_data_fixed_or/",
        "text": "I am running a mixed effects model to see if goal domain (categorical variable) predicts goal pursuit (ordinal data, measured on likert scale). I may also construct a time-series plot. I have some experience with lmer, but have never done a time-series plot.\n\nParticipants tracked their goal pursuit score and related goal domain 1 to 4 times a day, for 28 days. They started on different dates, so the range of 28 days is not fixed.\n\nDo I code time as a fixed or random effect in this case?\n\nCurrently, my data shows the date and time of each response in one column, e.g. \"18/11/22 12:41\". I'm not sure if I should leave it as it is, or dummy code the dates to days 1 to 28 and timestamp to times 1 to 4. If I do the latter, there would be a column for day and another for time - how do I then tell R to take both as one random/fixed effect?",
        "created_utc": 1671800649,
        "upvote_ratio": 1.0
    },
    {
        "title": "Open-source Best Practices in Responsible AI",
        "author": "Data_Nerd1979",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ztdrkf/opensource_best_practices_in_responsible_ai/",
        "text": " Discover and learn about open-source best practices in Responsible AI for free! Gift yourself with the best in Data Science Training with Ai+ today.\n\n[https://app.aiplus.training/courses/open-source-best-practices-in-responsible-ai?utm\\_campaign=Learn%20AI&amp;utm\\_source=Community&amp;utm\\_medium=Community&amp;utm\\_content=Ai%2B%20free%20Violeta%20Misheva](https://app.aiplus.training/courses/open-source-best-practices-in-responsible-ai?utm_campaign=Learn%20AI&amp;utm_source=Community&amp;utm_medium=Community&amp;utm_content=Ai%2B%20free%20Violeta%20Misheva)",
        "created_utc": 1671797411,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to best represent ratios and avoid undue influence in the following example",
        "author": "graphophobicbyproxy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/ztbzpt/how_to_best_represent_ratios_and_avoid_undue/",
        "text": "I'm planning on conducting an analysis of the ratio between public and internal reception of scientific papers in the science community. Public reception will be measured by the Altmetrics score (comprised of news mentions, Twitter posts, etc.), internal reception by the paper's citation count.\nAmong other things, I would like to compare the internal/external ratios of various scientific publications. At first, I thought of just calculating the ratio for each individual paper and doing statistic tests between journals from there. The problem I see here, however, is that papers with low counts in either of the two measures will unduly influence the overall outcome. An example: Paper A has 4 citations and an Altmetric score of 1, giving a ratio of 4. Paper B has 1000 citations and an Altmetric score of 750, giving a ratio 1.5.\nJournals with many papers with low counts in either citations or Altmetric score will therefore trend toward extreme ratios, which is not what I want to show in my analysis. So my question would be: What's a statistically sound way of differently weighing lower and higher scores so as not to have low scores (and high ratios) unduly influence overall outcomes?\nThe sample size will be in the tens of thousands, by the way. Thank you!",
        "created_utc": 1671790885,
        "upvote_ratio": 1.0
    },
    {
        "title": "Propose me a good book to understand inference statistics (both frequentist and Bayesian)",
        "author": "iamdynamite1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zt9n55/propose_me_a_good_book_to_understand_inference/",
        "text": "Hello, I would like you to consider that Im not a math or statistics but an ecologist major so maybe something not so hard or detailed for specialists, an intermediate level read would be great.\nThank you in advance",
        "created_utc": 1671781749,
        "upvote_ratio": 1.0
    },
    {
        "title": "I'm interested in a career in statistics.",
        "author": "OkWrap5597",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zt60a2/im_interested_in_a_career_in_statistics/",
        "text": "Just wanted some input:\n- Are you happy with your career?\n- How many hours a week do you work?\n- Is further education or professional licensure required?\n- Is the coding aspect hard for non-coders?\n- Is getting a job hard?\n\nThe reason I'm interested in statistics, is because I took a basic statistics class years ago and honestly loved it. I have a bachelor's in accounting and really dislike accounting.",
        "created_utc": 1671768600,
        "upvote_ratio": 0.99
    },
    {
        "title": "If my regression analysis is insignificant, do I still need to proceed to prediction model?",
        "author": "Hour_Woodpecker_906",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zt5qqj/if_my_regression_analysis_is_insignificant_do_i/",
        "text": "I'm doing analysis in RStudio.\n\nI'm done with glm() and summarizing the results. And the results were insignificant. So should I just report till this part? or do I need to go ahead with prediction model to check future outcomes?",
        "created_utc": 1671767757,
        "upvote_ratio": 1.0
    },
    {
        "title": "how many standard deviations is above/below average",
        "author": "Prior-Ordinary3729",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zt40xv/how_many_standard_deviations_is_abovebelow_average/",
        "text": "for example say of 1,357,867 songs examined the mean song length is 2:58 with a standard deviation of 59 seconds\n\nhow many sd would be above average aka long, and how many sd would be below average aka short",
        "created_utc": 1671762244,
        "upvote_ratio": 1.0
    },
    {
        "title": "Chi-squared test for survey analysis with nonprobability sampling?",
        "author": "Feynmanrenders",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zt2cde/chisquared_test_for_survey_analysis_with/",
        "text": "Hi, \n\nMy survey data consists of \\~350 completed responses, 95% categorical variables. While some demographic variables roughly match existing estimates of the population, the survey did not use a form of random sampling, but rather a mix of stratified but mostly convenience sampling (due to lack of resources and time). \n\nI consider the survey to be exploratory and a first step to analyze the topic, but due to sampling alone I only use descriptive analysis. I am not really sure if performing a chi square test makes sense in this case, since it seems to be best used for inferential statistics.\n\n**My question:** What additional value would such a test have in this context? I have seen mixed opinions on this matter so far. Is it sensible to use it, as long as I state that the chi-squared test results are only with regard to my specific sample?\n\nThankful for any feedback",
        "created_utc": 1671757170,
        "upvote_ratio": 1.0
    },
    {
        "title": "Tricking regression to remove intercept term by adding each term twice as (yi, xi) and (-yi, -xi)",
        "author": "BethStubbs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsz0ti/tricking_regression_to_remove_intercept_term_by/",
        "text": "So basically one can show the trick regression has a RSS exactly double that of the normal regression.\n\nBut in terms of the ratio of MSE between the two models models what would this be?\n\n\nI thought MSE = 1/n * RSS  so if the trick regression has twice the RSS but twice the data points, then MSE would be the same\n\nBut the book I read didn‚Äôt have this as the answer, think it was 4 or 1/4  (i cannot remember and no longer have the book)\n\nWhat should the answer be?",
        "created_utc": 1671748650,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you use repeat observations when calculating discrimination and calibration",
        "author": "iwantafarm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsxa9l/can_you_use_repeat_observations_when_calculating/",
        "text": " \n\nI have patient data (n=100), with a data point each year for 5 years, that I want to use to assess calibration and discrimination of a prognosis model.\n\nIs it statistically sound to use the \"repeats\" in one calculation of disc/calibration i.e. include patient data from all 5 years for all 100 patients, ,so there is 500 data points included?\n\nNote: the output of the model would be different for each year",
        "created_utc": 1671744280,
        "upvote_ratio": 1.0
    },
    {
        "title": "Beginner question about frequentist method",
        "author": "iamdynamite1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsx07u/beginner_question_about_frequentist_method/",
        "text": "If the hypothesis and parameters in a frequentist anaysis are fixed and the probability and p value are asigned to data, which can then make us reject or accept the null hypothesis, doesnt that mean if we took another random data it might be very different from the first one and thus maybe lead us to accept the H0 as data is the only unfixed thing is a frequentist analysis.\nThank you for bearing with my ignorance",
        "created_utc": 1671743574,
        "upvote_ratio": 1.0
    },
    {
        "title": "Ordered logit and probit regression",
        "author": "weirdonee2000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zswzqv/ordered_logit_and_probit_regression/",
        "text": "Hello everyone,\n\nI am trying to do a regression where my dependent variable has ordinal values of 0 to 5. The independent variables are age, gender and another variable which has ordinal values of 1 to 10. I am trying to find the correct way to do this regression and my professor is insisting that I need to use an ordered logit and probit model and is unwilling to help me in any other way.\n\nAs statistics is not my strong point I am unsure on what to do. With online guides I tried to do this analysis in Rstudio but the professor is unhappy with the results and I think ordered logit and probit model is not a good fit for my data. \n\nBased on the type of data, what regression do you think is best to use to find out if my independant variables are good predictors? \n\nThank you in advance for any recommendations!",
        "created_utc": 1671743538,
        "upvote_ratio": 1.0
    },
    {
        "title": "Am I dumb, and/or is this Lorenz curve incorrect?",
        "author": "caoxueqin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsqzgs/am_i_dumb_andor_is_this_lorenz_curve_incorrect/",
        "text": "I'm looking at a Lorenz Curve on the Irish government's statistics website, and there's something I don't understand.\n\nFigure 8.1 [here](https://www.cso.ie/en/releasesandpublications/ep/p-hfcs/householdfinanceandconsumptionsurvey2018/incomeandwealthinequality/): \n\nI'm interested in the numbers for 2018 (the green curve).\n\nThe top three percentiles are as follows:\n\n98 - 82.2%\n99 - 91.6%\n100 - 100%\n\nBased on that, the percentage of wealth held by the top two percentiles should be:\n\n99 - 9.4%\n100 - 8.4%\n\nBut that would imply that the top percentile is less wealthy than the one below it, which can't be right.\n\nWhat am I doing wrong here?",
        "created_utc": 1671728492,
        "upvote_ratio": 1.0
    },
    {
        "title": "Finding the preferred sample size",
        "author": "Bigshakandpeppapig",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsn19a/finding_the_preferred_sample_size/",
        "text": "Hi people, i am currently working on a biology lab report and need a bit of help figuring out how many samples i should have included in my experiment to say that it is representative of reality. What i have done is measured the soil depth and tree circumference on a total of 36 trees of a specific species, and found no correlation (r squared value of 0.002). My question is wether it is possible to find the sample size needed to say that the sample size is representative of the population",
        "created_utc": 1671718447,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to check normality in a data set?",
        "author": "TaMaody",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsl1zb/how_to_check_normality_in_a_data_set/",
        "text": " \n\nHello,\n\nI want to check the normality of my data to see what statistical tests I can do.\n\nmy data is divided by a nominal variable - Control and treatment (independent of each other)\n\nthen for each variable (Control/treatment), I have a time series(t1,t2,t3) for a couple of parameters (dependent on time series) with 3 replicates for each time point.\n\ndo I check normality for each variable (Control/treatment) separately or together? or do I check normality for each time point inside a variable? (this would mean I have 3 samples)\n\nhelp,\n\nthankyou",
        "created_utc": 1671712309,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I compare a weighted and an unweighted odds ratio?",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zskm8z/how_do_i_compare_a_weighted_and_an_unweighted/",
        "text": "I have a weighted and an unweighted logistic regression, both with only 1 identical predictor. I want to determine if the odds ratios of that predictor are significantly different from one another.",
        "created_utc": 1671710844,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about conditional density",
        "author": "lightsnooze",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zska8o/question_about_conditional_density/",
        "text": "Hi all,\n\nApologies in advance for this very basic mathematical statistics question.\n\nI'm having trouble with Bayes' theorem for computing the conditional density function.\n\nf(y|x) = { f(x|y).f(y) } / f(x)\n\nSo from my current understanding, f(x) should be a constant because we are treating X=x as known. \n\nQ1: Does this mean the numerator is the joint density of X and Y with a known value of X=x?\n\nQ2: Given that the denominator is a marginal density function, evaluating f(x) at a particular x should return a value extremely close to 0. Is this right?",
        "created_utc": 1671709756,
        "upvote_ratio": 1.0
    },
    {
        "title": "Advice for purchasing around $2k budget PC for Data analysis/ML",
        "author": "Lucky_Strawberry_185",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsfqu1/advice_for_purchasing_around_2k_budget_pc_for/",
        "text": "If there are statisticians working in the DS/ML fields, I need your advice!\n I will work as a freelance data scientist/machine learning engineer in the future. So, what do I need is to do basic to advanced data science projects with my computer. I'm still learning DS/ML util now. My buget limit is around 2000 dollars.\nPlease give me advice whether laptop or PC is suitable for my career. If possible, I honestly need both PC and laptop (the latter is due to the portability). I wanna know which operating system is necessary for DS/ML (Window or Linux or OS). It makes me satisfying if you offer specific advice related to which pc specs (cpu/gpu/...) are perfect for my job. Is cloud service really solve my problem other than purchasing expensive PC specs?\nThank you all for help!",
        "created_utc": 1671694006,
        "upvote_ratio": 1.0
    },
    {
        "title": "I had a question that my friend gave me that completely stomped me.",
        "author": "Hajilol",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zsa5r1/i_had_a_question_that_my_friend_gave_me_that/",
        "text": "This is how it went\n\n\"A student takes tests and has a 50% chance of passing each one. The student will keep taking tests until they have passed three in a row. What is the expected percentage of tests they will have passed after they stop taking them?\n\nHow would one go about solving this?\n\nps. sorry for the weird phrasing, i don't remember the exact question.",
        "created_utc": 1671677910,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which order for analysis? - Chi square and conditional frequency distributions.",
        "author": "Feynmanrenders",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zs9ypr/which_order_for_analysis_chi_square_and/",
        "text": "Hi all,\n\nI am currently analyzing survey data with mostly categorical variables. I have 6 independent variables that I use for analyzing conditional frequency distributions.\n\nI also want to use the chi square test to determine if there is a (high probability for a) relationship between two variables. \n\nSo my question is what would you do:\n\n1) Select combinations of interest and perform the chi square test first. Then plot the conditional frequency distributions for each independent variable + other variable that has a relation.\n\n2) Create conditional frequency tables and plots first for every independent variable with the other variables and then do Chi square selectively.",
        "created_utc": 1671677372,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hazard ratio - unit dependency",
        "author": "lattecoffeegirl",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zs96g5/hazard_ratio_unit_dependency/",
        "text": "I am struggling with hazard ratio calculation in a cox model.\n\nWhy is the hazard ratio dependent on the unity of the variable?\n\nI just computed 2 models.\n1. my variable A was with a mean of 2000\n-&gt; parameter estimation = 0.0001 and HR for A around 1.000\n2. I divided A by 1000 so √É=A/1000\n-&gt; parameter estimation = 0.1022 and HR for √É = 1.108\n\nBoth were significant, so this seems to be correct, but I don‚Äôt understand why scaling matters ü§î\n\nAnyone can help me? üòä",
        "created_utc": 1671675297,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to tell when to use T vs Z vs Chi square?",
        "author": "BriarKnave",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zs8ovy/how_to_tell_when_to_use_t_vs_z_vs_chi_square/",
        "text": "I know how to do it but I can't figure out when it's correct to use each one to get the correct confidence interval",
        "created_utc": 1671674040,
        "upvote_ratio": 1.0
    },
    {
        "title": "[confidence interval]",
        "author": "Anthonomos_97",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zs8ijl/confidence_interval/",
        "text": "I was rolling 2 dices of 6 and wanted to calculate the confidence interval.\nI used all the 36 possibles outcomes of the sum of those 2 dices as a sample.\nCalculating the confidence interval at 95% I obtained something going roughly from 6 to 8 but it doesn‚Äôt seem right. \nHow can I calculate it the right way ?",
        "created_utc": 1671673558,
        "upvote_ratio": 1.0
    },
    {
        "title": "very basic question: imagine you have N balls which are all different colours, you pick one out and place it back, how many ball pulls on average does it take to have picked all colours at least once",
        "author": "Bradas128",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zs5pss/very_basic_question_imagine_you_have_n_balls/",
        "text": "",
        "created_utc": 1671666643,
        "upvote_ratio": 1.0
    },
    {
        "title": "Question about marginal probability functions",
        "author": "TrovadorAngolano",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zs2pwq/question_about_marginal_probability_functions/",
        "text": "Hey guys so im in an econ bs and I have stats, and I'm learning the basics. So we're learning bivariate stats and my question is: if I have (X,Y) discrete variables and the  following Joint probability distribution : (X+Y)/32 (X=1,2 ; Y=1,2,3,4) how can I get an expression for the marginal function of X and Y? \n\nThanks guys\n\np.s. (I know how to calculate the values but not the expression)",
        "created_utc": 1671660521,
        "upvote_ratio": 1.0
    },
    {
        "title": "Any book recommendations for mixed effects &amp;/or bayesian hypothesis testing?",
        "author": "_siggy__",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zryuhx/any_book_recommendations_for_mixed_effects_or/",
        "text": "I would like to advance my understanding of statistics. I have been trained in typical psychology fashion: using SPSS to conduct null hypothesis significance testing, and a lot of my understanding centers on using t-tests, ANOVAs, a few regressions, etc. I think I have have a solid understanding of these, but I want to learn more about linear mixed effects modelling and Bayesian hypothesis testing (e.g. with bayesian alternatives to the standard tests). It would be great to learn how to implement these in R (or python) using real examples.\n\nIs there a modern book that covers these things? I understand that these are quite advanced topics but given that I am not a formally trained statistician I wonder if there are any resources that are somewhat accessible to someone like me!",
        "created_utc": 1671652218,
        "upvote_ratio": 1.0
    },
    {
        "title": "Dichotomous or continuous variable",
        "author": "frauensauna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zrsfqd/dichotomous_or_continuous_variable/",
        "text": "Hi all. I have a question regarding low frequency count variables. For example, I have 117 participants of which only 12% showed a specific behaviour of interest. Most (n=7) showed it only once, some (n=4) showed it twice, and two showed in 3 times, and only one participant showed it four times.\n\nWould it make sense to use these frequency counts in a linear model? Or would it be better to create a dichotomous variable indicating whether the participant showed it at least once?\n\nI have to decide in advance because I am writing a preregistration. The problem is that published studies usually do either of them. The ones having dichotomous variables state that they chose to do this because of the low frequency.\n\nWhat do you think? Would it even matter in the models?",
        "created_utc": 1671639020,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can we solve this problem? This is a question from my ststs final exam today. I dont know if I got it right. Tysm!",
        "author": "Lilvutra",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zron59/how_can_we_solve_this_problem_this_is_a_question/",
        "text": "Suppose A University uses a 3 face-dice to decide the amount of scholarship given out. The probability to get face i is p(i), where i = 1,2,3. Associated with the ith face is a reward of of $10^i.  \n1. On average, how much money should FUV expect to spend on a student.  \n\n2. Suppose that there is an average student named Alice, who is very special, in a way that she receives the average amount of money computed in part 1. On average, what is the difference that Alice should expect that her friends will receive compared to her scholarship?  \n\n3. Evaluate the values in part 1 and 2 using p1=1/2, p2=2/5, p3=1/10.",
        "created_utc": 1671633705,
        "upvote_ratio": 1.0
    },
    {
        "title": "Two questions concerning sample size estimation in simple study design",
        "author": "scholarship_dropout",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zrncnf/two_questions_concerning_sample_size_estimation/",
        "text": "I have a simple two-arm study design that compares intervention and control group and takes measures at three timepoints (T0, T1, T2).\n\nI found that an ANCOVA with group factor and timepoint factor as well as baseline values at T0 as covariate is the state-of-the-art way to analyze the results. Is that correct?\n\nWould it be feasible to calculate sample size for this study based on an unpaired t-test that compares intervention and control group outcome at T2 ? Or do I HAVE to calculate sample size using the ANCOVA analysis?\n\nI highly appreciate your help as I'm kinda lost here ü•≤",
        "created_utc": 1671631508,
        "upvote_ratio": 1.0
    },
    {
        "title": "Test Transitivity",
        "author": "vibr4ntt",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zrlqnv/test_transitivity/",
        "text": "I am wondering about the following.\n\n&amp;#x200B;\n\nSuppose a t-test is run on group A to test if group A mean is different from zero. Hypotheses:\n\nH\\_01: u1 = 0,\n\nH\\_a1: u1 != 0\n\n&amp;#x200B;\n\nSuppose in addition that a t-test is run on group B to test if group B mean is different from zero. Hypotheses:\n\nH\\_02: u2 = 0,\n\nH\\_a2: u2 != 0\n\n&amp;#x200B;\n\nSuppose in both cases we fail to reject null hypotheses H\\_01 and H\\_02 at significance level alpha. Does it follow automatically from this that a third hypothesis test run at the same significance level to test if the difference between the two groups is zero would be redundant?\n\nThat is, do the results from the first two tests mean that the following hypotheses are redundant?\n\nH\\_03: u1 - u2 = 0\n\nH\\_04: u1 - u2 != 0\n\n&amp;#x200B;\n\nI am leaning towards yes but I wanted to confirm.",
        "created_utc": 1671628898,
        "upvote_ratio": 1.0
    },
    {
        "title": "determining which prevalence (P) to use in sample calc",
        "author": "hananaski",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zrk0pv/determining_which_prevalence_p_to_use_in_sample/",
        "text": "This is an epidemiological study. Let's say disease A has two categories, x and y. The ones which belong to the x category has 10 risk factors, from 1 to 10. We do not know which of the 10 risk factors are responsible for disease Ax. In the future, we would like to develop a preventive measures that could prevent the incidence of disease Ax. Now, we do not know which of the 10 risk factors are actually responsible for disease Ax in a certain country, and that's what we're trying to determine. So does that mean my hypothesis should be:\n\nH1: Factor 1, 2, 3 are the main risk factors for disease Ax (assuming I can find the supporting literature :/ ).\n\nHence, should my P should be the cumulative proportion of factor 1, 2 &amp; 3 found in disease Ax ? \n\nOr should it be the prevalence of disease Ax [Ax/A(x+y)]? Or just the prevalence of disease A?\n\nThank you!",
        "created_utc": 1671626088,
        "upvote_ratio": 1.0
    },
    {
        "title": "Random experiments | Random experiment in statistics | Random experiment definition | Examples of random experiments | Random experiment definition in probability",
        "author": "ayshu12100",
        "url": "https://www.statistics.today/2022/12/Random-experiments.html",
        "text": "[removed]",
        "created_utc": 1671622944,
        "upvote_ratio": 1.0
    },
    {
        "title": "Hypothesis testing for data from non-normal distribution",
        "author": "bruh_to_you",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zrcq6w/hypothesis_testing_for_data_from_nonnormal/",
        "text": " I am working on no. of accidents in each state of my country and how pandemic has affected those numbers. The data for the past 10 years shows it is not normally distributed. Can I randomly select a few states to represent my entire population? What tests should be used? I am thinking Mann-Whitney, Kruskal-Wallis should work.",
        "created_utc": 1671604811,
        "upvote_ratio": 1.0
    },
    {
        "title": "Book for studying Causation",
        "author": "Capable-Operation-98",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zrbncf/book_for_studying_causation/",
        "text": "Please point me to good read for Causation, it could be book or a research paper.",
        "created_utc": 1671601297,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting Bonferroni: What does it say about differences between groups?",
        "author": "lucillabalmerino",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zrbe9u/interpreting_bonferroni_what_does_it_say_about/",
        "text": "I did a two-way ANOVA and know there is stat. significantly difference(s) between groups, so I did a Bonferroni to better understand the differences. But I do not know how to interpret the Bonferroni results. Any help?",
        "created_utc": 1671600477,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I determine which input variables are important for predicting the output?",
        "author": "user_--",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zr7c8s/how_can_i_determine_which_input_variables_are/",
        "text": "I have samples consisting of a very large set of input variables corresponding to one output variable, and from fitting a few different regression models to the data, there seems to be a predictive relationship. How can I determine which input variables are actually involved in the predictivity of the models, and which ones are irrelevant?",
        "created_utc": 1671588579,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confused by reporting population density",
        "author": "Royal_Difficulty_678",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zr6i2g/confused_by_reporting_population_density/",
        "text": "I'm currently looking at population density for different wildlife. I read a paper that reported the population density of polar bears as \" bears/1000 km2\". This confuses me.\n\nI know population density is calculated by population/area. In this case, bears / area.\n\nWhat I don't understand is why the population density is reported as \" bears/1000 km2\" instead of \"bears / km 2\". Where does this \"1000 km2\" come from?\n\nCan anyone explain this to me?\n\nPaper: [https://pubs.aina.ucalgary.ca/arctic/Arctic48-2-147.pdf](https://pubs.aina.ucalgary.ca/arctic/Arctic48-2-147.pdf)",
        "created_utc": 1671586323,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to write mathematical formula for logistic regression (mixed effects model)",
        "author": "copernicanrevolution",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zr5sn7/how_to_write_mathematical_formula_for_logistic/",
        "text": "Hi there,\n\nI am unsure how to write these models in mathematical notation when it  is a model with nested random effects. In particular I don't know how to show the subscripts or the random effects. This is how the models were  created in R: \n\n    Participation ~ 1 + Timepoint + (1 | CLUSTER_NUM/UNIQUE_ID)\n    \n    Participation ~ 1 + Timepoint*CONDITION + (1 | CLUSTER_NUM/UNIQUE_ID)\n\n They are both logistic regression models with a binary outcome variable  (Participation) and one (model 1) or two (model 2) categorical  predictors (that interact). Timepoint is a factor with 3 levels and CONDITION is a factor with 5  levels. The random effects are both categorical. \n\nAny help would be appreciated.\n\nThanks.",
        "created_utc": 1671584500,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a relationship between Survival Bias and Non-response Bias?",
        "author": "causam",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zr2loa/is_there_a_relationship_between_survival_bias_and/",
        "text": " The definition of survival bias is the logical error of concentrating on entities that passed a selection process. But wouldn't this also include non-response bias, where respondents chooses not to answer a  particular question or the entire survey. This would leave only the respondents left for the analysis stage. Thoughts on this?",
        "created_utc": 1671577344,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is similarity an objective property? Should other similar events be taken into account when evaluating the probability of an event?",
        "author": "eth_trader_12",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zr23zo/is_similarity_an_objective_property_should_other/",
        "text": "Suppose a rare event occurs. For example, suppose someone wins two lotteries in a year. This may have happened because it was rigged or because of chance. The defender of the chance hypothesis could point to how given that thousands of lotteries are played each year, one person winning twice is not that surprising. The defender essentially compares this event to other similar events (in this case lotteries) to rationalize it having occurred by chance. But how similar can the other events be?\n\nSuppose there is a person Joe who tells you to think of a number between 1 and 1,000,000. He then says he'll try to guess correctly. Suppose he fails 1,000,000 straight times. Another person by the name of Jane comes in and says she can do the same thing. On her fifth try, she guesses correctly. Suppose you are now tasked to figure out whether that happened by chance.\n\nIf you are an attacker of the chance hypothesis, you might say that the probability of Jane guessing it right within five tries is very low. Therefore, it probably didn't happen by chance. The defender of the chance hypothesis might say \"Well, both Joe's and Jane's events were guesses. The probability of atleast one guess being correct out of 1,000,005 guesses isn't that low.\" In this case, even though the events seem more dissimilar compared to the rigged lotteries case, the defender incorporates those trials.\n\nThis begs the question: when can we incorporate other trials? How similar can other trials be? And is similarity an objective property? Can it be said to be objectively true that the event of the New Jersey lottery occurring on Oct 8th and the New Jersey Lottery occurring on November 10th are more similar to each other than the events of Joe and Jane guessing a number? Why or why not?\n\nWhat about the naturalness of certain categories? For example, suppose Adam thinks of Jane and Jane calls her. You might put this event into the category \"thinking of someone and calling you\" since it seems natural to do so. Then, you can simply recognize that tons of people have thought of someone and they didn't call. So this particular event may not be surprising.\n\nBut what if say you invent some new game where you write down a word of five letters and ask another person in front of you to write it down as well. And they match. The probability of this happening by chance is of course very low. The defender of the chance hypothesis can again try the strategy of putting this into a category of events to increase the probability. But which category does this belong to if this game has not been played before? If it hasn't been played before, does this make this event more impressive than the event of you thinking of someone and having them call you?",
        "created_utc": 1671576149,
        "upvote_ratio": 1.0
    },
    {
        "title": "How should I structure my analysis? Trying to validate and evaluate an established risk prediction model",
        "author": "iwantafarm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zr23ii/how_should_i_structure_my_analysis_trying_to/",
        "text": " \n\nSo essentially I have patient data for a period of 5 years:\n\n* same patient each year\n* each person has 5 predicted risks (for each year) - e.g. in 2017 there risk was 2%, in 2018 the same patients risk was 8% etc\n* The equation predicts the risk of an event in 5 years\n* n is approx 100\n\nI want to asses how good the equation was at predicting the outcome. The outcome is already known. Note: there are no false positives\n\nI will conduct sensitivity and specificity, but would it make sense to produce a confusion matrix that includes all 5 years' false negatives, true positives etc?\n\nIf I were to use AUCROC analysis, would it be appropriate to make 5 aucroc curves for each year?\n\nThank you guys, Id appreciate any help",
        "created_utc": 1671576116,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics Introductory Course",
        "author": "euseguros",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zr0mfr/statistics_introductory_course/",
        "text": "Hi r/AskStatistics\n\nI made a free introductory course on inferential statistics. Not selling anything. Just compiled the main points for anyone that is interested. It's on YouTube. Thank you.\n\n[https://www.youtube.com/watch?v=euw-CFjQnDQ&amp;t=173s](https://www.youtube.com/watch?v=euw-CFjQnDQ&amp;t=173s)",
        "created_utc": 1671572592,
        "upvote_ratio": 1.0
    },
    {
        "title": "When doing multiple chi square tests for independence against the same target variable, do you need to apply a Bonferroni correction?",
        "author": "spigotface",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqy7dx/when_doing_multiple_chi_square_tests_for/",
        "text": "So if you had a dataset that had 3 explanatory variables (x1, x2, x3) and one response variable (y), you'd do 3 chi square tests for independence - x1 vs y, x2 vs y, and x3 vs y.\n\n&amp;#x200B;\n\nDo you need to apply a Bonferroni correction in this instance?",
        "created_utc": 1671566821,
        "upvote_ratio": 1.0
    },
    {
        "title": "What prerequisites of statistics to learn",
        "author": "Hopp5432",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqxsqi/what_prerequisites_of_statistics_to_learn/",
        "text": "Hello everybody! I am currently studying second year of undergrad in engineering mathematics and having taken courses in probability/statistics I find this subject to be something I‚Äôm really interested in. My current mathematical experience consists of courses in \n\n- Single variable calculus\n- Linear algebra\n - Multivariable calculus\n - Vector calculus\n - Complex analysis\n - Linear systems (ODEs and transforms)\n - Continuous systems (PDEs and operators) - TODO\n  - Matrix theory - TODO\n  - Set theory/Metric spaces - TODO\n\nAnd my stats experience is a course in mathematical statistics and next year I study courses in stochastic processes and Markov chains. \n\nI want to get into statistics/data science for my masters and am wondering what I can self study to get ahead. Currently I am planning to read about discrete math(combinatorics) and real analysis followed by measure theory. Basically my question is if this is enough or are there any other prerequisites for higher level statistics? Also do you have any recommendations for books about measure theory since it seems important in probability theory?",
        "created_utc": 1671565851,
        "upvote_ratio": 1.0
    },
    {
        "title": "R Package for bayesian estimation of a physical model",
        "author": "Parzfisch",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqx3gl/r_package_for_bayesian_estimation_of_a_physical/",
        "text": " Hello,  I am looking for an R package that can be used to estimate the  parameters of a physical model using a Bayesian approach. Are there any  suggestions from your side?\n\nDetails:  My data is given by y\\_i|m,S\\~ N(g(m), S), where the physical model g is a  function which maps from R to the power of 4 to R to the power of 21,  m=(m\\_1,...,m\\_4)' is a four-dimensional vector, S is a matrix of  dimension 21x21 and i=1,...,24.¬†\n\nI  have already used the nls.lm function from the R package minpack.lm to  estimate the parameters and this worked fine. However, my boss has prior  knowledge for the parameters m\\_j, so I would like to use this to derive  the corresponding posteriori probabilities. Unfortunately I haven't  found a suitable solution for this problem yet.\n\nHere are a few approaches which did not work so far:\n\n* rjags: Can't handle custom R-functions as far as I know.\n* rstan:  Similar problem as in rjags. Theoretically I could reprogram the  function g as a stan file but so far I didn't bother with that.\n* Acceptance  rejection algorithm: The kernel of the density is given by exp(-A)  where A is very large so the kernel is numerically zero. I could replace  this kernel by exp(-A +c) where c is a real number, but depending on  which hyperparameters are chosen the problem is present again.\n* Metropolis-Hastings  algorithm: Not sure if this algorithm is the right one for this  problem. The R packages I used so far did not really help me.\n\nHope someone here can help me and please let me know if something is unclear in the description.\n\nThanks in advance",
        "created_utc": 1671564159,
        "upvote_ratio": 1.0
    },
    {
        "title": "I've got data on Total Cholestrol and Circumference of the neck",
        "author": "sirczechs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqw2vr/ive_got_data_on_total_cholestrol_and/",
        "text": "I've got data on Total Cholestrol and Circumference of the neck\n\nI want to conclude my project saying that the cholesterol and circumference are co related. What statistical tests do I have to perform except finding correlation coefficients please let me know. I'm a student still, but fascinated by biostatistics.",
        "created_utc": 1671561758,
        "upvote_ratio": 1.0
    },
    {
        "title": "Estimating workplace injuries",
        "author": "GazuGaming",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqvpbc/estimating_workplace_injuries/",
        "text": "I‚Äôm trying to estimate the rate workplace injuries, for example sewing machine injuries with continuous trials within discrete period of use with non-zero chance of failure. If I have recorded 6 injuries over 30 hours of sewing machine operation with multiple operators in multiple factories then I can estimate the mean rate of future injuries is 0.2 injures per hour of operation for similar operators and machines and factory conditions. However the actual rate of failure is random and continuously changing due to physical conditions and state of operator, machines, etc. and If workers log 100 hours of operation I can expect 20 reportable injuries (y =0.2x with no injuries at zero operation). Perhaps the historical rate is 0.1 during daytime and 0.3 at night or varies similarly by location. How would I do this type of prediction with a better fitting non-linear function? Say I wanted to develop a prediction model for each factory with additional variables to better predict injuries in the future given an estimate of future opening time (for example if I expect 500 hours of use at a new factory over next 5 year period, how many injuries should I expect based on the mean and variance of injury probability estimated using a sample of records from similar factories?). Should I be using binomial or poison function or something else, and how?\nTyvm!",
        "created_utc": 1671560855,
        "upvote_ratio": 1.0
    },
    {
        "title": "Averaging Fits vs Fitting to All Data Simultaneously?",
        "author": "Meeegrees",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqtos8/averaging_fits_vs_fitting_to_all_data/",
        "text": "I have three data traces in a data set as pictured below. I obtain three traces every hour throughout a day and want to see how the slope of these lines changes over 24 hours. I could either:\n\n* fit y=mx+b to each data trace in a set, and then average m1, m2, and m3 to get m for that hour\n* fit y=mx+b to all three traces together\n\nI feel like while they may come out to the same, average m, I'm not sure if one method is more or less correct based on how I would then calculate the error. Are the methods different, and if so, how and which one is better suited? My gut feeling says that method one actively introduces more error when done in a program due to running a fitting method more times.\n\n[Fake Data as an Example](https://preview.redd.it/uqt09wsl237a1.jpg?width=1680&amp;format=pjpg&amp;auto=webp&amp;s=c9309dc2fb7796ba073d0dc223c99f0466349863)\n\nIf it matters, in reality, these lines always have monotonically increasing b values \\[from trace 1 to 3\\] and very little variance in m, where m is also maybe monotonically increasing. Not that I can picture it mattering.",
        "created_utc": 1671556019,
        "upvote_ratio": 1.0
    },
    {
        "title": "How does the likelihood function change if the random variables of the sample are identically distributed but NOT independent?",
        "author": "hawkeyeninefive",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqteol/how_does_the_likelihood_function_change_if_the/",
        "text": "",
        "created_utc": 1671555350,
        "upvote_ratio": 1.0
    },
    {
        "title": "Test for Unidimensionality?",
        "author": "Hell-Walker-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqry97/test_for_unidimensionality/",
        "text": "What is the best statistical analyses to use for unidimensionality of a test?",
        "created_utc": 1671551729,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help me settle a probability dispute",
        "author": "DrProfJoe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqr71u/help_me_settle_a_probability_dispute/",
        "text": "My friend and I disagree as to how to calculate the following probability:\n\nConsider a simple program that will select elements from the set {0, 1} with replacement. the probabilities of drawing a 0 and a 1 respectively are 0.3 and 0.7. If 100 values are drawn, what's the probability that the sum of all draws will be 60?\n\nI chose to model this as a Bernoulli trial resulting in 60 successes. \n\nMy friend insists it should be calculated with a z test of proportions. \n\nWhich, if either of us, is right? Note: He's a much better statistician than I am.",
        "created_utc": 1671549828,
        "upvote_ratio": 1.0
    },
    {
        "title": "If the P-value of my statistical test is far lower than 0.05, can I alter my threshold value of 0.05?",
        "author": "_dock_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqqj31/if_the_pvalue_of_my_statistical_test_is_far_lower/",
        "text": "So imagine I did a student T-test and got a statistical relevance of 0.001. For my experiment, I think the value is revelant for a P-value below 0.05. can I change my P value relevance to P&lt;0.001 to show that it is even more accurate than with a higher P-value? \n\nIf it would be the other way around (P&lt;0.001 --&gt; P&lt;0.05) it would be cheating the statistics I believe, as I didn't accept the results but want to use them. Is that also true in this case or do I just show that my experiment was far better than I initially expected?",
        "created_utc": 1671548113,
        "upvote_ratio": 1.0
    },
    {
        "title": "Anyone willing to help a struggling student?",
        "author": "Less_Lawfulness3590",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqqgaw/anyone_willing_to_help_a_struggling_student/",
        "text": "Hello there! My university professor gave us a take home exam. I‚Äôm going to be honest with you, stats is miles about my head. Any kind hearted souls wanna help me with the C?",
        "created_utc": 1671547906,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] ratio calculation",
        "author": "joshuas13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqnk5g/q_ratio_calculation/",
        "text": "hi All\n\nTrying to understand ratio calculation:\n\nWe have a total amount of X. \nBoth person A and B need to contribute but according to the level of involvement.\nperson A is involved 80%.\nperson B is involved 100%.\n\nhow much would person A and B contribute according to their involvement to reach X ?\n\nthanks",
        "created_utc": 1671540405,
        "upvote_ratio": 1.0
    },
    {
        "title": "Quickish question",
        "author": "Poor_StatsGuy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zqeebf/quickish_question/",
        "text": "Any recommendations on what test to run for multiple dichotomies questionnaire? \n\nCurrently stuck, but I‚Äôm trying looking into two various sets of symptoms and wondering if certain diagnosis increases or decreases based on the set of two sets of symptoms. Let‚Äôs say pediatric diagnosis of a heart condition. Physician want to know if symptoms such as heartburn, chest pain, abdominal pain vs back pain, shoulder pain, arm pain is more common with the pediatric heart condition.\n\nP.S. Thanks and sorry if it sounds like gibberish kind of hard formatting the question to ask.",
        "created_utc": 1671510687,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I get the solution and what is the answer by i84 calculator",
        "author": "EstablishmentVast581",
        "url": "https://i.redd.it/273yij8s2z6a1.jpg",
        "text": "",
        "created_utc": 1671488937,
        "upvote_ratio": 1.0
    },
    {
        "title": "What model should I use for a sample of 38 cyberattacks in a country?",
        "author": "StayHydrated588",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zq58hb/what_model_should_i_use_for_a_sample_of_38/",
        "text": "  Please help with the following: \n\nI have count data of 38 cyberattacks on a country. I have data for the months, year, type of attack (data theft, hack, dos, ransomware), target category (government, private, or military), etc. \n\nThere was a cyber law passed in 2016, and I‚Äôd like to study the before and after impact of that cyber law. I have coded the years before as 0 and after as 1 and created a new column for cyberattacks per year. \n\nStylized model: \n\nLaw (0/1) = type of attack + target category + (some control variables like population and GDP/capita). \n\nShould I study the count data by itself with a principal component analysis or Poisson regression, and then move to a linear probability model or logistic regression? Or should I study the dependent variable as count data which is 38 cyberattacks or should I group them by attacks per year and study the impact of the cyberlaw before and after .\n\nI am confused with which model to use how to split the data. Please assist, I do not have a statistical background and I need all the help I can get. I would appreciate any help and your time. Thank you.",
        "created_utc": 1671487852,
        "upvote_ratio": 1.0
    },
    {
        "title": "Could there be other way to use Run test?",
        "author": "stabelm",
        "url": "https://youtu.be/8P9A3x94XoI",
        "text": "",
        "created_utc": 1671485968,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Finding standard deviation move in dependent variable in response to a one standard deviation move of each macro driver (macro driver = independent variable)",
        "author": "acascuse-me-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zq4b24/q_finding_standard_deviation_move_in_dependent/",
        "text": " \n\nMethodology-wise would you just run a regression on daily move and then use standard deviation of the period as a multiplier? Does this sound like something you would use a multivariable regression for? Happy to share data if that is helpful.\n\nI'm not a stats guy so I'm not sure if there's a more sophisticated or appropriate way to do this.",
        "created_utc": 1671485748,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a way to control for variables in an ANOVA in R?",
        "author": "Goliof",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zq3lz0/is_there_a_way_to_control_for_variables_in_an/",
        "text": "An example: \n\n  \n\nMy outcome is whether a person wears glasses (1) or not (0). The prevalence of glasses in my population is 30%. Now I want to control for gender and age to get a weighted prevalence. \n\n  \n\nWould a grand marginal means model work for this? I'm not sure what I could use as the weights? I didn't have a treatment so propensity score methods won't work. \n\n&amp;#x200B;\n\n    emmeans(glm, specs = ~1)\n\nWould this work when I have included gender and age in the glm?",
        "created_utc": 1671484111,
        "upvote_ratio": 1.0
    },
    {
        "title": "if a test has a very high PPV and NPV, why should I be concerned about a lower sensitivity ?",
        "author": "Dorindon",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zq3a05/if_a_test_has_a_very_high_ppv_and_npv_why_should/",
        "text": "Hello, I am going bananas and have been working on this all evening.\n\nContext: looking at the rapid antigen test to detect covid in SYMPTOMATIC PATIENTS as compared to the gold standard (PCR test) at a time where there is  a relatively high prevalence of covid in the population.\n\nWhen I look at the table below, I would conclude that the rapid antigen test has a very high PPV and NPV, and is therefore reliable in both cases, positive and negative, to confirm or exclude covid.\n\nWhy would I care if the sensitivity is only 80% ? There must be something that I am missing because I reviewed multiple article including the CDC and Cochrane (all with similar results), and everyone concludes that a positive rapid antigen test + symptoms is sufficient to diagnose covid, but  a PCR test is indicated to conclude if the Rapid Antigen Test is negative in patients with symptoms.\n\nThis is not homework of any kind. I bought a covid rapid antigen testing kit and want to understand the stats behind it.\n\nthanks in advance for your time and help\n\nhttps://preview.redd.it/nuugp51t2x6a1.png?width=1826&amp;format=png&amp;auto=webp&amp;s=daa9f66196a53aa5d60d13fedcfdad8fcf327c71",
        "created_utc": 1671483351,
        "upvote_ratio": 1.0
    },
    {
        "title": "What statistical tests could I apply here",
        "author": "Guilty-March-4706",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zq1em2/what_statistical_tests_could_i_apply_here/",
        "text": "I have 5 consecutive versions of an interface that I made small improvements to.\n\nI started with version 1.0 to version 5.0\n\nAt each version of the interface I asked some people what they grade that particular interface versions; A to F (A best to F worst)\n\nWhat statistical tests could i apply here to see if there are improvements according to the grades people gave?\n\n&amp;#x200B;\n\nI'm doing these tests in Rstudio",
        "created_utc": 1671478981,
        "upvote_ratio": 1.0
    },
    {
        "title": "SURVIVAL CURVE PREDICTION - which method should I use?",
        "author": "surf_tech_hippie",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpygma/survival_curve_prediction_which_method_should_i/",
        "text": "Dear community,\n\nI am currently working on a python project to predict the number of terminated subscriptions for a given month.\n\n**Example:** It's End of Nov. 2022 and I know that 1000 new subscriptions started in Jan. 2022 (= Jan cohort) whereof 800 subscriptions are still active at the end of November. Now I want to know how many will most likely be left at the end of Dec. 2022 ( ... and of course all other cohorts as well)\n\n**Important:** There are many factors which impact when and if a subscriptions ends. One important factor is the minimum duration of a subscription which differs across rentals.\n\n&amp;#x200B;\n\nI know there are tons of statistical method out there like survival curve prediction (KaplanMeierFitter, Cox regression), time series analysis, random forest and so on but at this stage I am kind of lost which one to use. I applied the KaplanMeierFitter quite extensively but the predictions are pretty off.\n\n&amp;#x200B;\n\nDoes anyone have any suggestions how I could approach this topic (= which statistical method should I use)? Looking forward for any suggestions :)",
        "created_utc": 1671472457,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to calculate relative values?",
        "author": "cruelbankai",
        "url": "https://www.reddit.com/r/AskStatistics/comments/zpygkw/how_to_calculate_relative_values/",
        "text": "Say you have total ounces of fruit broken up by different kinds of fruit. \n\nYou have 20,000 ounces of strawberries and you have say 40,000 strawberries. \n\nSay you have 400 ounces of bananas but you have 500 bananas. \n\nIn total you have 40,500 fruit and 20,400 ounces.\n\nAs time moves forward and these fluctuate, how can you capture the relative amount of ounces for fruit? Is it literally just ounces / specific fruit count?",
        "created_utc": 1671472454,
        "upvote_ratio": 1.0
    }
]