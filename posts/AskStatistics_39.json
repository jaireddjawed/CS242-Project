[
    {
        "title": "What's the general procedure for building a linear model?",
        "author": "naovsky",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8crzqk/whats_the_general_procedure_for_building_a_linear/",
        "text": "So I have a formula with a lot of variables, and I'm looking to condense it. I already know there's a random variable, but otherwise I need to test the significance of each variable and interactions. I am working with R so it kind of throws a fit if I put too many variables into a formula, but how else do I do this?\n\nShould I introduce some variables and their interactions and then keep adding and subtracting? Is there another way to narrow down the factors? Thanks",
        "created_utc": 1523920722,
        "upvote_ratio": ""
    },
    {
        "title": "How can one test the hypothesis that the difference between two time series tends to zero (converge) with only four observations?",
        "author": "fischerjunior",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8crrhs/how_can_one_test_the_hypothesis_that_the/",
        "text": "I have two vectors of weight data of two different materials that underwent four processing steps, and I would like to test the hypothesis that those weight values show a tendency to converge. To put it simply with an example, what statistical test would give (A) p&lt;0.001 and (B) p=1 for the time series: A(1) = 3, 7, 11, 15; A(2) = 0, 5, 10, 15; and B(1)= 3, 4, 5, 6; B(2) = 6, 3, 8, 1?",
        "created_utc": 1523918727,
        "upvote_ratio": ""
    },
    {
        "title": "Mathematical Statistics MOOC",
        "author": "AhTerae",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cqv11/mathematical_statistics_mooc/",
        "text": "Hi everyone,\n\nI'm looking for a good massively open online course (or any similarly good resource, really) to learn probability theory and / or mathematical statistics. Does anyone know if such a thing exists? \n\nTo be clear, I'm not looking for statistics 101, I can do t-tests and stuff. I'm looking more for the conceptual and mathy stuff you'd want to know if you were going into statistics more heavily. For an example, the closest thing I can find is MIT Open Courseware's \"Theory of Probability\" class, which covers topics like \"infinitely divisible laws\" and \"martingales\" (this class would be great and I'd actually welcome more resources like it - the only problem is that it's a static record of lecture slides and stuff with no practice or explanation for ambiguous notation). Anything which covers stuff like this would be welcome.",
        "created_utc": 1523911383,
        "upvote_ratio": ""
    },
    {
        "title": "I need help calculating the average correct answers if given randomly",
        "author": "DutchNotSleeping",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cqoyq/i_need_help_calculating_the_average_correct/",
        "text": "Alright so the parameters are as follows.  \n38 players  \n21 make the team  \n\nSomeone guesses 21 out of the 38 players to make the team, totally random. How many will they get right on average?  \nI'm okay with a rounded number, it's just for some fun thing I'm doing with my friends  \n  \nEdit: formatting",
        "created_utc": 1523910067,
        "upvote_ratio": ""
    },
    {
        "title": "Mixed ANOVA setup + Plot labelling. SPSS",
        "author": "Gannicius",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cqklf/mixed_anova_setup_plot_labelling_spss/",
        "text": "Hi Guys, \n\nI'm looking for help setting up my Mixed Factorial ANOVA in SPSS for experimental analysis.\n\nI have 1 between subject factor and 2 within subjects and do not know how to appropriately label my plots, I may have messed up with my factor assignment. \n\n",
        "created_utc": 1523909168,
        "upvote_ratio": ""
    },
    {
        "title": "I'm looking for interaction effects and main effects. Are there any and why?",
        "author": "TacticalSheep",
        "url": "https://i.redd.it/r3vlyux0sbs01.png",
        "text": "",
        "created_utc": 1523908286,
        "upvote_ratio": ""
    },
    {
        "title": "Spearman's Correlation Coefficient in Excel",
        "author": "ringring3",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cq0th/spearmans_correlation_coefficient_in_excel/",
        "text": "Anyone have a good tutorial or work around to figure out Spearman's Correlation in Excel. So far we have only found Pearson's. If not, other suggestions would be great!  Thanks",
        "created_utc": 1523905023,
        "upvote_ratio": ""
    },
    {
        "title": "Densities in mean field equations",
        "author": "h0cusl0cus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cpnrw/densities_in_mean_field_equations/",
        "text": "Could someone explain [this part](https://i.imgur.com/7cy6en0.png) (png, excerpt) of [this paper](https://arxiv.org/pdf/1102.3931.pdf) (pdf)?\n\nHow do they arrive at the two equations? They have a table with the interactions and say that\n\n&gt; the probability of the interaction listed in row eight is equal to the probability that a node in state AB is chosen as speaker and a node in state B is chosen as listener times the probability that the speaker voices opinion A\n\nbut I'm probably missing something and don't arrive at their sums (RHS of equations).\n\nThank you.",
        "created_utc": 1523902351,
        "upvote_ratio": ""
    },
    {
        "title": "Simple Scatter Plot Statistics",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cpl00/simple_scatter_plot_statistics/",
        "text": "[deleted]",
        "created_utc": 1523901761,
        "upvote_ratio": ""
    },
    {
        "title": "[R] Retrieving information from a S4-object",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8coyt2/r_retrieving_information_from_a_s4object/",
        "text": "[deleted]",
        "created_utc": 1523897212,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate prediction interval?",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8coxfw/how_to_calculate_prediction_interval/",
        "text": "\nFrom this:\n\nhttps://i.imgur.com/GkruXOY.png\n\nThere's this :\nhttps://i.imgur.com/L1o1Yrs.png\n\nI'm not sure what we're really doing here though.\n\nWe have that\n\n    StandardDeviation(observed - predicted) =\n\n        sqrt(residualStandardDeviation^2 + standardErrorOfFit^2)\n\n\nBut isn't \n\n    observed - fitted\n    \nJust the error term anyway? \n\nSo why wouldn't this just be related to this : https://i.imgur.com/fwxiuUs.png\n\n**To summarise** : I don't understand the logic for this equation :\nhttps://i.imgur.com/L1o1Yrs.png , which is the standard deviation of the\ndifference between the observed and fitted values.\n\nThanks\n",
        "created_utc": 1523896912,
        "upvote_ratio": ""
    },
    {
        "title": "Scaling results with a massive outlier",
        "author": "discotaco34",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8coh7h/scaling_results_with_a_massive_outlier/",
        "text": "I have a couple data points that generally range between 2 and 46 with one huge outlier at 480. I'm trying to visualize these points on a size scale, where a bigger value is a bigger circle. Problem is the 480 value is way too massive compared to the values between 2-46. The best way I've figured to scale the values is to pick a smaller scale and give each value a value on that scale based on where they fall on the 2-480 range. \n\nSo, I changed the range from 2-480 to 3-80 (I used these numbers because they correspond well to the coding input values) my formula is newscalevalue = (xi - 2)/(480-3) * (80-3) +3 \n\nMy question is is this an ok way to go about solving this problem? Is there a better way to do so? I have also considered using a zscore but I need the final values to be entirely positive and ordered according to smallest to largest, which zscore doesn't quite provide. ",
        "created_utc": 1523893425,
        "upvote_ratio": ""
    },
    {
        "title": "Anthropometry - can I use Technical Error of Measurement (TEM) in this situation?",
        "author": "Snowwolf864",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8co2ul/anthropometry_can_i_use_technical_error_of/",
        "text": "Firstly, I apologise for my lack of understanding in the subject.\n\nI have measured an individual on 20 different sites e.g. abdominal girth, abdominal skinfold. I have been working at a %TEM of 1.5% for girths and 7.5% for skinfold measurements. I have two measurements for each site - how do I find my actual intra-tester %TEM or is this the wrong method, should I be looking at ICC instead for example? \nMost of the information I have found seems to expect there to be more than 1 person being measured -  i just have data for 1 personal and 2 measurements per site. Thank you in advance!",
        "created_utc": 1523890294,
        "upvote_ratio": ""
    },
    {
        "title": "Selecting Samples from a Population to Cover a Variable Space In Order to Evaluate the Impact of an Introduced Variable",
        "author": "BellyFloppinChubs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cnfjl/selecting_samples_from_a_population_to_cover_a/",
        "text": "The scenario:\n \nWe manufacture 30 different grades of material whose properties are roughly determined by 4 independent variables that are fixed for each grade. I am introducing another variable (component X) that will affect the properties of interest. I know the available range of these 4 variables and want to select a subset of these 30 grades to evaluate the impact of the introduced 5th variable.\n \nMy thoughts:\n1.       To try and select a subset of grades that would cover the high-low of each variable, this is a challenge because the values are basically continuous and it seems arbitrary to determine “high” vs “low”\n2.       To select a single grade and observe the impact; I fear this will miss interactions that will appear at different ranges of the variables\n3.       Select a subset that optimizes the variable space covered for the minimum samples from what is available; I’m not sure how to do this?\n \nAny guidance or resources you can point me towards will be helpful. I have Excel, Minitab, &amp; JMP available to use.",
        "created_utc": 1523884566,
        "upvote_ratio": ""
    },
    {
        "title": "one way ANOVA vs kruskal wallis",
        "author": "mooreessential",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cmh4i/one_way_anova_vs_kruskal_wallis/",
        "text": "I have a 4 group dataset. We were told that if the data is skewed to use kruskal wallis and if not to use a one way ANOVA. However, what if 3 of the datatsets are skewed and one is normal? What would be the most appropriate method to analysi the difference between the groups?",
        "created_utc": 1523873704,
        "upvote_ratio": ""
    },
    {
        "title": "(Question) Why do beta coefficicents for a 3 variable liear regression change when a fourth variable is added",
        "author": "DonMatteo13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cln6c/question_why_do_beta_coefficicents_for_a_3/",
        "text": "As per the title. We are told to analyse two linear regression models, one has 3 variables and the other has 4. For the model for the 4 variables, the 3 beta coefficients's included in the 3 variable model change when the 4th variable is added. Why does this occur? Why does the addition of a 4th variable impact the beta coefficient of others as isn't this the independent change that the one variable causes in the dependent variable? so why would adding another variable impact this?",
        "created_utc": 1523861537,
        "upvote_ratio": ""
    },
    {
        "title": "(Question) Normalizing data (area/population) to study differences in observation numbers",
        "author": "nerdywithnature22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8clbp6/question_normalizing_data_areapopulation_to_study/",
        "text": "Hello, \n\nI took statistics way too long ago and need some help remembering how to properly normalize my data to compare the results of an analysis I am conducting. \n\nI will try to explain as succinctly/clearly as possible. I am looking at the number of observations collected by citizens in two different cities. I am looking to determine if there is a significant difference in 1) the number of observations 2) the number of individual observers and 3) the average spread of the data across the city\n\nI have picked a large city and a medium size city (both in physical acres and population) to start comparisons with. \n\nThank you for any help in advance. ",
        "created_utc": 1523857175,
        "upvote_ratio": ""
    },
    {
        "title": "Need a good idea for a college statistics project that is relatively low-effort.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cjg2d/need_a_good_idea_for_a_college_statistics_project/",
        "text": "[deleted]",
        "created_utc": 1523837355,
        "upvote_ratio": ""
    },
    {
        "title": "SPSS help: Minute-by-minute correlations of a large number of frames/second as the cases?",
        "author": "peepeeparadise",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cjcvf/spss_help_minutebyminute_correlations_of_a_large/",
        "text": "I have time series data for 2 variables with about 50,000 cases (frames in a video). Each second is about 30 frames, so each minute is about 1800 frames. In SPSS, how do I get minute-by-minute Pearson Correlations for the two variables? ",
        "created_utc": 1523836500,
        "upvote_ratio": ""
    },
    {
        "title": "How would you work out the margin of error of the everyday poll, and would it be particularly accurate?",
        "author": "adamd22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cison/how_would_you_work_out_the_margin_of_error_of_the/",
        "text": "I'm pretty new to statistics, but I like learning more about them. I was wondering if, in polls we see in general, in the news, or in papers, that do not have obvious margins of error presented with them, it would be possible figure it out yourself, So, to setting your own confidence level, whether that would be significant in any way, whether there is some finesse to choosing this. Also to this extent, figuring out the margin of error, and how accurate this would be for telling people that X poll is \"this\" accurate to Y population. ",
        "created_utc": 1523831236,
        "upvote_ratio": ""
    },
    {
        "title": "Kappa Weighting - Linear vs Quadratic?",
        "author": "5k1rm15h",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ci87x/kappa_weighting_linear_vs_quadratic/",
        "text": "Hi, I'm trying to understand in which scenarios a linear vs a quadratic weighting for Kappa would be more appropriate.\n\nWould it be only based on the scale of what's being measured, ie. linear change in the outcome being measured across scale points would indicate linear weighting, and similarly for quadratic? Or is this completely wrong? I'd really like to get a good feel for when either weighting would be appropriate.\n\nThanks in advance!",
        "created_utc": 1523826249,
        "upvote_ratio": ""
    },
    {
        "title": "Mixed Effects Model help",
        "author": "AmyGee_DaLa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cgeza/mixed_effects_model_help/",
        "text": "Hi,\n\nI heard a few of my graduate advisors talking about the benefits of running a mixed effects model instead of a repeated measures.  However, I am having a hard time writing the formula in R (using lme4) -- converting it from SPSS.  \n\n[Here](https://i.imgur.com/bPBrQ7C.png) is how I analyze my data in SPSS: EBT100... is the name of the task, Genotype is my IV, and my within-subject factors are Day (5 levels) and Cue (9 levels).  \n\nIn R, this is the code that I am trying to run: In R, here is my code:\n\n    lmeModel &lt;- lmer(Att ~ Genotype*Day*Cue + (1|Subject)\n\nMy Genotype Effect is the same between R and SPSS (p~0.12), but all of my interactions are different (Genotype x Day, Genotype x Cue, Genotype x Day x Cue).\n\nR (lme4) Output:\n\n                     Sum Sq Mean Sq NumDF DenDF F.value    Pr(&gt;F)    \n    Genotype            488   243.9     2    32   2.272   0.11954    \n    Day               25922  6480.4     4  1408  60.356 &lt; 2.2e-16 ***\n    Cue               35821  4477.6     8  1408  41.703 &lt; 2.2e-16 ***\n    Genotype:Day       3646   455.7     8  1408   4.244 4.751e-05 ***\n    Genotype:Cue        736    46.0    16  1408   0.429   0.97560    \n    Day:Cue            5063   158.2    32  1408   1.474   0.04352 *  \n    Genotype:Day:Cue   3297    51.5    64  1408   0.480   0.99984  \n\nSPSS Repeated Measures ANOVA output:\n\n                       F.value   Pr(&gt;F)    \n    Genotype            2.272    0.120    \n    Day                 9.603    0.000\n    Cue                 83.916   0.000\n    Genotype:Day        0.675    0.712\n    Genotype:Cue        0.863    0.613    \n    Day:Cue             3.168    0.00  \n    Genotype:Day:Cue    1.031    0.411\n\nYou can see that the main effect of Genotype is the same for both R and SPSS. Additionally, in R, my DenDF output is not correct either. Any idea as to why this would be?\n\nEven more... Using ezANOVA, with the same dataset that I am using for lme4, this is my code:\n\n    anova &lt;- ezANOVA(data = dat,\n        wid = Subject,\n        dv = Att,\n        within = .(Day, Cue),\n        between = Genotype,\n        type = 3) \n\nezANOVA Output:\n\n                Effect DFn  DFd          F            p p&lt;.05         ges\n    2         Genotype   2   32  2.2715034 1.195449e-01       0.044348362\n    3              Day   4  128  9.6034152 8.003233e-07     * 0.103474748\n    5              Cue   8  256 83.9162989 3.938364e-67     * 0.137556761\n    4     Genotype:Day   8  128  0.6753544 7.124675e-01       0.015974029\n    6     Genotype:Cue  16  256  0.8624463 6.133218e-01       0.003267726\n    7          Day:Cue  32 1024  3.1679308 1.257738e-08     * 0.022046134\n    8 Genotype:Day:Cue  64 1024  1.0313631 4.115000e-01       0.014466102\n\nHow can I convert SPSS (or ezANOVA) to lme4?\n\nAny information would be greatly appreciated! Thank you!",
        "created_utc": 1523811234,
        "upvote_ratio": ""
    },
    {
        "title": "Why doesn't the TOST equivalence testing procedure use non-central t distribution to determine the p value?",
        "author": "changarid",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cg803/why_doesnt_the_tost_equivalence_testing_procedure/",
        "text": "I'm looking at Lakens' (2017) primer and he tests the hypothesis that there is a difference between two groups of magnitude `d` by subtracting `d` from the sample difference and then he computes the t value and p value based on this difference score with a procedure analogous to Welch t-test. This looks wrong to me. IMO one should be using non-central t distribution with non-centrality parameter based on `d` and t value based on the sample diffrence to determine the p value. Thoughts?\n\nLakens, D. (2017). Equivalence tests: a practical primer for t tests, correlations, and meta-analyses. Social Psychological and Personality Science, 8(4), 355-362.",
        "created_utc": 1523809549,
        "upvote_ratio": ""
    },
    {
        "title": "Need help understanding Naive Bayes and conditional probability.",
        "author": "RealMatchesMalonee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cfnzf/need_help_understanding_naive_bayes_and/",
        "text": "So, I've just started with Naive Bayes and a crucial concept is conditional probability, and if someone could help me clarify a few things, that'd be awesome.\n\nLet's build a spam classifier using Naive Bayes. Naive Bayes tells you upfront that it assumes that the probability of words appearing in the mail is conditionally independent given whether a mail is spam or not. Based on my understanding of the concepts of C.P, I'm going to write a few examples and my conclusions. Please tell me if they're correct or not...\n\n* So, the probability of the two words appearing is conditionally independent given that the mail is spam or not. I guess that means that if you tell me that \"buy\" is in the mail, then the probability of \"offer\" appearing as well, increases. That's because the likelihood of the mail being an advertisement , and therefore, a spam increases. We don't know right now that the mail is spam or not.\nBut if we replay this scene, and you start by telling me that this piece of mail is a spam, you won't be doing me any favours by then telling me that \"buy\" has also appeared, because when you told me \"Hey, this is a spam mail.\", my expectations of encountering spam related words (like \"buy\", \"offer\", \"limited time\" etc) automatically increased. So when you told me that \"buy\" appears as well, my expectations of encountering \"offer\" haven't changed. \n\n* A better example could be a world where all spam mail contain the word \"spam\". In such a case if you tell me that \nthat the mail is spam, then knowing that \"spam\" appears as well is of no help in determining if \"offer\" appears as well, because \"spam\" is appearing in all spam mails.\n\n* A negative example could be the phrase \"best deal\" used IRL. \"Best deal\" is a common phrase used in advertising in real life. So if one of the words from the phrase pops up, our minds automatically presume that the other is going to show up as well.  If you told me that the mail is spam, my expectations for encountering words you'd normally find in spam mail, increased. Then when you tell me that \"best\" is also present, my expectations of encountering \"deal\" increased even further. So, in case of \"best deal\", even \"buy\" and \"offer\", conditional independence is not held IRL. This also showcases the main beef that people have with Naive Bayes. Conditional independence of this type is not commonly, and in fact rarely, found in the real world.\n\nIs this correct?",
        "created_utc": 1523804568,
        "upvote_ratio": ""
    },
    {
        "title": "Need help understanding Naive Bayes.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cff84/need_help_understanding_naive_bayes/",
        "text": "[deleted]",
        "created_utc": 1523802204,
        "upvote_ratio": ""
    },
    {
        "title": "Question in relation the the median of the median and mode of logarithmic normal distribution",
        "author": "tjaymiller",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ceers/question_in_relation_the_the_median_of_the_median/",
        "text": "My question relates to the relation between the median and the mode of a logarithmic normal distribution.\nIn a particle size distribution, the median describes the particle size under which 50% of the quantity of particles are.\nThe mode though describes the most occuring particle size. \nI've read that the mode and the median are the same for the logarithmic normal distribution and found that somewhat confusing.\n\nIs the median and mode the same for a logarithmic normal distribution?",
        "created_utc": 1523789194,
        "upvote_ratio": ""
    },
    {
        "title": "Question about finding probabilities in a normal distribution",
        "author": "ztnq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cdjti/question_about_finding_probabilities_in_a_normal/",
        "text": "I have a battery life that is normally distributed(w/ random variable X), and if a certain amount of time (let's say 32 hours )has already passed, what are the chances\nthat it will die before 36.5 hours?\nInitially I thought the equation for the probability as P(32&lt;X&lt;36.5),because the dying could happen anytime in that interval, but now I  am thinking it would be P(X&gt;36.5|X&gt;32), because 32 hours  passed so I would have to normalize the sample space by that. \n\nEdit: I mean P(X&lt;36.5|X&gt;32)",
        "created_utc": 1523774585,
        "upvote_ratio": ""
    },
    {
        "title": "Need help deciding on what T-test to use",
        "author": "Tupiekit",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ccs28/need_help_deciding_on_what_ttest_to_use/",
        "text": "Hello all I have a question about what T-test I should use in a paper im writing for my International relations capstone. I apologize in advance for my description of what im analyzing....it might get kinda confusing.\n\nWhat im doing for my paper is im looking at how 3 seperate media sources (WSJ, Fox, NYT) cover two similar events (decision to bomb syria) that happened during President Obama and President Trump's respective administrations to see if there is any quantifiable bias.\n\nnot to get too specific but I have a program that breaks the content of article text into a quantifiable number. So what I did is I found articles that cover both Presidents during their respective reactions to similar events, got an average score per article, then compared (simple t.test) the two presidents against eachother. Using Fox as an example; I took all of the mean scores from each  Fox Obama article and put them into one data frame, I did the same for the Fox Trump articles. Next I did a simple t.test comparing the two means to see if the means were equal. I did this for the other two media sources.\n\nNow what I want to do is I want to compare each sources' coverage for a certain president. For example I want to compare how Fox covers Obama to how WSJ covers Obama and NYT covers Obama, but I am stuck on which t-test I should use to do this.\n\nAny help would be greatly appreciated, thank you.\n\n",
        "created_utc": 1523763942,
        "upvote_ratio": ""
    },
    {
        "title": "help a stats dummy in comparing two medical tests to see if one is better",
        "author": "ExcessiveHairGrowth",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cce43/help_a_stats_dummy_in_comparing_two_medical_tests/",
        "text": "Hello all, I ask for your guidance.  I am not good at statistics and I am not sure that I can explain my question well.\n\nSuppose I have a new screening quantitative medical lab test (Test B) that I am trying to show is better than another quantitative lab test (Test A) for the a specific disease (the higher the number, the more likely the disease).  All subjects also undergo a gold standard test to determine whether the disease is actually present.\n\nI have 15 patient controls and 30 disease controls.\n\nI ran both Test A and Test B on all 45 patients.  Based on the gold standard test I was able to show that Test B had better overall sensitivities and specificities for detecting disease from control than Test A, with stronger P-value. \n\nDo you think this is enough to show that test B may be better or do I have to do tests of correlation between tests A and B?  If I do, should I be separating calculating the correlation for controls and disease or should all values be combined?  Does showing positive or negative correlation even make a difference regarding my ultimate goal of pushing test B to be done instead of Test A?\n\nWhen running all 45 samples\n- Pearson R of 0.3 and P-value 0.045\n- Spearman R of 0.8 and P-value of 0 essentially\n\nWhen running control of 15,\n- Pearson R 0.0056, p value 0.98\n- Spearman R 0.584 p value 0.02\n\nWhen running disease of 30\n- Pearson R 0.07, p value 0.72\n- Spearman R 0.34, p value 0.086\n\n\nAppreciate any helpful thoughts in advance.",
        "created_utc": 1523759434,
        "upvote_ratio": ""
    },
    {
        "title": "Why ANOVA in Political Science?",
        "author": "abbamouse",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cc94p/why_anova_in_political_science/",
        "text": "I've been teaching social science stats for political scientists for some time.  We cover univariate stats (central tendency and dispersion), bivariate stats (measures of association and difference of means tests, making sure to run Hartley's F Test before the latter), and then spend most of our time on multivariate techniques: OLS, logit and probit, ordinal logit and ordinal probit, multinomial logit and multinomial probit, count models (Poisson and negative binomial, including their zero-inflated variants), duration models (Cox, Weibull, and Cox adjusting for non-proportional hazards), and selection models like Heckman, etc.  We sometimes even add a bit of tobit if we have time.  \n\n\nBut we don't cover ANOVA and the only time I see it in journals seems to be in articles on genopolitics (and occasionally some experimental study on political psychology), which are pretty niche.  \n  So...what can ANOVA do for me or my students better than these other techniques?  I'll learn it and teach it if I can be convinced it's useful in my discipline.  What kind of problems/puzzles is ANOVA (and similar acronyms like MANCOVA) the best-suited technique for addressing?  In other words, why is comparing *variance* so important?",
        "created_utc": 1523757853,
        "upvote_ratio": ""
    },
    {
        "title": "Sample size needed for Pearson's correlation?",
        "author": "amethyst397",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8cc2np/sample_size_needed_for_pearsons_correlation/",
        "text": "Sorry to ask this. I've been trying to figure it out and I'm getting super confused.\n\nAs part of my dissertation, I did a Pearson's correlation for the relationship between self-compassion and compassion for others. I split the file according to gender, because I expected that the relationship would be significant for males, but not females. My results showed that while the relationship was stronger for males, it was still insignificant. In my discussion I've said that previous research that found the relationship statistically significant for males had over 300 male participants, whereas my study had only 77 male participants. \nIs this a reasonable critique of my study? Or is 77 enough participants to establish a statistically significant Pearson's correlation? \n\nThanks in advance to anyone who answers. ",
        "created_utc": 1523755883,
        "upvote_ratio": ""
    },
    {
        "title": "Which test do I choose?",
        "author": "CanJesusSwimOnLand",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ca8gg/which_test_do_i_choose/",
        "text": "The study is looking at whether the use of different types of app before bed is related to total sleep time.\n\nI want to find if there is a relationship between any of my categorical variables (e.g use of social media: yes/no; use of video games: yes/no) on my ordinal variable (total sleep time: 4-5 hours/6-7 hours, etc). \n\nI have a low sample size (33) and if I split the file between yes/no for any specific variable, the distribution of total sleep time is not the same shape for each level in a histogram.\n\nMeans ranks Mann-Whitney U is probably the right test, but I have 11 categorical variables to check against the ordinal variable, so I don't want to run 11 separate tests. Is there a better method? Many thanks! \n\nEdit: clarity.",
        "created_utc": 1523737818,
        "upvote_ratio": ""
    },
    {
        "title": "Quite lost with what tests to run, help would be appreciate! SPSS",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c99b3/quite_lost_with_what_tests_to_run_help_would_be/",
        "text": "[deleted]",
        "created_utc": 1523729048,
        "upvote_ratio": ""
    },
    {
        "title": "Difference b/w P(X) and P(X=i)",
        "author": "RealMatchesMalonee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c81kq/difference_bw_px_and_pxi/",
        "text": "Hi. I'm a new to statistics, and recently, I've been wondering about the difference b/w P(X) and P(X=i), where X is a random variable. Is this different from p(X) and p(X=i) (lowercase p).",
        "created_utc": 1523717838,
        "upvote_ratio": ""
    },
    {
        "title": "what would be the closet statistical terms based on meaning for 1) the degree/relativeness of gains, 2) the degree/relativeness of uncertainty, and 3) the degree/relativeness of potential/possible gains/earnings",
        "author": "nixos_learner",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c70bs/what_would_be_the_closet_statistical_terms_based/",
        "text": "[removed]",
        "created_utc": 1523705729,
        "upvote_ratio": ""
    },
    {
        "title": "stats &amp; related majors: what % of usa population graduated with stats degrees within the last 5 years? (college level &amp; phd levels)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c6u8o/stats_related_majors_what_of_usa_population/",
        "text": "[removed]",
        "created_utc": 1523703201,
        "upvote_ratio": ""
    },
    {
        "title": "how to understand stats formulas",
        "author": "pappu_basanti",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c6ph8/how_to_understand_stats_formulas/",
        "text": "i have my semester exams for descriptive stats in 10 days and i have just started from basics. I'm having a hard time in remembering formulas how can i understand the formulas so that i dont have to rote learn all of them. im not looking for mnemonics type solution but can someone provide links, pdfs, etc. which has derivations or anything to understand the formulas in the simplest language possible. please help.....please. thanks in advance. ",
        "created_utc": 1523701094,
        "upvote_ratio": ""
    },
    {
        "title": "When comparing different groups, Why should I ever do an ANOVA (without contrasts)?",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c6lq6/when_comparing_different_groups_why_should_i_ever/",
        "text": "I wondered about this two years ago and forgot about it, but a short exchange here on AskStatistics reminded me of that thought again.\n\nA lot of the time, I see people running an ANOVA and some post hoc tests, which strikes me as somewhat odd. If I run an ANOVA I only get an answer to the global hypothesis \"is there an effect?\" But the only thing I can conclude from this at that stage is that \"something happened\". Of course that is why people follow up the ANOVA with a post hoc comparison to test for differences of groups.This approach seems to be the \"standard procedure\" in many papers and presentations I have seen.\n\nThe post hoc comparison is just a series of t-tests with an adjusted alpha criterion. Is there something I am missing? Why can't I just skip the ANOVA entirely and run the post-hoc test alone? I could even specify just a few t-tests for groups and directions I am really interested in to avoid reducing my alpha criterion too strongly. \n",
        "created_utc": 1523699390,
        "upvote_ratio": ""
    },
    {
        "title": "Beginner question: what method should I use to find correlation between 2 questionnaires?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c695d/beginner_question_what_method_should_i_use_to/",
        "text": "[deleted]",
        "created_utc": 1523693572,
        "upvote_ratio": ""
    },
    {
        "title": "Anyone still use @RISK?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c5ql7/anyone_still_use_risk/",
        "text": "[deleted]",
        "created_utc": 1523685797,
        "upvote_ratio": ""
    },
    {
        "title": "(Variance of the sum of non-independent X)/(n^2)-&gt;0 as n-&gt;infty ?",
        "author": "djjaneiro",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c5muh/variance_of_the_sum_of_nonindependent_xn20_as/",
        "text": "I am trying to prove Markov's weak law of large numbers for practice and was able to prove up to this point rather easily, though I was wondering how it is the case that (Variance of the sum of non-independent X)/(n^2 )-&gt;0 as n-&gt;infinity?",
        "created_utc": 1523684294,
        "upvote_ratio": ""
    },
    {
        "title": "what common statistical tests are commonly used to test the reliability of estimates (as an example for supply-demand curves in econ)?",
        "author": "nixos_learner",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c5l3n/what_common_statistical_tests_are_commonly_used/",
        "text": "[removed]",
        "created_utc": 1523683629,
        "upvote_ratio": ""
    },
    {
        "title": "What is a \"balance cohort\"?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c5hcl/what_is_a_balance_cohort/",
        "text": "[deleted]",
        "created_utc": 1523682268,
        "upvote_ratio": ""
    },
    {
        "title": "what % gradated from the educational system within the last 5 years?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c4xzu/what_gradated_from_the_educational_system_within/",
        "text": "[removed]",
        "created_utc": 1523675800,
        "upvote_ratio": ""
    },
    {
        "title": "What kind of t-test is used in linear regression?",
        "author": "aguyfromucdavis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c48yr/what_kind_of_ttest_is_used_in_linear_regression/",
        "text": "I'm trying to tie the concept of t-tests and their role in linear regression. I'm trying to understand if the t-test used in linear regression to test if the beta coefficients is 0 is a one-sample t-test, two-sample t-test (aka independent samples t-test), or paired t-test. \n\nI don't think it's a paired t-test as there's no concept of \"before &amp; after\", but I'm also conflicted whether it's a one-sample t-test as I'm not really comparing a sample to the sampling distribution of a population. I am neither assessing the effect of a treatment on two groups, which would be used in two-sample t-tests.\n\n[This link](http://support.minitab.com/en-us/minitab-express/1/help-and-how-to/basic-statistics/inference/supporting-topics/tests-of-means/types-of-t-tests/) seems to suggest there are four types, and that the fourth type is a t-test associated with regression outputs. Is that really the case?\n\nThanks for any input!",
        "created_utc": 1523668612,
        "upvote_ratio": ""
    },
    {
        "title": "ANOVA XLStat plugin for Excel",
        "author": "Yuan_Anxiong",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c3kk9/anova_xlstat_plugin_for_excel/",
        "text": "Five human subjects (A-E) listened to the same ten songs (1-10) and adjusted three parameters (a-c) in response for a total of 150 data points. My thesis advisor told me to separate this data into three one-way ANOVAs of 50 data points each, one per adjusted parameter.\n\nI have the XLStat plugin for MS Excel but I'm not sure what to plug into the prompt! Please help!",
        "created_utc": 1523661967,
        "upvote_ratio": ""
    },
    {
        "title": "Sample sizes for digital testing",
        "author": "londonstats",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c2vep/sample_sizes_for_digital_testing/",
        "text": "Hi all,\n\nI'm working on testing the impact of a website, and a few different products. There's a need for statistically significant samples, but I'd hugely appreciate guidance on how to calculate these (including the terminology of the tests we're looking to run, so I can explore these further). I've had a look through the FAQ on the side, and what we're looking to do doesn't seem to match with any of the suggested tests.\n\nWe're looking to run 4 tests:\n\n* **Test 1 - making the website globally relevant:** here, users will complete a survey after spending time on one of 4 versions of the website. The survey will explore certain aspects (colours, layout, language used). We’d like to be able to see, statistically, which is the best version of the website. If none are successful, we’d also like to be able to design a ’super’ version of the website drawing on the traits that users like most across the 4 versions. As this is a survey, instinctively the sample size calculation feels simple enough (total population of interest, confidence level, margin-of-error). However, am I missing anything - particularly around building a ‘super’ version of the site?\n\n* **Test 2 - identifying the most useful content:** we’ll have five different topics (money, sports, etc) and within each of these 3 sub-areas (e.g. for sports: biography of famous sportsperson, report of recent sports game/match etc). We want to see what this content is relevant across all of our markets. What test would we use to do this? It sounds like a test of x proportions, to see which is most popular. However, I think it may be more complex than that. Any thoughts on sample size (particularly elements needed to calculate it - such as baseline engagement rate, expected engagement rate etc) would also be very valuable.\n\n* **Test 3 - user preference for website navigation:** we want to see whether the way we present information to users is effective. Users will be asked to see information in a list or pictorially. We’d then like to survey them as to whether/not they found that method useful. What data would we need to do this? I think it’s something to explore later, as we don’t have current engagement rates of how users engage with pictures or lists yet. Unless we can do this another way? It seems that this, as a survey, should be simple with regard to sample sizes but I’m not quite clear on what test we run here, or the variables we put into that test. Thoughts welcome!\n\n* **Test 4 - testing email notifications:** we’ll be sending weekly email notifications, with an option to opt-out or reduce frequency. If they opt-out, they will be asked for their reasoning. This sounds like a survey approach, but I think there’s also value in finding the optimum amount (even frequency) of notifications, too. Any thoughts on either of these components?\n\nApologies for the length of the message, but any and all insight is very much appreciated - whether on a particular test, or component of a test. Similarly, any best practice/case studies/further reading is all gratefully received!\n\nThanks!",
        "created_utc": 1523655704,
        "upvote_ratio": ""
    },
    {
        "title": "Advice: Assessing the effect of unemployment on political ideology",
        "author": "pollinguk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c1hgw/advice_assessing_the_effect_of_unemployment_on/",
        "text": "Hi everyone,\n\nI have panel data for a few thousand voters, including their employment status and their political ideology at each time point. I would like to work out whether becoming unemployed leads to transient change in political ideology (e.g. you shift left then back to your mean level) or stable change (e.g. you shift to some new mean level).\n\nIf the IV of interest was time-constant I'd use a multilevel model of change or latent growth curve model, but in the case of a time-varying IV of interest, I'm not sure what to do.\n\nDo you have any advice?",
        "created_utc": 1523644535,
        "upvote_ratio": ""
    },
    {
        "title": "Units for slope",
        "author": "jokermaster091",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8c0aby/units_for_slope/",
        "text": "If I did a graph of biomass (in grams) (y axis) vs concentration (g/L) (x-axis), what units would my slope be?\nWould it be g/g/L? Would I keep it as that or change it to g/L?",
        "created_utc": 1523635324,
        "upvote_ratio": ""
    },
    {
        "title": "How to see if one value is statistically different from a group",
        "author": "raymanh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bzwq8/how_to_see_if_one_value_is_statistically/",
        "text": "I have 4 values of concentration of ammonium. Basically, I did an experiment and I want to know if I have accidentally changed the concentration of ammonium in some water. \n\nSo one value was measured before the experiment. This is 6.0631.\n\nThe other three values were measured after the experiment. These are; 6.1941, 6.18847, 6.23575.\n\nEssentially I want to know if I significantly altered the concentration of the water, which I don't want to have done. So I'm hoping that 6.0631 is not statistically different from the group of three above.\n\nWhat kind of test can I do here?\n\nI was thinking of a 2 paired Mann-Whitney U test, but errors were always returned.\n\nThanks",
        "created_utc": 1523632390,
        "upvote_ratio": ""
    },
    {
        "title": "Quick question on analytical setup",
        "author": "Takeurvitamins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bznnz/quick_question_on_analytical_setup/",
        "text": "I have cages with 4 individuals inside.\n\nThe cages are modified to manipulate conditions inside (four types, a, b, c, and d).\n\nFour of these cages are grouped together.\n\nThere are ten of these groups (random factor).\n\nThere is a probe in each of the cages that gives one measurement of the conditions (ostensibly modified by cage type) for the entire duration of the experiment.\n\nIs it appropriate to use just the value the probe gives as an independent variable? Should I only use cage type? Or should I combine the two?",
        "created_utc": 1523630439,
        "upvote_ratio": ""
    },
    {
        "title": "Equation derivation - variance, covariance, expectations, first differences (Panel data)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bybis/equation_derivation_variance_covariance/",
        "text": "[deleted]",
        "created_utc": 1523617658,
        "upvote_ratio": ""
    },
    {
        "title": "Constrained Minimization Problem?",
        "author": "Jcm487",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bxgdp/constrained_minimization_problem/",
        "text": "Heres the problem:  \n\nFor a fixed a and r, find the smallest value of k such that \n\nk * r *e^(-r(x-a)) &gt; probability density function of a standard normal evaluated at x ,    for all x &gt; a. \n\nThis is what I have deduced so far: \n\nk &gt; 1/r*sqrt(2pi) * e^(-x^2/2 + r*x - r*a), thus, this question is really asking, \n\nminimize f(x) = 1/r*sqrt(2pi) * e^(-x^2/2 + r*x - r*a), subject to \nconstraint x &gt; a, given that r and a are fixed constants. \n\nPretty much stuck here. I took the log of this function and subsequently took the derivative, set it equal to zero and solved for x, giving me x = r. I then reversed the log to get x = e^r as an optimum value, but when I conducted the second derivative test to check this x value it did not pass. Additionally, not sure this approach is the way to go given the constraint.  ",
        "created_utc": 1523605637,
        "upvote_ratio": ""
    },
    {
        "title": "Question on dummy variables",
        "author": "ah_lone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bx7y0/question_on_dummy_variables/",
        "text": "In my test, I have 2 categorical variables, gender and colour.\n\nGender = Female or Male\nColour = Red, Blue or Green\n\nIn this case, i will have one dummy for gender and 2 dummies for colour, else i will fall into a dummy variable trap.\n\nWould it be possible to re-estimate my model to include all 3 colours and not fall into a dummy variable trap?",
        "created_utc": 1523602406,
        "upvote_ratio": ""
    },
    {
        "title": "How can I find the Mean and Standard Deviation of this Normal Distribution",
        "author": "8-bit123",
        "url": "https://swz.salary.com/SalaryWizard/Mechanical-Engineer-I-Salary-Details.aspx?&amp;fromevent=swz.selectjob.freepop",
        "text": "",
        "created_utc": 1523592510,
        "upvote_ratio": ""
    },
    {
        "title": "If I randomly pick 500 numbers from 1:1000, then randomly drop 250 of those numbers, how does the variance of the remaining 250 numbers change as opposed to the previous set of 500 numbers?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bw7q4/if_i_randomly_pick_500_numbers_from_11000_then/",
        "text": "[deleted]",
        "created_utc": 1523590273,
        "upvote_ratio": ""
    },
    {
        "title": "How could I statistically analyse my dissertation data",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bvuhl/how_could_i_statistically_analyse_my_dissertation/",
        "text": "[deleted]",
        "created_utc": 1523586445,
        "upvote_ratio": ""
    },
    {
        "title": "Is anyone familiar with NIR calibrations or derivatives of lines?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bvftx/is_anyone_familiar_with_nir_calibrations_or/",
        "text": "[deleted]",
        "created_utc": 1523582539,
        "upvote_ratio": ""
    },
    {
        "title": "What are concurrent and construct validity?",
        "author": "femalebodyinspect007",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bvd3n/what_are_concurrent_and_construct_validity/",
        "text": "Hey everyone\nI'am having a hard time fully understanding concurrent and construct validitt, can someone please provide me with a explanation or an example?",
        "created_utc": 1523581830,
        "upvote_ratio": ""
    },
    {
        "title": "Erlang-k proof by observation?",
        "author": "needhelp19900991",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bupfl/erlangk_proof_by_observation/",
        "text": "FXi\\(xi\\) = 1 \\- e\\^\\(λx\\)\n\nand the Erlang\\-k distribution:\n\nP\\(X≤x\\) = int\\(x,0\\) \\[\\(λ\\(λt\\)\\^\\(k\\-1\\) /\\(k\\-1\\)! \\* e\\^\\(\\-λt\\)\\] dt\n\nHow can verify the Erlang distribution is correct? Lets say if k = 2.",
        "created_utc": 1523575824,
        "upvote_ratio": ""
    },
    {
        "title": "What's the difference between Adj SS and Seq SS, ?",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8buoig/whats_the_difference_between_adj_ss_and_seq_ss/",
        "text": "I mean, they're both exactly the same here : https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/how-to/fit-regression-model/interpret-the-results/all-statistics-and-graphs/analysis-of-variance-table/#seq-ms\n\nThey're also the same number on the output... \n\nso i'm wondering what the point in this difference is. \n\nI have Adj MS and only that (there's no Seq MS), should I expect more? \n\n",
        "created_utc": 1523575580,
        "upvote_ratio": ""
    },
    {
        "title": "URGENT: Help me interpret my ANOVA results!",
        "author": "gsbailey96",
        "url": "https://i.redd.it/8m0xxr5a8kr01.jpg",
        "text": "",
        "created_utc": 1523574711,
        "upvote_ratio": ""
    },
    {
        "title": "Question about what analysis yo use.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8buhwh/question_about_what_analysis_yo_use/",
        "text": "[deleted]",
        "created_utc": 1523573906,
        "upvote_ratio": ""
    },
    {
        "title": "How to combine multiple elicited expert priors?",
        "author": "UnderwaterDialect",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bu0lv/how_to_combine_multiple_elicited_expert_priors/",
        "text": "Once I have elicited priors from experts using the quantile method (i.e., what value would be equally likely to have the parameter be above and below; if the parameter were above/below that, what value would be equally likely to have the parameter be above and below).\n\nI have found resources to create a distribution reflecting each of these in isolation. But how would I develop a single prior distribution that accounts for all of the elicited opinions?",
        "created_utc": 1523569881,
        "upvote_ratio": ""
    },
    {
        "title": "What statistics test to use?",
        "author": "BrianGumble",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bscam/what_statistics_test_to_use/",
        "text": "A little background.\n\nMy experiment involved culturing bacteria on a medium and measuring specific chemicals (10 chemicals; e.g. lactic acid, acetic acid, etc) at time points 0h, 24h, 48h and 72h. \n\nAfter 72h, a new culture was started using an inoculant of the previous culture and the same chemicals were again measured at 0h, 24h, 48h and 72h time points.  \n\nWhat statistics would best be used to determine whether there was a significant difference/increase in these compounds, between culture batches?\n\nI was thinking a non-parametric (Krukas-Wallis test? equivalent to a one-way ANOVA?), as the data failed the normality test.  Any help would be greatly appreciated.\n\n\nThanks.",
        "created_utc": 1523556883,
        "upvote_ratio": ""
    },
    {
        "title": "I need a way to practice homework in the language my teacher taught it to us in",
        "author": "deadby100cuts",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8brvis/i_need_a_way_to_practice_homework_in_the_language/",
        "text": "My teacher apparently used a diferent method than my textbook and I'm trying to practice for a test. We are working on hypthosis testing. \n\nWhen he gives us problems they are either a Z or a T and he gives us what we need to know. \n\nfor Z we are given the standard deviation(sigma), significance level(alpha), Population size(n), what we are testing for (Mue knot),\n\nFor T we are generally given a set of data, from which we can get the sample mean, sample deviation, sample size, and we are given the significance rate. \n\nhowever in my book, I'm not getting that.  I have problems where it gives me the hypothosis we are testing , population size, X (whatever it means in this context), and alpha.\n\nSo my problems look like this.  (literally copying a problem from the book,no I'm not looking for someone to answer it for me thats not going to help me learn). (I don't know how to do subscript, so it will look a little weird)\n\n&gt; Test the Hypothesis using (a) the classical approach and (B) the P-value approach. Be sure to verify the requirements of the test. \n&gt; \n&gt; H0: P=.3 vs H1: P&gt;.3 \n&gt; \n&gt; n=200, x =75 , alpha = .05\n\nOk great. So I don't have the standard deviation so I'm going to use a T test right? well then where do I get the sample standard deviation so I can generate a test statistic? I'm assuming that X is the sample mean.  The formula is \n\n(x̅ - μ)/(Sample deviation/ √n)\n\nSo Ive got  (75-.3)/(S/√200)\n\nI still need the sample standard deviation? Except I'm not given any data from which to extract it. \n\nI really need practice for this test, but doing the worksheets he hands out over and over isn't going to cut it because its not that many problems and working the same problems again and again won't help me\n\n",
        "created_utc": 1523553376,
        "upvote_ratio": ""
    },
    {
        "title": "Does this analysis commit pseudoreplication? (xpost with added questions)",
        "author": "Takeurvitamins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8brs5z/does_this_analysis_commit_pseudoreplication_xpost/",
        "text": "Hi all, posted this in r/statistics with no luck yet, sorry about the crosspost, just trying to figure this out.\n\nI designed my experiment in a way that I thought would present me with options for analysis, but I'm starting to wonder if one of the options would be considered pseudoreplication.\n\nThe experiment was like this:\n\nI work with 2 mussel species (Z and Q).\n\nI put 4 of each species in a pipe in the river (each of the four got their own little chamber inside).\n\nEach pipe had a baffle on the front and back to change water flow into and out of the pipe\n\nI made four baffle types (A, B, C, D), with the intent that each one would allow faster/more water flow than the previous (ie A&lt;B&lt;C&lt;D).\n\nI grouped one of each pipe type together to make a replicate (right?) and attached these replicates to a cinderblock\n\nThere were 10 cinderblocks total\n\nI put these underwater, cleaned them every two weeks, and took pre- and post- measurements of mussel body size, and also got attachment strength.\n\nThe pipes looked like this\nSo, option 1 is:\n\nI could just analyze the data as a a 4x2 ANOVA, with the pipe type (A, B, C, D) vs. species (Z vs Q) with something like body length as the response variable (I have others), and Cinderblocks as a random blocking factor.\n\nBUT! in each pipe, I also included a hard plaster block that dissolved over time under water, ostensibly with higher dissolution due to increased water flow. I checked to see if the pipe baffle had any affect with a one way ANOVA (response variable = amount dissolved; four pipe types) and I found that the baffles basically did what I wanted them to (A&lt;B&lt;C&lt;D), but not as strongly as I had hoped (Only pipes A and D are significantly different).\n\nTherefore option 2 is:\n\nI could do an regression/ANCOVA using these plaster blocks, with species as the factor, plaster mass lost as the covariate, and body length as the response variable, with cinderblocks again acting as a random blocking factor.\n\nMy questions are:\n\n1) is the second option psuedoreplication, because there is only one measure of flow per pipe, but 4 mussels of each species per pipe?.\n\n2) if it's not psuedoreplication, how do I tell if I should go with the ANOVA or ANCOVA?\n\n3) should I try using both pipe type and clod loss (water flow)? Should I make it Pipe type vs species with clod loss as a covariate, or should I nest clod loss within pipe as each pipe has one clod measurement of flow?\n\nI'm using JMP for the analysis, which can't do GLMM unfortunately. Any help would be appreciated.",
        "created_utc": 1523552673,
        "upvote_ratio": ""
    },
    {
        "title": "How can I study statistics efficiently?",
        "author": "lksdshk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8brjyb/how_can_i_study_statistics_efficiently/",
        "text": "Engineering graduate here. I want to study it for myself since I want to work with data analysis in the long term, so I am on my first stepsc in Python and now I want to know some Statistics.\n\nHowever, when I tried once I found it unpractical since there is a lot of data to do the exercises. What I mean is that writing down charts takes a LOT of time, as well writing all the formulas like we do in College  and using a calculator...I am not interesting studying it in the traditional way because it takes a lot of time to build it, time that I want to use to work on it to learn. \n\nSo how can I do it? Is it possible?\nShould I work it on Excel?",
        "created_utc": 1523550954,
        "upvote_ratio": ""
    },
    {
        "title": "Propagating uncertainty through a Normal Distribution",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8brgvh/propagating_uncertainty_through_a_normal/",
        "text": "[deleted]",
        "created_utc": 1523550311,
        "upvote_ratio": ""
    },
    {
        "title": "On partial residual plots",
        "author": "ChaseTheMoonLikeFire",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8breol/on_partial_residual_plots/",
        "text": "3rd year stats student here. Why does it seem like resources on the internet about PARTIAL residual plots are rare/limited? Is it uncommon or unpopular? Or is it just Google's algorithm is against me because they show more search results with only \"residual plots\"?\n\nSide question: what are some prototypical shapes of partial residual plots? Like, common shapes and what they mean when they look like a certain way...\n?",
        "created_utc": 1523549811,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing the robustness in performance of several control systems",
        "author": "stuckonbandaids",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8br4rm/comparing_the_robustness_in_performance_of/",
        "text": "I am an engineer who wants to compare the performance (really the robustness of performance) of several different control systems. This is all using simulations an no real world data or measurements. The performance metric is 0-100%, and under ideal conditions the performance of the system will always be 100%. I plan to vary one parameter, such as distance from target, and predict the drop in performance (i.e. something &lt;100%). I don't care about the trend in the drop in performance as this parameter changes, I really only care about evaluating the performance of the system as a whole across the entire parameter space. \n\nI want to use a quadratic loss function with 100% as the target to compare the different control systems, so with any drop in performance the error is squared. This is because I want any major drops in performance to be penalized heavier than minor drops. Should I be performing my statistical analysis on the output of the loss function or raw performance? Realistically, the average performance might all be similar because they all start off at 100% and can only drop (nothing can go above 100, or with the loss function nothing can go below 0).\n\nAlso, is it necessary to run an ANOVA or should I go straight into performing a t-test across all pairs? But that doesn't sound right because I don't expect my performance measures to be normally distributed (I expect the performance of some of the control systems to be 100% across much of the parameter space). And would multiple comparisons need to be corrected for in this case?",
        "created_utc": 1523547716,
        "upvote_ratio": ""
    },
    {
        "title": "Time series analysis advice",
        "author": "sendhelpxd",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bqq4x/time_series_analysis_advice/",
        "text": "Hey guys,\n\nI'm getting started trying to learn more about data science. I'm currently working on a time series data set looking at revenue over several weeks where half of the stores have a fixed effect (additional marketing) and the remainder do not. I've started off by visually comparing the average of the two sets and noticing that there is constant nonstationary element in both averages.\n\nI went ahead and detrended the data using a moving average and now see clearly that both groups have clear autocorrelated character, but there is no apparent difference between the sets based on the fixed effect. Is there any other way I could tease out the differences between these two sets? I'd appreciate any advice you can give!",
        "created_utc": 1523544573,
        "upvote_ratio": ""
    },
    {
        "title": "Stats help for dissertation proposal.",
        "author": "TaylorH93",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bqmns/stats_help_for_dissertation_proposal/",
        "text": "Hello. I'm looking into woodland fragment sizes and how this might affect Silphidae (coleoptera) abundance/species richness. I will select 2 large sites and 2 small sites of roughly the same size (to allow me to compare) and place baited pitfall traps in each. Density of traps will be fairly low, around 6 in each woodland or so. For my proposal I need to suggest stats tests. Am I right in saying I could use One Way ANOVA to test for any differences in abundance (left hand side of image) and chi-square to test for associations (right hand side)? It's made up data/layout. I just wanted to visualise it.\n\nhttps://i.imgur.com/bQ2UVnW.png\n\nMy problem with the ANOVA is that they're supposed to be independent samples, but the sites 1 and 2 will be roughly the same size and 2 and 3 will be roughly the same size as each other. Is each one still classed as fully independent from each other? ",
        "created_utc": 1523543843,
        "upvote_ratio": ""
    },
    {
        "title": "One way ANOVA SPSS and minitab",
        "author": "throwawayolli",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bqd0a/one_way_anova_spss_and_minitab/",
        "text": "Hi, I am trying to compare 15 data points to see if they are all relatively the same. I tried performing a one way ANOVA (I'm not even sure that's the correct analysis) and its telling me I dont have enough degrees of freedom. Not sure what to do. ",
        "created_utc": 1523541655,
        "upvote_ratio": ""
    },
    {
        "title": "Regression Analysis",
        "author": "jokermaster091",
        "url": "https://www.reddit.com/r/AskAcademia/comments/8bn8ct/regression_analysis/",
        "text": "",
        "created_utc": 1523537960,
        "upvote_ratio": ""
    },
    {
        "title": "Paired or independent sample T-test",
        "author": "towelrag0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bpjpr/paired_or_independent_sample_ttest/",
        "text": "I am trying to figure out whether I should use a paired or independent sample T-test.\n\nMy study design is like this: \nI start with a single blood sample. I split this blood sample into two. One of these samples undergo procedure #1, while the other sample undergo procedure #2. After the two procedure, I analyze my samples and get a number, which I’m going to compare between the two. ",
        "created_utc": 1523533933,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence levels for unknown population - T distribution",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bowyl/confidence_levels_for_unknown_population_t/",
        "text": "[deleted]",
        "created_utc": 1523526020,
        "upvote_ratio": ""
    },
    {
        "title": "Is it worth it to memorize formulas that my calculator can do for me?",
        "author": "CuckedByJaredFogle",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bog8y/is_it_worth_it_to_memorize_formulas_that_my/",
        "text": "I am taking an intro stats class at my community college and I am really enjoying it.  My teacher told me that we do not need to memorize formulas that our calculators can do for us.  He also said that we do not need to use z, t or chi-squared tables because our calculators will be more accurate and faster.  What do the experts here think?  Am I being unwise to not follow my teachers advice when I spend a lot of time on these formulas and practice using these tables?",
        "created_utc": 1523519200,
        "upvote_ratio": ""
    },
    {
        "title": "Finding the right statistical test for a bit of an unusual case",
        "author": "saxyphone241",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8boaag/finding_the_right_statistical_test_for_a_bit_of/",
        "text": "I'm having a bit of trouble finding a proper stats test for data on a research project. I've already scoured multiple sites for this, but I'm still falling short of the right method. I'm measuring the effects of an environmental factor, measured quantitatively, on the performance of a primarily skill based test. Because the differences in skill of the participants could be a confounding factor for my data set, I wanted a test that could control for that. If not for this, I could just do a linear regression. Because I am using a set of control tests without the effect of the environmental factor, I considering using the participant's difference from the control for each interval on the scale of the environmental factor, and using those points in a linear regression. However, I'm not sure if that will do what I want.\n\nAny help would be appreciated.",
        "created_utc": 1523516905,
        "upvote_ratio": ""
    },
    {
        "title": "I've collected demographic survey data on my subreddit, what can I constructively use this data for?",
        "author": "dcman00000",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bnq54/ive_collected_demographic_survey_data_on_my/",
        "text": "My basic motivation is to grow my subreddit.\n\nTo that end, how can I use this data to grow it?",
        "created_utc": 1523509642,
        "upvote_ratio": ""
    },
    {
        "title": "Degrees of Freedom for T-Test",
        "author": "Mompy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bnky2/degrees_of_freedom_for_ttest/",
        "text": "Should I calculate the df for my paired unequal variance t-test before or after the removal of outliers, when determining the critical t value? ",
        "created_utc": 1523507943,
        "upvote_ratio": ""
    },
    {
        "title": "Qualitative F Test",
        "author": "Dreyfus2006",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bn6y0/qualitative_f_test/",
        "text": "Hello,\n\nI have two sets of nominal data and would like to compare their variability, but the F-test requires variance, which I cannot calculate for nominal data.  I calculated variation ratios for the data sets, but is there a better way to calculate their dispersion?  How would I compare the dispersion of one set to the other's?\n\nThank you!",
        "created_utc": 1523503650,
        "upvote_ratio": ""
    },
    {
        "title": "Help remembering how to interpret interaction terms along with their components.",
        "author": "Sociological_Duck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bn6k9/help_remembering_how_to_interpret_interaction/",
        "text": "Hi all.\n\nI used to have a helpful guide for interpreting interaction terms. I've lost it, and embarrassingly, forgot some of what I learned in stats class. When you have two variables, plus their interaction term, that leaves open several possibilities for which of them could be positive and which can be negative. Are there any relatively simple guides explaining this? Or can it be simply explained on here?",
        "created_utc": 1523503537,
        "upvote_ratio": ""
    },
    {
        "title": "What test should I use for this data? (Stats newbie, needs guidance)",
        "author": "ax0r",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bm6yg/what_test_should_i_use_for_this_data_stats_newbie/",
        "text": "It's been something like 15 years since I did stats at University, and I didn't really understand it then. I'm trying to educate myself, but I'm struggling.\n\nI'm trying to analyze a method of predicting the result of a gold standard test. I'm hoping that my prediction method can reliably detect at least some negatives, to reduce the time and expense involved in running the gold standard.\n\nAll data points include the result of the gold standard test and of the predictive test.  \nGold standard can return positive or negative.  \nPredictive test can return 0, 1, 2 or 3.  \nI'm collecting ancillary data to assess qualitatively and help in interpretation, but this is all that I need stats on.\n\nPopulation positive rate is low but unknown.\nPositive rate in this study hasn't been assessed yet, but is likely to be in the order of 1-5%. There is an expected selection bias (i.e population rate is far below 1%)\n\nI want to see to what extent a particular score correlates with a positive or negative gold standard outcome, and get some sort of CI or p value to go along with it.\n\nI'm new at this and don't really have a statistician available to help.",
        "created_utc": 1523494183,
        "upvote_ratio": ""
    },
    {
        "title": "What statistical tests should I run for this data? [SPSS]",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8blrpt/what_statistical_tests_should_i_run_for_this_data/",
        "text": "[deleted]",
        "created_utc": 1523490326,
        "upvote_ratio": ""
    },
    {
        "title": "jensen's inequality, Linear Regression, log transform",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bllgc/jensens_inequality_linear_regression_log_transform/",
        "text": "[deleted]",
        "created_utc": 1523488807,
        "upvote_ratio": ""
    },
    {
        "title": "Confused about Residual Degrees of Freedom?",
        "author": "MulticulturalHound",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bks1t/confused_about_residual_degrees_of_freedom/",
        "text": "Hi Guy,\n\nMy understanding of degrees of freedom for a regression model is that if you have 5 terms in your model, you have 6 total parameters. Therefore, you have 6 degrees of freedom in your model (including the constant). So, if you increase the degrees of freedom, you decrease model bias with the risk of increasing model variance.\n\nI'm confused as to what residual degrees of freedom are and what their significance is? What are they used for?\n\nThanks Guys!",
        "created_utc": 1523482075,
        "upvote_ratio": ""
    },
    {
        "title": "Expected Value of Random Matrices \"Raised\" to a Power",
        "author": "djjaneiro",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bkowm/expected_value_of_random_matrices_raised_to_a/",
        "text": "I have been able to find little literature related to the topic online (e.g., https://www.researchgate.net/profile/Theodore_Groves/publication/31400997_A_Note_on_the_Expected_Value_of_an_Inverse_Matrix/links/02bfe511db35022767000000/A-Note-on-the-Expected-Value-of-an-Inverse-Matrix.pdf), though, vaguely, I am working on something in which an inequality for \"powers\" other than the inverse might come in handy.\n\nWhile my search led me to a particular paper which had seemingly solved this problem, it seems to have since been redacted (see: Editor's Note: A Note on the Expected Values of Powers of a Matrix). Has anyone come across any useful literature on the topic of random matrix \"powers\"?",
        "created_utc": 1523481400,
        "upvote_ratio": ""
    },
    {
        "title": "Analyzing results of an estimation test (\"how many dots are there?\"): there's a correct value, an estimation, varying presentation times and two sets of dot arrays.",
        "author": "petriol",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bkjrx/analyzing_results_of_an_estimation_test_how_many/",
        "text": "Hello,\n\nI made a rather explorative online test for an uni course and didn't think the analysis through before. I would like to know your approaches to avoid monkey analysis. \n\n*(Being an entry level course it's allowed to fail but we need a good understanding of our failing.)*\n\nLet's say the test has 50 items, i.e. 50 pictures of different amount of dots. The participant has around 1s to estimate their quantity (and just types it in).  \nBut the pictures are also paired (unbeknownst to the participant): There's always 2 visualizations per correct value, 1 [chaotic](https://i.imgur.com/ECR3QXd.png) presentation and 1 [structured](https://i.imgur.com/YedLrTF.png) one.  \n*Sadly their order isn't randomized but has a complex structure to hide their pairedness. Still, everyone had the same order.*\n\n**My data per participant (n = 86):**    \n\n* chaotic_dots: 25x correct value\n* chaotic_dots: 25x estimation\n* structured_dots: 25x correct value\n* structured_dots: 25x estimation\n* **furthermore:** 3 different (this time randomized) presentation times each: 1200/1600/2000ms *(though maybe I disregard that information)*\n\nI'm not only interested in correct vs incorrect, but also how incorrect, how strongly the overshoots or undershoots are. I guess. Research interest is along the lines of \"are the structured ones significantly estimated better?\".\n\n \n**My questions are:**\n\n* 1) **SOLVED.** How do I best converge the correct and estimated value into one? Absolute difference doesn't say much I suppose. First I divided one with the other but that doesn't seem right. Percentage formular? Or should I use a fancy transformation? *(But, in that case, why should a mean count into an individual score?)* Anyway, let's call their combination \"***score***\".\n\n* 2) My lecturer always declared that there's the difference hypothesis and the correlation hypothesis and I'm a little stuck to decide how I should I test (or if I just could present both after one another? Can data qualify for both?)\n   \n* * 2.1) Correlation coefficient between chaotic and structured *scores*?  \n* * 2.2) Paired t-test with treating the chaotic and the structured (@ same person) as two paired samples, kinda like pre-test post-test data?  \n* * 2.3) Or something else?\n\n* 3) I'm not quite sure how to include the times. So every estimation value has one of three presentation times attached to it. My research interest would be like \"do people significantly estimate better at 2000ms than at 1200ms?\".   *I keep the option open to just disregard these data.*\n\n* 4) **SOLVED. Corrected my values to (est-correct)/correct.** A boxplot I made (then with still the mere quotient of correct/estimation...) showed greatly varying spans between items. Also, some pictures are consistently undershot, some overshot, some quite symmetrical. I guess simply an updated boxplot would suffice here though.\n\n* 5) There could be follow-up effects with one dot array influencing the estimation of its successor but how do I even.\n\nI feel like I have a total blackout, man. Some help would be greatly appreciated, thank you most dearly. And of course you don't need to do my entire homework folks, any single answer would be super nice already.\n\n**TL;DR: Made a test. n = 86. Each participant estimated dot quantities. 50 pictures of dots per participant. How to make 1 score out of correct quantity and estimation. Dot pictures also came in 2 distinct styles, 25 pictures each. Hypothesis: one dot style scored better estimations than the other. How to compare scores of dot group 1 and dot group 2.**\n\n",
        "created_utc": 1523480326,
        "upvote_ratio": ""
    },
    {
        "title": "Why does Variance Increase as you decrease the Number of Observations in your Sample?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bkjrp/why_does_variance_increase_as_you_decrease_the/",
        "text": "[deleted]",
        "created_utc": 1523480324,
        "upvote_ratio": ""
    },
    {
        "title": "Converting an average Likert scale score to a percentage? Is this wrong?",
        "author": "Spicy_Jimmies",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bkdlp/converting_an_average_likert_scale_score_to_a/",
        "text": "Hi all,\n\nI've created a five point Likert scale for a survey. Using the results I can easily convert to a weighted average (which I've done). What I want to know is, if it's okay to covert the result to a percentage by scaling it up. \nI can make the percentage by just dividing by five and multiply the result by 100, but It just feels wrong as I think it misrepresents the respondents answer. I'll explain.\n\nEssentially I'm scaling a 5 point Likert scale up to a 100 point scale. Where the values a respondent gives become equal to 1 = 20, 2 = 40, 3 = 60, 4 = 80 and 5 = 100. However this only gives the respondents 5 choices out of what should be 100. Where it should really be 1 = 1 to 20, 2 = 21 to 40, 3 = 41 to 60, 4 = 61 to 80 and 5 = 81 to 100.\n\nSo say someone answers a question with 4 and I convert this to fit a 100 point scale. I'd be changing it to 80, but if I had given this person a 100 point scale he may have answered 61, 80, or anywhere in between. And in a weighted average the difference between 61 and 80 could have quite an impact.\nThis of course ignores that the person may never have even selected an answer between 61 and 80 when given a greater choice.\n\nI feel like doing this misrepresents and misuses the person's response to the survey. Is this correct, or have I just gone down some rabbit hole? Thanks for any help in advanced!\n",
        "created_utc": 1523478960,
        "upvote_ratio": ""
    },
    {
        "title": "Covering a average Likert scale score to a percentage? Is this wrong?",
        "author": "Spicy_Jimmies",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bkaln/covering_a_average_likert_scale_score_to_a/",
        "text": "Hi all,\n\nI've created a five point Likert scale for a survey. Using the results I can easily convert to a weighted average (which I've done). What I want to know is, if it's okay to covert the result to a percentage by scaling it up. \nI can make the percentage by just dividing by five and multiply the result by 100, but It just feels wrong as I think it misrepresents the respondents answer. I'll explain.\n\nEssentially I'm scaling a 5 point Likert scale up to a 100 point scale. Where the values a respondent gives become equal to 1 = 20, 2 = 40, 3 = 60, 4 = 80 and 5 = 100. However this only gives the respondents 5 choices out of what should be 100. Where it should really be 1 = 1 to 20, 2 = 21 to 40, 3 = 41 to 60, 4 = 61 to 80 and 5 = 81 to 100.\n\nSo say someone answers a question with 4 and I convert this to fit a 100 point scale. I'd be changing it to 80, but if I had given this person a 100 point scale he may have answered 61, 80, or anywhere in between. And in a weighted average the difference between 61 and 80 could have quite an impact.\nThis of course ignores that the person may never have even selected an answer between 61 and 80 when given a greater choice.\n\nI feel like doing this misrepresents and misuses the person's response to the survey. Is this correct, or have I just gone down some rabbit hole? Thanks for any help in advanced!\n",
        "created_utc": 1523478293,
        "upvote_ratio": ""
    },
    {
        "title": "Using PCA to force two axis",
        "author": "PM_me_your_prose",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bjz8k/using_pca_to_force_two_axis/",
        "text": "So I've been learning about (and no doubt butchering) the joys of dimensionality reduction with PCA and TSNE and I was wondering;\n\nCan I use dimensionality reduction to flatten/merge dimensions that I know are in some way connected?\n\n**eg.** Can I use pca on a set of columns that represent the market share (percentage of shares, percentage of share price against their niche's total value)  that a company has to create a general 'market share' column... and then maybe plot it against another axis made from a set of columns that represent the total size of their market?\n\nI've done some reading (I'm not coming empty handed) that while PCA will retain linear relations in columns, TSNE will fuck that shit up and that there is likely a need for normalization so all the columns are equally weighted?\n\nIf what I'm suggesting is unholy in all ways statistical ... what would you recommend? Thank you so much for your help!",
        "created_utc": 1523475876,
        "upvote_ratio": ""
    },
    {
        "title": "When can I use phi coeffisient?",
        "author": "Scared-Famous",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bjuks/when_can_i_use_phi_coeffisient/",
        "text": "Hi. I have an independent variable which is nominal and categorical and has 2 values, and a dependent variable which is categorical and on the ordinal level, with 3 values.\n\nWhat kind of coeffisient should I use? Can I use phi?",
        "created_utc": 1523474871,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting test values in GLM (Python Statsmodels)",
        "author": "gimlet_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bjho3/interpreting_test_values_in_glm_python_statsmodels/",
        "text": "Hi,\nBeing fairly new to regression models, I have a few questions regarding interpretation of a GLM summary presented below. I have 22 independent variables x1-x18 and two  dependent variables: y1, y2.\nDo P&gt;|z| being 0.000 for the presented variables x1-x4 imply that these variables are statistically significant in forecasting my dependent variables: y1 &amp; y2 (Binomial model) --&gt; Can Null Hypothesis be rejected for them? What is the role the of Pearson chi2-tes value of 1.16 in this case with df 18? Or the log likelihood of -13669. ?\nAll help would be very appreciated!\n\nGeneralized Linear Model Regression Results\t\t\t\t\t\t\nDep. Variable:\ty1,y2\nNo. Observations:\t317\nDf Residuals: 298\t\t\nModel: GLM\nModel Family: Binomial\nDf Model:\t18\t\t\t\nLink Function:\tlogit\nScale:\t1.0\t\t\t\nMethod:\tIRLS\nLog-Likelihood:\t-13669.\t\t\t\nDeviance:\t24554.\t\t\t\nPearson chi2:\t1.16\t\t\t\nNo. Iterations:\t7\n\n\t\nvar, coef, std err, z, P&gt;|z|, [0.025, 0.975]\n\nx1     0.0016, 1.91E-05, 83.485,0.000, 0.002, 0.002\n\nx2     3,07E-02, 3.16E-07, 96.862, 0.000, 3.00E-05, 3.13E-05\n\nx3\t-25.661, 0.008, -332.962, 0.000, -2.581, -2.551\n\nx4\t-20.821, 0.008, -251.433, 0.000, -2.098, -2.066\n\n..\n\nx22\n",
        "created_utc": 1523472143,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing multiple groups against a control",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bi9jv/comparing_multiple_groups_against_a_control/",
        "text": "[deleted]",
        "created_utc": 1523462954,
        "upvote_ratio": ""
    },
    {
        "title": "[MWE included] Why doesn't R ANOVA give the same as hand computation?",
        "author": "linyeah",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bi8o5/mwe_included_why_doesnt_r_anova_give_the_same_as/",
        "text": "\n\nHere's an example :\n\n      t1 &lt;- c(10,12,15,19,21,25)\n      t2 &lt;- c(19,22,23,24,25,37)\n      ## using R\n      anova(lm(t1 ~ t2))\n\n      ## SS Total using 'hand' computation\n      globalMean &lt;- (mean(t1) + mean(t2))/2\n      res1 &lt;- t1 - globalMean\n      res2 &lt;- t2 - globalMean\n\n      ssError &lt;- sum(res1^2 + res2^2)\n      ssError\n\n\nthats 548 from the hand computation.\n\nSimilarly, the sum of squares between t1 and t2 is 192, and the sum of squares\nwithin is 356\n\nI don't see this represented by the output from R though.\n\nThanks\n",
        "created_utc": 1523462768,
        "upvote_ratio": ""
    },
    {
        "title": "Is it possible to add up effect sizes?",
        "author": "LouSalomee",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bhwzy/is_it_possible_to_add_up_effect_sizes/",
        "text": "If a study reports their effect sizes for men and women separately (they used the exact same instruments for both), is it possible for me to just add the effect sizes and then divide them by 2 (m/f) to get the overall effect size?\n\nExample: \n\nWomen:  d= .19\n\nMen: d= .29\n\nTotal: d= .24",
        "created_utc": 1523460289,
        "upvote_ratio": ""
    },
    {
        "title": "Don't understand Normal Random Variables",
        "author": "PoofLord",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bh6a9/dont_understand_normal_random_variables/",
        "text": "I'm completely lost on this topic and I'm hoping that someone could help me on the 'transformation' of normal random variables? ",
        "created_utc": 1523454399,
        "upvote_ratio": ""
    },
    {
        "title": "in machine learning, what does it actually mean to fit a distribution?",
        "author": "cheekyyucker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8bf5s3/in_machine_learning_what_does_it_actually_mean_to/",
        "text": "Ive heard it in the context of feature engineering",
        "created_utc": 1523430366,
        "upvote_ratio": ""
    }
]