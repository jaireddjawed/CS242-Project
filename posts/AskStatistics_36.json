[
    {
        "title": "What implications can we draw when two groups have different variances?",
        "author": "rayray2kbdp",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h8lw5/what_implications_can_we_draw_when_two_groups/",
        "text": "Suppose I have Country A and Country B. The mean weight of both countries is relatively the same. The standard deviation (or error) of Country A is much higher than that of Country B. What does this say about the weight of Country A, and why might the variation be different?",
        "created_utc": 1525537855,
        "upvote_ratio": ""
    },
    {
        "title": "Sampling Distribution of r-Squared",
        "author": "justbeane",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h87mk/sampling_distribution_of_rsquared/",
        "text": "I would like to learn about the distribution of r-squared in a various types of linear regression problems (simple or multiple, independent or dependent predictors). Part of my motivation would be to understand how to create a confidence interval for the r-squared value in a fitted linear regression model. \n\nCan any point with to a resource where I can find information about the distribution of r-squared? Ideally something with derivations?\n\nThanks!",
        "created_utc": 1525534112,
        "upvote_ratio": ""
    },
    {
        "title": "Questions relates to probability mass function",
        "author": "king_booker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h6jsc/questions_relates_to_probability_mass_function/",
        "text": "I have the following assignment to solve :- \n\nThe random variable X takes values -1, 0, 1 with probabilities 1/8, 2/8, 5/8 respectively.\n(a) Compute E(X).\n(b) Give the pmf of Y = power(X,2) and use it to compute E(Y). \n(c) Instead, compute E(power(X,2) directly from an extended table.\n(d) Compute Var(X).\n\nI am not able to solve questions b and c. Can anyone please help here? ",
        "created_utc": 1525513477,
        "upvote_ratio": ""
    },
    {
        "title": "Help with interpreting three way MANOVA",
        "author": "OkResult",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h64oh/help_with_interpreting_three_way_manova/",
        "text": "Hello!\nI ran into a problem while analyzing my results for my master thesis. Normally I would ask my supervisor, but it is the weekend, so she is enjoying her time off.\n\nI'm writing within psychology about stress and personality factors in two different office environments.\n\nHere is my problem, I have three dependent variables (two stressors and one for global stress) and three independent variables (one for office type, one for personality factor X, and the third for personality factor Y, each of these variables is divided into two levels). I'm interested in comparing the different personality factors and how they score on the stressors and global stress between the two offices and in combination within each office and between offices. I did a three-way MANOVA in SPSS, but  I got \nstuck in trying to interpret the output. \n\nIs this even the correct method of analysis? What analysis would you recommend? I really feel stuck here and would appreciate any help!!!",
        "created_utc": 1525506725,
        "upvote_ratio": ""
    },
    {
        "title": "Soccer Statistic",
        "author": "sport3000",
        "url": "http://sportstatist.com/soccer-statistic-eng/",
        "text": "",
        "created_utc": 1525501693,
        "upvote_ratio": ""
    },
    {
        "title": "Do 2^k factor design experiments and CRD experiments need to have homogenous experimental units?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h5j77/do_2k_factor_design_experiments_and_crd/",
        "text": "[deleted]",
        "created_utc": 1525498431,
        "upvote_ratio": ""
    },
    {
        "title": "I am trying to do some very basic data analysis in R with some forestry data and somethings not quite right...",
        "author": "urbrick_8",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h4l6o/i_am_trying_to_do_some_very_basic_data_analysis/",
        "text": "I need to perform a function on the data in one column based on the values in another column or two. I have been using tapply and it seemed to work when I had only one column to base the calculation on, but not when I used it for two. I am not sure what is going on. To summarize the data: I have measurements of woody debris diameters for each site. I need to add up all of the values for the debris per site. That seemed to work ok. I then need to add the measurements for the wood based on diameter classes as well as per site. I can include the data and formulas I have used so far, if that helps. ",
        "created_utc": 1525487250,
        "upvote_ratio": ""
    },
    {
        "title": "Reading data that isn't a table into R (communities and crime data by Redmond)",
        "author": "ECTD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h4ji6/reading_data_that_isnt_a_table_into_r_communities/",
        "text": "Hello,\n\nI'm having trouble reading communities and crime data into R and I cant figure out how to read it in since it doesn't come in the typical table format. Does anyone know how to do this?\n\nHere is my idea:\n&gt;cacd &lt;-read.table(\"/Users/firstname/Desktop/cac.txt\", sep = \",\",header=FALSE, dec=\".\", na.strings = \"?\")\n\nData is from: http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt\n\nedit:\nI got help. The answer is, can't believe I didn't realize this, using read.csv... \n\ncacd &lt;-read.csv(\"/Users/first/Desktop/cac.txt\", na.strings=\"?\")\n",
        "created_utc": 1525486758,
        "upvote_ratio": ""
    },
    {
        "title": "Use of False Discovery Rate if some of the variables are suspect a priori.",
        "author": "TheRealBeakerboy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h2krx/use_of_false_discovery_rate_if_some_of_the/",
        "text": "I was asked to compare some production samples produced in different processes. Going into the test, I expected 2 chemical compounds would be different. Since my instrument also provides results on 20 other compounds, I decided to see which other chemicals may differ. I thought I would use a standard t-test on the 2 I suspected would be different, but adjust the criteria using FDR for the others. I figure my a priori expectation on these two are different than the “screening” that I am performing with the other 20. Is this reasonable? My results show that the two have p&lt;.05, but if they were to be subjected to FDR they would not be significant. However, 3 other compounds were significantly different after FDR was applied.",
        "created_utc": 1525467650,
        "upvote_ratio": ""
    },
    {
        "title": "Help explaining the relation of the exponential and gamma distribution. (X is an exponential R.V., mean = 5)",
        "author": "tyler067",
        "url": "https://i.redd.it/to68wj1ujwv01.png",
        "text": "",
        "created_utc": 1525467417,
        "upvote_ratio": ""
    },
    {
        "title": "question about covariance and variance",
        "author": "tyler067",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h2h4b/question_about_covariance_and_variance/",
        "text": "When finding the variance for ---&gt; Var(4X - Y), when and why would you use this formula --&gt; 16Var(X) + Var(Y) - 4Cov(X,Y) instead of this formula ---&gt;16Var(X) + Var(Y)?",
        "created_utc": 1525466808,
        "upvote_ratio": ""
    },
    {
        "title": "Use of Failure Detection Rate if some of the variables are suspect a priori.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h21ff/use_of_failure_detection_rate_if_some_of_the/",
        "text": "[deleted]",
        "created_utc": 1525463231,
        "upvote_ratio": ""
    },
    {
        "title": "SPSS - which criteria should I use?",
        "author": "almost_february",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8h0m0b/spss_which_criteria_should_i_use/",
        "text": "Hello everyone,\nI would really appreciate your help. Did my research on explicit (Likert scale ) and implicit attitudes (IAT) towards mental illness and have no idea what criteria to use for my data. I have 3 uneven groups (43, 46 and 40) and want to compare means. Which is the best criteria?",
        "created_utc": 1525451797,
        "upvote_ratio": ""
    },
    {
        "title": "Calculating predicted probabilities in binary logistic regression",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gywaj/calculating_predicted_probabilities_in_binary/",
        "text": "[deleted]",
        "created_utc": 1525437353,
        "upvote_ratio": ""
    },
    {
        "title": "How to evaluate expB&lt;1 in logistic regression",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gykov/how_to_evaluate_expb1_in_logistic_regression/",
        "text": "[deleted]",
        "created_utc": 1525433994,
        "upvote_ratio": ""
    },
    {
        "title": "So I am learning regression analysis and I am able to calculate estimates and check inferences but still I don't know how all of those formulas and procedures were invented. Should I be worried that I don't know all of these?",
        "author": "Wickedmittal",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gye2o/so_i_am_learning_regression_analysis_and_i_am/",
        "text": "",
        "created_utc": 1525431905,
        "upvote_ratio": ""
    },
    {
        "title": "Standard Normal Distribution Interval of a Sample Mean",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gxyai/standard_normal_distribution_interval_of_a_sample/",
        "text": "[deleted]",
        "created_utc": 1525426012,
        "upvote_ratio": ""
    },
    {
        "title": "Get Z-Score values with a Ti-83+",
        "author": "Quaranges",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gxtj1/get_zscore_values_with_a_ti83/",
        "text": "Hello so, I have a exam soon and I need some guidance. I have a Ti-83+ and for instance I want to get the value \nof P[Z&lt;1.57]\n\nor P[-1.25&lt;z&lt;1.57]\n\nI've been reading documentation and I still can't get the right answer from the functions.\n\nfor the first one I should get about \n.9418 and then second one I should get .8543\n\nany help would be greatly appreciated!\n",
        "created_utc": 1525424195,
        "upvote_ratio": ""
    },
    {
        "title": "Interpreting the normal distribution",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gxgiy/interpreting_the_normal_distribution/",
        "text": "[deleted]",
        "created_utc": 1525418916,
        "upvote_ratio": ""
    },
    {
        "title": "Question regarding power levels and confidence intervals?",
        "author": "karlanthonyhsiao",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gwynr/question_regarding_power_levels_and_confidence/",
        "text": "I'm looking at the formula here: https://imgur.com/a/kfdpz0v\n\nThe formula allows you to calculate sample size necessary to meet some certain confidence interval requirement. I'm confused regarding the t^2Ei / 2, g(n-1) aspect. I think it might have something to do with df's, but not sure. Any insight? ",
        "created_utc": 1525412439,
        "upvote_ratio": ""
    },
    {
        "title": "Testing for the relevance of a factor variable with multiple levels for a linear model.",
        "author": "utty2298",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gvskl/testing_for_the_relevance_of_a_factor_variable/",
        "text": "Hi everyone!\n\nCurrently the project im working on requires me to use automated model selection methods to come up with a regression model to check associations in the data. \nThe problem i am running into is when using best subsets. There is a factor variable in my data which has 4 levels. Upon running this method the output is only showing two of the levels are significant. But when running stepwise regression it suggests that the full factor variable is relevant. \nCan anyone suggest or give me guidance to which tests or method i can use to check whether its i should keep the full factor variable in my regression model or whether the model should only contain a certain level of that variable.\n\nThe language i am using for this project is R.\n\nThank you",
        "created_utc": 1525399646,
        "upvote_ratio": ""
    },
    {
        "title": "Research on factors affecting housing prices in U.S. after 2008 recession",
        "author": "georgewh1te",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gvl3v/research_on_factors_affecting_housing_prices_in/",
        "text": "I'm plan to write econometric research paper on factors that affect house prices in U.S. I would like to get help with what variables would work the best and some theory help with bulding and testing the model. Panle or cross sectional types are preffered. I was thinking to rather look at the period after 2008 recession since it is a form of a start of new period afer regulations. \nAs of right now I have data from FRED:\n1. producer price index for special indexes: construction material, index 1982=100, quaterly, not seasonally adjusted\n\n2. Median Sales Price for New Houses Sold in the United States, Dollars, Quarterly, Not Seasonally Adjusted\n\n3. New Privately\\-Owned Housing Units Completed: Total, Thousands of Units, Quarterly, Seasonally Adjusted\n\n4. University of Michigan: Consumer Sentiment, Index 1966:Q1=100, Monthly, Not Seasonally Adjusted\n\n5. Total Population: All Ages including Armed Forces Overseas, Thousands, Monthly, Not Seasonally Adjusted",
        "created_utc": 1525397569,
        "upvote_ratio": ""
    },
    {
        "title": "What are some polls I can do for these hypothesis tests?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gtv5b/what_are_some_polls_i_can_do_for_these_hypothesis/",
        "text": "[deleted]",
        "created_utc": 1525382203,
        "upvote_ratio": ""
    },
    {
        "title": "Using Statistics to create a Composite or Summary Score to rank Fantasy Hockey players",
        "author": "Ciparoo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gt2nq/using_statistics_to_create_a_composite_or_summary/",
        "text": "OK so I'm trying to create a score to rank fantasy hockey players before a league draft. The data points are:\n\nGoals\nAssists\nPower Play Points (PPP)\n+/-\nHits\nBlocks\n\nAll of these are counting stats with the exception of +/-, which can be positive, negative, or zero. This score would also take account of how many games each player played, so it would really be Goals/game, Assists/Game, etc, but not sure how that would work with +/-. I would like to be able to use this on the past 2 or 3 seasons to have more data than just last season, but there would be some players (like rookies) who wouldn't have that long of a record. \n\nI thought about calculating a z-score for each category and adding those up to create the player score. Does that make sense? Can I do that? Maybe I'm totally off base here, please help! ",
        "created_utc": 1525375921,
        "upvote_ratio": ""
    },
    {
        "title": "Questons about how use data with small groups",
        "author": "OkResult",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gskcr/questons_about_how_use_data_with_small_groups/",
        "text": "Hello!\nIm doing my master thesis and have run in to some problems. I study psychology and has conducted a study where i looked at personality factors and how a combination of these affect how you deal with stressors in an office environment. I have conducted the study in two different offices and are going to compare the results from these offices. The participants from each office is assigned to one out of four groups (depending on the combination of personality factors), and here is my problem. I had no way to forsee how big these groups where going to be, I assumed some of the groups would be smaller since these combinations are less common, but the differences in group size is more severe than I would have expected. The biggests group is 60+ participants and the smallest just 4. I need help figuring out what to do here, do I give up and admit that there are no significant result or is there some method that could be used that would allow me to still use my data?\n\nWhat I want to do is to compare the groups within each office and look at group differences between the offices. I figured a 2x2x2 ANOVA of some sort is the best way to go, but how do I get there with such different group sizes?",
        "created_utc": 1525371896,
        "upvote_ratio": ""
    },
    {
        "title": "t-test independence questions",
        "author": "hunaja00",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8grz1x/ttest_independence_questions/",
        "text": "I'm interested in comparing the mean of two groups, where one is a much larger group and includes the other group. The data is about average graduate salaries of a particular university compared to the national average, however the national average will surely include data from the particular uni.\n\nCan a 2-sample t-test be used in this scenario or are these two groups not considered independent?\n\nThanks!",
        "created_utc": 1525367323,
        "upvote_ratio": ""
    },
    {
        "title": "Difference in Difference",
        "author": "wutangforever2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8grdoj/difference_in_difference/",
        "text": "Hi,\n\nIm conducting a difference-in-difference model using Stata. Im having problem to interpret the intercept in the regression. They are showing a negative slope, but I am excepting a positive slope. When conducting a test such as difference in difference where I look at the treatment effect by using control variables, are there any concerns I specifically must look after? Are there any risk for trending or seasoning? I think the estimator might be inconsistent.",
        "created_utc": 1525362636,
        "upvote_ratio": ""
    },
    {
        "title": "How do you use stored regression residuals as moving-average error terms and then forecast?",
        "author": "ajskelt",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gr6hb/how_do_you_use_stored_regression_residuals_as/",
        "text": "I was reading a paper on electric load forecasting and trying to understand their methodology.\n\nThe equation they use is here: https://imgur.com/BovEtjQ\n\nThe part I'm struggling to understand is underlined. They describe their process as:\n\n\"In the estimation, each equation is initially estimated ignoring the moving-average error terms and the regression residuals stored. The equations are then re-estimated using the regression residuals from the previous step as observed moving average error terms. This process is then iterated until convergence...\"\n\nCan anyone ELI5 how this calculation process is being done? My understanding is that the first time a regression is done without the moving average error terms. Then with training entry, you add a variable that is the residual from the entry 1 day ago (εh d−1)* and 7 days ago (εh d−7)* these will be used in the next regression, and both will get a coefficient.\n\n*I can't subscript apparently, so sorry\n\nThen you also add the actual error (εh d). Somehow this doesn't get a coefficient, so I'm not sure how this is different from the intercept/kept separate. Also, when it comes time to forecast, you can see the forecast and predict using your model, you could calculate the (εh d−1) and (εh d−7) however I'm not sure how you'd calculate the (εh d) to incorporate into the model, since you won't have an actual value.\n\nAny thoughts on this?\n\nFull Paper: https://pdfs.semanticscholar.org/b802/b3935a1526c1ca4314393a8295f649de3339.pdf",
        "created_utc": 1525361051,
        "upvote_ratio": ""
    },
    {
        "title": "Funny statistics idea using AOL Search Data Leak",
        "author": "pagrafy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gqyvx/funny_statistics_idea_using_aol_search_data_leak/",
        "text": "My professor wants us to present him funny and maybe idiotic statistics we'll visualize using the AOL Search Data Leak in 2006. \n\nWe're also allowed to use other open databases, but the main focus point should still be the data leak. \n\nAny ideas for fun topics I can talk about?",
        "created_utc": 1525359331,
        "upvote_ratio": ""
    },
    {
        "title": "Assessing whether assumptions for the Mann Whitney test are being met",
        "author": "Roxy123456Q",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gqwc4/assessing_whether_assumptions_for_the_mann/",
        "text": "I'm a medical student trying to analyze a research population for the first time. I know litterally zero about statistics and am mostly learning through Youtube videos. Sorry if this is a really stupid question!  \n\n\nOkay, so I want to compare the age of menopause between two groups, the survivors \\(of a certain cancer\\) and the controls. \n\n\nI performed a normality test in SPSS and it gave me a value of 0.019. Since this is less than 0.05 it is established that the data is not normally distributed. So I cannot use the t\\-test to compare my groups, but need to use the Mann Whitney.  \n\n\nThe next thing I did was to check whether the assumptions for the Mann Whitney test are being met.   The thing is the histograms look really different.\n\nhttps://i.redd.it/eomsf9cdknv01.png\n\nHowever, when I do the Test of Homogeneity of Variance \\(In SPSS analyze \\-\\&gt; Descriptive statistics \\-\\&gt; Explore \\-\\&gt; Dependent list \\(Age\\_menopause\\) and factor list \\(Survivol\\_Control\\) \\-\\&gt; Plots \\-\\&gt; check: factor levels together and untransformed\\) it gives me the following numbers:\n\n\nhttps://i.redd.it/3hy16dneknv01.png\n\nSo, if I understand correctly. Because 0.358 is bigger than 0.05, the 0 hypothesis is not rejected and thus the data is distributed normally after all.\n\nMaybe I'm missing something, but how is it possible that the test of homogeneity of variance says the distributions are similar when if you look at the histograms they are totally not. Or did I miss something?\n\nAnd if all of this is correct, it means that the Mann WHitney test can be used to assess whether there is a difference in age of menopause between the survivors and controls, right?\n\nThanks in advance!",
        "created_utc": 1525358768,
        "upvote_ratio": ""
    },
    {
        "title": "A couple of silly questions",
        "author": "Stray_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gqjtm/a_couple_of_silly_questions/",
        "text": "Hi r/AskStatistics, this might be a bit of a silly question but I'm still trying to get my head around understanding concepts in quantitative papers. I've only just got my head around OR and P values, and I'm trying to critically appraise some papers for working towards my dissertation, but I've just got a couple of questions that I'm struggling to find the answers to.\n\nIn terms of studies doing power calculations, how will papers present to the reader that they've actually done one? I'm sure that this paper I'm analysing has met the sample size, but I'm struggling to find where or if they've done a power calculation? How is it usually presented? Will they always present it?\n\nAnother question that I feel stupid for asking, but I've seen examples of previous years' dissertations referring to something as \"(r=0.35, p=0.01)\". I understand what the p-value means (\"understand\" being a very strong word), but what is this \"r\" value? The only thing I've managed to turn up is stuff relating to thermo-physics and stuff, and I doubt this paper is talking about that haha.\n\nI'd appreciate any help in getting my head around this, and anything presented in an ELI5 format would be amazing!",
        "created_utc": 1525355810,
        "upvote_ratio": ""
    },
    {
        "title": "Is this a valid calculation?",
        "author": "And12rew",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gqedm/is_this_a_valid_calculation/",
        "text": "I have 83 years worth of correlations, and want to calculate the change in correlations from start to finish. I have put them into excel and used a trendline to calculate an R\\^2 value. \n\nThe question is if I am manipulating my data to much, making it useless.\n\nThe original values are not considered significant.",
        "created_utc": 1525354452,
        "upvote_ratio": ""
    },
    {
        "title": "Has the science of statistics evolved? I am currently reading an old text from the 1900's that mentions stats and the numbers proposed there sound false (explanation below).",
        "author": "ParticularPhotograph",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gqczl/has_the_science_of_statistics_evolved_i_am/",
        "text": "This text \\(by a German guy named Rhine\\) describes a parapsychological experiment. It is described as follows \\(by another author\\): \"In each series of experiments the pack is laid out 800 times, in such a way that the subject cannot see the cards. He is then asked to guess the cards as they are turned up. The probability of a correct answer is 1 in 5. The result, computed from very high figures, showed an average of 6.5 hits. The probability of a chance deviation of 1.5 amounts to only 1 in 250,000\". This supposedly proves that there is some type of parapsychological effect at work. \n\nI am wondering if statistics as a science has evolved, because this doesn't sound quite right, even though I'll admit I am no expert. Is it possible to arrive at 6.5 correct hits from a modern statistical point of view? ",
        "created_utc": 1525354095,
        "upvote_ratio": ""
    },
    {
        "title": "How much Trigonometry do I need to know to apply for a Masters in Statistics? And how much will I know after?",
        "author": "iwannaknowreally",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gp82g/how_much_trigonometry_do_i_need_to_know_to_apply/",
        "text": "This is because my undergrad doesn't cover functions in trig. thanks.",
        "created_utc": 1525341612,
        "upvote_ratio": ""
    },
    {
        "title": "Calculate p-value in binary data",
        "author": "Rayashy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gort6/calculate_pvalue_in_binary_data/",
        "text": "I'm sorry if this is a stupid question, but i have low background in statistics, and it's imperative that i get this right.\n\nSo i have medical data that studies the presence of a pathology in patients. I have a table that represents the symptoms/attributes for each patient, and whether the pathology is present or not.\n\nWhat i need to compute is the correlation between each symptom and the pathology. For instance, if \\[symptom1\\&gt;7&amp;#37;\\] or \\[symptom1\\&lt;7&amp;#37;\\] how does that affect the result.  Apparently i'm supposed to calculate the p\\-values for each case, and i'm not sure how to do it. \n\nAll the ressources i have found online use continuous values, as in the value of a certain measure. But here i have a binary value \\(is the pathology present yes/no\\). I appreciate any help i can get and thank you.",
        "created_utc": 1525335045,
        "upvote_ratio": ""
    },
    {
        "title": "What's a good application or program to record members in the sports club for statistic purposes ?",
        "author": "gnosza",
        "url": "https://www.reddit.com/r/answers/comments/8gomts/whats_a_good_application_or_program_to_record/",
        "text": "",
        "created_utc": 1525334439,
        "upvote_ratio": ""
    },
    {
        "title": "FDR and q-value",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8goky6/fdr_and_qvalue/",
        "text": "[deleted]",
        "created_utc": 1525332274,
        "upvote_ratio": ""
    },
    {
        "title": "Why are zero-sum constraints used?",
        "author": "ayeandone",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8goji8/why_are_zerosum_constraints_used/",
        "text": "I often see in factorial models a set of zero sum constraints that say the sum of µ + αi + βj + αβij all ai = 0, sum of all Bj = 0, etc.\n\nI am wondering why are these constraints used? What purpose do they serve and what do they indicate? ",
        "created_utc": 1525331714,
        "upvote_ratio": ""
    },
    {
        "title": "Kruskal Wallis and homoscedasticity?",
        "author": "kayanh",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gnz9m/kruskal_wallis_and_homoscedasticity/",
        "text": "Ive been told that Kruskal-Wallis test assumes equal variances (homoscedasticity), however, i cant seem to find any credible source stating this. Does anyone know a credible source for this? \n\nEDIT:\nI went back to my lecturer and he showed me a book claiming that both mann whitney u and kruskal wallis test assumes equal variances/similarly shaped distributions (cant remember exactly which, or if it was both). Biostatistical analysis fifth edition by Jerrold h. Zar. \nHowever, i had three other statistical course books next to me, each claiming different things. One said nothing about any assumptions being needed to preform kruskal wallis, one said that the shapes of the distributions had to be similar and one that said that kruskal wallis SHOULD be used when variances are heterogenous. \n\nLife is hard as a beginner to statistics i guess",
        "created_utc": 1525324501,
        "upvote_ratio": ""
    },
    {
        "title": "CDF for discrete distribution questions",
        "author": "ztnq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gm11t/cdf_for_discrete_distribution_questions/",
        "text": "For continuous I know P(X&lt;a) is the cdf and an integral of the density function from -infinity to a, and that P(X&gt;a)=1-P(X&lt;a)= the integral of the density function from a to infinity;and is also the survival function.\n\nI have a few questions regarding the cdf and 1-cdf for discrete probability distributions.\nSo are both P(X&lt;a) and P(X&lt;=a) cdfs? I usually see a definition with P(X&lt;=a) but not P(X&lt;a).\nWould P(X&gt;=a) be the sum of the pmf from x=a to infinity?(that's what I think it is, but when I look it up I don't see any claim about it,while there's a bunch for the continuous version,calling it the survival function)\nWhat would P(X&gt;a) be the sum from? From the next value bigger than 1 to infinity  over the density function I assume. Is there a good math notation for saying that?\nThanks.",
        "created_utc": 1525304926,
        "upvote_ratio": ""
    },
    {
        "title": "Sample distribution questions",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8glrhh/sample_distribution_questions/",
        "text": "[deleted]",
        "created_utc": 1525302505,
        "upvote_ratio": ""
    },
    {
        "title": "Basic probability distribution question",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8glovc/basic_probability_distribution_question/",
        "text": "[deleted]",
        "created_utc": 1525301851,
        "upvote_ratio": ""
    },
    {
        "title": "hello! i saw the following questions in some local math competition and was wondering if you could explain how to solve them please",
        "author": "mathnoobie123",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8glhiu/hello_i_saw_the_following_questions_in_some_local/",
        "text": "A rain of meteors is falling on a round area, with a radius of 10 km, so that each meteor is falling in a random please inside the are, without any dependancy on other meteor falls. assuming that the number of meteors that fall inside the round area is a poisson random variable with the parameter 2, and that the three assumptions of the poisson prccess are valid in the area where the meteors fall:\n\n1)what are the odds thatexactly 3 meteors will fall inside that area?\n\n2)inside the round area we mark a round circle with the radius of 6 km, inside the round area mentioned above. what are the odds that inside the little circle mentioned(the 6km), won't fall any meteor?\n\n3)if we know that in the biggercircle fell exactly 4 meteors(the 10 km), what are the odds that exactly one of them fell inside the small area(the 6km) inside the 10 km?\n\njust to clarify, the small circle(6km radius) is inside the bigger circle(10km radius)\n",
        "created_utc": 1525300086,
        "upvote_ratio": ""
    },
    {
        "title": "What is the probability of this happening?",
        "author": "hyughatu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gl8vo/what_is_the_probability_of_this_happening/",
        "text": "An event with a .0014792899 chance of happening, happening 2 out of 8 times. Also is there a formula for this?",
        "created_utc": 1525298022,
        "upvote_ratio": ""
    },
    {
        "title": "Crosspost from r/learmath Question about two related probabilities being utilized to form one probability of an event happening.",
        "author": "moremolotovs",
        "url": "https://www.reddit.com/r/learnmath/comments/8gi358/taking_into_account_two_related_probabilities/?st=JGPFLU0A&amp;sh=6da9238c",
        "text": "",
        "created_utc": 1525285591,
        "upvote_ratio": ""
    },
    {
        "title": "When do I divide my S.D by the square root of my sample (n)?",
        "author": "MotorReality4",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gil9n/when_do_i_divide_my_sd_by_the_square_root_of_my/",
        "text": "Hey guys, so my question is pretty much my title. Sometimes in books I would see that standardization formula is Z=(x-mu)/sigma and other times I would see something along the lines of Z=(x-mu)/(sigma/sqrt(n)).\n\nWhats the difference? When do I divide my sigma by the sqrt of n?",
        "created_utc": 1525277410,
        "upvote_ratio": ""
    },
    {
        "title": "How to test validity and reliability of data with categorical and discrete variables?",
        "author": "KIDE777",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gi18m/how_to_test_validity_and_reliability_of_data_with/",
        "text": "Hi, I'm new to statistic, so I don't really know anything about it. I have SPSS but also yet to fully understand it.\n\nI have a simple survey about relationship between car model with coolness and sex appeal. IIRC car model (\"Honda Civic\", \"Bugatti Chiron\") is categorical variable while coolness and sex appeal are discrete ones (both using scale 1-7)\n\nThe teacher asked for validity and reliability tests on the survey data. How to do it? What test should I use?\n\nThank you",
        "created_utc": 1525273082,
        "upvote_ratio": ""
    },
    {
        "title": "Pigeon population",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ghusc/pigeon_population/",
        "text": "[deleted]",
        "created_utc": 1525271644,
        "upvote_ratio": ""
    },
    {
        "title": "How many iterations of Monte Carlo Simulation?",
        "author": "Neoflash_1979",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ghsq0/how_many_iterations_of_monte_carlo_simulation/",
        "text": "So I've built a simple computer model of a tic-tac-toe game and I want to run random playouts to find the winning probability for the first player and the second player. How can I find out how many iterations I need to run before my overall results are fairly representative of reality? Lets say I wanted to be 99% confident that my results were within a 2% margin of error or something like that.\n\nI guess it's the same thing as asking how many times do I need to roll a dice before the accumulated results are representative of the real odds.",
        "created_utc": 1525271169,
        "upvote_ratio": ""
    },
    {
        "title": "Business Statistics vs. Elementary Statistics",
        "author": "HalliganHooligan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ghe3n/business_statistics_vs_elementary_statistics/",
        "text": "I'm struggling with course selection for this summer semester. Approximately 18 months ago I took Elementary Statistics to fulfill my math requirement. I made an A, however, it took quite a bit of time to achieve that even with it being my only course and the professor allowed us to bring a note card with formulas to tests. I question whether a Business Statistics course will be doable with my other writing intensive course this summer.\n\nSo, my question is what exactly is the difference between the courses? Should I expect Business Statistics to be more difficult or the same? I'm honestly debating on dropping the course before the semester starts since it is essentially only a filler course (no other coursework is available this summer). I know I could complete it, but I'm questioning if the headache would be worth it.",
        "created_utc": 1525267662,
        "upvote_ratio": ""
    },
    {
        "title": "Back-to-back procedures, was it worth doing the second one?",
        "author": "imamonion",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gh4ei/backtoback_procedures_was_it_worth_doing_the/",
        "text": "I have a dataset of people who had back-to-back procedures and I have whether or not polyps (precancerous lesions) removed during each procedure. Since any polyps found in the first procedure are removed, the second procedure will only yield additional polyps.\nThis is generally what my data looks like:\n\nSubject | Procedure 1, polyp found | Procedure 2, polyp found | Overall, polyp found\n-------|------------------------|------------------------|--------------------\n1 | 1 | 0 | 1\n2 | 1 | 1 | 1\n3 | 0 | 1 | 1\n4 | 0 | 0 | 0\n\nWhere 1 = a polyp was found.\nI want to compare the Procedure 1 column and the overall column to find out if the increase in detection rate (60.8% vs. 67.5%) is significant. \nI found a paper that did a similar study [here](https://www.ncbi.nlm.nih.gov/pubmed/18389446):\n\n&gt; From the clinical point of view, we tested the equivalence between one colonoscopy or two back−to−back colonoscopies to get an optimal diagnosis of patients with polyps, adenoma, polyps or adenoma larger than 5 mm, and advanced adenoma. Compared with the first colonoscopy, there was an increased diagnosis rate by back−to−back colonoscopy for patients with polyps (42% [95% CI 37±48] vs. 66% [60±71]; P &lt; 0.0001) or with adenoma (26 % [21±31] vs. 36 % [30 ± 41]; P &lt; 0.02), as well as with polyps larger than 5mm (26% [20±31] vs. 31% [25± 36]; P &lt; 0.001), or adenomas larger than 5 mm (16 % [11 ± 20] vs. 18 % [14 ± 23]; P &lt; 0.01), but these rates were not significant for patients with advanced adenoma (8 % vs. 9 %).\n\nI'm pretty sure they used the McNemar's test to make these comparisons. I'm not sure if this was a valid use of the test since the detection rate can only increase with the finding of additional polyps during the second procedure. \n\nHere's how I set up the McNemar's test with my data:\n\n | No| Yes\n---|---|----\nNo | 64 | 13\nYes |0 | 118\n\nThe \"Yes/No\" cell will always be 0 since if Procedure1=1, Overall=1 whether or not you found a polyp in the second procedure. Would McNemar's test apply? If not, is it possible to do the analysis with a different test or at all?\n\nThanks in advance!",
        "created_utc": 1525265206,
        "upvote_ratio": ""
    },
    {
        "title": "Poisson regression to estimate relative risk for binary outcomes (in SPSS)",
        "author": "abcbrakka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ggny4/poisson_regression_to_estimate_relative_risk_for/",
        "text": "Hi,\n\nI am trying to calculate relative risks for a condition \\(prevalence 60&amp;#37;\\) in my population. My first choice is negative binomial log link in SPSS but for some models I am getting covergence problems.\n\nNow, after being tipped on Reddit and reading about it, I want to try a modified \\(quasi\\) Poisson regression analysis with robust standard errors, as described here:\n\n[https://stats.idre.ucla.edu/sas/faq/how\\-can\\-i\\-estimate\\-relative\\-risk\\-in\\-sas\\-using\\-proc\\-genmod\\-for\\-common\\-outcomes\\-in\\-cohort\\-studies/](https://stats.idre.ucla.edu/sas/faq/how-can-i-estimate-relative-risk-in-sas-using-proc-genmod-for-common-outcomes-in-cohort-studies/)\n\n[https://stats.stackexchange.com/questions/18595/poisson\\-regression\\-to\\-estimate\\-relative\\-risk\\-for\\-binary\\-outcomes](https://stats.stackexchange.com/questions/18595/poisson-regression-to-estimate-relative-risk-for-binary-outcomes)\n\nNow, on the first website there is an example written in SAS, and I am having problems translating it to SPSS \\(only package I am familiar with\\). In SPSS I go to GLM, choose 'Poisson loglinear', but there is no robust standard error option \\(there is a 'Robust estimator' for the covariance matrix, but I think this isn't the right option.\n\nAre there any redditors who have experience doing modified Poisson for binary outcomes in SPSS? ",
        "created_utc": 1525260401,
        "upvote_ratio": ""
    },
    {
        "title": "Hierarchical (blockwise entry) order",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gffyn/hierarchical_blockwise_entry_order/",
        "text": "[deleted]",
        "created_utc": 1525244150,
        "upvote_ratio": ""
    },
    {
        "title": "Help with interpreting a p-value histogram",
        "author": "[deleted]",
        "url": "https://i.redd.it/u94t9epi3ev01.png",
        "text": "[deleted]",
        "created_utc": 1525243884,
        "upvote_ratio": ""
    },
    {
        "title": "SOS! I am a stats n00b and I need help with my biostatistics",
        "author": "saltyjm",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gfdfr/sos_i_am_a_stats_n00b_and_i_need_help_with_my/",
        "text": "Biostatistics subject is mandatory in my degree and I legit have no idea what I am doing. If anyone can help explain what the eff I do for the questions below, I will be forever at your mercy!\n \n**A study was performed on a sample of 147 adults to measure the effect of a health promotion intervention on the outcome measure “Number of portions of fruit per day” which is measured on a continuous scale. At baseline, the sample mean of “number of portions of fruit per day” was 4.05 with a sample standard deviation of 1.23. \nAssume that the outcome measure “number of portions of fruit per day” is normally distributed within the population and that the sample mean and sample standard deviation provide reasonable estimates of the population parameters.** \n\n&gt;a) Calculate the 98% reference range for the “number of portions of fruit per day”. \n&gt; \n&gt; b) Calculate and interpret a 98% confidence interval for the population mean “number of portions of fruit per day”. \n&gt; \n&gt; c) Describe in words the difference between the reference range, computed in part (a), and the confidence interval computed in part (b). \n&gt; \n&gt; d) Estimate the proportion of the population that consume between 3 and 5 portions of fruit per day. \n&gt; \n&gt; e) Estimate the proportion of the population that consume less than 3.5 or more than 4.5 portions of fruit per day. \n&gt; \n&gt; f) Calculate and interpret a (two-sided) p-value to test the null hypothesis that the population mean “number of portions of fruit per day” is 3.83.\n",
        "created_utc": 1525243233,
        "upvote_ratio": ""
    },
    {
        "title": "Regression on detail data vs. summary",
        "author": "Trek7553",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gejyj/regression_on_detail_data_vs_summary/",
        "text": "I am trying to predict returning student enrollments. For simplicity, let's say I have two predictors: Program and Gender.\n\nOne way to approach this would be to create a dataset showing how many students were in each program for each gender in the historical data and then what the return rate was. This might look something like this:\n\nProgram | Gender | Avg Credits\n---|---|----\nMath| Male | 3.2\nMath | Female | 3.4\nScience | Male | 5.1\n\n[etc.]\n\nThen I could use a linear regression to determine what the average credits is based on historical data for each combination of program and gender (I would have several examples of each combination, one from each year). I would then use this along with current student counts to create my forecast.\n\nOR\n\nI could build a dataset with each student in it and indicate their program and gender and actual credits (historical). \n\n\nStudent | Program | Gender | Credits\n---|---|----|----\n1 | Math | Female | 3\n2 | Science | Female| 6\n[etc.]\n\nThen I could apply the resulting formula to all of my current students to project their individual credits. Then I would sum the results.\n\nThe problem with the second option is that there is too much variation with individual students to accurately predict at such a granular level. However, my theory is that I could make student-level predictions and then sum the predictions to smooth out those variations. Is that true?\n\nAre there other pro's and con's I should consider?\n\nThe real model has quite a few more predictors, and some are numeric (which I can't use very well in the first option unless I create bins).\n\nThanks in advance!\n",
        "created_utc": 1525233374,
        "upvote_ratio": ""
    },
    {
        "title": "Any statistical justification for dropping weekend data?",
        "author": "Pimp_Fada",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gdmde/any_statistical_justification_for_dropping/",
        "text": "Hi\n\nI'm doing some GARCH modelling for comparison of two financial datasets. One dataset has no weekend data. Unfortunately, R's \"rugarch\" package doesn't take any data with missing data which means I have two choices:\n- drop weekend data\n- fill weekend data for the dataset with missing dats\n\nI'm leaning towards dropping weekend data as filling data however good the data filling techniques nullifies the comparison as it's technically invokes a \"look ahead bias\". Is there any justification for dropping the weekend data tho?",
        "created_utc": 1525224122,
        "upvote_ratio": ""
    },
    {
        "title": "Statistics Question Using Census Data",
        "author": "Buckybadger907",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gdlv2/statistics_question_using_census_data/",
        "text": "What are the implications for making inferences if a logistic regression analysis is done using census data, rather than data from a representative sample of a population?\n\na)  Significance tests would not be justified, so no conclusions could be made from the data.\nb)  Logistic Regression results would describe relationships in the population, but inferences would not mean anything for the population.\nc)  Significance tests would only indicate if there were significant relationships in the census data, but the results could not be generalized to a population.\nd)  It would be important to report the p-values for estimated coefficients so that decision makers could draw their own conclusions about significance.\n",
        "created_utc": 1525223978,
        "upvote_ratio": ""
    },
    {
        "title": "Hypothesis testing for convenience sampling?",
        "author": "bigblackbrick",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gdejt/hypothesis_testing_for_convenience_sampling/",
        "text": "Help, for a Statistics project we used a non-random sample which narrowed down our choices to analyze our data. Our study involved the reaction of people on litter being thrown right in front of them. However, we are having trouble because first, our data is not normally distributed second, we used convenience sampling. Which hypothesis test can we use since we cannot use Chi-Square (correct me if I'm wrong)?",
        "created_utc": 1525222068,
        "upvote_ratio": ""
    },
    {
        "title": "I have limited time to study for a Methods and Statistics final; I’m dyscalculic. I need help prioritizing.",
        "author": "loonaboots",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gd9c5/i_have_limited_time_to_study_for_a_methods_and/",
        "text": "For background, it’s an environmental studies course. i also know this is a bit of a weird question, so if you have no clue what to do that’s fine. \n\nwe’re using the fourth edition of “statistics in plain english,” and [this is a link to a thing that lists the chapters](http://tcurdan.com/content.htm)if that helps. we’ve been assigned 1-8 and 12, and assignments have focused on z and t scores, correlation, descriptive data, and standard error. \n\ni don’t know what this test will look like- the final is the first and only test i’ve had in this class, and i can’t find a review sheet or anything on the course page. \n\nso basically, i’m looking for suggestions for what to focus on- i have no clue what my brain is going to be able to retain, especially so for formulas. so, any guidance (whether it’s a ranking of what’s easiest or what pops up the most in other things) on how to do this would be great. \n\nliterally, anything would be good. ",
        "created_utc": 1525220695,
        "upvote_ratio": ""
    },
    {
        "title": "help with multiple interaction regression model",
        "author": "tyler067",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gcm1g/help_with_multiple_interaction_regression_model/",
        "text": "I have the answer to the problem but I do not understand how my professor got the answer. the problem and the answer are in the two links. the problem is on both the first and second link.\n\nlink 1 ---&gt; https://i.stack.imgur.com/WqoMy.png\\\nlink 2 ---&gt; https://i.stack.imgur.com/qSV1u.png",
        "created_utc": 1525214667,
        "upvote_ratio": ""
    },
    {
        "title": "A good test to use please",
        "author": "kiri_katana",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gbpmv/a_good_test_to_use_please/",
        "text": "I have unequal N's (46, 42, 14, 8) for people reporting religion, and I want to study the effect religion has on the scores of a questionnaire. I tried using a regression model with dummy coding, but the model wasn't good. Any suggestions? ",
        "created_utc": 1525207156,
        "upvote_ratio": ""
    },
    {
        "title": "Non-normality &amp; 1-way ANOVA/KW problem",
        "author": "PudendalCleft",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gb9od/nonnormality_1way_anovakw_problem/",
        "text": "Hi all,\n\nI have two studies, both n=19. Both look at a quantitative value for protein expression in the brain. Study 1 and 2 are from the same experiment but study 2 carried on after study 1 (in an effort to determine whether time after the experiment concluded had any effect on protein expression). Each study has four groups, so small sample sizes!\n\nFor study 1, I have Gaussian distribution so I 1-Way ANOVAd it with a Dunnett's (all compared against the controlliest control group). For study 2, I do not have normal distribution so I'm thinking that I need to run a KW test with a Dunn's posthoc. Running an ANOVA on study 2 gives non-significant results for all multiple comparisons. The KW gives 1 out of 3 comparisons as significant. This same comparison is significant in study 1's ANOVA.\n\nBecause I will be comparing study 1 with study 2, is it 'good' to compare the ANOVA &amp; the KW?\n\nMy supervisor ran the ANOVA and just assumed normal distribution. That, or he thought that the ANOVA was robust enough to overcome the non-normal distribution. This clearly isn't the case, especially not with 4-6 samples per group. He's checked for outliers but there aren't any. I tested for normality using Shapiro-Wilkes and the other normality tests don't have enough samples to run.\n\nI'd appreciate any thoughts &amp; I'm happy to provide more info. if necessary.",
        "created_utc": 1525203696,
        "upvote_ratio": ""
    },
    {
        "title": "At what point is an event so statistically unlickely to happen that you can disregard the statistic.",
        "author": "Controlled01",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8gb7b4/at_what_point_is_an_event_so_statistically/",
        "text": "An example might be the odds of all my atoms simultaneously passing through a wall due to quantum tunneling are so small that we dont need to include it in a discussion of what might happen to me tomorrow.  So how do I decide to draw that line?  Specifically inn a pool of say 7 billion simmilar events it is shown that 1 out of every 2000 have a non simmilar outcome would I be able to say that is an inconsiquential number?  ",
        "created_utc": 1525203234,
        "upvote_ratio": ""
    },
    {
        "title": "Penn State online stat course materials?",
        "author": "svyset",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ga3z6/penn_state_online_stat_course_materials/",
        "text": "Great materials used to be accessible for free. Now it requires university account access. Anyone has a backup that can be shared?",
        "created_utc": 1525194792,
        "upvote_ratio": ""
    },
    {
        "title": "What does it mean for questionnaire responses to be 'floored to zero'?",
        "author": "animal-magnetism",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g9x6w/what_does_it_mean_for_questionnaire_responses_to/",
        "text": "I've been trying to figure this out for myself and I'm not finding a lot of clear examples online.\n\nThe best that I've come up with is that responses that become negative when scored are transformed to a value of 0, but I don't see how it would make sense in relation to the scoring of this particular questionnaire (see: https://www.phenxtoolkit.org/toolkit_content/PDF/PX230104.pdf).\n\nI also don't understand how this would work or how it would change the distribution of the responses...\n\n\nSo, basically. Help! Thank you so much in advance.",
        "created_utc": 1525193328,
        "upvote_ratio": ""
    },
    {
        "title": "Need help guiding toward various aspects of statistic",
        "author": "chick6nsalt",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g9c2n/need_help_guiding_toward_various_aspects_of/",
        "text": "Hey I'm new here. I need some help on which topic of statistics to learn next.\nI understand standard deviation, some binomial distribution and poisson distribution.\nI want some advise on what topic to learn next as I can't seem to connect the dots.",
        "created_utc": 1525188805,
        "upvote_ratio": ""
    },
    {
        "title": "Question regarding Bonferroni correction test.",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g8tob/question_regarding_bonferroni_correction_test/",
        "text": "[deleted]",
        "created_utc": 1525184633,
        "upvote_ratio": ""
    },
    {
        "title": "Confidence interval",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g8fgc/confidence_interval/",
        "text": "[deleted]",
        "created_utc": 1525181132,
        "upvote_ratio": ""
    },
    {
        "title": "Insurance rating factor extraction",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g8bkg/insurance_rating_factor_extraction/",
        "text": "[deleted]",
        "created_utc": 1525180137,
        "upvote_ratio": ""
    },
    {
        "title": "Novice, looking for a solution",
        "author": "king_booker",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g7702/novice_looking_for_a_solution/",
        "text": "Hi,\n\nI have been given a statistic problem which states :- \n\n**boldLet X1, X2, X3, X4, X5 be independent U (0, 1) random variables. Let X = X1 + X2 + X3 and Y = X3 + X4\n+ X5. Use the runif() function to simulate 1000 trials of each of these variables. Use these to estimate Cov\n(X, Y).**\n\nThis is the solution I used. Let me know if I am not on the right track here. Or if there is a better way to achieve this?\n\n    list_x&lt;-c()\n    list_y&lt;-c()\n    for(i in 1:1000)\n    {\n    #set.seed(123)\n    a &lt;- runif(5,0,1)\n    element_x = a[1] + a[2] + a[3] \n    element_y = a[3] + a[4] + a[5]\n    list_x &lt;-c(list_x,element_x)\n    list_y &lt;-c(list_y,element_y)\n    }\n    print(list_x)\n    print(list_y)\n    cov(list_x,list_y)\n    plot(list_x,list_y)",
        "created_utc": 1525167378,
        "upvote_ratio": ""
    },
    {
        "title": "Total statistics novice looking for some guidance or just to be pointed in the right direction with regards to variable \"weighting\"",
        "author": "MintPolo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g6383/total_statistics_novice_looking_for_some_guidance/",
        "text": "Hi all,\n\nI was directed here from r/statisctics and was hoping someone can advise me as I have no maths knowledge other than what I've (likely incorrectly) learned from intense googling. \n\nBasically, I want to assign a score to each of 30 variables to reflect their weight of influence.\n\nI was originally told to establish  the R^2 value for each as a means of doing so. Though this has helped, I believe that they aren't a true reflection of each attributes importance. \n\nI then decided to take the mean for each attribute in my sample. The top 3 mean values are in fact what I would expect to be most relevant, but doesn't help me see how relevant, and more importantly, does not match the order of importance suggested by the r^2 values.\n\nIs there a way to amalgamate both these findings into scores that reflect more closely the importance of each predictor variable?\n\nHere is a spreadsheet showing my findings thus far. Just to clarify, the predictor variables are listed along the top, with the response variable on the left under \"snr_avg_rating\". \n\nhttps://imgur.com/a/62FfWZl\n\nr^2 order just means the order of value that the r^2 calculations gave and was designed to show it against the mean values as a check. \n\nVisions for example was 15th but is certainly one of the most important attributes, as reflected by the mean. \n\nI hope i've explained the problem clearly, I must admit i'm very exhausted... been trying to solve this for a considerable amount of time now. \n\nI'm guessing that one major problem is my sampling, but I have limited data to work with and hope that there is a way to put these figures together to find a best fit answer.  \n\nThank you so much for any help \n\n\n",
        "created_utc": 1525152283,
        "upvote_ratio": ""
    },
    {
        "title": "How do I tell between a 2 sample T-test and a paired t-test?",
        "author": "steeze17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g5xjy/how_do_i_tell_between_a_2_sample_ttest_and_a/",
        "text": "Is there a way to easily identify between a 2 sample T-test and a paired t-test? I keep on mixing those up. Thanks in advance ",
        "created_utc": 1525150364,
        "upvote_ratio": ""
    },
    {
        "title": "(2SLS) What does it mean if my instruments are strong (F = 10) but the R^2 in first stage regressions are trash? (R^2 = .01)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g5r2y/2sls_what_does_it_mean_if_my_instruments_are/",
        "text": "[deleted]",
        "created_utc": 1525148214,
        "upvote_ratio": ""
    },
    {
        "title": "Self studying, looking for solutions to All of Statistics?",
        "author": "tending",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g4tm8/self_studying_looking_for_solutions_to_all_of/",
        "text": "Got to the first question in Chapter 2 and realized I'm unclear about how much the book wants me to prove versus what I'm allowed to use from what's been presented. Checked the back and it doesn't even have odd number solutions.\n\nI'm a programmer that's been out of school for 10 years, not someone looking for homework answers I swear ;p",
        "created_utc": 1525138958,
        "upvote_ratio": ""
    },
    {
        "title": "Can Someone Help me do the Math on this Utility Calculation?",
        "author": "GuzzlingHobo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g4kn9/can_someone_help_me_do_the_math_on_this_utility/",
        "text": "So suppose there's a pipe that breaks off into two different chambers.  In this first chamber, there is one person, in the other there is five million people.  Going down this pipe is poisonous particles that will cause instant death upon inhale.  If it goes into the first chamber, due to the space of the chamber the one person will surely die.  However, if it goes into the second chamber, each person will suffer a one in a million chance of death \\(no one is guaranteed to die in the second chamber\\).  You have time to close the pipes access to one room, i.e. cannot close off access to both.\n\nThis is a problem we faced in my consequentialism class, and some were reluctant to choose closing off the first valve instead of the second because of the chance no one would die.  It's been a while since I took stats so I can't do the relevant math, but to me it seems *incredibly* likely that one or more people will die if the gas goes into the second chamber.  How would I figure out what the chances are of no people, one person, two people, and so on dying if the gas goes into the second chamber?",
        "created_utc": 1525136633,
        "upvote_ratio": ""
    },
    {
        "title": "How to approach time series data when tracking recovery?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g3z2j/how_to_approach_time_series_data_when_tracking/",
        "text": "[deleted]",
        "created_utc": 1525131210,
        "upvote_ratio": ""
    },
    {
        "title": "Econometrics help!",
        "author": "thank_you_based_mod",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g3opw/econometrics_help/",
        "text": "I have these variables \\-\\&gt;\n\nweek\n\nprice\n\nquantity\n\n2 supply shocks\n\nand dummy variables that are 4 weeks\\(essentially a month\\) each \\(season1 being first 4 weeks, season 2 being the next, etc etc\\) but there are 13 seasons, and I am planning on omitting the 13th and keeping 1\\-12.\n\nHow do I estimate the demand function?\n\nI was planning on running an ivregression using the 2 supply shocks as instrumental variables for price, and using the seasons dummy variables, but not using the week variable. Is this the right way to do this?\n\nThanks!\n\nedit1: essentially the two shocks are cartel and weather\\(which are assumed to be exogenous to demand\\). I have to find the price elasticity of demand\\(so i considered using logs\\). The cartels are expected to have tampered with the prices, but sometimes the cartels fell apart and the prices were determined more competitively. I am estimating the price elasticity of demand for the shipment of grain, which i assume to be mostly competitive\\(at least after controlling for cartels\\).",
        "created_utc": 1525128697,
        "upvote_ratio": ""
    },
    {
        "title": "Multi-Variable Correlation Analysis (Multiple Linear Regression possibly)",
        "author": "PierrethePenguin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g3kho/multivariable_correlation_analysis_multiple/",
        "text": "I am doing a project for work that is deeper in the realm of statistics than I am used to and would like some help making sure the results I present are indeed relevant and correct. \n\nWe are looking at our marketing mix and trying to identify which marketing programs are resulting in the best sales lift. \n\nCurrently I have my data set up as follows (1 indicates inclusion; ie, bucket 1 products were included in Ad Type 1, 2, and 3):\n\n[Crappy Image due to work firewall restrictions on Imgur; Sorry!](https://imgur.com/a/uBmijQo)\n\nOverall, I have 23 Buckets and 7 ad types each containing a different number of products. \n\nThe Q3 - Q4 % increase is there as a \"baseline\". There were a variety of different ads running during Q4 but I did not include those in this analysis as I do not have access to all of the necessary data. \n\nQ1 is when the recorded ads were run. \n\nI am trying to determine which \"mix\" is best. In other words, I am trying to determine which ad type correlates the most with an increase in sales. \n\nI did a quick R-squared analysis between the ad type and the Q4 - Q1 Sales % Increase and got the following results:\n\nAd Type 1: 0.02\nAd Type 2: 0.07\nAd Type 3: 0.04\nAd Type 4: 0.01\nAd Type 5: 0.01\nAd Type 6: 0.00\nAd Type 7: 0.11\n\nThis is part of a larger analysis. I had already hypothesized that Ad Type 7 would have the greatest correlation. Since there are so many variables involved, 0.11 seems to show at least more correlation than some of the other variables. \n\nI would appreciate any help in my approach and any further steps I could take to determine correlation or the best mix possible. \n\nThanks!\n\nedit: to add link to image of spreadsheet setup ",
        "created_utc": 1525127683,
        "upvote_ratio": ""
    },
    {
        "title": "I want to analyze a subreddit, does anyone have ideas on where to begin?",
        "author": "justletmedieinpeace",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g3dth/i_want_to_analyze_a_subreddit_does_anyone_have/",
        "text": "I'm pretty solid on the actual statistics of it, I just don't know if anyone has a convenient tool for scraping reddit comments, etc.\n\nThank you in advance, I love this sub.",
        "created_utc": 1525126118,
        "upvote_ratio": ""
    },
    {
        "title": "Checking for chance level performance.",
        "author": "Gannicius",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g1ta5/checking_for_chance_level_performance/",
        "text": "Hi all\n\nI am trying to identify if my participant mean score differ significantly from chance performance(.5) for a within subjects factor. Should I be using a one sample t-test with the population mean set to .5 for this? \n\nI have mean performance scores on test1 and test2 (same test, just after time period) for each pp. This has caused me to question my choice of one sample, would it be better to go paired sample? \n\nThanks in advance",
        "created_utc": 1525113872,
        "upvote_ratio": ""
    },
    {
        "title": "When taking the mean of a set of means, how do you get the overall standard deviation?",
        "author": "PeaPodBod",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g1ggi/when_taking_the_mean_of_a_set_of_means_how_do_you/",
        "text": "I have a set of 10 mean values with corresponding standard deviations for each. I now want to take the mean of all those means to get an overall average. How then do I get the final standard deviation in this case? Any references would be very helpful. Thanks!",
        "created_utc": 1525111198,
        "upvote_ratio": ""
    },
    {
        "title": "How many power analyses do I need to do for multiple correlations?",
        "author": "GetFrozty",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g1d7i/how_many_power_analyses_do_i_need_to_do_for/",
        "text": "I looked up multiple guides but they don't seem to address this. I have 10 subscales/variables that I ran correlations on (6+4 from 2 measures). Most were Spearman, some are Pearson. I was told to run a priori &amp; post hoc power analyses using G Power. \n\n* For post-hoc, do I run one for each of the correlations I'm interested in reporting? Are there differences in running power analyses for Spearman/Pearson?\n* For a priori, should I be finding justification/effect sizes for every correlation I'm interested in reporting?\n\nAny feedback or advice would be supremely appreciated.",
        "created_utc": 1525110508,
        "upvote_ratio": ""
    },
    {
        "title": "How would I work this out please? (Quantity in population with given trait)",
        "author": "KateBlanche",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g0kme/how_would_i_work_this_out_please_quantity_in/",
        "text": "I'm trying to work out the likelihood of there being a certain number of people in a random group with a given trait.\n\nFor example, say the chances of having Trait A are 1:5,000, and I have a random group of 150 people. I want to work out the probability of there being, for example, 25 people with Trait A in my 150, if the group is a representative sample of the whole population.\n\nThanks in advance for your help. It's been a long time since I did Stats and I can't work out which of the part remembered bits are the bits I need, so if you could show me how I would work out the answer that would be awesome!\n\n",
        "created_utc": 1525104383,
        "upvote_ratio": ""
    },
    {
        "title": "Paired samples t test",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8g09p4/paired_samples_t_test/",
        "text": "[deleted]",
        "created_utc": 1525102019,
        "upvote_ratio": ""
    },
    {
        "title": "Doubt on a public service exam",
        "author": "MERAXNA",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fzngz/doubt_on_a_public_service_exam/",
        "text": "Hello! My wife showed me a question on a public service exam (to get a job), and I believe the her teacher's answer to be wrong. Here is the question:\n\n&gt; The probability of rainning on Saturday is 50%. The probability of raining on Sunday is 30%. The probability of raining on both days is 20%. What is the chance of raining on Saturday or Sunday?\n\nThe official answer is 60%. The solution the teacher gave was made using sets. Basically, he did P(OnlyStaurday)=P(Saturday) - P(Both). So, the chance of raining on saturday only was 50-20=30. He did same for sunday-only, which gives 10. So 30+20+10 = 60.\n\nI believe the answer to be wrong because the events are dependent, and for this subtraction to work they have to be independent.",
        "created_utc": 1525096893,
        "upvote_ratio": ""
    },
    {
        "title": "What does r Mean in this Instance, I Know Nothing of Statistics? McAdams et al. (2008): ex. \"r(125) = .22\"",
        "author": "HeyThereImApollo",
        "url": "https://i.redd.it/pjqhr9fjx1v01.png",
        "text": "",
        "created_utc": 1525096679,
        "upvote_ratio": ""
    },
    {
        "title": "Analysing individual-level panel data with differing timings for each variable",
        "author": "pollinguk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fyfbk/analysing_individuallevel_panel_data_with/",
        "text": "I have panel data from a popular election study as well as linked data, collected seperately, on all of the respondents. The problem I have is that the linked data were recorded at different points in time to the panel data. As such, whenever there is panel data the linked data is NA. Similarly, whenever there is linked data, the panel data is NA. Ideally, I'd like to test the effect of one of the linked variables on one of the panel variables.\n\nIs there a model that can handle this kind of problem? Preferably one that I can implement in R.\n",
        "created_utc": 1525083568,
        "upvote_ratio": ""
    },
    {
        "title": "Need help with my Fixed Effects model on GRETL",
        "author": "Meowspacito",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fy4pl/need_help_with_my_fixed_effects_model_on_gretl/",
        "text": "Hey, redditors. I am a student who's currently writing his Bachelor's thesis, and I need some help with my Fixed Effects model. \n\nIs there any way to take care of heteroskedasticity and autocorrelation in GRETL software? I found this discussion (https://www.statalist.org/forums/forum/general-stata-discussion/general/833393-how-to-correct-for-heteroscedasticity-and-autocorrelation-in-the-same-regression-command-in-a-fixed-effects-panel-data-model), yet it concerns STATA, and I probably can't do the same in GRETL.\n\nAnother way might be to simply use robust standard errors. However, this concerns only the heteroskedasticity, and still does not alleviate it altogether, if I'm not mistaken.\n\nThanks for your help.",
        "created_utc": 1525079331,
        "upvote_ratio": ""
    },
    {
        "title": "Non significant interaction terms",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fxy67/non_significant_interaction_terms/",
        "text": "[deleted]",
        "created_utc": 1525076458,
        "upvote_ratio": ""
    },
    {
        "title": "What kind of hypothesis test to use?",
        "author": "bigblackbrick",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fxuno/what_kind_of_hypothesis_test_to_use/",
        "text": "Hello, so I have this project on how people would react to people littering in front of them and the reactions were divided to Positive (Reacted) or Negative (Did Not React). We have this two proportions (say we have 2 positive reactions/38 negative reactions and 40 sample size, so we have 2/40 and 38/40? Please correct me if Im wrong)  but we only have one sample? So im now stuck which kind of hypothesis test to use. Please help Thank you",
        "created_utc": 1525074947,
        "upvote_ratio": ""
    },
    {
        "title": "Help with converting MLE to r/d",
        "author": "tiltul",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fxufm/help_with_converting_mle_to_rd/",
        "text": "I am currently working on a meta-analysis and at the stage of converting all studies' effect size into one common matrix. I came across a model that one of its estimations are of my interest. The estimations coefficients I'm interested in is an estimation of expected utility coefficients and further variables. The estimation is conducted using maximum likelihood. My question is: How do I convert this coefficient to r or d' to be able to compare it with the rest of the effect size I have? ",
        "created_utc": 1525074838,
        "upvote_ratio": ""
    },
    {
        "title": "What test to use in SPSS?",
        "author": "Archangel768",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fwl4j/what_test_to_use_in_spss/",
        "text": "Hi, I am trying to predict levels of depression (0-42) in the workplace based on two variables. The first is amount of hours worked. The people are grouped into two groups, high work hours and low work hours. The other variable is levels of work engagement which is measured from 0-6 (0 being low engagement and 6 being high).\n\nI need to answer whether or not high levels of work engagement is associated with lower depression in both low and high work hour groups.\n\nI'm not sure what test in SPSS I would use to test this.",
        "created_utc": 1525058400,
        "upvote_ratio": ""
    },
    {
        "title": "Inferring results from linear range survey data",
        "author": "DeadPukka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fvrst/inferring_results_from_linear_range_survey_data/",
        "text": "I've been doing some market research for my startup company, and I'm a software developer, by trade, not a data scientist/statistician.\n\nI've asked a few questions with a 1-10 range of values, but for a two-value question; for example a range of like vs. dislike of a topic.\n\nI've been playing around with the numbers, using histogram charts and some bucketing of data, but I'm assuming there's a common pattern for evaluating this type of data to infer some meaning from it (other than just averaging the results).\n\nWould anyone be able to point me to any web resources/tutorials that apply to this type of situation?  (It's been many, many years since I took a statistics class in grad school.)  Thanks!",
        "created_utc": 1525049827,
        "upvote_ratio": ""
    },
    {
        "title": "Coefficient restriction and cointegration",
        "author": "h0cusl0cus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fuq01/coefficient_restriction_and_cointegration/",
        "text": "We have [these data](https://www.dropbox.com/s/j19ee67vzlzjcg9/data.csv?dl=0). RL and RS are six-month and three-month rates, and S the spread (RL−RS).\n\nGiven that RL and RS are cointegrated, what is the best way to test b1=2 in ΔRSt=b0+b1St−1+ut? Dynamic OLS with HAC standard errors and then a Wald test? Apologies if I'm confusing anything.",
        "created_utc": 1525039677,
        "upvote_ratio": ""
    },
    {
        "title": "Help with written report interpreting analytic methods and results. (Paid)",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fum8x/help_with_written_report_interpreting_analytic/",
        "text": "[deleted]",
        "created_utc": 1525038714,
        "upvote_ratio": ""
    },
    {
        "title": "What is in a name...of this graphical display (no roses here).",
        "author": "Makidoll",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fu6q8/what_is_in_a_nameof_this_graphical_display_no/",
        "text": "Hi all, \n\nI've reached that point in the year where i have too much information in my brain and some has fallen out... I've been asked to list graphical displays are in a news article that was provided to my group and i cant recall the appropriate name of this graph. \n\nhttps://imgur.com/a/s8TMPOO\n",
        "created_utc": 1525034886,
        "upvote_ratio": ""
    },
    {
        "title": "What statistical test should I use for this data? So lost!",
        "author": "[deleted]",
        "url": "https://i.redd.it/qgjlixhotwu01.png",
        "text": "[deleted]",
        "created_utc": 1525034756,
        "upvote_ratio": ""
    },
    {
        "title": "[AP Statistics] Randomized Study homework check",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8fu1z4/ap_statistics_randomized_study_homework_check/",
        "text": "[deleted]",
        "created_utc": 1525033768,
        "upvote_ratio": ""
    },
    {
        "title": "Convention for Combined Graphs",
        "author": "5k1rm15h",
        "url": "https://www.reddit.com/r/AskStatistics/comments/8ft9js/convention_for_combined_graphs/",
        "text": "Hey everyone, \n\nI was wondering if I could ask your opinion on what the general convention or best practice for combined graphs were?\n\nI'm combining the output of a few different graphs into a multi-panel output. \n\nI'm measuring the same variable but using different time periods &amp; strata in each panel. \n\nThere is some noticeable difference when the y-axis is restricted to the range used but some of the graphs appear fairly smooth when scaled to the range across all graphs.\n\nSome of the graph's periods are within but shorter than other periods. \n\n&amp;nbsp;\n\nIs accepted convention to format each panel's axis with the entire domain and range from data across all panels, so that an apples to apples comparison could be made? Or is convention to limit the range &amp; domain to the relevant period &amp; values within each panel in order to highlight detail?\n\n&amp;nbsp;\n\nThanks in advance!\n",
        "created_utc": 1525026922,
        "upvote_ratio": ""
    }
]