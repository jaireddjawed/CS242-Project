[
    {
        "title": "Did I calculate this correctly",
        "author": "lawrenceugene",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f4434/did_i_calculate_this_correctly/",
        "text": "My grandmother told me me that her and her cousin, both born in the same day three years apart, are about to (If things the due date is correct) have grandchildren who were born three years apart to the day. I personally hate when people throw around this type of coincidence and attribute it to some magical property such as fate or luck, as it ruins the value of and the potential to appreciate the sheer coincidence. So I pulled out a calculator to see exactly how likely this is and how often it should be expected to happen and it went something like this. I just KNOW that this is all sorts if messed up to I'm hoping someone can critique it and help me clear up any misunderstandings. I treated the initial birth of my grandma's cousin as insignificant, it's only when my grandma was born exactly three years later that it had any significance. I decided that since there are about 9,125 years of fertility in a woman's life, it was a 1/9,125 chance they would be bon on the same day. I know this is wrong already due to fertility decreasing and there being an average age for which Americans choose to conceive which is made even more important due to them being from the same family and sharing the same values, but lets keep going. I figured all I had to do was calculate the odds of this happening twice, therefore I multiplied it by 2 to achieve an end probability of 1/18,250. What am I missing. I feel like all I calculated for was the probability that two sets of people would be born on the same day, and not three years apart on the same day, and the other set three years apart on the same day as the other set. How would I go about a correct calculation?",
        "created_utc": 1536721719,
        "upvote_ratio": ""
    },
    {
        "title": "Help with a problem",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f404z/help_with_a_problem/",
        "text": "[deleted]",
        "created_utc": 1536720761,
        "upvote_ratio": ""
    },
    {
        "title": "Chances of conception after salpingectomy",
        "author": "pbsnowflake453",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f3ou7/chances_of_conception_after_salpingectomy/",
        "text": "Let's say an average woman has a 20% chance of conceiving in any given cycle. (Lots of different studies have different numbers, but this is the one I'm going with for this question, so just roll with it.)\n\n[This study](https://academic.oup.com/humrep/article/28/4/937/653121) found that approximately 1/3 of the women who conceived after a salpingectomy did so with an egg from the contralateral ovary (opposite the side with the remaining tube). \n\nDoes this mean that a woman who has had a salpingectomy has a 6.7% (1/3 of 20) chance of conceiving during the cycles where ovulation happens on the side with the missing tube? If not, what are the chances during that cycle?\n\nAlso, what are the chances in any given cycle, not knowing on which side ovulation occurred?",
        "created_utc": 1536718088,
        "upvote_ratio": ""
    },
    {
        "title": "How to build an assessment that measures conceptual knowledge in a subject?",
        "author": "dfd0226",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f2zbn/how_to_build_an_assessment_that_measures/",
        "text": "Hi r/AskStatistics\n\nThis may be the wrong venue, so tell me if there is a better place to post (or x-post) this.\n\nI am trying to **build a way to assess student learning** in an **app to teach chemistry**. I want to measure how people do on formative questions in the app (each invokes one or more concepts) and then map that onto how much students understand particular concepts.\n\nI have used **factor analysis** to back out how many factors an assessment loads onto, but **this problem is going the other direction** where I will tag items as invoking a specific concept beforehand.\n\nThis feels like something **psychometrics** has tackled. Can anyone **recommend any reading** on this?",
        "created_utc": 1536712432,
        "upvote_ratio": ""
    },
    {
        "title": "To grow or not to grow Saturation and population density",
        "author": "Terrible_Tie",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f2pzw/to_grow_or_not_to_grow_saturation_and_population/",
        "text": "Let's say I have propane cages at retail locations, but I don't want to over saturate a market... What should I be googling or how could I calculate if it is a good market for me to grow in? \n\nI'm already located at various retailers (drug, mass, grocery) and am about $10/propane. In order to be profitable, I need to make at least $7000 in sales a month per propane cage or sell at least 700 propane tanks a month. Let's pretend I don't have expenses.\n\nI have population density and population by 5 digit zip-code from 2010 census, a historical sales file, and a list of locations I'm already at. \n\nIs there a way to calculate at what point have I added too many propane tanks in a town? \nOr based on population or population density how can I calculate whether a town has potential for my propane cages?",
        "created_utc": 1536710372,
        "upvote_ratio": ""
    },
    {
        "title": "Mean squared prediction error",
        "author": "Brown-Banannerz",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f2a8o/mean_squared_prediction_error/",
        "text": "It's so hard finding information on this :(\n\nCan anyone provide me with a quick rundown of how to calculate MSPE? If you know of an R package that will do it, that would be great, but I'm fine with doing it by hand through an equation too. I don't have an equation though. \n\nThe best I found so far is from this wikipedia page but I don't know if the equation is correct\n\nhttps://en.wikipedia.org/wiki/Mean_squared_prediction_error",
        "created_utc": 1536706890,
        "upvote_ratio": ""
    },
    {
        "title": "Converting predicted probabilities from Classification and Regression Tree to odds ratio",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f18bv/converting_predicted_probabilities_from/",
        "text": "[deleted]",
        "created_utc": 1536698836,
        "upvote_ratio": ""
    },
    {
        "title": "Bingo. What are the odds of getting bingo with 7 randomly selected numbers?",
        "author": "spiffynsnazzy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9f14vl/bingo_what_are_the_odds_of_getting_bingo_with_7/",
        "text": "A local bar does a bingo night, and at the end they play one round with 7 numbers drawn and if you get bingo you win $2000.\n\nI've been trying to think of the likelihood of this happening. Let's say 50 boards?",
        "created_utc": 1536698136,
        "upvote_ratio": ""
    },
    {
        "title": "How to calculate de standard deviation of a number that is a sum of two others ?",
        "author": "SnippyFrenchmen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ey84v/how_to_calculate_de_standard_deviation_of_a/",
        "text": "Hi all,\n\nSo my question is pretty simple, but I can't find the answer by myself. Here is some context : I have a value C, that is the substraction of the number A and B, roughly (A-B=C). \n\nA and B are values that are measured 5 times each, and to obtain C we actually use the average of the values of A and B since it isn't necessarily  A\\_1-B\\_1=C\\_1.\n\nSo since I can calculate mean and standard deviation of A and B, what I get from the subtraction is the mean value of C, but how do I get C's standard deviation?\n\nThanks everyone!",
        "created_utc": 1536677476,
        "upvote_ratio": ""
    },
    {
        "title": "Multicollinearity confusion",
        "author": "Ruwatchingclosely",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9exiqn/multicollinearity_confusion/",
        "text": "So for my master's thesis, I am examining the influence of union density (% of the workforce in a union) and top marginal tax rates on pre-tax CEO pay. These two independent variables are very highly correlated in my sample. When I include both variables in the regression, union density has a significant negative association with CEO pay, and top marginal tax rates are insignificant. The VIF is very high for both variables. However, it is my understanding that multicollinearity does not bias coefficients, and it is accurately captured in the standard errors (see Stephen Voss' multicollinearity paper). On the other hand, in this case, multicollinearity may suggest union density and top tax rates are causally related and not independent--this would violate a key OLS assumption. Further, union density and top tax rate changes are both associated with left government ideology (a third variable I can include in my analysis), but they shouldn't be related to each other EXCEPT for their relation to left government ideology. So my question is this: if I include left goverment ideology as a control variable, and union density is still significantly associated with CEO pay, and tax rates are not, will my coefficients be unbiased? Can I then ignore multicollinearity? Another question: do I even need to control for government ideology, if union density and tax rates are only related to each other because of their association with government ideology? If I only include union density and tax rates in the regression are my coefficients unbiased? Previous research mostly suggests top tax rates are not associated with CEO pay changes, but a few authors find they are (and in opposite directions). Thanks very much in advance for any insight.\n\nHere is an excerpt from Voss' multicollinearity paper that has me concerned: At some point the idea of “variable packages” becomes hazy. Two variables may not capture the exact same underlying concept, but researchers nevertheless may be aware that they are causally related to each other in the larger population, such that multicollinearity in the sample is no accident. This sort of multicollinearity, although theoretically meaningful, nonetheless can pose an obstacle to the analyst who wishes to distinguish two or more concepts statistically; it can hinder an analysis based on fine theoretical distinctions. The appearance of such multicollinearity may be helpful, because it offers a warning that concepts may not be as theoretically distinct as a modeler initially assumed, but it still risks leaving the analyst with regrettably hesitant causal conclusions. The analyst would only be able to generalize about the overlapping variables as a package, even if a project’s needs demand otherwise.",
        "created_utc": 1536672280,
        "upvote_ratio": ""
    },
    {
        "title": "Question about general dominance weights in dominance analysis",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ew5lg/question_about_general_dominance_weights_in/",
        "text": "[deleted]",
        "created_utc": 1536659821,
        "upvote_ratio": ""
    },
    {
        "title": "Using CART to infer likelihood of group membership",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ew0bl/using_cart_to_infer_likelihood_of_group_membership/",
        "text": "[deleted]",
        "created_utc": 1536658178,
        "upvote_ratio": ""
    },
    {
        "title": "Help on Statistics HW",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9er9pg/help_on_statistics_hw/",
        "text": "[deleted]",
        "created_utc": 1536614739,
        "upvote_ratio": ""
    },
    {
        "title": "Can we compare r/Iwantout to r/Igotout?",
        "author": "Consumeradvicecarrot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ep83p/can_we_compare_riwantout_to_rigotout/",
        "text": "Subscribers are 287k vs 4,3k respectively. I believe it speaks to the graduation rate of redditor country nomads. It indicates that a mere 1.4% of r/Iwantout redditors actually got out. What do you think?\n\n\n(Disclaimer: This is of course only a back of the enveloppe calculation, ignoreing circumstances and false positives and negatives. And taking for granted that all r/Igotout subscribers came from r/Iwantout.)",
        "created_utc": 1536600676,
        "upvote_ratio": ""
    },
    {
        "title": "Stats question :)",
        "author": "FoggyPancake",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9emdps/stats_question/",
        "text": "Hi just had a few questions for tutorial practice but I cant wrap my head round this one. Any explanation would help\n\n An on-line retailing firm conducts a study into the number of on-line purchases of footwear made during all of last year.   The study wants to determine the proportion of Indians over the age of 25 who purchase footwear on-line. The managing director of the on-line retailing firm believes this proportion of Indians is 15%. A random sample of 126 Indians over the age of 25 is selected. If the managing director’s claim is true, what is the sample proportion value below which 10% of all sample proportions would exist? \n",
        "created_utc": 1536580223,
        "upvote_ratio": ""
    },
    {
        "title": "Piecewise linear regression with a 0 intercept",
        "author": "Metatronx",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9elq0g/piecewise_linear_regression_with_a_0_intercept/",
        "text": "Hey, so I am trying to run a piecewise linear mixed model with longitudinal data. I am using the lme4 library in R.\n\nI have the below given data structure and I create two new variables (T1, T2)  from the time variable to allow for the piecewise model.\n\nMy issue is that when I fit this model (Y\\~ T1+T2+(t1|patient)+ (T2|Patient). The model does not converge.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n|Patient|Time|T1|T2|\n|:-|:-|:-|:-|\n|1|0|0|0|\n|1|1|1|0|\n|1|2|2|0|\n|1|3|0|3|\n|1|4|0|4|\n\nHowever, when  I remove for each person the time=0 measuremnt there is no issue with the convergence.\n\nSo, I am thinking that because I am replacing the values larger than 2 with zeros in T1, there is no way in distinguishing between the intial value 0 and the 0 which are due to the time points being larger than 2.\n\nI hope it can be understood what I mean.\n\n&amp;#x200B;\n\nWould that be a right theory, and if so how could I work around that issue, retaining my 0 time point measuremnt and having a model that converges?\n\n&amp;#x200B;\n\n&amp;#x200B;",
        "created_utc": 1536574141,
        "upvote_ratio": ""
    },
    {
        "title": "Any controversy over using weights in multi-level models? And a question about combining weights.",
        "author": "Sociological_Duck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9eit23/any_controversy_over_using_weights_in_multilevel/",
        "text": "I know there is some debate about using weights when controlling for factors accounted for in the weights. I usually turn to precedent in the literature and user guides from datasets. \n\nAre there similar controversies when weighting cross-national data used in multi-level modeling? The nature of the HLM models often controls for issues accounted for in cross-national weights. I'm going with weighted data, because the user guide for the European Social Survey essentially demands it. \n\nAlso, it requests that I combine two weights for what I'm doing. In SAS, will newweight=(oldweight1)\\*(oldweight2); suffice?",
        "created_utc": 1536546229,
        "upvote_ratio": ""
    },
    {
        "title": "Help a College Admissions Office Predict who will attend!",
        "author": "canadianwonk",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9egsgg/help_a_college_admissions_office_predict_who_will/",
        "text": "Hello.  I work for a college admissions office at a small non-profit college and we need help predicting which student will attend our college.  We don't have the expertise or money to hire a professional data analytics person, so it's just me (who studied computer science).\n\n&amp;#x200B;\n\nHere is the issue:   We have a certain number of applicants per year, say 3000 applicants.   Of those, we make admissions offers from anywhere from 800 - 1200 students per year.    However, we only have spots for about 200 students to accept our offer (matriculate).      Thus, it is very important to us to be able to predict who will actually accept and attend (aka matriculate).   If too many students matriculate (say &gt; 250), then we literally don't have room for them and we're in big trouble.    However, if too few students matriculate (say &lt; 150), then we run into severe financial problems.   It changes from year to year how many accept, running from about 12% to 22%.  Thus, in about February, we have made a bunch of offers, but have not heard back from most of the students yet (until april).     We don't hear back until about June or July  from all students.\n\n&amp;#x200B;\n\nThus, say in february we have made 800 outstanding offers (and have only heard one way or the other from\\~50), but we have to decide whether we should make another 200 offers or not to meet our enrollments.    It would be super helpful if in February, we could analyze the 800 outstanding acceptance offers and make a good model of how many those will actually accept, so we know whether or not to make any additional offers.\n\n&amp;#x200B;\n\nPreviously the admissions office just made educated guesses about the number.   Since I joined, I have tried to improve this by doing some basic data analysis.    I have run logistic regressions on the past 3 -5 years worth of admitted/matriculated students, and found some predictive features/variables that I have used to make a simple regression model with about 6 variables.    The model basically uses: SAT, high school gpa, geographic location (near or far from the school)., and scholarship amount to predict the probability that the student will attend.    Once I have the probabilities for each of the students that we make, I then run a series of monte carlo simulations using the probability of each student to estimate the range of possible lass sizes.  However, my monte carlo simulations don't do so well on back testing, as they tend to over or under predict by quite a bit actual enrollment, so I am probably doing something a little of.\n\nI have a few questions I would love to get your thoughts on.\n\n&amp;#x200B;\n\n1. Scholarship:   We offer good students scholarship money.   Thus, the problem is scholarship is usually  co-variate with SAT and high school gpa - students with high gpa/sat get high scholarship money ususally.  So that seems to be problematic and maybe I should remove it from my model.    However, occasionally scholarship money is not covariate with gpa/sat.   For instance, we sometimes offer scholarship money to people with low gpas/sats for other reasons (e.g. they are promising in other ways), and in such a case, a student getting extra scholarship money (vs not) seems to be casually related to such a student's likelihood to actually come.    It seems like taking out scholarship money from the model wouldn't work there. So I am trying to figure out how to take in account scholarship money, given that it is usuaally a covariate with high GPA/sat (high gap and sat get more momney and therefore high money not telling any extra information), but for other students it is not.  Any thoughts about to think about scholarship money in such a situation and include it in the model?\n\n&amp;#x200B;\n\n2)  Ranges of sat scores seem to be predictive.   For example, we don't get many students of very high sat scores, but we do reasonably well with students around the median.   However, the strength of the applicant pool depends on year to year depending upon how many people apply.   For instance, somebody with an 1100 sat might have a 20% chance of accepting our offer.   However, some years we got many more overall applications, and then people with 1100 might only have a 10% chance of accepting that year, since the size of the pool was so much bigger that year.   This is problematic, because if I am trying to estimate the size of my incoming class by looking at past years, that 20% metric that was learned in my logistic regression is going to steer me wrong.  In other words, the size of the applicant pool can change things from year to year, and I haven't figured out how to control and include the size of the applicant pool in my model.\n\n&amp;#x200B;\n\n3) Am I generally doing the right approach (logistic regression) on the past few years students?   Are there better ways I could be doing this.   Are there papers I should be reading on people who have done this?  Are there other machine learning or statistical methods that might be better suited for this?   Are there r or python libraries that are specifically designed for something like this?\n\n&amp;#x200B;\n\nMany thanks for any help that you have here and for helping out a small college!\n\n&amp;#x200B;",
        "created_utc": 1536529327,
        "upvote_ratio": ""
    },
    {
        "title": "Could someone please help me out with a few questions for my intro statistics course?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ef2zk/could_someone_please_help_me_out_with_a_few/",
        "text": "[deleted]",
        "created_utc": 1536516590,
        "upvote_ratio": ""
    },
    {
        "title": "Distribution of unordered books in a library",
        "author": "Hari_a_s",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ecmg7/distribution_of_unordered_books_in_a_library/",
        "text": "I am trying to come up with a specialized sorting algorithm for librarians, assuming that they can carry a fixed number of books at a time. The objective is to minimize the distance walked by the librarian. What I cannot figure out is what will be the best statistical distribution of unordered books in a library (with relation to the real world) - it isn't completely random or normally distributed. The majority of books would be in place, and the one's misplaced would be within the same section.\n\nP.S - I'm a sophomore Computer undergrad who has never taken a statistics class.",
        "created_utc": 1536496280,
        "upvote_ratio": ""
    },
    {
        "title": "Are the statistics in this video correct?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ec8hb/are_the_statistics_in_this_video_correct/",
        "text": "Found this.\n\n[https://www.youtube.com/watch?v=W\\_VtY7XHDCc&amp;t=1s](https://www.youtube.com/watch?v=W_VtY7XHDCc&amp;t=1s)",
        "created_utc": 1536491613,
        "upvote_ratio": ""
    },
    {
        "title": "Can anyone point me in the direction of a more laymanized explanation of the Markov Point Process?",
        "author": "Emperor_Friendpatine",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ebrmy/can_anyone_point_me_in_the_direction_of_a_more/",
        "text": "Hi everyone. Not sure if this is the right sub for this. But I'm enrolled in Spatial Statistics this semester, and we were tasked to make a report on the Markov Point Process. Unfortunately, I'm not a hardcore maths person; I'm having a hard time understanding what the Markov Point Process is all about since all of the resources just dive right into the mathematical explanation of it. Would appreciate any possible resources/articles/books I can read to gain a better understanding of this topic.  \n\nThanks!",
        "created_utc": 1536484908,
        "upvote_ratio": ""
    },
    {
        "title": "Examining decrease in time between episodes over time",
        "author": "IHaveQuestions999",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9eay8n/examining_decrease_in_time_between_episodes_over/",
        "text": "[ETA title should read ~increase~ in time between episodes, but it's the same idea]\n\nI'm working with a client that provides people at risk of homelessness with brokerage (emergency funds) and they are interested to know about the patterns of people using their service. They think that after people enter the service, their interaction drops off over time. \n\nThey have provided me with their administrative data showing every brokerage date for every client over the last few years (~4000 entries). I can easily calculate the time between dates, but I'm not sure what test I would use to examine the change in the days between events over time.\n\nMy usual work is with survival analysis and I also do quite a bit of logistic regression/other linear modelling and some generalised modelling. I have studied time-series analysis but haven't had any actual jobs using it specifically so my hunch is it's got something to do with time-series but the structure of the data don't really suit it. Any ideas on how I can approach this? I'd love to be able to provide an illustration of this pattern of interaction to the client but also can't quite get my head around how to structure the data.",
        "created_utc": 1536473576,
        "upvote_ratio": ""
    },
    {
        "title": "Can someone help me decide what stats to do and how to interpret them?",
        "author": "NotTheAndesMountains",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9e5n0g/can_someone_help_me_decide_what_stats_to_do_and/",
        "text": "Full disclosure I'm not really well informed on stats. I am analyzing 3 groups of rats with surgery/treatment (surgery+drug, surgery+saline, naive (sham surgery)+drug): their behavior at 3 time points -baseline before any treatment/drug, and then 2 other times some days after the procedure. Additionally I'm measuring MRI parameters measured at 3 time points all after the procedures. My adviser recommended\n\nrepeated measures anova to compare each group individually to itself over time for all measurements, and then\n\nsome other higher level test to compare each group together at each time point.\n\nUsing JMP, I used MANOVA repeated measures for (1) but I'm not sure how to interpret the results (what do the results of \"between subjects\" and \"within subjects\" actually tell me?). I put the scores they had (3 sets b/c 3 days) as the Y response and put the group (surgery/treatment) as the model effect. Does this test or the way I do it seem right?\n\nAnd for the second point (2), I'm not totally sure what to run - an anova with t test or something because it's comparing groups at individual time points? God stats are so confusing to me... any help would be appreciated!",
        "created_utc": 1536425696,
        "upvote_ratio": ""
    },
    {
        "title": "Conditional Probability Question!",
        "author": "DaeguDude",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9e4d21/conditional_probability_question/",
        "text": "Hello guys, I was studying on the chapter about Conditional Probability. And I came upon this one example. I think I understood the concept of it. But there's something I wonder from the example. Here's the example.(By the way, this is the example from some youtube channel!)\n\n&amp;#x200B;\n\n*Number of Men = 5000*\n\n*Number of Women = 5000*\n\n*The % of men and alcoholic = 2.25%(0.225)*\n\n&amp;#x200B;\n\n||*Alcoholic*|*Non-Alcoholic*|*Number of People*|\n|:-|:-|:-|:-|\n|*Men*|*225*|*4775*|*5000*|\n|*Women*|||*5000*|\n||||*10000*|\n\n*So, what Example is is, when I'm given a person I picked is a man, to figure out the % of him being an alcoholic.*\n\n*So I use conditional probability*\n\n*= 0.0225 / (5000/10000)*\n\n*= 0.0225 / 0.5*\n\n*=* ***0.045***\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSo the answer is going to be the 4.5% of the chance that he is going to be an alcoholic when I know I'm given a man.\n\n**But my real question here is(This could be really obvious and silly), Is there any way to know numbers of blank cells in the table??? If it's not can you explain why I can't know, or If it is, can you explain the theory to know and show me the process???**\n\n&amp;#x200B;",
        "created_utc": 1536413477,
        "upvote_ratio": ""
    },
    {
        "title": "Probability that n throws of 2d3 exceed a given sum?",
        "author": "BeowulfShaeffer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dyyir/probability_that_n_throws_of_2d3_exceed_a_given/",
        "text": "I could use a little help with some probability. It's been twenty years since stats class and I haven't quite rebooted it back into my brain. \n  \nThe problem I am trying to solve is pretty simple to state.  I am throwing pairs of 2d3 dice that each contain face values { 1,2,3 }. Each throw yields a distribution like:\n  \n2 3 4    \n3 4 5  \n4 5 6  \n  \nSo, 2 or 6 with probability 1/9, 3 or 5 with probability 2/9 and 4 with probability 1/3.  \n  \nThe problem I have is this:  \n  \n&gt; Given a sum **S** and a number of rolls *n* **what is the probability *p*(n,S) that the sum of *n* throws (of 2d3) will exceed S**?\n  \nFor example:  \n  \n* *p*(1,7)=0 (since it's impossible to rolls a 7 in one throw)\n  \n* *p*(2,3) = 1 (since it's impossible to roll twice and get anything less than 4).   \n\nI need to calculate values like *p*(5,12).\n  \nThanks in advance, I'm a little embarrassed to be asking for help on this.\n  \n\n\n\n\n\n",
        "created_utc": 1536357363,
        "upvote_ratio": ""
    },
    {
        "title": "Sample size for crossover trial",
        "author": "needinganswersldn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dyo5j/sample_size_for_crossover_trial/",
        "text": "Hi all,\n\nCan anyone point me to a sample size calculator (or formula) for a crossover trial? Ideally one for 4 different treatments (well, 3+control). All the resources I've found so far are for 2x2 trials.\n\nThank you!",
        "created_utc": 1536355155,
        "upvote_ratio": ""
    },
    {
        "title": "Help! Save my soul!",
        "author": "InALandFarAwayy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dwdjg/help_save_my_soul/",
        "text": "Hi Reddit, thanks so much in advance for your help!!\n\n&amp;#x200B;\n\n1) I am measuring a variable for this bunch of subjects. After I've done and gathered the data, there is some variability present in the data.\n\nThe sources of variability are:\n\n\\-Random error of the measurement instrument\n\n\\-Natural Variability of the variable we have measured\n\n&amp;#x200B;\n\nNOT:\n\n\\-Systematic error of the instrument.\n\n&amp;#x200B;\n\n2) I have 3 instruments called X, Y, Z, that are used to measure a variable for a large group of subjects. The fact is that instrument X has no systematic error, then:\n\n\\-Possible to determine if Y or Z has a smaller systematic error\n\n\\-Possible to determine which of the instruments Y or Z is more reliable.\n\nBoth statements are false.\n\n&amp;#x200B;\n\nAm I correct?\n\nIf I am not, please help explain to me why.\n\nThanks so much!",
        "created_utc": 1536340233,
        "upvote_ratio": ""
    },
    {
        "title": "Advice on store coupon research design needed!",
        "author": "bzuckercorn1969",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dtmhg/advice_on_store_coupon_research_design_needed/",
        "text": "I'm involved in a bit of social science research in my job but don’t have formal training in stats or research design. We have the opportunity to work with a supermarket to test out some product coupons in their stores. I want to propose a trial that would test the effect of a diet soda coupon on future purchases of regular sugary soda. \n\nFor the main analysis, I’d like to focus on the effect of the coupon on health-minded customers. The store has already done a segmentation analysis identifying these customers based on healthy food purchases in the previous year. The hypothesis is that for some health-minded customers who still purchase sugary sodas, the coupon might be enough of a nudge to get them to start buying more diet soda and less of the added-sugar kind. The store has offered a 10% control group. These customers would otherwise receive the coupon but will get nothing. \n\nMy primary outcome measure is probably the difference in purchases of sugary soda between the health-minded customers and the control who get no coupons. But I’m stuck in describing the research design. Is this where I’d use a difference in difference method?\n\nI know I shouldn’t expect a huge result from such a limited intervention but I’m hoping the project will lead to more ambitious ideas with this store in the future that might be more impactful. Hoping to get some further stats support locally at my job but may not be for another month and would like to clarify the research design as much as I can. \n\nVery grateful for any advice!",
        "created_utc": 1536321254,
        "upvote_ratio": ""
    },
    {
        "title": "What statistical test should I use? First year doctoral student in education research",
        "author": "gigiheheblop",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dqwua/what_statistical_test_should_i_use_first_year/",
        "text": "Hi all, I have a data set with: Independent variable: numerically coded teacher actions 3 ratings per teacher, 300+ teachers Dependent variable: 3 measures of outcomes per student, about 20 students per teacher\n\nWhat statistical test can I use to show the relationships between these variables? Some ideas I have are ANCOVA and t test. Will these tests prove correlation or causation? Thank you!",
        "created_utc": 1536295277,
        "upvote_ratio": ""
    },
    {
        "title": "Question about time series detrending and shifts",
        "author": "forgotmypassword314",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dqp9q/question_about_time_series_detrending_and_shifts/",
        "text": "Hello all!  Please let me know if this is not the appropriate place for this post, and where a better match for a subreddit would be!\n\nSo, I am casually working on a time series analysis - basic ARIMA models.  I've been trying out different ways of detrending, just for curiosity's sake.  So there are two types of \"regular\" detrending that I've tried: linear de-trending (fitting a linear regression and working on the residuals) and the diff method.\n\nSo, I've looked at the residuals and for both methods, they've both passed the Dickey-Fuller test, suggesting that the de-trended data are now stationary.  However, when I fit an ARIMA model to the one via linear detrending, it seems that the ARIMA model estimations/predictions/forecasts are 1 step (e.g., month, day, etc.) ahead of where they should be, whereas for the diff, the estimations seem to match the data.\n\nI was so unconfident in these results that I tried it in both Python and R.  They both give the same results.\n\nIs this a real thing?  Is it a well-known situation that I just don't know about?  Is it a result of implementation (e.g., diff will always be 1 datapoint shorter, so the programming somehow accommodates for this?)\n\nThanks so much!",
        "created_utc": 1536293439,
        "upvote_ratio": ""
    },
    {
        "title": "I want to learn statistics so I can better understand research and studies done in the humanities. Are there any (hopefully free) online resources that can teach me?",
        "author": "All_Tan_Everything",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dpulp/i_want_to_learn_statistics_so_i_can_better/",
        "text": "It's all in the title. The last time I took statistics, I was a Freshman in undergrad. I am now getting a certificate that requires a statistics course as a prerequisite and I'm afraid I've forgotten everything. On top of that, I'm going to grad school for a topic in the humanities and feel like I've always missed out on how to read statistics in terms of economic and social science studies. Can you point me to any online resources that could help me out?",
        "created_utc": 1536286667,
        "upvote_ratio": ""
    },
    {
        "title": "Question about Bayesian Statistics",
        "author": "macoit18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dpkxw/question_about_bayesian_statistics/",
        "text": "I was doing some exercises but after studying bayesian statistic topics I still find difficult to apply it in practice.. Can you help me get a solution so that I can understand the whole process?\n\nHere is an exercise:\n\nhttps://i.redd.it/umtvjuvk1qk11.jpg\n\nI know that (a) is *f*(*θ*|*x*) but how is it computed in practice?\n\nWhat is (b)? Is it just the marginal distribution of *X*?  \nAnd also, in (c) why do I need the observed sample?",
        "created_utc": 1536284753,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing a non-uniform quantitative categories/intervals of data to discrete quantitative data",
        "author": "der_swedishchef",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dpebz/comparing_a_nonuniform_quantitative/",
        "text": "Hi all.  It's been a while since I've taken a statistics class, please excuse any butchery of technical terminology!  If I am not clear I would be happy to clarify.\n\nAs you may have implied from the title, I have a project that involves comparing quantitative categories of data to discrete quantitative data points.  \n\nHere's a sample plot of some of my data.  \n\nhttps://imgur.com/a/spJMLo3\n\nThe quantitative categories/intervals shown here are the green bars.  The red dots show forecast data.  The categories/intervals here are \"32+ degrees F,\" \"25-32 degrees F,\" and \"10-25 degrees F.\"  \n\nI suppose you could say that the forecast temperature is the dependent variable and the observed temperature is the independent variable since the forecast model has to somehow calculate the temperature from observed temperatures.  I don't have access to the inner workings of the forecast model though, so as far as I know the forecast and observed temperature are independent of one another.  For all intents and purposes, the data was randomly sampled. \n\nI am trying to show that the forecast data is a good substitute for in-situ observational data.  I know I can make generalized observations about whether the forecast data falls within the range of the observational data, but are there any specific tests I can use to draw comparisons between the data or otherwise prove the forecast data is a good substitute for observational data?  I checked the UCLA link in the sidebar but I wasn't able to get much out of it (perhaps I am easily confused).\n",
        "created_utc": 1536283389,
        "upvote_ratio": ""
    },
    {
        "title": "question about rolling dice",
        "author": "anarcho-monarchist2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9donmx/question_about_rolling_dice/",
        "text": "ok, so, quick question because I'm not a statistics person but I need this real quick.\n\nsuppose I rolled one x-sided die, one y-sided die, and one z-sided die. what're the odds that they're all the same number? \n\nI really should learn some statistics at some point\n",
        "created_utc": 1536278234,
        "upvote_ratio": ""
    },
    {
        "title": "Can a mathy person help me think more about my algorithm?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dmc8g/can_a_mathy_person_help_me_think_more_about_my/",
        "text": "[deleted]",
        "created_utc": 1536263464,
        "upvote_ratio": ""
    },
    {
        "title": "Sampling distribution of the slope paramter in simple linear regression",
        "author": "sdfgergs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dlljw/sampling_distribution_of_the_slope_paramter_in/",
        "text": "I am reading a bit about inferences (test and confidence interval) about the slope parameter in simple linear regression.\n\nAssuming the model: Y=b_0 + X*b_1 + epsilon \n\nWhere epsilon is a random variable distributed as N(0,sigma^2)\n\nWhy is the sampling distribution of the sample slope b_1(hat) normally distributed?  \n\nb_1 (hat) = SP_xy / SS_xx (SS=sum of squares, SP= sum of products)\n\n\n\n",
        "created_utc": 1536258991,
        "upvote_ratio": ""
    },
    {
        "title": "How do you implement a hold out group when a product feature is 100% launched?",
        "author": "maxismyboxersname",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dkk36/how_do_you_implement_a_hold_out_group_when_a/",
        "text": "I work as a product analyst at a medium sized tech company. We have a good culture of experimentation but occasionally, we launch features without creating long term holdout groups. For example, for search we have a holdout group that’s about 1% of users so that we can understand lift in certain metrics.\n\nToday, I was asked how we could create a holdout group for a feature that’s already launched to 100% of users, Now, one way we might be able to do it is to look at users who have never seen this component (penetration of the feature is relatively low) and then randomly sample those users. Inherently though,  I can see problems that would arise from that approach. \n\nI would love any suggestions or resources that might help guide me! ",
        "created_utc": 1536252993,
        "upvote_ratio": ""
    },
    {
        "title": "Hotspot Mapping for Capture-Recapture and Population Size Estimation",
        "author": "chilling_soft",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dk5h9/hotspot_mapping_for_capturerecapture_and/",
        "text": "The overall objective for my project is to estimate population size of three demographic groups using a capture-recapture method. We used a formative assessment (meeting with local gov officials, community gatekeepers, etc) to identify potential hotspots for each of the 3 groups, and came up with a list of about 8,000.\n\nWe then dispatched enumerators for hotspot mapping and validation; essentially they seek out the pre-listed potential hotspots and either confirm or invalidate them. We programmed tablets that contained the list (select hotspot name from a drop-down menu), and a survey to be completed for each hotspot. Unfortunately, we had a number of issues that have produced a large amount of duplicates during data collection for hotspot mapping and listing. Some were technical (drop-down menus of hotspot names were switched for two of the provinces, so all of the names had to be manually entered; a programming error in the tablets that caused multiple entries if data wasn't refreshed immediately after finishing a hotspot), and some were human (enumerators didn't recognize a name in the drop-down menu, and manually entered a name that we can't validate; different enumerators visiting the same hotspot incidentally).\n\nSo currently I have a list of 20,000 hotspots that are probably 50% duplicates, and we need to begin capture-recapture in 24 hours. Apart from entries that are identical in all fields, I can't be 100% which are duplicates and which are not. If we initiate capture-recapture with a duplicate-riddled dataset, how will this affect estimates of population size?\n\nThanks and happy to clarify anything.",
        "created_utc": 1536250478,
        "upvote_ratio": ""
    },
    {
        "title": "What is the likelyhood of a pilot to die in its career?",
        "author": "Chanat0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dj4zh/what_is_the_likelyhood_of_a_pilot_to_die_in_its/",
        "text": "Hi guys,\n\nI don't know if this is the right place for something like this l, if not I will take it down.\n\nI was talking with some friends and we were wondering what's the chance of a pilot to die in its career since its the second safest way of transportation in the world.\n\nI know that there's 1 in a million chance of a pilot to be in a major accident (it doesent mean he dies I know but let's go with that) and I did an average of 180 to 140 flights per year, and let's say a pilot stars it's career at 25 so 40 years of duty.\n\nI don't know if this is enough for something but thanks anyway. ",
        "created_utc": 1536244092,
        "upvote_ratio": ""
    },
    {
        "title": "Normal Distribution Help",
        "author": "jamesteckno",
        "url": "https://i.redd.it/vbu4kpd8bmk11.jpg",
        "text": "",
        "created_utc": 1536239497,
        "upvote_ratio": ""
    },
    {
        "title": "Help on what statistics to use",
        "author": "phyxsly",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dgfx6/help_on_what_statistics_to_use/",
        "text": "Good Day.\n\nI do not have a good background on statistics so i need your help if i am doing it right for my analysis.\n\nHere's the data profile:\n\nI have N participants. Each participant has taken 12 problem solving tasks. Each task is scored from 1-100.\n\nFor each task, the following variables were looked at:\n\nV1 - unit is time e.g 43ms\n\nV2 - occurrence count just a number from 1 to N (integer)\n\n&amp;#x200B;\n\nMy research question is, Is there a connection between the variables and the participants' scores?\n\nI did a Pearson's Correlation on the following for V1:\n\nParticipant 1, Problem 1 Score, Problem 1 V1\n\nParticipant 1, Problem 2 Score, Problem 2 V1\n\n.............................................................................\n\nParticipant 1, Problem 12 Score, Problem 2 V1\n\nParticipant 2, Problem 1 Score, Problem 1 V1\n\nParticipant 2, Problem 2 Score, Problem 2 V1\n\n.............................................................................\n\nParticipant 2, Problem 12 Score, Problem 2 V1\n\n............................................................................\n\n..............................................................................\n\n.............................................................................\n\nParticipant N, Problem 1 Score, Problem 1 V1\n\nParticipant N, Problem 2 Score, Problem 2 V1\n\n.............................................................................\n\nParticipant N, Problem 12 Score, Problem 2 V1\n\n&amp;#x200B;\n\nThe same process I did for V2.\n\nDid i do it correctly? Please help.",
        "created_utc": 1536223095,
        "upvote_ratio": ""
    },
    {
        "title": "independent samples t test",
        "author": "cmndr_keen",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dfz83/independent_samples_t_test/",
        "text": "Hi, \n\nI'm currently taking spss course and toying around with data I've gathered but strugle understanding what's going on.\n\nI've conducted a survey at my work place (hospital).\n\nIn age distribution the two major groups were:\n1) age 26-35 46%\n2) age 35-46 37%\n\none of the questions asked if hospital computer usage should be more strict. I've conducted ind. t-test in which I compared 2 major age groups vs mentioned question. and got sig. = 0.044\n\nWhat's the proper way to interpret that data? (I'm still struggling at statistics course).\n\n[pic](https://imgur.com/a/zr27Lvd)\n\nThank you\n\n",
        "created_utc": 1536218723,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing bivariate correlations with differing significance levels?",
        "author": "halcyonq",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9debpv/comparing_bivariate_correlations_with_differing/",
        "text": "Is it acceptable to compare correlation values that have different significance levels? (One is .01 and one is .05).",
        "created_utc": 1536204542,
        "upvote_ratio": ""
    },
    {
        "title": "Is this situation an example of joint probability?",
        "author": "o-rka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dcpwn/is_this_situation_an_example_of_joint_probability/",
        "text": "Suppose one had a tree with 3 leaves and 2 internal nodes.  The 2 internal nodes \\[y1, y2\\] and classification models and the 3 leaves \\[class\\_A, B, and C\\] are the classes.  Each time the tree bifurcates at the internal nodes you get the probability of going left or right.  \n\n&amp;#x200B;\n\nLet's say you had the following probabilities:\n\n    observation 1:\n    y1 -&gt; y2 = 0.85\n    y2 -&gt; class_C = 0.9\n    \n    and observation 2:\n    y1 -&gt; class_A = 0.95\n\n&amp;#x200B;\n\n**Would it be correct to say that \\`observation 1\\` has a joint probability of \\`0.85 \\* 0.9 =  0.765\\`  and \\`observation 2\\` has a joint probability of \\`0.95\\`?**  I'm not sure if I am using the terminology correct in this case.\n\n&amp;#x200B;\n\n          /-class_C\n       /y2\n    -y1   \\-class_B\n      |\n       \\-class_A\n\n&amp;#x200B;",
        "created_utc": 1536193023,
        "upvote_ratio": ""
    },
    {
        "title": "How strong is the normality assumption for ANOVA?",
        "author": "ajhalthor",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dcn8b/how_strong_is_the_normality_assumption_for_anova/",
        "text": "The t-test and ANOVA have an assumption that the data (or their residuals) must be normally distributed. However, I've read that this condition isn't really that important. How so? In some places, I read the Central Limit Theorem Comes into play and so sample means become gaussian. But Isn't this only true for 1 sample t-tests where we compare a group to some population? How is this valid for comparing multiple groups as in ANOVA or 2 sample t-test?",
        "created_utc": 1536192562,
        "upvote_ratio": ""
    },
    {
        "title": "General Procrustes Analysis - Appropriate Statistical Test for Repeated Measures",
        "author": "ailinggradstudent",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dcj95/general_procrustes_analysis_appropriate/",
        "text": "I have a question regarding General Procrustes Analysis (GPA) and was wondering if anyone has any advice. I am  trying to test whether my data collection method is replicable between researchers.\n\nI am working with a dataset where each point has 15 XY landmarks (so it is a 2D dataset). I have 45 specimens that each had these landmarks collected by 6 different people. \n\nWhat I ultimately want to do is  see whether there is a significant difference between the shape profiles of these 45 specimens between these 6 people after a GPA. (Think a linear mixed effects model or repeated measures MANOVA but for a GPA).\n\nA colleague originally suggested I use the morphol.disparity function in the R geomorph package, but it appears that this function does not account for repeated measures and would simply pool my data together. \n\nI have also run into the lm.rrpp function in the RRPP package. But again it doesn't seem appropriate for a paired dataset.\n\nDoes anyone have any experience or advice for this problem?\n\nThanks!",
        "created_utc": 1536191753,
        "upvote_ratio": ""
    },
    {
        "title": "Do I need a reference category with aggregates made from a system of dichotomous variables?",
        "author": "Sociological_Duck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dbwps/do_i_need_a_reference_category_with_aggregates/",
        "text": "If I'm comparing race groups, I'll use a system of dummies with a reference category. Black, Native American, Asian, with whites as the reference. \n\n&amp;#x200B;\n\nIf I nest the data in states, and aggregate the mean black, Native, Asian, and white populations, do I keep the same organizational scheme?",
        "created_utc": 1536187492,
        "upvote_ratio": ""
    },
    {
        "title": "What is good literature (PDF books available online) to study Viterbi algorithm?",
        "author": "DjMiladinovic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9dapb6/what_is_good_literature_pdf_books_available/",
        "text": "I  need to write about 20 pages for my university project and I need some  good literature (not too hard to understand, some basic level).   \n \n\nTheme is \"Viterbi alogrithm\". I can also write few pages about Markov Chains and Hidden Markov Models.   \n \n\nThanks!",
        "created_utc": 1536179916,
        "upvote_ratio": ""
    },
    {
        "title": "What statistical method should I use?",
        "author": "Grantmitch1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9d9n5g/what_statistical_method_should_i_use/",
        "text": "Hey guys,\n\nReally simple question: what statistical method should I use?\n\n&amp;#x200B;\n\nWhat I want to do is correlate a series of conditions with two outcomes\\* - this can be a single analysis for both outcomes or two separate analyses for each outcome, it doesn't matter. Essentially, what I want to do is calculate the effect that a series of party strategies has on the vote share and/or reputation of a radical right party.\n\nThe attached image shows what this would look like in tabular form \\[[https://image.ibb.co/iY61Sz/example.png](https://image.ibb.co/iY61Sz/example.png)\\]\n\nImportantly, I need bundle cases according to their time period, also shown in the table. The effect should show me the impact on the vote share and/or reputation of the radical right party of the various other parties' strategies.\n\nSo using the example, in 1998, when the CDU, CSU, SDP, and FDP adopted various strategies, what was the effect? and again for 2002, 2005, 2009, 2013, and 2017, for each country under investigation. It's not a time series analysis, the parties for each time period need to be bundled together. \n\nI will also have similar data for 10 or so other European countries that will have different numbers of mainstream parties. So in the German example I've chosen four (no theoretical basis, just an example), whereas the UK might only have three, the Netherlands 6, etc.\n\nMy goal is to determine what strategies are good for weakening the radical right, essentially. I think it might be ideal to think of each period being a separate case - so Germany 1998, Germany 2002, Germany 2005.\n\nAny advice would be most welcome!\n\n&amp;#x200B;\n\n\\* I am used to QCA terminology, so condition would translate to independent variable, while the outcomes would be dependent variables.",
        "created_utc": 1536173548,
        "upvote_ratio": ""
    },
    {
        "title": "Best way to compare a matrix with a set of standards?",
        "author": "i_am_f",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9d7912/best_way_to_compare_a_matrix_with_a_set_of/",
        "text": "I'm using R for most of my analysis, but if something else can work better I don't mind switching =) \n\nI have a dataframe of 100 variables which I tested for, and a set of 15 dataframes (100 variables as well) which are standards I want to compare to. Each variable is unique and does not repeat. I want to see which standard my matrix best matches.\n\nI'm not a statistician so I'm not entirely sure if i'm wording it properly, I'm sure there are better terms to use. Sorry if that was a bit confusing!",
        "created_utc": 1536158972,
        "upvote_ratio": ""
    },
    {
        "title": "Is there a difference between poverty rate and percent in poverty? Thanks!",
        "author": "hovva91",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9d6ws4/is_there_a_difference_between_poverty_rate_and/",
        "text": "Edit: I emailed the Census and they confirmed Poverty Rate is the same as Percent in Poverty. ",
        "created_utc": 1536156684,
        "upvote_ratio": ""
    },
    {
        "title": "Independent events are necessarily non-exclusive",
        "author": "atr101",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9d48ja/independent_events_are_necessarily_nonexclusive/",
        "text": "Is the statement presented in the title correct, or wrong? why?",
        "created_utc": 1536133771,
        "upvote_ratio": ""
    },
    {
        "title": "Hull Moving Average",
        "author": "luchins",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9d1zvt/hull_moving_average/",
        "text": " I was  studying the Hull Moving Average  from a statistical point   and  I can't  figure out  what's  the difference  with Weighted moving average  and  which  statistical ''properties'' make  this indicator  less  lag  if compared to  others (SMA, EMA). \n\nI would  an  answer  from a  statistical  point, here below the formula\n\n1. Calculate a Weighted Moving Average with period n / 2 and multiply it by 2\n2. Calculate a Weighted Moving Average for period n and subtract if from step 1\n3. Calculate a Weighted Moving Average with period sqrt(n) using the data from step 2\n\nHMA= WMA(2\\*WMA(n/2) − WMA(n)),sqrt(n))",
        "created_utc": 1536113047,
        "upvote_ratio": ""
    },
    {
        "title": "Comparison of two studies on Australia's buyback program",
        "author": "SniffingSarin",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9czuup/comparison_of_two_studies_on_australias_buyback/",
        "text": "I'm currently having a debate with someone and we both brought up papers to support our arguments about Australia's gun control program\n\nHere's the study I brought up - http://c8.nrostatic.com/sites/default/files/Lee%20and%20Suardi%202008.pdf\n\nHere's his - https://jamanetwork.com/journals/jama/fullarticle/2530362\n\nWe each question the validity of each other's studies but don't have a background in statistics\n\nCan someone well versed in statistics look at these studies and tell me how \"good\" each is in terms of methodology, bias, conclusion, etc? As well as say your qualifications.\n\nThank you!",
        "created_utc": 1536096090,
        "upvote_ratio": ""
    },
    {
        "title": "Why is using the median best when using Sums of Absolute of Values opposed to mean? (laymen's terms)",
        "author": "teeny657",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cztqg/why_is_using_the_median_best_when_using_sums_of/",
        "text": "Its hard  to understand this conceptually, can any one given an example as to why the median is best? thanks!",
        "created_utc": 1536095878,
        "upvote_ratio": ""
    },
    {
        "title": "How \"good\" is a Bachelor in Stats, Specilization in Statistical Machine Learning and Data Mining Stream?",
        "author": "jayster3658",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cyy80/how_good_is_a_bachelor_in_stats_specilization_in/",
        "text": "I hope this is the correct subreddit:\n\nAnd by good I mean: employable, and good salary. I know SQL, Python, and Java. My plan was to do a Bachelor in CS but I did't make the cut for my University, and now I will be placed in Stats Coop. I just want to know if I should transfer to another University or stay and do my undergrad specialization in Stats coop. Also I can not minor in CS because it falls in the same department so I will have to minor in economy (next best minor I can take). \n\nThank-you for your help.",
        "created_utc": 1536089827,
        "upvote_ratio": ""
    },
    {
        "title": "A $300,000 property is insured against fire by paying a premium equal to C dollars. The probability of complete loss due to fire in a given year is 0.004.",
        "author": "TASITFOME",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cxvob/a_300000_property_is_insured_against_fire_by/",
        "text": " (1) Determine the value of C if the company wishes to get break even (on average).   \n\n\n(2) Determine the value of C if the company wishes to have net gain (on average) of $150.  \n\n\nI'm having trouble conceptualizing and solving this question.   \n\n\nAny help will be greatly appreciated!  ",
        "created_utc": 1536082558,
        "upvote_ratio": ""
    },
    {
        "title": "Dependent t test",
        "author": "zaffff",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cumzt/dependent_t_test/",
        "text": "Hi everyone. I usually don’t ask help from people in the internet but I’ve really run out of options. I apologise in advance if my questions seem too “basic.” It’s what our teacher called them earlier before he shrugged me off because he “had better things to do.” (Yes, these kinds of teachers do exist and I’m unfortunate enough to be one of his students). I’m really having trouble with my statistics.\n\nSo my question is what can a dependent t test do? Is it limited to testing whether or not two paired means are significantly different? Can it also detect whether there had been a significant increase or decrease between two paired means?\n",
        "created_utc": 1536059447,
        "upvote_ratio": ""
    },
    {
        "title": "Introducing new variables into the model renders insignificant variables significant",
        "author": "Sociological_Duck",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ct5ty/introducing_new_variables_into_the_model_renders/",
        "text": "I understand why adding new variables to the model can render significant variables insignificant. \n\n&amp;#x200B;\n\nIn my case, I start with religious group affiliation. Several of these are insignificant. Once I add any indicator of religiosity (frequency of prayer, religious service attendance, or subjective religiosity), several of the religious categories become significant. Cross-national data, if that matters.\n\n&amp;#x200B;\n\nHow can I explain this, and ultimately, how should this effect my research design? Ideally, I'd like to keep these variables in the model. It's fairly common in the literature, so I could appeal to that, but I don't like appeals to authority. I'd rather understand why.",
        "created_utc": 1536043339,
        "upvote_ratio": ""
    },
    {
        "title": "what are all the concepts of elementary stats?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9csrx5/what_are_all_the_concepts_of_elementary_stats/",
        "text": "[deleted]",
        "created_utc": 1536039167,
        "upvote_ratio": ""
    },
    {
        "title": "How to estimate population size for a restaurant or a coffee shop?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9csla3/how_to_estimate_population_size_for_a_restaurant/",
        "text": "[deleted]",
        "created_utc": 1536037262,
        "upvote_ratio": ""
    },
    {
        "title": "What is the plot comparing expected joint biavariate distribution and actual scatter plot or 2D density called?",
        "author": "ran88dom99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9crmno/what_is_the_plot_comparing_expected_joint/",
        "text": "So far I just compare contour plots by permuting one variable but the lines are not easy to distinguish. Also, besides absolute density I may want to see the probability. And finally maybe find pockets of unusual distributions in multiple dimensions at once. What is this entire subject called? Multivariate improbability pockets?\n\n2 contour R code:\n\nggplot(mtcars, aes(x = hp, y = wt)) + stat_density2d () +\nstat_density_2d(mapping = aes(x = mtcars$hp[sample(1:length(mtcars$hp))],\ny = mtcars$wt),\ncolor = \"green\", geom = \"density_2d\",\nposition = \"identity\" , contour = TRUE, n = 100, h = NULL,\nna.rm = FALSE, show.legend = F, inherit.aes = F)\n\nYeah with 32 data points the green contour can change very much but actual data is all 1k and up.",
        "created_utc": 1536028149,
        "upvote_ratio": ""
    },
    {
        "title": "Strange Homework Question",
        "author": "mapsandclocks",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cqltu/strange_homework_question/",
        "text": "Yes, this is for homework. I apologize. I've hit a wall.\n\nIn this question, we are being asked to calculate an ANOVA table from a table with the mean, standard deviation, and sample size of each group.\n\nHow is this possible? I thought ANOVA was based on calculating the individual deviations from the group mean, and using that as the basis for your test. We have no data about the individual observations, just the group mean and standard deviation. Is this possible to do?\n\nThank you!",
        "created_utc": 1536019689,
        "upvote_ratio": ""
    },
    {
        "title": "How do you calculate the probability of a sample size with a specific mean?",
        "author": "teeny657",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cqdxm/how_do_you_calculate_the_probability_of_a_sample/",
        "text": "&amp;#x200B;\n\nhttps://i.redd.it/o5cou2wb04k11.png",
        "created_utc": 1536017917,
        "upvote_ratio": ""
    },
    {
        "title": "If you know the values of N, Sum of Squares, and Variance, can you figure out what the mean is?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cq0wf/if_you_know_the_values_of_n_sum_of_squares_and/",
        "text": "",
        "created_utc": 1536014945,
        "upvote_ratio": ""
    },
    {
        "title": "Generating random sample from a continuous distribution X using R",
        "author": "sdfgergs",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cpp9p/generating_random_sample_from_a_continuous/",
        "text": "I am trying to use the inverse transformation method in R.  \n\nto generate 1000 random variables from the density f(x)=3x^2 0&lt;x&lt;1\n\nClearly F(x)=x^3 and F^-1 (u)= u^(1/3)\n\nusing the following R code:\n\nn=1000\n\nu=runif(n)\n\nx=u^(1/3)\n\nhist(x,prob=TRUE)\n\n\\#check if histogram of the draws from f(x)=3x^2 approximate the pdf f(x)=3x^2\n\ny=seq(0,1,0.01)\n\nlines(y,3*y^2)\n\nI get the following histogram and line: https://gyazo.com/6b3eddf144cf9cd56ae3bf4f9370325e\n\nWhich is exactly what I wanted.\n\nHowever I am running into trouble trying to do the similar thing for the double exponential with parameter lambda and gamma. Exp(lambda, gamma) CDF: https://gyazo.com/398b33c4c90156130ba00ea76cffdf2e x&gt;gamma\n\nHere is my code:\n\n\\#f(x)=lambda * exp(-lambda*(x-gamma))\n\n\\#F(x)=1-exp(-lambda(x-gamma))\n\n\\#F^1 (u)=gamma - log(1-u) * lambda^(-1), 0&lt;u&lt;1\n\nit is given that gamma and lambda should be 1.\n\ngamma=1\n\nlambda=1\n\nn=1000\n\nu=runif(n)\n\nx=gamma-log(1-u)*lambda^(-1)\n\nhist(x, prob=TRUE)\n\ny=seq(0,1,0.01)\n\nlines(y,lambda*exp(-lambda*(y-gamma)))\n\nAnd I only get the following picture: https://gyazo.com/a6f649f13d81d9c667d2fc5968e79694\n\nPretty sure I should be getting some line as well.",
        "created_utc": 1536012357,
        "upvote_ratio": ""
    },
    {
        "title": "Help Interpreting, standardised beta coeffecient for categorical predictor",
        "author": "anandoknows",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cp8sp/help_interpreting_standardised_beta_coeffecient/",
        "text": "Hello how do I interpret the standardised beta coefficient if its a categorical predictor. What do I say? Lets say sex for example do i say, being male affects the dependent variable by standardised beta value?",
        "created_utc": 1536008881,
        "upvote_ratio": ""
    },
    {
        "title": "MSE, SSE, RMSE: What do they have in common conceptually?",
        "author": "teeny657",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cp7tu/mse_sse_rmse_what_do_they_have_in_common/",
        "text": "Thank you!",
        "created_utc": 1536008684,
        "upvote_ratio": ""
    },
    {
        "title": "Question about Residual \"Patterns\" in Linear Regression",
        "author": "ice_shadow",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cokew/question_about_residual_patterns_in_linear/",
        "text": "So I learned from stats classes that if you see a U-shape pattern in the residual plot, that the linear model is not the right model for the data. \n\nWell in chemistry/physics/engineering, sometimes we fit data that already has a theoretical linear model. \n\n&amp;#x200B;\n\nFor example, Beer's Law is A= ELC so when doing spectrophotometry you plot A vs C and do a linear fit. However, sometimes a U shape pattern still appears. Does this mean the linear model is incorrect? The R\\^2 is still usually high near 0.99. \n\n&amp;#x200B;\n\nI'm currently working with contrast agents and doing some relaxivity measurements where I need to fit the inverse of the relaxation times (1/T2) in NMR to the concentration of concentration agent C. The relaxivity is supposed to be the slope of this line according to the literature. However, while I get a high R\\^2 value, my residual plot has a U-shape. \n\n&amp;#x200B;\n\nWhen I change it to a quadratic model, that aspect goes away but theoretically the literature says its linear.\n\n&amp;#x200B;\n\nDoes that mean I discard the linear model which is theoretical? Maybe there are other factors that affect the relaxivity as it is a pretty complex thing but I am not sure how to obtain a value from the quadratic plot which implies relaxivity is concentration dependent instead of independent as the theoretical linear model suggests. ",
        "created_utc": 1536003972,
        "upvote_ratio": ""
    },
    {
        "title": "MANOVA: Differences in Covariance Matrices?",
        "author": "TheOtherCount",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cnvn5/manova_differences_in_covariance_matrices/",
        "text": "Hi all,\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI've got two zero-mean vector-valued datasets, and I'm interested in knowing how to do the statistics as to detect statistically significant differences in the covariance matrices for each sample! I know Random Matrix Theory has something to do with it.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nAny suggestions would be a life-saver!\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThank you so much,\n\nTOC",
        "created_utc": 1535999108,
        "upvote_ratio": ""
    },
    {
        "title": "How to filter in SPSS by valid N?",
        "author": "throwawa341",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cnlgg/how_to_filter_in_spss_by_valid_n/",
        "text": "I've done some searching but unable to find this.\n\n&amp;#x200B;\n\nAny SPSS maestros - I have a really big database (335 variables) that I need to analyse. Of these, about 10 variables are crucial, I don't want any data gaps. I put these together in the descriptive statistics and found that out of total n of 699, I have a valid n of 421. \n\nCan I filter by this specific valid n (valid by those 10 variables) and exclude the remaining cases, as in only analyse the 421 cases where all 10 variables I need are filled? \n\n&amp;#x200B;\n\nThanks!",
        "created_utc": 1535997138,
        "upvote_ratio": ""
    },
    {
        "title": "Probability that an 8-character password has exactly 2 integers?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cnghd/probability_that_an_8character_password_has/",
        "text": "[deleted]",
        "created_utc": 1535996205,
        "upvote_ratio": ""
    },
    {
        "title": "Prep for Oral Exam, please help",
        "author": "Frontlines95",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cnbzo/prep_for_oral_exam_please_help/",
        "text": "So tomorrow I'm having an oral exam in Probability and Statistics. Last time I failed somewhat and the professor gave me literally 2 questions to study and answer. Only thing is, they are very vague.\n\n1. fg distribution\n2. event X:Ω-&gt;R\n\nI don't understand what he means by a distribution of a probability density and the 2nd question looks like a Random Variable question, but I don't understand the meaning of event in it.",
        "created_utc": 1535995359,
        "upvote_ratio": ""
    },
    {
        "title": "How to test if a die is fair, with fractions",
        "author": "mafffsss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cmvvx/how_to_test_if_a_die_is_fair_with_fractions/",
        "text": "I'm wondering what the best approach is here \n\nsay I'm tolled a die is rolled 20 times and there are 6 6's, and asked to determine whether the die is fair\n\nGetting a 6 can be considered a random selection with replacement from the list \n\n0,0,0,0,0,1\n\nwith expected result of 1/6 sixes from one selection (which makes no physical sense ). \n\nThen over the course of 20 rolls we'd expect 10/3 sixes, which also makes no physical sense. \n\nDo I try to make physical sense of this or continue the computation? And if I do make sense of 10/3, should I round down, or up? (obviously convention with values would be 3.3 - &gt; 3.0 , I'm not sure if there's a difference at all here )\n\nContinuing, we see that the standard deviation here is sqrt([5/6] * [1/6]) = sqrt(5) / 6 \n\nThen the standard error is sqrt(20) * sqrt(5) / 6 = 5/3 ~= 1.7\n\nSo, I now have that I'm expecting something around 3, which varies by around 2. I'm not putting exact values on them because I'm not sure that physically makes sense. \n\nIf anyone can shed some light on how to interpret and write up something like this, that deals with physical things where fractional values are senseless, that would be appreciated. ",
        "created_utc": 1535992220,
        "upvote_ratio": ""
    },
    {
        "title": "Combine different socio-economical variables into one",
        "author": "chrismilan",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9clg7o/combine_different_socioeconomical_variables_into/",
        "text": "I wanted to know whether it was possible and, if so, what would be the best approach to combine different socio-economic variables in order to make an index that would represent the social situation of different States / regions.  \n\nFor instance, if I have an independent variable (e.g. numbers of sold television) that I want to adapt  to GDP per capita, Populations, Burglary rate, unemployment rate, etc. I can create a number\\_of\\_sold\\_television\\_per\\_100k\\_population variable but how to create a formula that would allow me to create a new variable taking into account not just the population but all the other socio-economic ones? \n\nI hope it makes sense and if it doesn't that you will explain me why !\n\nThank you :)",
        "created_utc": 1535981122,
        "upvote_ratio": ""
    },
    {
        "title": "Choosing sample size to determine the effectiveness among different ad types",
        "author": "Mrbumby",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ckb1i/choosing_sample_size_to_determine_the/",
        "text": "Hi,\n\n&amp;#x200B;\n\nI want to test different ads on Facebook to see which one is the most effective. Therefore I want to launch 3 different ads to the same target audience.\n\nSo the result would look something like this:\n\n1. Ad: Views: 120 Clicks: 5\n2. Ad: Views: 130 Clicks: 3\n3. Ad: Views 100 Clicks: 4\n\nNow I'm wondering how do I get number of samples (Views) I have to get to have valid result.  \n\n\nI'm think that the pwr package for R might be a good start, but I'm not sure which statistical test applies:  \n\n\nPWR: [https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html](https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html)\n\n​",
        "created_utc": 1535969613,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical Forecast resources",
        "author": "Spilkn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ck5u1/statistical_forecast_resources/",
        "text": "Long story short, in a couple of months I will be taking over management of a number of Demand Planners in my company. The company relies heavily on statistical forecasting of product demand (3000+ products).\n\nAlthough I have some experience in demand planning and statistical forecasting I haven’t been involved in the practice of it for a number of years.\n\nCan anyone point me in the direction of any decent online resources / free courses to refresh / learn?",
        "created_utc": 1535968013,
        "upvote_ratio": ""
    },
    {
        "title": "Second opinion about how I'm analysing this.",
        "author": "sholopinho",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9ck3b2/second_opinion_about_how_im_analysing_this/",
        "text": "Hey. \n\nI've made a questionnaire aiming to asses a knowladge level. I've asked 4 knowladge questions, 3 of them were multiple choises questions and the last was an open question that the participants answered a certain value.\nEach correct answer in the multiple choises questions got 1 point and a wrong answer got 0. \nThe open question was scored in a way that the closer the participant was to the correct answer, the more points he got for the question. For example, if the correct answer was 10 and he answered 15 or 5 then he got 0.5 points. \nEventually, the points were summed so each participant had a \"knowladge level\" score.\n166 questionnaires were collected. \nI feel that that there's a smarter way to do it, specially the open question part. \nI'm aiming to conduct a liner regression on the scores with some demographic variants.\n\nI'd be happy to have some other opinions or ideas:) ",
        "created_utc": 1535967126,
        "upvote_ratio": ""
    },
    {
        "title": "ELI5: Density scale",
        "author": "Blepmorty1231",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9citf1/eli5_density_scale/",
        "text": "why is it important?\n\nWhy does it matter?\n\nWhat is it?\n\n​\n\nEtc  \n\n\nADD:I forgot to mention that this concept i am having trouble on is the in the context of histogram graphs",
        "created_utc": 1535952784,
        "upvote_ratio": ""
    },
    {
        "title": "Am I thinking this probability question through correctly?",
        "author": "EtOHMartini",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cgmhp/am_i_thinking_this_probability_question_through/",
        "text": "A committee is being formed. Ten members put forward their names. Four will be chosen. Each committee member can vote for four of the nominees. Eleven members vote for exactly the same four nominees.\n\n10P4 = 210 possibilities.\nEleven members choosing exactly the same: 11^210 = 35027750054222100000000000\n\nThe probability of this outcome is 1/35027750054222100000000000\n\nIs this valid mathematical reasoning?\n\n",
        "created_utc": 1535931631,
        "upvote_ratio": ""
    },
    {
        "title": "How to find how many numbers it will take to generate all values if generated randomly?",
        "author": "AltruisticOpposite",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cd5gv/how_to_find_how_many_numbers_it_will_take_to/",
        "text": "I play league of legends and I've been collecting champion(characters in the game) shards and I want to know how many more I need to get to get them all on average. There are 141 champions currently and I possess 71 of their shards already. ",
        "created_utc": 1535904284,
        "upvote_ratio": ""
    },
    {
        "title": "Help interpreting unusual Hierarchical Regression results",
        "author": "anandoknows",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cd1ok/help_interpreting_unusual_hierarchical_regression/",
        "text": "Hello, I have some unusual (to me) results that I dont really know how to explain. Its probably to do with losing degrees and thus losing precision etc but, i just want to clarify this.\n\n&amp;#x200B;\n\nIve ran a hierarchical regression, the dependent variable is- Levels of relaxation following a solitary activity (Post Relaxation)\n\n&amp;#x200B;\n\nto explain the variance the predictors i added in \n\nstep1: Trait-mindfulness, pre-levels of relaxation (model not significant)\n\nStep 2 (model not significant): gender (dummy Male reference\\* significant contribution), age, meditation conditon and colouring condition added\n\n&amp;#x200B;\n\nstep 3( model not significant): only gender significant\\*, relgion and meditation experience added but not signficant\n\n&amp;#x200B;\n\nStep 4 (model is SIGNIFICANT): TRAIT-mindfulness is now significant, gender is signficiant and colouring condition is significant BUT the new variables added which are \"depression\" \"stress\" and pre-levels of STATE-mindfulness are NOT signficiant\n\n&amp;#x200B;\n\nStep 5 (model is SIGNIFICANT): Only pre levels of state mindfulness is now significant, the added variable distraction and enjoyment are not significant \n\n&amp;#x200B;\n\nI dont understand what is going on with these results or why they could, if you guy know the potential reasons for this, i would appreciate it greatly.",
        "created_utc": 1535903468,
        "upvote_ratio": ""
    },
    {
        "title": "Post hoc test after no significant difference in ANOVA",
        "author": "drpushanmaity",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9cbtia/post_hoc_test_after_no_significant_difference_in/",
        "text": "so, I did ANOVA and found the p value to be &lt;0.05 . However if I run some post hoc tests anyway ie LSD HSD sceffe's .... Then I get significant mean differences on LSD in some sets.... I might sound stupid ( believe me I am regarding statistics) but can anyone please explain this to me?",
        "created_utc": 1535892725,
        "upvote_ratio": ""
    },
    {
        "title": "Statistical Problem",
        "author": "Cyalas",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9c3zmp/statistical_problem/",
        "text": "Hello,\n\nI'm not so good in statistics (I wish I were), but I've a statistic problem and don't have an idea about how to approach it. I've some local measures (15x21 points) of local velocities. By local velocities I mean velocity in each cell (represented by the magnitude :  [https://imgur.com/a/wNGLVea](https://imgur.com/a/wNGLVea)). With this data, I can compute the discharge (flux across the section). The problem is this is a lot of measures (each case is a measure) and I want to measure (in next steps) in fewer points without affecting the result (the discharge) very much. So I thought that (maybe) there are two parameter I should optimize: the number of points and the position. \n\n**In brief: What's the minimum points and their position that can give us the better discharge.** I have no idea about how to approach the problem in a statistic way (I guess it's an optimization problem?). I could do it manually (test all the possibilities: remove one point, test all the combination, remove 2 pts test all the combination, etc) but I guess there is an other statistical (so faster) way to apply ?\n\nThank you!",
        "created_utc": 1535816415,
        "upvote_ratio": ""
    },
    {
        "title": "Are there any assumptions for Fisher's exact test that aren't shared by Pearson's 2x2 Chi-squared test?",
        "author": "PainIntoPower",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9c39vi/are_there_any_assumptions_for_fishers_exact_test/",
        "text": "Besides requiring more computational power for Fisher's exact test, isn't it just a more superior version of the Chi-squared test?",
        "created_utc": 1535810695,
        "upvote_ratio": ""
    },
    {
        "title": "Standard error with a list",
        "author": "mafffsss",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9c33rf/standard_error_with_a_list/",
        "text": "I have the following : \n\nhttps://i.imgur.com/UYgaXDn.png\n\n\n\nI feel as though the approach would be to take each list, compute the standard deviation of the list, then have \n\nse = sqrt(n) * sd\n\nAnd the value which is closest to 5 would be the correct list. \n\nusing python \n    \n    In [2]: l1 = [51,57,48,52,57,61,58,41,53,48]\n    \n    In [3]: l2 = [51,49,50,52,48,47,53,50,49,47]\n    \n    In [4]: l3 = [45,50,55,45,50,55,45,50,55,45]\n    \n    In [7]: math.sqrt(10) * statistics.stdev(l1)\n    Out[7]: 18.80898130622118\n    \n    In [8]: math.sqrt(10) * statistics.stdev(l2)\n    Out[8]: 6.359594676112971\n    \n    In [9]: math.sqrt(10) * statistics.stdev(l3)\n    Out[9]: 13.84437310486346\n    \n\nSo I would state that (ii) is the list which is the sequence of observed values. \n\nI'm not sure if this approach is correct though, thanks\n    \n",
        "created_utc": 1535809116,
        "upvote_ratio": ""
    },
    {
        "title": "Comparing the rating of two things with consideration to sample size?",
        "author": "OneManApocalypse",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bz6qs/comparing_the_rating_of_two_things_with/",
        "text": "I've been Googling this for months without any luck, so ideally someone here can point me in the right direction where I can do more research.\n\nSay I have two games, Factorio and Grand Theft Auto V:\n\nhttps://store.steampowered.com/app/427520/Factorio/\nhttps://store.steampowered.com/app/271590/Grand_Theft_Auto_V/\n\nEach game gets reviewed, and has a different score.\n\n* Factorio: 98% positive reviews with ~30k reviews\n* GTAV: 67% positive with ~353k reviews\n\nIs there an acceptable way that I can create a single rank of each game by combining their sample size against their overall score? If I had a list of 10 total games with differing review counts and overall scores, is there a meaningful way to rank that list so that a game with 10 reviews and 90% positive can be weighted against a game with 10,000 reviews and a 85% score?",
        "created_utc": 1535764482,
        "upvote_ratio": ""
    },
    {
        "title": "Question about Sample Size/G*Power",
        "author": "psycho1391",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bw00y/question_about_sample_sizegpower/",
        "text": "Hi all. I'm planning on testing out a mediation model using multiple regression\n\nMy IVs: Attachment (3 levels--secure, anxious, avoidant) and Relationship Satisfaction, where relationship satisfaction is the proposed mediator \n\nMy dv: Glycemic control\n\nWhen I run G*Power with an effect size of .5, alpha set at .05, 2 predictors, one tail and power at .95, it is saying that I only need 24 participants. \n\nAm I doing something wrong? \n\nBacking up: Am I even running the correct statistical test? \n\nPicture for reference: https://imgur.com/a/ihn1xWT \n\n",
        "created_utc": 1535739328,
        "upvote_ratio": ""
    },
    {
        "title": "Can't understand the result output",
        "author": "Renyudaishu",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bt4pv/cant_understand_the_result_output/",
        "text": "I am using the cox's proportional hazards model, and one of my hazard ratio is 6.31e-09 \n\n&amp;#x200B;\n\nThe whole line reads like this:\n\nHaz. Ratio   Std. Err.      z    P&gt;|z|     \\[95% Conf. Interval\\]\n\n6.31e-09   2.17e-09   -54.98   0.000     3.22e-09    1.24e-08\n\n&amp;#x200B;\n\nWhat does this mean?\n\n&amp;#x200B;",
        "created_utc": 1535718075,
        "upvote_ratio": ""
    },
    {
        "title": "Negative Confidence Intervals (Ecology)",
        "author": "TheLadyOfSmallOnions",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9brgr0/negative_confidence_intervals_ecology/",
        "text": "Hey all, I'm working on some stats based around sampling tree density (number of trees per meter squared). The confidence intervals I got for density were -0.0427 to 0.0026. Now according to the smart maths person I was talking with, this means that the data is dodgy, because there is a negative confidence interval? I thought I understood, but looking back at the notes I have no idea what is going on. But here's what I think I understand:\n\n- The confidence intervals are -0.0427 and 0.0026. The tree density overall was 0.006.\n- The reciprocal of the data is 1/D (D being tree density). I'm not sure if reciprocals are important here but I know 1/D is involved here in some way. \n- The lower confidence interval cannot be negative. I'm a little foggy on this part - is it because negative trees are impossible? I know some stats can have negative confidence intervals, but is it different in ecology?\n- Because of this we choose 0 as the lower confidence interval.\n- But 1/0 (as in 1/D) is not a valid fraction so we have to choose a positive number close to zero e.g. 1/0.001. \n- ???????\n- So the upper confidence interval is undefined/infinite? \n\nHelp me please!\n\n",
        "created_utc": 1535699322,
        "upvote_ratio": ""
    },
    {
        "title": "Please help me solve these probability questions",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bq2rk/please_help_me_solve_these_probability_questions/",
        "text": "[deleted]",
        "created_utc": 1535684952,
        "upvote_ratio": ""
    },
    {
        "title": "Logistic regressions issues",
        "author": "TempUN18",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bklga/logistic_regressions_issues/",
        "text": "Hi there, I'm having a confusing time with some logistic regressions I'm running, and any help would be appreciated.\n\n​\n\nI'm looking at a binary dependent variable and independent variables on a scale, with two covariates. I first ran a series of binary logistic regressions (1 independent variable and 2 covariates), and everything seemed fine, although for some I had a problem of complete or quasi-complete separation (massive SE and beta), so I also ran Firth regressions to correct for separation. I'm doing this in SPSS.\n\n​\n\nI noticed something odd about the Firth regressions that I can't seem to understand... when putting the 3 variables (IV and covariates) into the \"independent variables\" box, I seem to get completely different output for each variable (and the overall test), depending on the their order in the box. *\\[EDIT: with further tests, this seems to happen specifically when I select a small subset of my cases, not the whole dataset, but is still weird.\\]* This isn't the case with regular binary logistic regressions, where I have the IV and 2 covariates in \"Block 1\" and the order doesn't affect results.\n\n​\n\nSo my main confusion: **is the order of the variables supposed to matter? If so, why, and in which order do they need to be?**\n\n​\n\nI've also got some other questions about Firth regressions, if anyone knows anything:\n\n\\-   Is it more correct to just run the  \"standard\" logistic regressions, forget about the complete separation  variables, and just report the   non-\"separated\" significant results, or  to use the Firth regressions?\n\n\\-   If  using Firth regressions, is it more correct to run Firth  regressions   for EVERY variable of interest (for consistency), or only  those   variables with complete separation (as the non-separates don't  need   correection)\n\n\\- In SPSS, for the  logistic  regression the main p value I was looking at to determine  significance  was just the p value for the independent variable amongst  the  covariates  in the \"Step 1 Variables in the Equation\" box. For Firth,  the SPSS  output also specifies a \"significance\" value above, for  the  test as a  whole. Is it relevant whether this value is &lt;0.05 (or   whatever alpha) if I'm only concerned with my one independent variable?\n\n​\n\n​\n\n​\n\nThanks a ton in advance, I really appreciate any input!\n\n​\n\n​",
        "created_utc": 1535642837,
        "upvote_ratio": ""
    },
    {
        "title": "Need help in correcting for autocorrelation in uninterrupted, non-seasonal timeline analysis",
        "author": "thenakednucleus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bk6n1/need_help_in_correcting_for_autocorrelation_in/",
        "text": "Hi everyone,\n\nI’m new to timeline analysis, so please go easy on me.\n\nI scraped social media posts from Reddit and coded the individual posts for several attributes (such as, for example, anger words). Now I want to test whether these attributes change over time.\n\nI set the time of the first post of each individual as 0, so that the time since the first post is my independent variable and the attribute (i.e. anger) is my dependent variable.\n\nNow if my independent variable was anything else than time, I could simply use least-squares regression. The way I understand it though, I won’t be able to reliably use a simple regression analysis because error terms are gonna be autocorrelated.\n\nEvery solution for this problem I’ve found so far assumes that measurements are either evenly spaced out over time or that there is the same amount of measurements (posts) per individual (these vary between 1 and several thousand though!).\n\nCould anyone here point me towards an algorithm or correction for regression that works well with this kind of data?\n\nThanks in advance\n",
        "created_utc": 1535639971,
        "upvote_ratio": ""
    },
    {
        "title": "Stata: How do i set two cutoffs using rdrobust?",
        "author": "[deleted]",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bjcrz/stata_how_do_i_set_two_cutoffs_using_rdrobust/",
        "text": "Or any other ways to do a discontinuity design regression with two cut-offs using stata.",
        "created_utc": 1535633546,
        "upvote_ratio": ""
    },
    {
        "title": "Estimating average speed of all cars on a highway?",
        "author": "Noumenon72",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bgnmr/estimating_average_speed_of_all_cars_on_a_highway/",
        "text": "Was just wondering if there was some mathy way to formalize \"I am driving 80 and passing *n* cars per minute while *m* cars are passing me, so I estimate the average speed is 75.\"",
        "created_utc": 1535604567,
        "upvote_ratio": ""
    },
    {
        "title": "Looking for advanced correlation analysis for possible: linear, non-linear and lag components.",
        "author": "pych_phd",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9beest/looking_for_advanced_correlation_analysis_for/",
        "text": "Situation: \nmarket price analysis for various competitors. \n\nsimilar situations:\nlooking for patterns in neuron firing rates, where I am reasonably sure analysis has been done but not sure how. When this part of the neuron rate changes is there other parts that do too, and how?\n\n\nAim: \ninvestigate how the prices change together or separately over time.  Does one competitor follow another directly, maybe a lag? or... \n\nSo in this situation there is no x/predictor variable except maybe time.  All the variables are outcome variables and the aim is to find out how they change together.   \nThe linear, non-linear and lag aspects may all require separate modeling. \n\nConsidered methods:\n\nAMOS modeling.  The version I did treated the variables as separate and thus might not work in this situation. \n\n * iteratively run, run standard correlation pearsons/find a non-linear correlation methods, for all seasons. \n\nWikipedia lists theses three possible sources for non linear:\n\n      Croxton, Frederick Emory; Cowden, Dudley Johnstone; Klein, Sidney (1968) Applied General Statistics, Pitman. ISBN 9780273403159 (page 625)\n      Dietrich, Cornelius Frank (1991) Uncertainty, Calibration and Probability: The Statistics of Scientific and Industrial Measurement 2nd Edition, A. Higler. ISBN 9780750300605 (Page 331)\n      Aitken, Alexander Craig (1957) Statistical Mathematics 8th Edition. Oliver &amp; Boyd. ISBN 9780050013007 (Page 95)\n\n * Some variation of factor analysis but for time based situation - I.e. is competitor 1 so similar in pricing over time to competitor 2, based on linear/non linear/lag analysis that they could be considered one entity. \n\n * Confidence interval analysis of the slop/differentiation/correlation of the data over time. \n\nsimilar to:\n \n * Create a market mean and then modeling that as \"0\" and looking at the different competitors relative to this. Kind of like a GLM but takes into account the change in mean overtime. \n\nThoughts?\n \n\n \n",
        "created_utc": 1535585367,
        "upvote_ratio": ""
    },
    {
        "title": "Book reccomendations for questionaire construction",
        "author": "JohnCamus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bectk/book_reccomendations_for_questionaire_construction/",
        "text": "I know, I know. There are a lot of requests for general statistical books. I am currently looking for some in depth literature on test theory and survey construction. Could anybody of you guys reccomend something?",
        "created_utc": 1535584897,
        "upvote_ratio": ""
    },
    {
        "title": "Finding the MLE of a \"simple\" binomial",
        "author": "grizzlebritches",
        "url": "https://imgur.com/zXT9G9e",
        "text": "",
        "created_utc": 1535577230,
        "upvote_ratio": ""
    },
    {
        "title": "Is there an accepted abbreviation for explained variance ratio from PCA?",
        "author": "o-rka",
        "url": "https://www.reddit.com/r/AskStatistics/comments/9bcras/is_there_an_accepted_abbreviation_for_explained/",
        "text": "[http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n\n&amp;#x200B;\n\n**explained\\_variance\\_ratio\\_** : array, shape (n\\_components,)\n\n&gt;Percentage of variance explained by each of the selected components.",
        "created_utc": 1535573295,
        "upvote_ratio": ""
    }
]