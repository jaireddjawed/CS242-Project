[
    {
        "title": "convolution and sum",
        "author": "Sen_7",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kcxli/convolution_and_sum/",
        "text": "I have the following question:\n\nX1,X2 are i.d.d and both \\~U(0,1)  \nI wanted to calculate the cdf of X1+X2  \n\n\nI solved this question by drawing the square and calculating the area of the two triangles,  \nfor 1&lt;t&lt;2 the answer was 1- ((2-t)\\^2)/2  \n\n\nI have tried to also solve it using convolution  and had some problems, would appreciate help please:  \n\n\nhttps://preview.redd.it/ndycqh3bf1ea1.jpg?width=948&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2f0a1cbf3ae06be931eec575eac07ce6234945a9\n\nIm pretty sure that my problems are in the upper and lower bounds of the integral (the from -&gt; to)  \nbut Im not sure how Im suppused to think about it",
        "created_utc": 1674586350,
        "upvote_ratio": 1.0
    },
    {
        "title": "Best way to simplify change over time",
        "author": "JoWVMD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10kbx79/best_way_to_simplify_change_over_time/",
        "text": "\nHi!  I am considering a research project that would seek the answer the following question:  Is someone who is over weight and looses weight then regains it more or less likely to develop diabetes in the future.  This would be a retrospective study.  I can pull reports on patient’s weight and hba1c (measure of blood sugar) from our EHR.  It would be nice if I could boil an individuals BMI over time down to one or two variables that would tell me wether or not it went down a significant amount (&gt;5% of initial) then back up (at least to initial) again.  \n\nIf it helps to know more specifically what my reports look like column A - patient ID, B date of BMI, C - BMI \nColumn A will be the same for patient who have multiple instances of BMI recorded in the EHR.  \n\nThat way I could easily group patients by those who have lost and regained and those who have stayed roughly the same.  \n\n\nCan anyone suggest analysis that could do this? \n\nThanks!",
        "created_utc": 1674583925,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which is a bigger evil in Linear Regression? Endogeneity or Multicollinearity ? [Question]",
        "author": "venkarafa",
        "url": "/r/statistics/comments/10k3l01/which_is_a_bigger_evil_in_linear_regression/",
        "text": "",
        "created_utc": 1674577832,
        "upvote_ratio": 1.0
    },
    {
        "title": "How can I run a metanalysis on pre-post results in Revman?",
        "author": "aimlesssouls",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10k91v1/how_can_i_run_a_metanalysis_on_prepost_results_in/",
        "text": "Hello! I want to conduct a met-analysis using the pre-test for a treatment and a control, and the post-test of a treatment and a control after 6 weeks. I have the mean of the tests, number of participants, and SDs. When I try to use revman 5.4, I can only compare the treatment and control group without comparing pre/post. I'm not sure if this makes sense, this is my first met-analysis. What software can I use to conduct a met-analysis for pre-post results?",
        "created_utc": 1674576794,
        "upvote_ratio": 1.0
    },
    {
        "title": "A Question on Randomization and Variance Minimization for Randomized Clinical Trials",
        "author": "Hell-Walker-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10k859b/a_question_on_randomization_and_variance/",
        "text": "Randomization is considered a \"gold-standard\" for mitigating selection bias. However, there is still a chance that subjects in an RCT may not be comparable because of covariates. Variance minimization is able to work around this. Is there an element of \"randomization\" in the variance minimization procedure? Would the research design remain a randomized clinical trial even if you used variance minimization for the allocation of subjects to treatments?",
        "created_utc": 1674574419,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to know which variable is leading to MDS grouping",
        "author": "SushiRoll9298",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10k4qnu/how_to_know_which_variable_is_leading_to_mds/",
        "text": "Hey all!  \n\n\n  I'm currently running an MDS analysis on some data. I'm using isoMDS from the MASS package in R, my input is a Jaccard distance matrix and the analysis seems to be running fine, I'm having stress values around 0.1\n\nMy question is the following: When I plot the result, I get to see a group that is clearly apart from the rest, I have tried colouring and assigning shaped to different variables in my metadata but they don't seem to explain this grouping (see pictures). Is there a way to infer/calculate which of my variables might be responsible for this?  \nNote: I'm aware of the existence of envfit but this produces a set of vectors showing the impact of different environmental variables, not quite what I'm looking for.  \n\n\nThanks for any help!!!  \n\n\nhttps://preview.redd.it/6wfkd05smzda1.jpg?width=1085&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=af40bdd41f6152f5473459a6a4d585ac61555b21\n\nhttps://preview.redd.it/s9nm725smzda1.jpg?width=1085&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=51603f89efbc0eec3d43d02d328c12ddcb5d0c25",
        "created_utc": 1674564619,
        "upvote_ratio": 1.0
    },
    {
        "title": "Using Copula to determine dependence structure between risk factors for a disease",
        "author": "TinyPenguin22",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10k3jd8/using_copula_to_determine_dependence_structure/",
        "text": "1. I am working on a dataset from a disease survey. I tried to identify risk factors from that survey for a sub-category of the disease. As a first step, I took 4 states for data observation from this survey.    \n2. For each state, I tried to see if the data collected on every possible predictor of the disease (for example, age, gender, smoking status, diabetes status, and socio economic status) was representative of the total population surveyed in the state, for example, I found that in state X, testing efforts are more focused on 60-74-year-olds.    \n3. With this information, I looked up for some baseline data for each predictor on national data sources.  \n4. Since I now had an idea of how the representativeness of the population was for each state, the next step was to see if this surveyed population was representative of the country's population and could be replicated in a synthetic population.  \n\n\nI have tried to use Copula probability theory to describe the dependence structure between random variables while keeping the marginals fixed in R.   \n\n\nI built a correlation matrix for synthetic data, using each predictor's marginal distribution. I am trying to use this matrix to fit my Copula but I keep getting this error:  \n\n\nclayton\\_copula\\_fit &lt;- fitCopula(copula = claytonCopula(), data = pobs(syn\\_data), method = \"mpl\")\n\nError in (copula, u = data, method = method, start = start,  : \n\n  The dimension of the data and copula do not match  \n\n\nCan someone please help? thank you!",
        "created_utc": 1674560439,
        "upvote_ratio": 1.0
    },
    {
        "title": "Connect data from 2 samples",
        "author": "yasumasa",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10k1c1h/connect_data_from_2_samples/",
        "text": "Hi, so I have this racing game where cars are listed from cheapest to most expensive. List A has all the cars I have already unlocked and List B has only locked cars. The total amount is about 140 and I already have 100 for List A unlocked.\n\nSo I wanted to calculate the amount of money you need to buy all cars but didn't want to add them up. My idea was to take samples from each list that are proportionally distributed and make a fit for both and calculate the integral. How can I merge both samples from both lists?\n\nYes I could just do it the \"correct\" way and add them up one by one but I wanted to know what this problem is called and what to do if the lists were bigger.",
        "created_utc": 1674550991,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is there a simple test for trend in an uneven spaced time series?",
        "author": "StargazingGecko",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jxd86/is_there_a_simple_test_for_trend_in_an_uneven/",
        "text": "Hi. I have a time series which small sample size and values that seem to increase over time, but the measurements were made at uneven intervals. \n\n I am trying to obtain a p-value related to the hypothesis that there is a trend, and I found some bootstrap-based tests for testing it ([https://cran.r-project.org/web/packages/funtimes/vignettes/trendtests.html](https://cran.r-project.org/web/packages/funtimes/vignettes/trendtests.html)), but I am assuming that even them takes the regularity of the time series as a assumption. Am I right? Can someone point me in the direction of finding what tests, if any, could be used in my case?  It is for a more exploratory analysis, so the methods don't need to be complex or particularly powerful.\n\nBest",
        "created_utc": 1674535426,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I manipulate to get the next step? ISLR2 LDA",
        "author": "No_Canary_5299",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jucab/how_do_i_manipulate_to_get_the_next_step_islr2_lda/",
        "text": "&amp;#x200B;\n\nhttps://preview.redd.it/qysy82olfwda1.png?width=1300&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=41adca87aa925a91e786b2d4d42e55112411afc6",
        "created_utc": 1674525913,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics survey question (at least 50 people). Number of songs in your favorite playlist?",
        "author": "Medium-Hold1683",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jspt3/statistics_survey_question_at_least_50_people/",
        "text": "",
        "created_utc": 1674521213,
        "upvote_ratio": 1.0
    },
    {
        "title": "Does nearest neighbor matching match based on inputs or outputs?",
        "author": "malachai926",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jozul/does_nearest_neighbor_matching_match_based_on/",
        "text": "Let's say you have two predictor variables, X1 and X2, and an output variable, Y. One data point has X1 = 12, X2 = 60, Y = 3.3. If you wanted to find its \"nearest neighbor\", would you look for another data point that has, say, X1 = 12.1, X2 = 60.1, and potentially something like Y = 1,000,000, or would you look for a row where, say, X1 = -934,234, X2 = 123,456,789, but Y = 3.31? Which of these two is what the \"nearest neighbor\" algorithm is looking for?",
        "created_utc": 1674511567,
        "upvote_ratio": 1.0
    },
    {
        "title": "Why couldn't we always use PMF instead of histograms?",
        "author": "muskagap2",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jntd9/why_couldnt_we_always_use_pmf_instead_of/",
        "text": "Hello,  I'm curious about this thing: if PDF gives us theorethical  distribution, which is precise probability distribution for each X then  why we should use empirical histograms based on observed data? The issue  is that we can plot PMF and histogram and PMF will show precise  probabilities for each X whereas histogram might not. Histograms are  sample size-sensitive, so  we can be misled by the difference in sample  size. Not sure if I explained it clearly.\n\nLet's  say we have a dataset with 'age' column. We could measure probability  precisely for each discrete variable in age column instead of plotting a  histogram.\n\nAll in all, why not using underlaying PMF but histograms to estimate probability?",
        "created_utc": 1674508712,
        "upvote_ratio": 1.0
    },
    {
        "title": "Sampling variable populations with different signal to noise ratios feels wrong to me, but I can't exactly explain why it's counterproductive. Could anyone help me explain to my colleagues why this approach isn't productive?",
        "author": "American-living",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jnbmn/sampling_variable_populations_with_different/",
        "text": "I'm currently working on a project that uses very large data sets where there's a very low signal to noise ratio, but the extent to which that varies is dependent upon certain conditions.  \n\n\nTo explain what I'm interested in examining:  \n\n\nLet's say we have 10 big barrels of marbles. There are gray marbles (which are the noise). These are &gt;95% of all the marbles in each barrel. Then we have a bunch of different colored marbles (which are the signal). I'm concerned about the different colors and proportions of these colored marbles to one another, we don't care about their relationship to the number of gray marbles.  \n\n\nIn each of the barrels there can be very different numbers of marbles. It can be 10-20k, 3-5k, or sometimes only a few hundred. Currently, we try to sample the same number of marbles from each barrel, regardless of the number of marbles that are in each barrel. We do that 10 for each barrel and then average out the numbers of colored marbles in each group.\n\nRecently, I realized that the numbers of colored marbles that we were counting in each of the samplings seemed to be proportional to the number of marbles that were in a given barrel. For example:  \n\n\nIf we have a barrel with 10k marbles and another barrel with 3k marbles, we would sample 2.5k marbles from each barrel 10 times. I found that the number of colored marbles we would count in our samplings for the 10k marbles would be roughly 100 on average for the barrel with 10k marbles whereas the barrel with 3k marbles would sample roughly 330 colored marbles on average.   \n\n\nAs it stands, we try to compare the 330 number to 100 number, but we're actually interested in the number of colored marbles overall in each sample. It seems like both barrels have about 400 colored marbles in them and that is the number we should be comparing.   \n\n\nMy knowledge of statistics is rudimentary and patchy enough that I don't have the language to describe why our current approach isn't productive? Or maybe at least point me in the direction of some reading that might be helpful?",
        "created_utc": 1674507546,
        "upvote_ratio": 1.0
    },
    {
        "title": "Reporting regression with bootstrap",
        "author": "Alicia-Emily",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jn8qk/reporting_regression_with_bootstrap/",
        "text": "I am sorry if this is a dumb question, but I am quite new to bootstrappping. I just ran a multiple linear regression analysis with bootstrap in SPSS. I did bootstrap, because my residuals were not normally distributed (and the sample size is small, so no central limit theory at work here). But now I am not sure how to set up my results table (which std errors, CI, p-values). And what to do with r\\^2?\n\nI could not find an example table, so i thought I would ask here.",
        "created_utc": 1674507349,
        "upvote_ratio": 1.0
    },
    {
        "title": "Non-Parametric Test help needed",
        "author": "Anonym1111273",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jn40w/nonparametric_test_help_needed/",
        "text": "I want to test whether \"takeover premiums\" are differently influenced by the variable \"Beta\" during economic crisis vs normal periods. To test that, I used a Generalised Least Squares Regression (GLS) because the takeover premiums are not normally distributed. However, I understand that one underlying assumption is a linear relationship between the dependent variables and the takeover premiums and this is not the case. **Can I use a different test to test my hypothesis?**\n\n Furthermore, I'm not sure whether I used the right code in R: financial\\_model &lt;- gls(\\`X1.Month.Equity.Premium\\` \\~ crisis + Beta, data = financial\\_subset, method = \"ML\") linearHypothesis(financial\\_model, c(\"crisis + Beta = 0\")) Does the Code above test a linear relationship between the independent variable \"X1.Month.Equity.Premium\" and the two dependent variables or just between crisis and Beta? Thank you for your help.",
        "created_utc": 1674507040,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help",
        "author": "cristinnam",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jlgiq/help/",
        "text": "Heey, is here someone who is willing to help me with statistics this trimester. I need to pass. Not for free. Thank u a lot.",
        "created_utc": 1674503048,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can anyone help me determine what the standard error says about these sampling means?",
        "author": "GspotGing",
        "url": "https://i.redd.it/vyw95gzmzvda1.jpg",
        "text": "",
        "created_utc": 1674502497,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistical procedures for determining cutoffs for variables with four levels",
        "author": "Aud_my_Percept",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jkjvz/statistical_procedures_for_determining_cutoffs/",
        "text": "I have an audio dataset of normal and disordered voices that has been evaluated in terms of the voices' normality as they relate to different qualities (e.g., the amount of roughness in the voice). The raters used two different scales. One is an Equal Appearing Interval Scale of 0-3 that is often interpreted as normal (0), mild (1), moderate (2), and severe (3). The other scale is a visual analog scale (0-100). I would like to take an individual voice quality (e.g., breathiness) and determine the range and cut-off for what would be considered normal, mild, moderate, and severe. I have looked at the descriptives and (for most of the qualities of interest), there is very little overlap when looking at the mean and standard deviations. I am not sure what a better way to determine ranges/cutoffs for each category would be. I looked into ROC but I've only seen discussions that use a binary variable (yes/no). I need something that would work with 4 levels of the categorical variable. Any ideas?",
        "created_utc": 1674500829,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical test for multivariate outlier detection is this?",
        "author": "Quiet-Station-3950",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jh98b/which_statistical_test_for_multivariate_outlier/",
        "text": "    [SAS code]\n    mahalanobis= uss(of prin1--prin15);\n       sign_chi= 1-probchi(mahalanobis,15);\n    run;\n    \n    data table_02;\n       set table_01;\n       val_weight= (sign_chi&gt; 0.0000000001);",
        "created_utc": 1674493049,
        "upvote_ratio": 1.0
    },
    {
        "title": "More information about a recursive estimation of the covariance matrix",
        "author": "MasonBo_90",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jh495/more_information_about_a_recursive_estimation_of/",
        "text": "In the paper \"[From Probabilistic Forecasts to Statistical Scenarios of Short-term Wind Power Production](http://pierrepinson.com/docs/pinsonetal_wpfscenarios_fin.pdf)\", the authors offer a recursive estimation for the covariance matrix.\n\nThey note that $$x$$",
        "created_utc": 1674492712,
        "upvote_ratio": 1.0
    },
    {
        "title": "Chi2 sample size, depending on DoF",
        "author": "Daily_Jesus",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jg624/chi2_sample_size_depending_on_dof/",
        "text": "We created a questionnaire trying to understand motivations for an action.\n\nWe want to understand if there is a difference between the observed frequency of nominal variables in our questionnaire and the frequency according to a theory.\n\nI'm really unsure about what the Degree of freedom would be to calculate the sample size, or if this is the wrong approach and we would have to work with the variance (which the theory does not give and we do not know for the to be asked population).\n\nWould be great if someone can clear up where we are going wrong or how to calculate a sample size in such a case.\n\n\nMore detail on the questionnaire:\nWe ask them to choose the top 3 factors of 8 for a cause.\nThe factors are categorized in 3 type A factors, 1 type B factor, 4 type C factors.\n\n We want to test if they are significantly different than a theory (e.g. randomly picked from the list of 10), so e.g. type A and B factors are significantly more important.\n\nThanks again",
        "created_utc": 1674490338,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with Statistics on spss for psychology project",
        "author": "Vegetable-Bug-7518",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jd07k/help_with_statistics_on_spss_for_psychology/",
        "text": "Hi, I'm doing a research project on the uncanny valley. My research involves 6 different cgi characters varying in realism, that have been presented to participants (nominal data). I then have 1 set of 3 likert scales that assess the perceived realism of the character and 1 set of 4 likert scales that assess the warmth felt towards the character.\n\nI want to compare the relationship between the perceived realism of the character presented to the participants, and the warmth felt towards the characters. I'm just wondering how I would input and analyse the data for this in spss?\n\nI then also have a single likert scale to assess the perceived attractiveness of the character, a likert scale to assess the perceived threat of the character, and a likert scale to assess the disgust felt towards the character. I'm wondering how I would compare the relationship between the warmth felt towards the characters and the disgust, attractiveness, and threat of the characters, using spss?\n\nI know this is a lot. I've not used spss in over 3 years, and would really really appreciate some guidance. Thank you!",
        "created_utc": 1674482052,
        "upvote_ratio": 1.0
    },
    {
        "title": "How would a smart statistician go about asking this question?",
        "author": "Odd_Look_360",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jcl8q/how_would_a_smart_statistician_go_about_asking/",
        "text": "Hi all, long time lurker (head-scratcher, more like), first time poster. My question is one of methodology; \n\nI have a sample of 200 patients with a certain genetic marker present in the blood which (according to guidelines) should prompt referral for a painful procedure to rule out malignancy. If my 200 patients all have negative results (i.e. no malignancy), how would I argue that the procedure is not necessary? \n\nWould it be sufficient to just state \"100% of patients investigated were malignancy-free\" or would I need some sort of control? If so, should I a)subject healthy controls to a painful procedure (our REB will be delighted, I'm sure) or b) compare them to patients with the sort of malignancy I'm looking for?\n\nI hope this question makes more sense on paper than it does in my head... Thanks in advance, Mark",
        "created_utc": 1674480791,
        "upvote_ratio": 1.0
    },
    {
        "title": "Non-Parametric Test help needed",
        "author": "Anonym1111273",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jchcp/nonparametric_test_help_needed/",
        "text": "I want to test whether \"takeover premiums\" are differently influenced by the variable \"Beta\" during economic crisis vs normal periods. To test that, I used a Generalised Least Squares Regression (GLS) because the takeover premiums are not normally distributed. However, I understand that one underlying assumption is a linear relationship between the dependent variables and the takeover premiums and this is not the case. **Can I use a different test to test my hypothesis?**\n\nFurthermore, I'm not sure whether I used the right code in R:\n\nfinancial\\_model &lt;- gls(\\`X1.Month.Equity.Premium\\` \\~ crisis + Beta, data = financial\\_subset, method = \"ML\")\n\nlinearHypothesis(financial\\_model, c(\"crisis + Beta = 0\"))\n\nDoes the Code above test a linear relationship between the independent variable \"X1.Month.Equity.Premium\" and the two dependent variables or just between crisis and Beta?\n\nThank you for your help.",
        "created_utc": 1674480471,
        "upvote_ratio": 1.0
    },
    {
        "title": "If bivariate analysis results are significant, but in the opposite direction than hypothesized, can a logistic regression be used to control for possible confounding baseline characteristics? (retrospective, pre/post study)",
        "author": "jackruby83",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jcc4y/if_bivariate_analysis_results_are_significant_but/",
        "text": "Doing a retrospective study to see if a treatment reduced the rate of a condition... The sample size was appropriate to detect a clinically significant difference... There isn't a compelling reason for the treatment to *increase* risk, but in bivariate analysis, the result was significant in the opposite direction than hypothesized - \"treatment increased risk\". The groups are pre-intervention and post-intervention over several years, so baseline risk factors and practice changes should be considered, and in fact several risk factors and important demographic differences are significant in bivariate analysis of baseline characteristics - you might expect the post-group (ie, treatment) would be at higher risk of the condition. Is a logistic regression, including the well established risk factors and other significant baseline characteristics, appropriate in this case? We performed one, and the treatment was no longer associated with the endpoint (which is better than harm), and a couple of the well established risk factors were, as we would expect.",
        "created_utc": 1674480037,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to accurately find Median? (Grade 11) Because I keep getting different Medians depending on the median class. So I wonder if there is a way to find the 'most accurate ' median.",
        "author": "thebluemoongirl",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jc0xc/how_to_accurately_find_median_grade_11_because_i/",
        "text": "",
        "created_utc": 1674479099,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multilevel regression analysis with bounded dependent variable",
        "author": "Executer13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jav1z/multilevel_regression_analysis_with_bounded/",
        "text": "Hey all,\n\nI have a dataset with patient data; these patients filled in the EQ-5D questionnaire, some of which filled it in more than once over months to years (longitudinally). The EQ-5D questionnaire is basically a score that is frequently used to assess patient's health related quality of life and the one that I am using consists of five questions where each has 5 possible answers (so there are 5^5=3125 possible health states). Based on the unique combinations of responses to the 5 questions, it is possible to compute a utility score for each patient, which has the following properties:\n\n* It is a continuous variable;\n* Its maximum is 1.000, which represents a state of perfect health; its minimum is country specific but is always &lt; 0 (for instance, it's −0.148 for Canada). This is due to the fact that EQ-5D requires anchoring at 0.000 = dead, but some people may consider some health states worse than death, so a negative utility score is possible.\n\nAdditionally, there often is a ceiling present (as most people are in a perfect health state, i.e. 1.000) and the data distribution might be skewed.\n\nI want to assess how patient's characteristics (e.g., sex, age, symptoms, medications, etc.) affect the utility score. Due to the nature of my data, I want to run a multilevel mixed-effects regression clustered by patient and hospital.\n\nHowever, I am unsure what to do. I have seen Tobit being used in cases like this one; however, I am unsure how to implement a multilevel Tobit regression in R (I am aware of the censReg package, but it does not seem to work for this purpose?).\n\nCould someone please lead me in the right direction? I would appreciate any reading materials and as I would like to perform the analysis in R, suggestions of R packages are equally appreciated.\n\nThank you and please let me know if something is not clear.",
        "created_utc": 1674475336,
        "upvote_ratio": 1.0
    },
    {
        "title": "Conditional Probability Question",
        "author": "WIILLLZ",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10jadtc/conditional_probability_question/",
        "text": "The total workforce is comprised of 2500 workers. 20% are female.\n\nThe probability of any given female worker being rostered to work on any given day is P(F) = 0.70\n\n4% of the female workforce are on a higher wage. Let’s donate these females has F2’s.\n\nHow would I calculate the probability, that say 5,  F2’s are rostered to work on the same day? How would I extend this to find the probability that 5 were rostered on the same day at least once in 2 weeks?\n\nHow would I also calculate the new probability or likelihood as the females as a percentage of the overall workforce increases? F2’s will also increase as they will stay 4% of this higher total.\n\nThank you for any help 🙏🏼",
        "created_utc": 1674473626,
        "upvote_ratio": 1.0
    },
    {
        "title": "Controlling for unobserved confounders using time dummies",
        "author": "Expensive_Charity293",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10j8q1p/controlling_for_unobserved_confounders_using_time/",
        "text": "Hey everyone,\n\nit is my understanding that in fixed effect models you can control for unobserved confounders that are constant across entities but vary over time by including time dummies. I was wondering:\n\n\n1. Why this works.\n\n2. Whether this also works when using splines of time instead of dummies.",
        "created_utc": 1674467008,
        "upvote_ratio": 1.0
    },
    {
        "title": "Violation of normality assumption in moderated regression",
        "author": "Rosie-daisy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10j7hi6/violation_of_normality_assumption_in_moderated/",
        "text": "I performed a moderated regression analysis in SPSS, with mean-centered predictors and an interaction term. I noticed that my residuals are not normally distributed (negatively skewed). In a normal multiple regression, i would try to transform (log, sqrt or so) my predictors. But I am not sure how this works for moderated regression.\n\nAny suggestions on how to do this? For example, do you transform the mean centered variables or the original ones? And what about the interaction term?",
        "created_utc": 1674461804,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are there jobs that combine web development with statistical programming?",
        "author": "Accomplished_Pipe563",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10iyh1c/are_there_jobs_that_combine_web_development_with/",
        "text": "",
        "created_utc": 1674432267,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can a histogram have individual numbers instead of ranges in the x-axes",
        "author": "Throwaway123652",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10iy0ri/can_a_histogram_have_individual_numbers_instead/",
        "text": "Or would that require a bar chart and a number is considered a ‘category’ as well? Or would either of the two work?",
        "created_utc": 1674431070,
        "upvote_ratio": 1.0
    },
    {
        "title": "Trying to draw causal inference from connection between meteorological data and health outcomes: which approach / test to use?",
        "author": "BiscuityOk3118",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10iwwhp/trying_to_draw_causal_inference_from_connection/",
        "text": "",
        "created_utc": 1674428181,
        "upvote_ratio": 1.0
    },
    {
        "title": "Minitab DOE",
        "author": "Epan320",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10iwhw6/minitab_doe/",
        "text": " \n\nI have a series of data and try to use Minitab DOE to get factorial analysis.\n\nThe problem is I only have 3 factors, but each level is different(20 groups). Is that possible that I can get the factorial analysis there? I tried it, but the software removed two factors due to \"cannot be estimated\". And no graph is showing.",
        "created_utc": 1674427160,
        "upvote_ratio": 1.0
    },
    {
        "title": "Lesser Known Summary Statistics",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10itqj8/lesser_known_summary_statistics/",
        "text": "What are some lesser known summary statistics for a set of about 7 - 15 numbers. Besides the well known mean, range, standard deviation, mode, etc etc is there any other way to usefully summarize or describe this data?",
        "created_utc": 1674420336,
        "upvote_ratio": 1.0
    },
    {
        "title": "Linear regression with multiple-response variable",
        "author": "Rosie-daisy",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10is7ws/linear_regression_with_multipleresponse_variable/",
        "text": "I need to do a multiple linear regression analysis with 3 continuous predictors and 1 categorical multiple-response predictor. I would normally dummy code the categorical variable in k-1 variables, so that I have one reference category. However, now I have a multiple response variable for which participants selected multiple answers.\n\nSo, now I have separate variables for each category (1 = present, 0 = not present). But I am not sure how to handle them in my regression. Do I enter them all, or choose a reference category, or …?",
        "created_utc": 1674416569,
        "upvote_ratio": 1.0
    },
    {
        "title": "Variance question",
        "author": "Traditional_Soil5753",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10is3q8/variance_question/",
        "text": "Is there a term for the variance of the sample variance?",
        "created_utc": 1674416284,
        "upvote_ratio": 1.0
    },
    {
        "title": "Probing Categorical x Binary Interaction?",
        "author": "dankyverno",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10irovg/probing_categorical_x_binary_interaction/",
        "text": "(Realized I tried to post this in the wrong sub originally).\n\nI'm running a GEE in SPSS using a continuous predictor, a categorical predictor, and a binary predictor with a count outcome. Negative binomial distribution with log link function, exchangeable WC matrix. Initial results gave me a significant categorical x binary interaction, but the parameter estimates table is giving me the \"this parameter is set to zero because it is redundant\" spiel for all but 2 of the parameters related to the interaction. I figured my next step is to run contrasts, but I'm not sure if I should use LSD, Bonferroni, etc. adjustments or even if doing pairwise comparisons is the right move. Any help would be appreciated! Thanks in advance.",
        "created_utc": 1674415227,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpretation of coefficients in Multinomial Logistic Regression (ISLR2)",
        "author": "ZestyBoots",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10il7zq/interpretation_of_coefficients_in_multinomial/",
        "text": "Hi all, I am reading the [An Introduction to Statistical Learning](https://www.statlearning.com) and I am wondering about the interpretation of the coefficients in Multinomial Logistic Regression. On page 140 of the textbook, it says this \"a one-unit increase in X\\_j is associated with a β\\_stroke\\_j increase in the log odds of stroke over epileptic seizure. Stated another way, if X\\_j increases by one unit, then Pr(Y = stroke|X = x) / Pr(Y = epileptic seizure|X = x) increases by e\\^β\\_stroke\\_j\".\n\nSee the image at [https://imgur.com/nBhbI0d](https://imgur.com/nBhbI0d).\n\n&amp;#x200B;\n\nShouldn't the Pr(Y = stroke|X = x) / Pr(Y = epileptic seizure|X = x) be **multiplied** by e\\^β\\_stroke\\_j instead of **increased**? [Working](https://imgur.com/uE6V1r0)",
        "created_utc": 1674398700,
        "upvote_ratio": 1.0
    },
    {
        "title": "Proper use of cross correlation? Is there a better way?",
        "author": "GretschElectromatic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10il50i/proper_use_of_cross_correlation_is_there_a_better/",
        "text": "I have a spreadsheet of votes cast.  The columns are the candidates (0 if not voted for, 1 if voted for).  The rows are individual ballots, one row for each ballot cast.  The ballots are grouped in batches of 50 because 50 is how many ballot the optical ballot scanner can handle.  There are 250 batches of 50 ballots (12,500 ballots total).\n\nMy question is if one, or more of the 250 batches of 50 ballots were voted exactly the same?  I don't want to look through all 12,500 ballots.  Would a correlation calculation tell me if the ballots in a batch were all the same?\n\nI don't care if ballot #10 in a batch is correlated to ballot # 31.  I'd like to know how close all 50 ballots are to being the same.",
        "created_utc": 1674398456,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to create a new variable from residual (e) of regression.",
        "author": "ludwigjet",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10iir20/how_to_create_a_new_variable_from_residual_e_of/",
        "text": "One of the variable of my study is the residual of the regression model, and I have all the required data (variables) for the regression model. But I don’t know how to calcule the residual of each sample. I know there’s a Excel Add-In for regression,  but I believe it’s for a bulk data (population), not for sample function. Is there any way to calculate it, preferably in Excel or Stata?  \n\nThanks in advance",
        "created_utc": 1674391052,
        "upvote_ratio": 1.0
    },
    {
        "title": "Class mark has a fatal flaw",
        "author": "_Devilogy_",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ii05s/class_mark_has_a_fatal_flaw/",
        "text": "As we know that while calculating mean of a grouped frequency distribution we assume that the frequency is centered around the class mark(25-30 has class mark as 27.5). Hence we multiply the cm with freq and add them...By doing this we try to get the best possible probabilistic or nearest estimate of the actual mean of the data.\n\n&amp;#x200B;\n\nBut but but...\n\nThe class interval cant have the value 30 as it lies in the succeeding class...\n\nHence the 'true' class mark shall be given as (upper - 1 + lower) / 2 (25-30 should have it as 27)\n\nIt also makes sense as follows...\n\nthe values that the distribution can hold are 25,26,27,28,29. Hence the middle most value is 27 and its also the best value if we look at a probabilistic way to get the closest value to the actual mean.\n\n&amp;#x200B;\n\nI would be glad if someone can help me out with this...",
        "created_utc": 1674388388,
        "upvote_ratio": 1.0
    },
    {
        "title": "Determining Maximum Potential Correlation between Ordinal Variables Restricted by a Categorical Variable",
        "author": "GuybrushManwood",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ihyj2/determining_maximum_potential_correlation_between/",
        "text": "Consider two variables, `y` and `x1`. `y` is measured on an ordinal level (on a 5-point likert-scale), and `x1` is a categorical variable (dichotomously score to 0 or 1). The point-biserial correlation between these two variables is low and amounts to `.26`. Assume that `x1` has a negligible measurement error. Now, the true nature of the phenomenon behind `x1` is not categorical, but also ordinal. I want to find out if it is \"worth\" measuring the phenomenon behind `x1` with an ordinal scale and hence wonder, what the maximum potential correlation between `y` and `x2` could be, if `x2` is ordinal, but restricted by `x1`. Is this possible?\n\n**tl;dr**: What is the highest potential correlation that could exist between the ordinal variable `y` and a new ordinal variable `x2`, if `x2` is restricted by the categorical variable `x1` which has a low point-biserial correlation of`.26` with y and is assumed to have negligible measurement error? Is this possible to determine?\n\nBonus: I'd prefer calculating this instead of doing simulations.",
        "created_utc": 1674388223,
        "upvote_ratio": 1.0
    },
    {
        "title": "Quartile formula",
        "author": "Novel_Swimming_125",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ig00o/quartile_formula/",
        "text": "&amp;#x200B;\n\n[Here I can see that the L is what ever comes before the quartile class and it gets added to the mess next. The i\\/f part gives the \\\\\"density\\\\\" of \\\\\"stuff\\\\\" in the quartile class. But what is the next part and why is it getting multiplied.](https://preview.redd.it/ku9jphyjfkda1.png?width=360&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a0e66ff45a2ba1ca3346eafed807e66cd5e277bf)",
        "created_utc": 1674380751,
        "upvote_ratio": 1.0
    },
    {
        "title": "Can you forego random sampling to generalise your findings to the population if you use an exact permutation test over a t-test?",
        "author": "AstralWolfer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10i7306/can_you_forego_random_sampling_to_generalise_your/",
        "text": "Consider that I have 10 people whom I randomly assigned into 2 groups: treatment and control, and days to recover as my dependent variable. I sampled the 10 people using convenience sampling. I am looking whether there is a significant difference between the 2 group means. \n\nAm I  able to Infer my results to the general population by using an exact permutation test? Because the permutation test only requires random assignment, not random sampling as a prerequisite. Or is my logic totally off here?",
        "created_utc": 1674349734,
        "upvote_ratio": 1.0
    },
    {
        "title": "Multicollinearity",
        "author": "ilsapo",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10i6keg/multicollinearity/",
        "text": "Hi, we just learned about  multicollinearity,but we have skipped learning about VIF  \nwe learend the following sayings:  \nif we have model:  \nYi\\_hat = B\\_0har +B1\\_hat \\*X1\\_i + B2\\_hat\\*X2\\_i + ei  \n\n\nand lets say X1\\_i and X2\\_i have pearson coefficent of 0.99 (high multicollinearity)  \nthen the following might happen:\n\nA. R\\^2 wont change by much  \nB. the Statistical siganificance of the model will change  \nC. the mean/expected value of B1\\_hat wont change by much  \nD.the variance of the model will change  \n\n\nI understand  why R\\^2 wont change by much (since X1 and X2 are highly correlated they represent pretty much the same \"information\", the SST will remain the same, and the SSE wont change by much so R\\^2 will be the same)  \n\n\nB. the Statistical siganificance will change since  the degrees of freedom change  \n\n\nIm a bit confused about why the mean/expected value wont change much, and why the variance will  \n\n\nthank you all",
        "created_utc": 1674348294,
        "upvote_ratio": 1.0
    },
    {
        "title": "Several variables measure same thing. What next step?",
        "author": "ran88dom99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10i4pt0/several_variables_measure_same_thing_what_next/",
        "text": "I have several variables that measure approximately the same thing and I want to get that latent variable. Specifically, my enthusiasm for completing anki flashcard reviews. The variables include speed of completing each review, number of reviews in session, time from previous session etc.   I do not think pca is really a great option because I do not know how to weight the variables and there is not a lot of correlation between the variables. Is there an alternative to pca in such a case?",
        "created_utc": 1674343148,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] Comparing kappa coefficients across different strata",
        "author": "le_pyropygian",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10i2pk5/q_comparing_kappa_coefficients_across_different/",
        "text": " \n\nHi all,\n\nI think this is a fairly noob question, but my google search didn't turn up much. I posted this on r/statistics before i realized that this subreddit exists.\n\nIn my project, I am comparing outputs of a software vs the output of a survey of humans, to see if the software gives similar results as a human. The output is categorical, so I used cohen's kappa to do this. While subdividing the data in to 4 strata and doing separate kappa on each of the strata, one of these values was much lower than the other three. I am wondering if there is a way to demonstrated that it is significantly lower than the other three groups.\n\nI saw that there is a way to do this in SAS, but I am using R (Rstudio) and do not have access to SAS. Please let me know if you have a solution for this. Thank you",
        "created_utc": 1674337890,
        "upvote_ratio": 1.0
    },
    {
        "title": "Not homework, but for a Math IA: Investigation of the change in population",
        "author": "wayvthot",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10i2dzr/not_homework_but_for_a_math_ia_investigation_of/",
        "text": "My IA is based on investigating the change in the population of tigers in a certain country. I have found two different models. Fairly simple, however, it borders on too simple. The ideal way to jazz it up would be to evaluate the model/actual vs predicted population using some kind of statistical test - something along the lines of, but not limited to, Goodness of Fit or Chi-Squared test.\n\nWould really appreciate any guidance; frankly, I'm in a bit of a fix right now.\n\nThanks in advance.",
        "created_utc": 1674337055,
        "upvote_ratio": 1.0
    },
    {
        "title": "When do we start? An Introduction To Statistics Request.",
        "author": "SilencerSerhii",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10i2d3b/when_do_we_start_an_introduction_to_statistics/",
        "text": "Could you suggest some good books/materials on basics of statistics? Are there some great channels and books that cover most core principles for complete beginners?",
        "created_utc": 1674336986,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help Needed for SPSS (not homework?)",
        "author": "bittertrusts",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10hyurs/help_needed_for_spss_not_homework/",
        "text": "Hi, I'm doing a Clinical Psychology research project these days and though I'm still collecting data right now, I'm worried about my analysis. \n\nThe project I'm working on is a collaboration and I am collecting data from my country's population while this project has already been carried out in a couple of other countries. Since I wasn't involved in the planning of research, I'm still catching up with the intricacies of it. Additionally, it doesn't help that quantitative research isn't my strong suit. \n\nThe problem I'm having is with inputting randomized data into SPSS. The study includes (among other variables) a 3x3 design for 1 main variable. Each participant is shown 3 different scenarios (person options) and each scenario further has 3 different levels. To counter sequence effects, these are all presented randomly and that randomization is automated. \n\nThe study is being conducted on Qualitrics however, I need to keep a separate Data record for myself (meaning manual input). While I have coded the rest of the data, I'm struggling with the randomization of this particular variable. I was told that I need to note down each different order, code it in values and run that separate variable as continuous. The automated table generated by qualitrics seems too complicated for me to understand and use for running any analyses.\n\nQuite honestly, I'm lost. Any input would be appreciated!",
        "created_utc": 1674328068,
        "upvote_ratio": 1.0
    },
    {
        "title": "Statistics Course",
        "author": "Bat_Academic",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10hxp0i/statistics_course/",
        "text": " Hi guys,I need help finding a good statistics courses I know a little bit about statistics I understand Multiple Linear Reggresion and DOE and I'm Intrested in statistics,I took Applied statistics Engineering course, btw I am studying Industrial Engineering,and thank you.",
        "created_utc": 1674325170,
        "upvote_ratio": 1.0
    },
    {
        "title": "Carrying Standard Errors through an equation",
        "author": "TimelyTill7611",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10htrfm/carrying_standard_errors_through_an_equation/",
        "text": "Hi,\n\nIf you have a set of values (x₁,x₂,... and y₁,y₂,...) that all have their own individual SEs, but then you are trying to calculate a further value from this set of values using an equation (such as the one below), how would you then calculate the error for the resulting answer (z)?\n\nz = (x/(1-x))/(y/(1-y))",
        "created_utc": 1674315032,
        "upvote_ratio": 1.0
    },
    {
        "title": "Coding a categorical variable",
        "author": "frauensauna",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ht3j5/coding_a_categorical_variable/",
        "text": "Hi all. I want to fit a logistic mixed-effects model using one categorical predictor. I am trying to figure out what the best contrast coding would be for this variable. I am not really interested in comparing the different levels to each other separately, but I am interested in knowing how the levels differ significantly from the overall mean. I tried sum coding, but I am confused why the last level is never compared. I want to know how all levels are doing compared to the mean. Or should you make the \"least interesting\" level the last one?\n\nTo very shortly explain my data, the categorical predictor is 7 different types of infant gestures (e.g., index-finger pointing, giving, showing). The binary outcome indicates whether their caregiver responded to the gesture or not which I annotated during caregiver-infant interactions. The question I want to answer is whether specific infant gestures are more likely to elicit a caregiver response compared to others.",
        "created_utc": 1674313196,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I compute for a sample size from a population of 227?",
        "author": "martianweb",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10hs0c1/how_do_i_compute_for_a_sample_size_from_a/",
        "text": " \n\nI'm thinking of using Slovin's formula which requires I know the value of my margin of error but to solve for margin of error, you'd also have to know the sample size. My study is \"The Relationship between Sleep Quality and Sustained Attention of 4th Year Students\".\n\n1. I do not know the proportion of the population that has a certain characteristic of interest\n2. I do not know what confidence interval is appropriate for my study\n3. I have 3 parameters (reaction time, frequency of lapses, number of false starts)",
        "created_utc": 1674309991,
        "upvote_ratio": 1.0
    },
    {
        "title": "How do I work with Bootstrap weights in Survey data for statistical analysis?",
        "author": "halfchewedgum99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10hrc9x/how_do_i_work_with_bootstrap_weights_in_survey/",
        "text": "I asked this on Stack Data Science, and will probably ask on a data science subreddit as well. I'm stuck and confused, so any help even if just pointing me to the right direction would be greatly appreciated. I'll just copy and paste my question here, but if you need a link to answer on the DS stack site (for points or something) I will provide that too.\n\nI'm working with a survey dataset for some statistical analysis. My issue is that there are weight columns, but I have no idea how they did the weighting or what the weights are for. I'm not familiar with Sampling theory, and don't have the time to study it in depth which is why I'm asking this question. This is NOT homework, btw. The dataset is quite large, so I'll give as much information here as possible:\n\n\\- I have about 130k rows (User ID's)\n\n\\- I have about 1400 columns (1000 are weights, the rest are questions of the survey)\n\n\\- The weights are introduced as \"person weights\" (not weight as in mass in human body)\n\n&amp;#x200B;\n\nThe issue:\n\n\\- If there are 1000 weights, does that mean those are for the 130k users? Then why are they in columns?\n\n\\- Some of the weight cells are 0. I understand that a weight of 0 means that case won't be entering the analysis. Is that right?\n\n&amp;#x200B;\n\nFor example, the dataset looks like this:\n\n&amp;#x200B;\n\n|USERID|STATE|HOURS ON IG|WEIGHT1|WEIGHT2|WEIGHT3|WEIGHT4|\n|:-|:-|:-|:-|:-|:-|:-|\n|10005A|CA|7|0|648293|8194|881328.49|\n|10006B|WY|12|459939.1|0|264726|0|\n\n&amp;#x200B;\n\nHow do I interpret this and apply it into my analysis? Let's say I want to see if there is a relationship between state and hours on instagram. How do I include these weights in my study? Do these weights related to each user or each feature (state, hours on ig)? The numbers don't match either way! But let's say it is about each case, so should I interpret the weights as:\n\n&amp;#x200B;\n\n\\- for someone in CA that uses Instagram for 7 hours, we have 0 people in w1, 648293 people in w2, 8194 people in w3, and so on?\n\n&amp;#x200B;\n\nI'm eternally confused as to what to do with these weights, so any help would be much appreciated.",
        "created_utc": 1674307983,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confidence intervals question",
        "author": "DrSteve34",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10hr8kx/confidence_intervals_question/",
        "text": "*May be a still question, apologies in advance if it is...*\n\nSay I have (mean) 500 bacteria in 1 mL of fluid, with a 95% confidence interval of 450 - 550 bacteria in that 1 mL of fluid. \n\nIf I want to harvest 50 bacteria from my fluid, I need to draw up 0.1 mL. Does this mean that the 95% confidence interval, of the bacteria in the 0.1 mL volume, is 45 - 55?\n\nMany thanks for any advice! It seems reasonable to me, but there may be some statistical reason this reasoning wrong.",
        "created_utc": 1674307681,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this solution correct? the S.D = 2 (not 72)",
        "author": "GogaReborn",
        "url": "https://i.redd.it/h2x4rjz5tdda1.jpg",
        "text": "",
        "created_utc": 1674300413,
        "upvote_ratio": 1.0
    },
    {
        "title": "Interpreting output of logistic regression",
        "author": "nicknacknick99",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10hp3u2/interpreting_output_of_logistic_regression/",
        "text": "i did a logistic regression with r studio.\n\ni'm just not sure how to interpret my output because i'm a bit confused with logodds, odds and probability. \n\nI have for example: \n\nintercept: -0.76\n\ncoefficients: minutes=0.0004552 and penalty rate= 0.93.\n\nAnd my dependent variable is whether he hits the penalty or not (0/1).\n\nThat would be my logodds. \n\nI know how to calculate the odds and the probability. but what would be the best way to interpret the result in this case?\n\n&amp;#x200B;\n\nFor example, if I want to say that the probability of hitting the penalty is ... if the minutes increase by one unit?",
        "created_utc": 1674300141,
        "upvote_ratio": 1.0
    },
    {
        "title": "statistical significance on ad click through rate",
        "author": "doomhunter13",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ha2le/statistical_significance_on_ad_click_through_rate/",
        "text": "I have data that takes the form:\n\nAd Name | Number of impressions | Number of clicks\n___________________________________________\nad 1.     |.   10000            |     4\n\nad 2     | 15000 |          3\n....\n\nWhats the best way to tell if the differences in click through rates has statistical significance? I tried using the side bar tool, but i'm a little lost.",
        "created_utc": 1674252564,
        "upvote_ratio": 1.0
    },
    {
        "title": "Exponentially Moving Average Volatility in Excel",
        "author": "Educational-Mango-88",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ha0u8/exponentially_moving_average_volatility_in_excel/",
        "text": " \n\nHey guys,\n\nI am writing my Master Thesis about Volatility models. I am trying to estimate 1 day ahead vola and am using Excel for it.\n\nNow my problem: I can't seem to be able to calculate it right, because I can't come up with a formula in Excel that works.\n\nWhat I've done so far:\n\n1. I calculated the log returns for the index I am looking at\n2. I squared the returns\n3. I established a weighting column (my formula is (1-lambda)\\*lambda\\^corresponding day)\n4. Then I use SUMPRODUCT with the squared returns and the weighting column and square root the results\n\nMy questions are:\n\n1. Is the weighting calculated right, since if i apply the weightings for 50 days total my total weights add up to 95,47% not 100%\n2. Secondly is my way of doing it even the right approach and if not what would be the right approach in Excel?\n\nI would be very glad if people could help me out with this problem (I know it's probably hilariously simple for people in this sub, but I am struggling :D)\n\nThanks guys",
        "created_utc": 1674252477,
        "upvote_ratio": 1.0
    },
    {
        "title": "Time series analysis: How does one calculate the contribution of a subset of observations to the overall change of an indicator over time?",
        "author": "PublicDataAnalyst",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10h8ezr/time_series_analysis_how_does_one_calculate_the/",
        "text": "I would like to know how to calculate the contribution of a subset of observations to the overall change of an indicator.\n\nAs an example I will use two groups of test results (that are part of a larger population) and two years.\n\n**Year 1**\n\nGroup A in year 1 has 10 positive test results out of 100 total tests in that group\n\n= 10% positive rate \n\nGroup B in year 1 has 25 positive tests out of 250 total tests in that group\n\n= 12.5% positive rate\n\nIn year 1 the total positive tests in the population is 150 and total tests 1000 (there are more observations beyond group A and B)\n\n= 15% positive rate\n\n**Year 2**\n\nGroup A in year 2 has 12 positive test results out of 200 total tests in that group\n\n= 4.8% positive rate \n\nGroup B in year 2 has 10 positive tests out of 350 total tests in that group\n\n= 2.9% positive rate\n\nIn year 2 the total positive tests in the population is 100 and total tests 1600 \n\n= 6.25% positive rate\n\n&amp;#x200B;\n\n**Question**\n\nHow would I go about calculating how the changes in group A and B (to both the numerator and the denominator) effected the change in the overall rate?\n\nIts fairly straightforward to calculate the % change to the numerator and the denominator when they are examined separately. However, I would like to rank how changes in different groups in the population effected the change in the overall rate year-over-year.\n\n&amp;#x200B;\n\nThank you!",
        "created_utc": 1674249419,
        "upvote_ratio": 1.0
    },
    {
        "title": "GAMs for ecology",
        "author": "Comprehensive_Face68",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10h8a2n/gams_for_ecology/",
        "text": "&amp;#x200B;\n\nI’m hoping you can help me solve a question about General Additive models for ecology... I have sightings data with 1kmx1km grid cells overlayed on a map, survey effort calculated for each grid cell, sightings in each grid cell and environmental variables for each grid cell. Is using sightings/unit effort (SPUE) as the response variable with environmental data as the explanatory the same as using effort as an explanatory variable and count of sightings/grid as the response with GAMs. Since GAMs show partial effects of each variable, will I get the same results (ecologically) with both methods? Note: Effort has a linear positive relationship with sightings count.\n\n i.e. is: SPUE \\~ depth+SST the same as: sightings count \\~ effort+depth+SST \n\nI know this is really simple and straight forward but I’m struggling to communicate with someone so it’s best to speak to an expert!",
        "created_utc": 1674249092,
        "upvote_ratio": 1.0
    },
    {
        "title": "Causal Inference Question: Ignorability",
        "author": "repigyou",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10h5p9w/causal_inference_question_ignorability/",
        "text": "Hello, I am fairly new to the idea of causal inference and had a question about the ignorability assumption.\n\nThrough my sources, the ignorability assumptions states that once we condition on a set of variables, then treatment outcome and treatment assignment are independent of each other.\n\nAt first I get that within a certain subgroup, treatment assignment is \"random\"- but then I am stumped about how the treatment outcome and treatment assignment are independent? Shouldn't the outcome of the treatment be dependent on wether the person received the treatment or not? Thanks!",
        "created_utc": 1674242580,
        "upvote_ratio": 1.0
    },
    {
        "title": "Subtracting two Gaussian Distributions results in???",
        "author": "Financial-Back313",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10h2sud/subtracting_two_gaussian_distributions_results_in/",
        "text": " Subtracting two Gaussian Distributions results in ???\n\n A. Mean is subtracted and standard deviation is added\n\nB. Mean is subtracted and variance is added\n\nC. Mean is subtracted and standard deviation is subtracted\n\nD. Mean is subtracted and variance is subtracted\n\n&amp;#x200B;\n\nanyone knows correct answer???",
        "created_utc": 1674235750,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to measure reasons for enjoyment with different variables",
        "author": "justhereformythesis",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10h2d8o/how_to_measure_reasons_for_enjoyment_with/",
        "text": "Hi! I am working on the data analysis of a survey I conducted for my thesis.\n\nI want to accept or dismiss multiple hypotheses that are dependent on one condition: Enjoyment.\n\nThe goal is to prove/disprove (simply put):\n\n*People* ***enjoy*** ***(Fixed variable)*** *because they* ***feel/can/do XYZ****.*\n\nWhich kind of correlation analysis would be needed here?\n\nI am using SPSS as a tool. \n\nThank you a lot in advance!",
        "created_utc": 1674234740,
        "upvote_ratio": 1.0
    },
    {
        "title": "Logit vs probit regression",
        "author": "Kooky_Anteater6166",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10h0m1e/logit_vs_probit_regression/",
        "text": "How do I choose  between the two?",
        "created_utc": 1674230577,
        "upvote_ratio": 1.0
    },
    {
        "title": "Quasi Experiment in the Bible ?",
        "author": "Program_Constant",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10h0ly5/quasi_experiment_in_the_bible/",
        "text": "",
        "created_utc": 1674230571,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which test should I use?",
        "author": "lokulkakul",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gzi5f/which_test_should_i_use/",
        "text": "I'm doing a study on Magnesium levels in Type2 Diabetes Mellitus patients(HbA1C values)?",
        "created_utc": 1674227875,
        "upvote_ratio": 1.0
    },
    {
        "title": "G power sample size calculation 2x3 within groups ANOVA",
        "author": "Money_Lie8845",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gyu9p/g_power_sample_size_calculation_2x3_within_groups/",
        "text": "Hi there, I’m trying to do a g power calculation for sample size however I am unsure of the input. \n\nI’m running a psychology study and planning to do a 2x3 within groups ANOVA \n\nIf someone could provide some steps to follow that would be great 😊 \n\nI’m unsure with:t\n1: which ANOVA ‘statistical test’ to select \n2:  the number of groups &amp; number of measures to enter",
        "created_utc": 1674226230,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with probability for an experiment",
        "author": "Own-Comfortable4600",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gvlvg/help_with_probability_for_an_experiment/",
        "text": "Hi, I am a biologist/chemist with basic math knowledge and was hoping someone would help me with a question I need answers to for an experiment I'm planning. \n\nEssentially, I'm trying to synthesize molecules using a split and pool method of synthesis. these molecules will have 9 units each, and will be synthesized on beads. these 9 units are made up of 4 different building blocks. So, the total number of permutations will be 4 to the power 9 which gives me  262144  unique sequences. \n\nHowever, since the nature of the synthesis method is stochastic (I've attached an image from wikipedia to better understand it) --&gt; but essentially say I start out with 262144 beads (on which I want to synthesize my molecules) and divide them into 4 equal piles and attach one of the 4 different building blocks to each pile, and then pool the four piles together and then split them again and repeat the second step (and do this 9 times) I'll have molecules having 9 units each in the end. BUT, the problem is that there will be many duplicates if I use only 262144 because there are chances two identical beads follow the same path / get split into the same pile during the split and pool process. I don't want this, so my questions is ultimately,\n\nwith a 5% error rate or with 95% confidence, how many beads must I start out with to ensure I make all my 262144 unique sequences using the split-and-pool synthesis method?\n\nPlease help if you're a math whiz, my head hurts trying to figure this out, Thanks In Advance",
        "created_utc": 1674217325,
        "upvote_ratio": 1.0
    },
    {
        "title": "When using MLE, how do you turn a function f into L?",
        "author": "EmergentPhysics",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gqra3/when_using_mle_how_do_you_turn_a_function_f_into_l/",
        "text": "I am trying to do MLE, and I know the rough idea is to turn f into L, then use logarithm (usually), derive, set it to 0 and calculate it. The idea that I am using this method to find the extreme of the curve is not entirely lost on me, which is why you derive and set it to 0. I understand logarithm does not change the position of the extreme but makes calculation simpler, though. But I don't really understand much beyond that.\n\nI understand there is some sort of likelihood which describes how well the parameter 'fits' the function or data. The thing is, I have no clue how to actually turn the function into this probability function (I'm guessing some sort of pdf).\n\nWhat I really need are some really really simple examples that show the progression from the function to L. Something like f(x;parameter)=parameter\\*x+b, and how to convert that function to L(parameter).\n\nDo you know of any book or place online with plenty of elementary exercises. Or at least a couple of elementary before moving on to harder examples?\n\nOn a side note, why is setting derivative to 0 guaranteed to find the maximum? Couldn't it be a different extreme?",
        "created_utc": 1674200034,
        "upvote_ratio": 1.0
    },
    {
        "title": "Help with Statistical Significance Calculation",
        "author": "DrDaringPhD",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gnx08/help_with_statistical_significance_calculation/",
        "text": "I know basic math and a very elementary understanding of algebraic equations.\n\nI am hoping someone in this community can help with a Google Sheets calculation I would like to do please. I have a list of 1v1 matches and the score. I have a column that calculates if player A won or lost. I do everything based off of player A. Then I have a column for possession and a column to calculate if player A had the most possession during the match. And a column that compares if player A’s winning and possession are equal (column M). In other words, did he win AND have majority possession. All these columns are binary — 1 or 0. I'm trying to determine if having possession the majority of the time is as important as people seem to think it is.\n\nCurrently I have a very simplistic formula: =SUM(M:M)/COUNT(M:M)\n\nBut I would like to account for statistical relevance. I.e. *p*&lt;0.05. I am not sure how, and don’t understand it, to incorporate that into the formula. And should it be done in the =SUM(M:M)/COUNT(M:M) formula or before when determining if the win and possession were statistically relevant?\n\nAny help from you math geniuses would be appreciated. Thanks.",
        "created_utc": 1674190642,
        "upvote_ratio": 1.0
    },
    {
        "title": "Teaching Myself Bayesian Regression",
        "author": "engr4lyfe",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gkwrp/teaching_myself_bayesian_regression/",
        "text": "I’m trying to teach myself Bayesian Regression.  But, it has been ~15 years since I’ve taken a math or statistics class.\n\nI’ve looked up videos on YouTube and also Googled for some resources.  However, everything seems to be very math/notation heavy, and not focused well to beginners.\n\nDoes anyone have a suggestions for resources for beginners to learn Bayesian Regression?  Is there a “Bayesian Regression for Dummies”?\n\nAny help on a good place to start would be much appreciated.",
        "created_utc": 1674181894,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is it possible to use Deep Learning Imputation for a data set of 3 years?",
        "author": "Cath4_skool",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gkfb8/is_it_possible_to_use_deep_learning_imputation/",
        "text": "Hello to all amazing statisticians! I wanna start with a preface that I am a clueless Grade 12 student who only has the basic knowledge of Statistics (and I'm not even good enough at that). A college friend of ours convinced us to continue a research paper of theirs, insisting that it was easy as our final project in research. It was not 😭. Anyways, a long explanation ahead...\n\nSo we have this data for our region's air quality for 3 years. The air monitoring station was supposed to record every day. But because of the lack of proper maintenance on the stations and the pandemic making everything harder, the years (namely 2019, 2020, 2021), were full of missing data. We want to subject the data to ANOVA because we wanted to know if the pandemic had a significant effect on the air quality over the years in our locality (and it also was the method suggested by our research adviser). At first, we just cleansed off the dates with no data and we used IBM: SPSS to subject it to Oneway ANOVA and Kruskall-Wallis just to be sure. But when we presented the data to our school's statistician, he was not impressed, to say the least. He complained about how the data had unequal data and our method just being wrong. We brought up imputations to compensate for the missing data and he agreed. So we started to search for what imputations to use, and deep learning imputations caught my attention. Is it reliable? And is it okay for us to use it? Will we have to use imputations to fill all the daily data that was empty? Or do you know any better methodologies that might fit our research more?\n\nHere's the [data](https://docs.google.com/spreadsheets/d/1e8gWQudFpR1um94C0-5VkwJhSFaxFDxg/edit?usp=share_link&amp;ouid=105278812431067067681&amp;rtpof=true&amp;sd=true) for those who want to see it for yourselves.\n\nThank you so much for reading my word vomit and I hope you can spare some time to help us with our problem. Please help us, our high school graduation is hanging on this 😭.",
        "created_utc": 1674180584,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is this correct?",
        "author": "beardly1",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gkasu/is_this_correct/",
        "text": "Hi everyone, so basically I have two matched samples (before and after treatment). I want to observe whether the effect I found (difference in the mean) is statistically significant. My samples are not parametric and therefore I thought of using a Wilcoxon matched pair test to find whether the differences between the means of the samples are stat significant. But the problem is that the assumption of symmetry for Wilcoxon does not hold, as well as the assumption of independence of observations (in my second sample, the after treatment, individuals are exposed to others opinions before submitting a guess, thus observations are influenced by other observations). I have settled on performing a bootstrap test for establishing statistical significance for the difference between the two means of the matched samples. Is that correct or am I missing something?\n\nThanks to everyone in advance!",
        "created_utc": 1674180248,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which statistical test for pre/post results between groups?",
        "author": "sugarmansugarcubes",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gk1zk/which_statistical_test_for_prepost_results/",
        "text": "It’s been a long time since my job has required me to do anything other than report on descriptives, so I’m feeling super rusty.\n\nI’ve got pre- and post-pilot screening results for an intervention and control group. At first I was thinking, easy! Repeated measures ANOVA, with Time as my within-groups variable and Group as my between-subjects factor. But Mauchly’s test of sphericity is coming up blank (I’m assuming because time is only two levels?), so this doesn’t seem to be the right path.\n\nI’m running this all on SPSS, if that makes any difference. Any guidance is appreciated!",
        "created_utc": 1674179595,
        "upvote_ratio": 1.0
    },
    {
        "title": "Odds my roommate is a creep?",
        "author": "chrononaut-",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ggox9/odds_my_roommate_is_a_creep/",
        "text": "This is a real situation that just happened. The first girl to live here with us was just moved in and on her 3rd night here, my other roommate Doug opened her bedroom door while she was sleeping at 5am. She awoke and he said 'oops sorry' and left her room. He claims he was drunken sleepwalking. But Doug has a history of being a narcissistic asshole so we don't believe him. There's audio of his voice so we know it was him. \n\nWhat are the odds of his 'sleepwalking' mistake being a total coincidence given:\n\nI live in a 4 bedroom apartment with 4 occupants in each room. \n\nRoommate Doug has lived here for 1145 days.\n\nHe went into the girls room on the 3rd night she has been here. \n\nHe's never done this before. \n\nHe drinks every night but rarely sleepwalks. (I've never heard of or seen him sleepwalking)\n\nI have (1142/1145)^1145*(3/1145)/3= 1/23089\n\nDoes this check out?",
        "created_utc": 1674170829,
        "upvote_ratio": 1.0
    },
    {
        "title": "What is deviance?",
        "author": "vsr0",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gc195/what_is_deviance/",
        "text": "I am comparing two logistic regression models which have the same predictors with one having an additional predictor. For instance, mod0 = OUTCOME ~ GENDER and mod1 = OUTCOME ~ GENDER + RACE. In R, using\n\n    anova(mod0, mod1, test = \"LRT)    \n\nproduces a deviance and a p-value. I have some vague understanding that deviance tells me something about the fit of the models, but I don't quite get it.\n\nCan someone please help me understand what deviance is in this context? What would a deviance of 3 represent?",
        "created_utc": 1674159540,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are Samples of Convenience an appropriate method for presuming Amazon has a systematic ergonomic hazards rather than outliers among 1,300 facilities",
        "author": "Mizzou0579",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10gaocn/are_samples_of_convenience_an_appropriate_method/",
        "text": "MSDs (musculoskeletal disorder) is an injury often caused by repetitive motion and affects ¼ of the global population. How can OSHA charge Amazon with ergonomic safety hazards without performing statistical tests of significance rather than comparing counts to an industry numerical average (or median)?\n\n[Three Amazon Warehouses charged with Ergonomic Safety Hazard]https://www.dol.gov/newsroom/releases/osha/osha20230118#:~:text=WASHINGTON%20%E2%80%93%20The%20U.S.%20Department%20of,and%20New%20Windsor%2C%20New%20York%20%E2%80%93\n\n[Comparing injury rates](https://www.peoplesworld.org/article/data-show-amazon-workers-suffer-double-the-injuries-of-other-warehouse-workers/#:~:text=Frumin%20testified%20the%20overall%20injury,rate%20was%207.7%20per%20100%2C000.) \n\n[Rates](https://injuryfacts.nsc.org/work/industry-incidence-rates/work-related-incident-rate-trends/)",
        "created_utc": 1674156356,
        "upvote_ratio": 1.0
    },
    {
        "title": "Bootstrapping When You Have a Perfect Predictor",
        "author": "sadfasn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ga6ib/bootstrapping_when_you_have_a_perfect_predictor/",
        "text": "I have 4 probabilities: A, B, C, and D.\n\nI want to test whether the probability of A is greater than any of the other probabilities.\n\nI need to use a non-parametric approach since I have a pretty small sample size (like 40 observations altogether)\n\nMy thought is to use a non-parametric bootstrap to generate a test statistic. However, one of my categories (D) has a probability of 0 on my sample (therefore the bootstrapped values for D will have no variability).\n\nHow should one approach this problem?",
        "created_utc": 1674155175,
        "upvote_ratio": 1.0
    },
    {
        "title": "Which G*Power for two group crossover",
        "author": "AKACensored",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10g46nl/which_gpower_for_two_group_crossover/",
        "text": "Heyho,\n\nCould anyone tell me the best model to use for a calculation of needed sample size for a simple crossover trial in G\\*Power? (two groups, once crossed over, continous outcome)\n\nI'm sorry for this probably very easy question but I'm a bit overwhelmed by the names and shorthands of statistical models in G\\*Power.\n\nI don't think that you need this information, but I have the following variables from a trial study (one group, tested in both situations):  \n\\- mean and SD of situation one  \n\\- mean and SD of situation two  \n\\- wanted power  \n\\- wanted significance level  \n\n\nCheerio\\^\\^",
        "created_utc": 1674141309,
        "upvote_ratio": 1.0
    },
    {
        "title": "G power analysis for sample size 2x3 ANOVA (within groups)",
        "author": "Money_Lie8845",
        "url": "https://www.reddit.com/r/statistics/comments/10g3d0y/g_power_analysis_for_sample_size_2x3_anova_within/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf",
        "text": "",
        "created_utc": 1674139389,
        "upvote_ratio": 1.0
    },
    {
        "title": "Are unequal sample sizes better if you have a larger sample? (Comparative cross-sectional study)",
        "author": "AstralWolfer",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10g054u/are_unequal_sample_sizes_better_if_you_have_a/",
        "text": "I am conducting a study to see if vegetarians have better mental health measures than omnivores. The prevalence of vegetarians in my target population is 20% while meat eaters are 80%. Assume that given a certain power and alpha, my software recommends a minimum sample size of 30 for each group (Toy example).\n\nConsider the following distribution of participants:\n1) 30 vegetarians and 30 omnivores\n2) 30 vegetarians and 200 omnivores \n3) 200 vegetarians and 30 omnivores \n\nIs there a particular scenario which is better suited to my objectives ? What are the pros and cons of each?",
        "created_utc": 1674130227,
        "upvote_ratio": 1.0
    },
    {
        "title": "Excel statistics",
        "author": "FuturePlayerMod",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fzbtj/excel_statistics/",
        "text": "Anybody knows a good guide on how to create statistic graphics / data out of 2 or more data sets (e.g. Age and Product purchased) \nI know how to create a graphics for the age distribution and a data on what products were bought, but I struggle to combine: What people buy what products at what age. \nNot sure what the correct term is for my problem? Combining statistics of surveys? \nThanks",
        "created_utc": 1674127534,
        "upvote_ratio": 1.0
    },
    {
        "title": "[UK] What's the best way to access a variety of ONS data?",
        "author": "sodawaterlime",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fvvr5/uk_whats_the_best_way_to_access_a_variety_of_ons/",
        "text": "The office of national statistics (ONS) has a huge range of public data. I'm interested in using a range of these sources over time for analysis (personal interest, not commercial). \n\nIt currently looks like the only way to do this is to download csv files from the website and do my own wrangling to get joinable time series. Is anyone aware of an easier/more programmable way to do this (e.g a sql database, API)?",
        "created_utc": 1674114243,
        "upvote_ratio": 1.0
    },
    {
        "title": "How is minimizing expected value of weighted loss equivalent to minimizing expected loss over weighted distribution?",
        "author": "RecentUnicorn",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fulb2/how_is_minimizing_expected_value_of_weighted_loss/",
        "text": "[removed]",
        "created_utc": 1674109516,
        "upvote_ratio": 1.0
    },
    {
        "title": "[Q] z-score less than 1.96 (z = 1.10) is showing significance (p&lt;.001) in multilevel model",
        "author": "bennettsaucyman",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fscc3/q_zscore_less_than_196_z_110_is_showing/",
        "text": "I ran a multilevel model, and found that there was a difference between two observations in an interaction. But the z-score R popped out was 1.10, p&lt;.001. It was my understanding that a p-value of .05 always coincided with a z-score of +-1.96 in a normal distribution (which is what z-scores are from). \n\nIs my understanding of z-scores wrong, or did I do something wrong in my model?",
        "created_utc": 1674102312,
        "upvote_ratio": 1.0
    },
    {
        "title": "Would this sports statistics tweet be an example of cherry-picking? more context in comments",
        "author": "drmehmetoz",
        "url": "https://twitter.com/pff/status/1615861602296676354?s=46&amp;t=SVVA1vbt_Bg-00-kgygogQ",
        "text": "",
        "created_utc": 1674097316,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to test if the average difference between items prescribed in a 24-year period is significant?",
        "author": "UnusualEffort",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fp6ew/how_to_test_if_the_average_difference_between/",
        "text": "I have 24 years of annual items prescribed per year of a particular drug. I have worked out the average annual difference by calculating the difference between each number and the next number in the list and dividing by the number of differences to get a result of the average difference of 8.41. How would I go about determining if this is a statistically significant difference? I have attached an image of my annual items prescribed data if required.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/mlehpggpowca1.png?width=63&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d2ffd400726f02217010c6e2fe0990d62a6369bf\n\n||||\n|:-|:-|:-|\n||||",
        "created_utc": 1674093150,
        "upvote_ratio": 1.0
    },
    {
        "title": "Expected mean value from a Monte Carlo sample",
        "author": "PittSE17",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fn2qb/expected_mean_value_from_a_monte_carlo_sample/",
        "text": "I’m currently working with a fault tree model which assigns distributions to the events. When we quantify this model we use the mean of those distributions as the point estimate for each event. After we obtain a mean value for the model, we run a Monte Carlo analysis to determine the distribution parameters of the overall mode (i.e. the mean, 5th%, 95th%). It is recognized that if some of the events are correlated that the mean generated from the Monte Carlo sample could be larger than that predicted by simply using the point estimate quantification. My question here is whether or not it would ever be reasonable to see a mean that is less than the point estimate. Any thoughts or recommendations for text books on the impact of correlation n Monte Carlo sampling would be appreciated.",
        "created_utc": 1674087494,
        "upvote_ratio": 1.0
    },
    {
        "title": "Confidence Intervals",
        "author": "leglump",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fl8x0/confidence_intervals/",
        "text": " \n\nSo...i think i miss read googles Confidence Interval formula but I ended up with similar results, can anyone explain?\n\nSo the equation is:\n\nsample mean +/- (Confidence level) (sample standard deviations/number of observations)\n\nBeing rusty on stats, I put the Confidence level to .95 and got a results of 3.39 - 3.42. But after brushing up more, i realized that the .95% should actually be a critical value. Since my population standard deviation is unkown I am using the T critical value and not the Z. Using the T critical value I get bounds of 3.38 - 3.43.\n\nWhy are they similar, are you allowed to use the alpha significance level as the confidence level/critical value?",
        "created_utc": 1674082948,
        "upvote_ratio": 1.0
    },
    {
        "title": "Is linear regression an acceptable approach for a model with a continuous (not binary) outcome from 0 to 1?",
        "author": "math135135",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fkjs0/is_linear_regression_an_acceptable_approach_for_a/",
        "text": "Example: suppose I'm studying a group of subjects drinking water and want to model an outcome of what percent of water in a cup was drank after 10 minutes for each subject.  Explanatory variables would be something like thirst amount (thirsty vs not thirsty) and minutes since they last ate food.  \n\nObjective is to figure out the effect of thirst and time since food (meaning I want a parameter estimate for each variable that is easy to interpret; objective is not necessarily to predict future water drinking habits)\n\nIs it appropriate to do a linear model for % of water drank with two independent variables, even though the outcome technically should not be able to go outside of the 0-1 (or 0% to 100%) range?\n\n(technically in this example the outcome could be changed to \"amount of water drank in mL\", but I'm just using this as an example of a case where the outcome is a percent)\n\nthanks!",
        "created_utc": 1674081264,
        "upvote_ratio": 1.0
    },
    {
        "title": "How to find median and percentiles with 'weighted' scores?",
        "author": "Noedel",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fkg7y/how_to_find_median_and_percentiles_with_weighted/",
        "text": "I'm doing some GIS based population mapping. \n\n**What I have**\n\nA grid across the region. Each cell in the grid has the following values:\n\n- The Grid ID\n- The esimated resident population (ERP)\n- The distance from the middle of the grid cell to the nearest bus stop\n\n[Image](https://i.imgur.com/pEf1fAF.png)\n\nNow it's quite easy to calculate the percentile rank for the distance column - but the median distance then does not consider the fact that some cells (usually those in dense areas with better outcomes) have way more people in them than those with (usually) poor outcomes on the city fringe. \n\nHow do I calculate the median distance to a bus stop for the median resident? And how would I go about finding the deciles? Could use GIS/Python, but Excell is easier for me. \n\nAny help is greatly appreciated!",
        "created_utc": 1674081026,
        "upvote_ratio": 1.0
    },
    {
        "title": "GAM vs GAMLSS",
        "author": "ragold",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10ficrc/gam_vs_gamlss/",
        "text": "I'm trying to understand the difference between these two models. When would you use GAMLSS instead of GAM? What are the advantages of GAMLSS over GAM?",
        "created_utc": 1674075092,
        "upvote_ratio": 1.0
    },
    {
        "title": "Will a Chi square work when categories are very different in size?",
        "author": "Greedy-Resident3596",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fg93m/will_a_chi_square_work_when_categories_are_very/",
        "text": "I am comparing the responses of survey respondents who are over the age of 65 with the responses of those under the age of 65. However, I only had eight respond who are over 65 while I had 126 respondents who are younger than 65.   \n\n\n1. General stats question: Will the Chi square even work when categories are very different in size?  \n\n2. Do I just need to toss my data on this since I had so few respondents over 65? Or is there anything I can still do with this data? Any advice would be appreciated!",
        "created_utc": 1674070122,
        "upvote_ratio": 1.0
    },
    {
        "title": "Looking for the best way to group data into classes.",
        "author": "Ok-Plastic-2992",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fcku7/looking_for_the_best_way_to_group_data_into/",
        "text": "I'm trying to find the best way to group data into four classes (**Great, Good, Moderate, Poor**). The data I am looking at is percentage impervious surface. I have been using the median of the entire dataset as a middle breakpoint, then the median of the lower half as the first breakpoint and the median of the upper half of the third break point.\n\nThis gives me frequency distributions that are even throughout the classes, but I don't know that it is a good way to truly group, as I might have something like 0-24.45784% receive a score of poor, while 35.xxx% scores excellent.\n\nAny suggestions?",
        "created_utc": 1674061590,
        "upvote_ratio": 1.0
    },
    {
        "title": "Should this be considered a probability or a non-probability sample?",
        "author": "nwars",
        "url": "https://www.reddit.com/r/AskStatistics/comments/10fc8fg/should_this_be_considered_a_probability_or_a/",
        "text": "Hello, I have a doubt about determining the sample type in one scenario.\n\nCan I consider a group of existing elements to be the sample of a group of \"**will exist in the future**\" elements (forming the overall population)?\n\nFor example, can I consider all the children born one day in one hospital as the sample from a bigger population of children \"**to be born**\" in the same hospital during the next month? If so, should be this considered a probability sample or a non-probability sample?\n\nFinally, would the answer change if instead of humans we were talking about something where the variability doesn't change over time, like industrial elements that are basically generated from the same machine every time in the same way?\n\nSome material I can read to clear my mind about it?",
        "created_utc": 1674060805,
        "upvote_ratio": 1.0
    },
    {
        "title": "what does the sign &lt; denote? can someone explain the solution to this?",
        "author": "udkwho89",
        "url": "https://i.redd.it/293jcjvahvca1.jpg",
        "text": "",
        "created_utc": 1674060481,
        "upvote_ratio": 1.0
    }
]