{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade praw\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "We import PRAW and authenticate with the Reddit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "config = json.load(open(os.path.join(os.getcwd(), 'config.json')))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id= config['clientID'],\n",
    "    client_secret= config['secret'],\n",
    "    password= config['password'],\n",
    "    user_agent=\"CS242 scraper assignment at UC Riverside\",\n",
    "    username= config['username'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Top 100 Hot Posts of Several Subreddits\n",
    "\n",
    "We retrieve the top 1000 hot posts of several subreddits. We use the `subreddit` method to retrieve the top 1000 hot posts of a subreddit. We use the `hot` method to retrieve the top 1000 hot posts of the subreddit. We use the `limit` method to limit the number of posts retrieved to 1000. For each post, we retrieve the top 1000 maximum top-level comments. We set the `comment_sort` attribute to \"top\" to retrieve the top comments. We set the `comment_limit` attribute to 100 to limit the comments retrieved to a maximum of 1000. We use the `replace_more` function to remove instances of 'more comments' when iterating through the comments. Afterward, we write these posts into a JSON file in the `posts` directory until we reach 500 MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = config['subreddits']\n",
    "\n",
    "# used to continue from the last subreddit we were retrieving data from\n",
    "# so that we won't have to go back to the begining\n",
    "start_subreddit = 'lifeofnorman'\n",
    "\n",
    "# start the subreddits list with the one we want to start at\n",
    "subreddits = subreddits[subreddits.index(start_subreddit):]\n",
    "\n",
    "# prevent duplicate posts by creating a set that tracks the id of all posts\n",
    "existing_posts = set()\n",
    "\n",
    "# we only need 500 MB worth of data, so we'll stop once we reach it\n",
    "MAX_SIZE = 500 * 1000000\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    subreddit_posts = []\n",
    "    for post in reddit.subreddit(subreddit).hot(limit=1000):\n",
    "        try:\n",
    "            # prevent existing posts and NSFW posts from being added\n",
    "            if not post.over_18 and post.id not in existing_posts:\n",
    "                existing_posts.add(post.id)\n",
    "                \n",
    "                subreddit_post_comments = []\n",
    "                post.comment_sort = \"top\"\n",
    "                post.comment_limit = 1000\n",
    "                post.comments.replace_more(limit=0)\n",
    "                for comment in post.comments:\n",
    "                    subreddit_post_comments.append({\n",
    "                        \"author\": comment.author,\n",
    "                        \"text\": comment.body, \n",
    "                        \"upvotes\": comment.score,  \n",
    "                        \"created_utc\": comment.created_utc \n",
    "                    })\n",
    "                #end for\n",
    "                \n",
    "                subreddit_posts.append({\n",
    "                    \"title\": post.title,\n",
    "                    \"author\": post.author,\n",
    "                    \"url\":  post.url,\n",
    "                    \"text\": post.selftext,\n",
    "                    \"upvote_ratio\": post.upvote_ratio,\n",
    "                    \"created_utc\": post.created_utc,\n",
    "                    \"comments\": subreddit_post_comments\n",
    "                })\n",
    "            #end if \n",
    "        except Exception as err: \n",
    "            print(\"Something went wrong: \", err)\n",
    "    #end for\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(os.path.join(os.getcwd(), 'posts')):\n",
    "            raise OSError(\"'/posts' folder not found.\")\n",
    "        #end if\n",
    "        \n",
    "        subreddit_df = pd.DataFrame(subreddit_posts)\n",
    "        subreddit_df.to_json('posts/' + subreddit + '.json', default_handler=str, orient='records')\n",
    "        print(f'Successfully wrote r/{subreddit} posts to file.')\n",
    "    except Exception as err:\n",
    "        print('Something went wrong: ', err)\n",
    "\n",
    "    try:\n",
    "        cur_size = 0\n",
    "        for subreddit_json_file in os.scandir(os.path.join(os.getcwd(), 'posts')):\n",
    "            cur_size+=os.path.getsize(subreddit_json_file)\n",
    "        print('current size', cur_size)\n",
    "        if cur_size >= MAX_SIZE:\n",
    "            print('Reached 500 MB. Terminating subreddit data retrieval.')\n",
    "            break\n",
    "        #end if      \n",
    "    except Exception as err:\n",
    "        print('Something went wrong: ', err)\n",
    "#end for\n",
    "\n",
    "print('Successfully retrieved data for all subreddits.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
