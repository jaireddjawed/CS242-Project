{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade praw\n",
    "!pip install pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "We import PRAW and authenticate with the Reddit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "config = json.load(open(os.path.join(os.getcwd(), 'config.json')))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id= config['clientID'],\n",
    "    client_secret= config['secret'],\n",
    "    password= config['password'],\n",
    "    user_agent=\"CS242 scraper assignment at UC Riverside\",\n",
    "    username= config['username'],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Top 100 Hot Posts of Several Subreddits\n",
    "\n",
    "We retrieve the top 100 hot posts of several subreddits. We use the `subreddit` method to retrieve the top 100 hot posts of a subreddit. We use the `hot` method to retrieve the top 100 hot posts of the subreddit. We use the `limit` method to limit the number of posts retrieved to 100. Afterward, we write these posts into a JSON file in the `posts` directory until we reach 500 MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = config['subreddits']\n",
    "\n",
    "# used to continue from the last subreddit we were retrieving data from\n",
    "# so that we won't have to go back to the begining\n",
    "start_subreddit = 'ucr'\n",
    "\n",
    "# start the subreddits list with the one we want to start at\n",
    "subreddits = subreddits[subreddits.index(start_subreddit)+1:]\n",
    "\n",
    "# prevent duplicate posts by creating a set that tracks the id of all posts\n",
    "existing_posts = set()\n",
    "\n",
    "# we only need 500 MB worth of data, so we'll stop once we reach it\n",
    "MAX_SIZE = 500 * 1000000\n",
    "\n",
    "for subreddit in subreddits:\n",
    "  subreddit_posts = []\n",
    "  for post in reddit.subreddit(subreddit).hot(limit=100):\n",
    "    try:\n",
    "      # prevent existing posts and NSFW posts from being added\n",
    "      if not post.over_18 and post.id not in existing_posts:\n",
    "        existing_posts.add(post.id)\n",
    "        subreddit_posts.append({\n",
    "          \"title\": post.title,\n",
    "          \"author\": post.author,\n",
    "          \"url\":  post.url,\n",
    "          \"text\": post.selftext,\n",
    "          \"upvote_ratio\": post.upvote_ratio,\n",
    "          \"created_utc\": post.created_utc\n",
    "        })\n",
    "\n",
    "      # todo: add comments here, there is a comments attribute on the post variable\n",
    "      # comments are in CommentForest (https://praw.readthedocs.io/en/stable/code_overview/other/commentforest.html#praw.models.comment_forest.CommentForest) attribute on PRAW\n",
    "      # we have to determine how many levels deep we will retrieve comments\n",
    "\n",
    "    except Exception as err: \n",
    "      print(\"Something went wrong: \", err)\n",
    "\n",
    "  try:\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), 'posts')):\n",
    "      raise OSError(\"'/posts' folder not found.\")\n",
    "\n",
    "    subreddit_df = pd.DataFrame(subreddit_posts)\n",
    "    subreddit_df.to_json('posts/' + subreddit + '.json', default_handler=str, orient='records')\n",
    "    print(f'Successfully wrote r/{subreddit} posts to file.')\n",
    "  except Exception as err:\n",
    "    print('Something went wrong: ', err)\n",
    "\n",
    "  try:\n",
    "    cur_size = 0\n",
    "    for subreddit_json_file in os.scandir(os.path.join(os.getcwd(), 'posts')):\n",
    "      cur_size+=os.path.getsize(subreddit_json_file)\n",
    "    if cur_size >= MAX_SIZE:\n",
    "      print('Reached 500 MB. Terminating subreddit data retrieval.')\n",
    "      break\n",
    "  except Exception as err:\n",
    "    print('Something went wrong: ', err)\n",
    "\n",
    "print('Successfully retrieved data for all subreddits.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4034451351b0580bd91c94b09b2bdad4dd3d33f567dfebbe1624ca6ae0f1212"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
